semantics for an agent communication language we address the issue of semantics for an agent communication language in particular the semantics of knowledge query manipulation language kqml is investigated kqml is a language and protocol to support communication between intelligent software agents based on ideas from speech act theory we present a semantic description for kqml that associates cognitive states of the agent with the use of the language s primitives performatives we have used this approach to describe the semantics for the whole set of reserved kqml performatives our research offers a method for a speech act theory based semantic description of a language of communication acts languages of communication acts address the issue of communication between software applications at a level of abstraction that could prove particularly useful to the emerging software agents paradigm of software design and development 1 introduction this research is concerned with communication between software agents 13 error driven pruning of treebank grammars for base noun phrase identification finding simple non recursive base noun phrases is an important subtask for many natural language processing applications while previous empirical methods for base np identification have been rather complex this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task in particular we present a corpus based approach for finding base nps by matching part ofspeech tag sequences the training phase of the algorithm is based on two successful techniques first the base np grammar is read from a treebank corpus then the grammar is improved by selecting rules with high benefit scores using this simple algorithm with a naive heuristic for matching rules we achieve surprising accuracy in an evaluation on the penn treebank wall street journal 1 introduction finding base noun phrases is a sensible first step for many natural language processing nlp tasks accurate identification of base noun phrases is arguably the most critical comp protein structure prediction with evolutionary algorithms evolutionary algorithms have been successfully applied to a variety of molecular structure prediction problems in this paper we reconsider the design of genetic algorithms that have been applied to a simple protein structure prediction problem our analysis considers the impact of several algorithmic factors for this problem the conformational representation the energy formulation and the way in which infeasible conformations are penalized further we empirically evaluate the impact of these factors on a small set of polymer sequences our analysis leads to specific recommendations for both gas as well as other heuristic methods for solving psp on the hp model 1 introduction a protein is a chain of amino acid residues that folds into a specific native tertiary structure under certain physiological conditions a protein s structure determines its biological function consequently methods for solving protein structure prediction psp problems are valuable tools for modern molecula analog neural nets with gaussian or other common noise distributions cannot recognize arbitrary regular languages we consider recurrent analog neural nets where the output of each gate is subject to gaussian noise or any other common noise distribution that is nonzero on a large set we show that many regular languages cannot be recognized by networks of this type and we give a precise characterization of those languages which can be recognized this result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise on the other hand we present a method for constructing feedforward analog neural nets that are robust with regard to analog noise of this type 1 introduction a fairly large literature see omlin giles 1996 and the references therein is devoted to the construction of analog neural nets that recognize regular languages any physical realization of the analog computational units of an analog neural net in technological or biological systems is bound to encounter some form of imprecision or an the diagnosis frontend of the dlv system this paper presents the diagnosis frontend of dlv which is a knowledge representation system under development at the technische universit t wien the kernel language of the system is an extension of disjunctive logic programming dlp by integrity constraints it offers frontends to several advanced knowledge representation formalisms the formal model of diagnosis employed in the frontend includes both abductive diagnosis over dlp theories and consistency based diagnosis for each of the two diagnosis modalities generic diagnoses single error diagnoses and subset minimal diagnoses are considered we illustrate the use of the frontend by showing the dlv encodings of several diagnosis problems thereafter we discuss implementation issues diagnostic reasoning is implemented on the dlv engine through suitable translations of diagnostic problems into disjunctive logic programs such that their stable models correspond to diagnoses for the six kinds of diagnostic reasoning problems emerging from above such reductions are provided modeling emotions and other motivations in synthetic agents we present cathexis a distributed computational model which offers an alternative approach to model the dynamic nature of different affective phenomena such as emotions moods and temperaments and provides a flexible way of modeling their influence on the behavior of synthetic autonomous agents the model has been implemented as part of an extensible object oriented framework which provides enough functionality for agent developers to design emotional agents that can be used in a variety of applications including entertainment e g synthetic agents for interactive drama video games etc education e g intelligent tutoring systems and human computer interfaces introduction emotions are an essential part of our lives they influence how we think and behave and how we communicate with others several researchers have acknowledged their importance in human thinking minsky 1986 toda 1993 and recent neurological evidence seems to support these ideas ledoux 1996 damasio guaranteeing no interaction between functional dependencies and tree like inclusion dependencies functional dependencies fds and inclusion dependencies inds are the most fundamental integrity constraints that arise in practice in relational databases a given set of fds does not interact with a given set of inds if logical implication of any fd can be determined solely by the given set of fds and logical implication of any ind can be determined solely by the given set of inds the set of tree like inds constitutes a useful subclass of inds whose implication problem is polynomial time decidable we exhibit a necessary and sufficient condition for a set of fds and tree like inds not to interact this condition can be tested in polynomial time 1 introduction the implication problem for fds and inds is the problem of deciding for a given set sigma of fds and inds whether sigma logically implies oe where oe is an fd or an ind the implication problem is central in data dependency theory and is also utilised in the process of database design since it can be used to test wheth improving retrieval on imperfect speech transcriptions this paper presents the results from adding several forms of query expansion to our retrieval system running on transcriptions of broadcast news from the 1997 trec 7 spoken document retrieval track 1 introduction retrieving documents which originated as speech is complicated by the presence of errors in the transcriptions if some method of increasing retrieval performance despite these errors could be found then even low accuracy recognisers could be used as part of a successful spoken document retrieval sdr system this paper presents results using four query expansion techniques described in 3 on 8 different sets of transcriptions generated for the 1997 trec 7 sdr evaluation the baseline retrieval system and the techniques used for query expansion are described in section 2 the transcriptions on which the experiments were performed in section 3 and results and further discussion are offered in section 4 2 retrieval systems 2 1 baseline system bl our baseline system bargaining with deadlines this paper analyzes automated distributive negotiation where agents have firm deadlines that are private information the agents are allowed to make and accept offers in any order in continuous time we show that the only sequential equilibrium outcome is one where the agents wait until the first deadline at which point that agent concedes everything to the other this holds for pure and mixed strategies so this material is based upon work supported by the national science foundation under career award iri 9703122 grant iri 9610122 and grant iis 9800994 y this is based upon work supported by the epsrc award gr m07052 interestingly rational agents can never agree to a nontrivial split because offers signal enough weakness of bargaining power early deadline so that the recipient should never accept similarly the offerer knows that it offered too much if the offer gets accepted the offerer could have done better by out waiting the opponent in most cases the deadline spoken document retrieval for trec 7 at cambridge university this paper presents work done at cambridge university on the trec 7 spoken document retrieval sdr track the broadcast news audio was transcribed using a 2 pass gender dependent htk speech recogniser which ran at 50 times real time and gave an overall word error rate of 24 8 the lowest in the track the okapi based retrieval engine used in trec 6 by the city cambridge university collaboration was supplemented by improving the stop list adding a bad spelling mapper and stemmer exceptions list adding word pair information integrating part of speech weighting on query terms and including some pre search statistical expansion the final system gave an average precision of 0 4817 on the reference and 0 4509 on the automatic transcription with the r precision being 0 4603 and 0 4330 respectively the paper also presents results on a new set of 60 queries with assessments for the trec 6 test document data used for development purposes and analyses the relationship between recognition accuracy as defined by a pre processed term error rate and retrieval performance for both sets of data 1 querying network directories hierarchically structured directories have recently proliferated with the growth of the internet and are being used to store not only address books and contact information for people but also personal pro les network resource information and network and service policies these systems provide a means for managing scale and heterogeneity while allowing for conceptual unity and autonomy across multiple directory servers in the network in a way far superior to what conventional relational or object oriented databases o er yet in deployed systems today much of the data is modeled in an ad hoc manner and many of the more sophisticated queries quot involve navigational access in this paper we develop the core of a formal data model for network directories and propose a sequence of e ciently computable query languages with increasing expressive power the directory data model can naturally represent rich forms of heterogeneity exhibited in the real world answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity we present external memory algorithms for the evaluation of queries posed in our directory query languages and prove the e ciency of each algorithm in terms of its i o complexity our data model and query languages share the exibility and utility of the recent proposals for semi structured data models while at the same time e ectively addressing the speci c needs of network directory applications which we demonstrate by means of a representative real life example this work was done when the authors were at at t labs variorum a multimedia based program documentation system conventional software documentation systems are mostly based on textutal descriptions that explain or annotate the program s source code typically they also support interactive browsing of high level control flows and name based searching of program primitives such as variable declarations and function definitions because these systems rely solely on texts it is difficult for program authors to describe overall algorithm structures and detailed implementation considerations of the programs in an interactive and flexible fashion variorum is a novel software documentation system that allows program authors to record the process of walking through their own code using multimedia technology specifically text audio and digital pen drawing this approach greatly improves the interactivity and flexibility in the software documentation process in addition to broaden its applicability and to reduce the implementation complexity variorum is designed to inter operate with the www t case based classification using similarity based retrieval classification involves associating instances with particular classes by maximizing intra class similarities and minimizing inter class similarities the paper presents a novel approach to case based classification the algorithm is based on a notion of similarity assessment and was developed for supporting flexible retrieval of relevant information validity of the proposed approach is tested on real world domains and the system s performance is compared to that of other machine learning algorithms 1 introduction classification involves associating instances with particular classes based on the object description the classification system determines whether a given object belongs to a specified class in general this process consists of first generating a set of categories and then classifying given objects into the created categories for the purpose of this paper it is assumed that the categories are known a priori from a prescribed domain theory 38 various reasoning tech relational transducers for electronic commerce electronic commerce is emerging as one of the major websupported applications requiring database support we introduce and study high level declarative specifications of business models using an approach in the spirit of active databases more precisely business models are specified as relational transducers that map sequences of input relations into sequences of output relations the semantically meaningful trace of an input output exchange is kept as a sequence of log relations we consider problems motivated by electronic commerce applications such as log validation verifying temporal properties of transducers and comparing two relational transducers positive results are obtained for a restricted class of relational transducers called spocus transducers for semi positive outputs and cumulative state we argue that despite the restrictions these capture a wide range of practically significant business models 1 introduction electronic commerce is emerging as a major web s real world data is dirty data cleansing and the merge purge problem the problem of merging multiple databases of information about common entities is frequently encountered in kdd and decision support applications in large commercial and government organizations the problem we study is often called the merge purge problem and is difficult to solve both in scale and accuracy large repositories of data typically have numerous duplicate information entries about the same entities that are difficult to cull together without an intelligent equational theory that identifies equivalent items by a complex domain dependent matching process we have developed a system for accomplishing this data cleansing task and demonstrate its use for cleansing lists of names of potential customers in a direct marketing type application our results for statistically generated data are shown to be accurate and effective when processing the data multiple times using different keys for sorting on each successive pass combing results of individual passes using transitive c logical transactions and serializability the concept of logic databases can serve as a clear and expressive foundation of various information systems however the classical logic language only refers to a single database state although in modern information systems it is necessary to take the dynamics into account for this purpose several update languages were proposed among them 1 3 4 8 9 11 which allow to specify complex transitions from one database state to another from the evaluation point of view a complex state transition can and should be considered as a transaction up to now the isolation property of transactions has been poorly addressed in the context of logic update languages although it is an important problem even for classical sequential transactions see 2 for instance in this paper we investigate how the serializability of logical transactions can be supported and what this means for the implementation of a transaction manager 1 introduction and motivation in 11 12 we propose an up ifile an application of machine learning to e mail filtering the rise of the world wide web and the ever increasing amounts of machine readable text has caused text classification to become a important aspect of machine learning one specific application that has the potential to affect almost every user of the internet is e mail filtering the worldtalk corporation estimates that over 60 million business people use e mail 6 many more use e mail purely on a personal basis and the pool of e mail users is growing daily and yet automated techniques for learning to filter e mail have yet to significantly affect the e mail market here i attack problems that plague practical e mail ltering and suggest solutions that will bring us closer to the acceptance of using automated classification techniques to filter personal e mail i also present a filtering system ifile that is both effective and efficient and which has been adapted to a popular e mail client results are presented from a number of experiments and show that a system such as ifile could become a run time detection in parallel and distributed systems an application to safety critical applications as systems are becoming more complex there is increasing interest in their runtime analysis understanding their dynamic behavior and possibly controling it as well this paper describes complex distributed and parallel applications that use run time analyses to attain scalability improvements with respect to the amount and complexity of the data transmitted transformed and shared among different application components such improvements are derived from using database techniques when manipulating data streams namely by imposing a relational model on a data stream filters and constraints on the stream may be expressed in the form of database queries evaluated against the data events comprising the stream streams may then be filtered using runtime optimization techniques derived from query optimization methods this paper also presents a tool termed cnet which offers 1 means for the dynamic creation of queries and their application to distributed data streams 2 permits the shopbots and pricebots shopbots are agents that automatically search the internet to obtain information about prices and other attributes of goods and services they herald a future in which autonomous agents profoundly influence electronic markets in this study a simple economic model is proposed and analyzed which is intended to quantify some of the likely impacts of a proliferation of shopbots and other economically motivated software agents in addition this paper reports on simulations of pricebots adaptive pricesetting agents which firms may well implement to combat or even take advantage of the growing community of shopbots this study forms part of a larger research program that aims to provide insights into the impact of agent technology on the nascent information economy 1 introduction shopbots agents that automatically search the internet for goods and or services on behalf of consumers herald a future in which autonomous agents become an essential component of nearly every facet o foundations of spatioterminological reasoning with description logics this paper presents a method for reasoning about spatial objects and their qualitative spatial relationships in contrast to existing work which mainly focusses on reasoning about qualitative spatial relations alone we integrate quantitative and qualitative information with terminological reasoning for spatioterminological reasoning we present the description logic alcrp d and define an appropriate concrete domain d for polygons the theory is motivated as a basis for knowledge representation and query processing in the domain of deductive geographic information systems 1 introduction qualitative relations play an important role in formal reasoning systems that can be part of for instance geographic information systems gis in this context inferences about spatial relations should not be considered in isolation but should be integrated with formal inferences about structural descriptions of domain objects e g automatic consistency checking and classification and infer pruning classifiers in a distributed meta learning system jam is a powerful and portable agent based distributed data mining system that employs meta learning techniques to integrate a number of independent classifiers concepts derived in parallel from independent and possibly inherently distributed databases although metalearning promotes scalability and accuracy in a simple and straightforward manner brute force meta learning techniques can result in large inefficient and some times inaccurate meta classifier hierarchies in this paper we explore several techniques for evaluating classifiers and we demonstrate that meta learning combined with certain pruning methods can achieve similar or even better performance results in a much more cost effective manner keywords classifier evaluation pruning metrics distributed mining meta learning this research is supported by the intrusion detection program baa9603 from darpa f30602 96 1 0311 nsf iri 96 32225 and cda 96 25374 and nysstf 423115 445 y supported in part by ibm a multimodal approach to term extraction using a rhetorical structure theory tagger and formal concept analysis this paper reports on knowledge extraction using rhetorical structure theory rst and formal concept analysis fca the research is multimodal in two ways i it uses a text tagger to identify key terms in free text these terms are then used as indexation filters over the free text ii it aims to normalise the contents of multiple text sources into a single knowledge base the aim is semi automated extraction of semantic content in texts derived from different sources and merging them into a single coherent knowledge base we use rst 7 to automate the identification of discourse markers in multiple texts dealing with a single subject matter marcu 8 10 has shown that rst can be used for the semiautomated mark up of natural language texts marcu uses discourse trees useful to store information about the rhetorical structure and has shown that the identification of discourse markers from prototypical texts can be automated with 88 precision 9 we have adapted marcu s algorithm in our approach although our work draws on recent results from natural language processing progress in that field is not the objective the research is motivated by the analysis of texts generated by different sources their translation to a formal knowledge representation followed by a consolidation into a single knowledge corpus our interest is in the analysis of this corpus to determine the reliability of information obtained from multiple agencies 11 and then to visually navigate this knowledge this involves fca 14 15 17 18 6 for browsing and retrieving text documents 2 3 4 1 fca is typically a propositional knowledge representation technique i e it can only express monadic relations recently wille 16 has shown that fca can be used to repres sensor fault detection and identification in a mobile robot multiple model adaptive estimation mmae is used to detect and identify sensor failures in a mobile robot each estimator is a kalman filter with a specific embedded failure model the filter bank also contains one filter which has the nominal model embedded within it the filter residuals are postprocessed to produce a probabilistic interpretation of the operation of the system the output of the system at any given time is the confidence in the correctness of the various embedded models as an additional feature the standard assumption that the measurements are available at a constant common frequency is relaxed measurements are assumed to be asynchronous and of varying frequency the particularly difficult case of soft sensor failure is also handled successfully a system architecture is presented for the general problem of failure detection and identification in mobile robots as an example the mmae algorithm is demonstrated on a pioneer i robot in the case of three different sensor failures unsupervised learning from dyadic data dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads i e pairs with one element from either set this includes event co occurrences histogram data and single stimulus preference data as special cases dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision in this paper we present a systematic domain independent framework for unsupervised learning from dyadic data by statistical mixture models our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery mixture models provide both a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data as well as structural information about data inherent grouping structure we propose an annealed version of the standard expectation maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains response generation in collaborative negotiation in collaborative planning activities since the agents are autonomous and heterogeneous it is inevitable that conflicts arise in their beliefs during the planning process in cases where such conflicts are relevant to the task at hand the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs this paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise and of selecting appropriate evidence to justify the need for such modification furthermore by capturing the negotiation process in a recursive propose evaluate modify cycle of actions our model can successfully handle embedded negotiation subdialogues 1 introduction in collaborative consultat the cmunited 97 simulator team the soccer server system provides a rich and challenging multiagent real time domain agents must accurately perceive and act despite a quickly changing largely hidden noisy world they must also act at several levels ranging from individual skills to full team collaborative and adversarial behaviors this article presents the cmunited 97 approaches to the above challenges which helped the team to the semifinals of the 29 team robocup 97 tournament 1 introduction the soccer server system 5 used at robocup 97 2 provides a rich and challenging multiagent real time domain sensing and acting is noisy while interagent communication is unreliable and low bandwidth in order to be successful each agent in a team must be able to sense and act in real time sensations arrive at unpredictable intervals while actions are possible every 100ms furthermore since the agents get local noisy sensory information they must have a method of converting their sensory inputs into a good w aspects of interface agents avatar assistant and actor this paper introduces the interface agent research project being carryied out at atr media integration communications research labs we are interested in virtual interface agents to support human creative activities by mediating between human users and computer cyberspace in this paper we categorize interface agents into three types avatars assistants and actors based on their functionality and discuss a design framework of agents and related applications we present the current status and objectives for the following topics design of an asynchronous hierarchical agent architecture a character locomotion design tool applications in a virtualized museum and a group discussion environment in which virtual agents would inhabit and act 1 introduction a virtual interface agent or interface agent is defined in this paper as an autonomous agent which mediates between a human user and computer cyberspace an interface agent differs from an ordinary interface since it is expecte oms rapid prototyping system for the development of object oriented database application systems we present an object oriented data model and system that supports the development of database application systems through a combination of rapid prototyping and refinement prototyping is performed on an abstract application model and is independent of any implementation platform supporting not only the design stage but also analysis and requirements modelling the underlying model om has a two level structure which reflects the two aspects of object oriented database application systems data modelling and programming we are therefore able to cleanly integrate ideas from both the database and software engineering communities and achieve compatibility with object models used in both keywords object oriented database systems rapid prototyping database design 1 introduction the design and implementation of object oriented database application systems presents a software engineering challenge in that it encompasses both application program development and database design exception handling in agent systems a critical challenge to creating effective agent based systems is allowing them to operate effectively when the operating environment is complex dynamic and error prone in this paper we will review the limitations of current agent local approaches to exception handling in agent systems and propose an alternative approach based on a shared exception handling service that is plugged with little or no customization into existing agent systems this service can be viewed as a kind of coordination doctor it knows about the different ways multi agent systems can get sick actively looks system wide for symptoms of such illnesses and prescribes specific interventions instantiated for this particular context from a body of general treatment procedures agents need only implement their normative behavior plus a minimal set of interfaces we claim that this approach offers simplified agent development as well as more effective and easier to modify exception handling behavior t greedy strikes back improved facility location algorithms a fundamental facility location problem is to choose the location of facilities such as industrial plants and warehouses to minimize the cost of satisfying the demand for some commodity there are associated costs for locating the facilities as well as transportation costs for distributing the commodities we assume that the transportation costs form a metric this problem is commonly referred to as the uncapacitated facility location ufl problem applications to bank account location and clustering as well as many related pieces of work are discussed by cornuejols nemhauser and wolsey recently the first constant factor approximation algorithm for this problem was obtained by shmoys tardos and aardal we show that a simple greedy heuristic combined with the algorithm by shmoys tardos and aardal can be used to obtain an approximation guarantee of 2 408 we discuss a few variants of the problem demonstrating better approximation factors for restricted versions of the a framework for describing visual interfaces to databases in the field of hci there exist many formalisms for analysing describing and evaluating interactive systems however in developing and evaluating user interfaces to databases we found it necessary to be able to describe presentation and interaction aspects that are catered for poorly or not at all in current formalisms this paper presents a framework for the systematic description of data model presentation and interaction components that together form a graphical user interface the utility of the framework is then demonstrated by showing how it can be used to describe two existing visual query interfaces these examples show that the framework provides a systematic method for the concise description of graphical interfaces to databases that can be used either during interface design or as a communication aid 1 introduction research in user interfaces for databases is gaining momentum with many recent conferences and workshops 10 21 22 23 36 however many papers on datab conflicts in a simple autonomy based multi agent system this paper shows that in some situations conflict can deliberately be left in an autonomy based multi agent system this study supported by experimental results has two major outcomes first it proves that conflict does not necessarily alter the global outcome of the system in a qualitative way second it shows that it is possible to effect the way the global task is achieved by appropriately modifying the environment of the agents introduction our work fits in the framework of bottom up artificial intelligence brooks 1986 brooks 1991 and more particularly in that of autonomous agents pfeifer 1995 we are concerned with collective phenomena and their issues and more precisely the way to carry out solutions that allow an autonomy based multi agent system to achieve a global task by virtue of emergence and self organization emergence offers indeed a bridge between the necessity of complex and adaptive behavior at a macro level the one of the system and situationbased from resource discovery to knowledge discovery on the internet more than 50 years ago at a time when modern computers didn t exist yet vannevar bush wrote about a multimedia digital library containing human collective knowledge and filled with trails linking materials of the same topic at the end of world war ii vannevar urged scientists to build such a knowledge store and make it useful continuously extendable and more importantly accessible for consultation today the closest to the materialization of vannevar s dream is the world wide web hypertext and multimedia document collection however the ease of use and accessibility of the knowledge described by vannevar is yet to be realized since the 60s extensive research has been accomplished in the information retrieval field and free text search was finally adopted by many text repository systems in the late 80s the advent of the world wide web in the 90s helped text search become routine as millions of users use search engines daily to pinpoint resources on the internet however r placing a robot manipulator amid obstacles for optimized execution this paper presents an efficient algorithm for optimizing the base location of a robot manipulator in an environment cluttered with obstacles in order to execute specified tasks as fast as possible the algorithm uses randomized motion planning techniques and exploits geometric coherence in configuration space to achieve fast computation the performance of the algorithm is demonstrated on both synthetic examples and real life cad data from the automotive industry the computation time ranges from under a minute for simple problems to a few minutes for more complex ones 1 introduction the base placement of a robot manipulator is an important issue in many robotics applications given a description of a robot manipulator and its environment the goal is to find a base location for the manipulator so that specified tasks are executed as efficiently as possible in this paper we present an algorithm that makes use of randomized motion planning techniques to compute simultaneously processing fuzzy spatial queries a configuration similarity approach increasing interest for configuration similarity is currently developing in the context of digital libraries spatial databases and geographical information systems the corresponding queries retrieve all database configurations that match an input description e g find all configurations where an object x 0 is about 5km northeast of another x 1 which in turn is inside object x 2 this paper introduces a framework for configuration similarity that takes into account all major types of spatial constraints topological direction distance we define appropriate fuzzy similarity measures for each type of constraint to provide flexibility and allow the system to capture real life needs then we apply pre processing techniques to explicate constraints in the query and present algorithms that effectively solve the problem extensive experimental results demonstrate the applicability of our approach to images and queries of considerable size 1 introduction as opposed to visu an agent based approach to the construction of floristic digital libraries this paper describes an agent assisted approach to the construction of floristic digital libraries which consist of very large botanical data repositories and related services we propose an environment termed chrysalis in which authors of plant morphologic descriptions can enter data into a digital library via a web based editor an agent that runs concurrently with the editor suggests potentially useful morphologic descriptions based on similar documents existing in the library benefits derived from the introduction of chrysalis include reduced potential for errors and data inconsistencies increased parallelism among descriptions and considerable savings in the time regularly spent in visually checking for parallelism and manually editing data keywords agents agent based interfaces floristic digital libraries fna chrysalis introduction constructing the vast data repositories that will support knowledge intensive activities in digital libraries poses problems of enormo bias variance and error correcting output codes for local learners this paper focuses on a bias variance decomposition analysis of a local learning algorithm the nearest neighbor classifier that has been extended with error correcting output codes this extended algorithm often considerably reduces the 0 1 i e classification error in comparison with nearest neighbor ricci aha 1997 the analysis presented here reveals that this performance improvement is obtained by drastically reducing bias at the cost of increasing variance we also show that even in classification problems with few classes m5 extending the codeword length beyond the limit that assures column separation yields an error reduction this error reduction is not only in the variance which is due to the voting mechanism used for error correcting output codes but also in the bias keywords case based learning classification error correcting output codes bias and variance email ricci irst itc it aha aic nrl navy mil phone 39 461 314334 fax 39 461 302040 bi dynamic logic programming in this paper we investigate updates of knowledge bases represented by logic programs in order to represent negative information we use generalized logic programs which allow default negation not only in rule bodies but also in their heads we start by introducing the notion of an update p oplus u of a logic program p by another logic program u subsequently we provide a precise semantic characterization of p oplus u and study some basic properties of program updates in particular we show that our update programs generalize the notion of interpretation update we then extend this notion to compositional sequences of logic programs updates p 1 oplus p 2 oplus dots defining a dynamic program update and thereby introducing the paradigm of emph dynamic logic programming this paradigm significantly facilitates modularization of logic programming and thus modularization of non monotonic reasoning as a whole specifically suppose that we are given a set of logic program modules each describing a different state of our knowledge of the world different states may represent different time points or different sets of priorities or perhaps even different viewpoints consequently program modules may contain mutually contradictory as well as overlapping information the role of the dynamic program update is to employ the mutual relationships existing between different modules to precisely determine at any given module composition stage the declarative as well as the procedural semantics of the combined program resulting from the modules on using regression for range data fusion in the paper we consider an occupancy based approach for range data fusion as it is used in mobile robotics we identify two major problems of this approach the first problem deals with the combination rule which in many cases assumes the independence of range data contrary to the usual situation the second problem concerns the redundancy of stored and processed data which results from using the grid representation of the occupancy function and which is the main obstacle to building 3d occupancy world models we propose a solution to these problems by proposing a new range data fusion technique based on regression this technique uses the evidence theory in assigning occupancy values which we argue is advantageous for fusion and builds the occupancy function by fitting the sample data provided by a sensor with a piecewise linear function having developed a general framework for our approach we apply it to building 3d world models from visual range data where 3d world models soft margins for adaboost recently ensemble methods like adaboost have been applied successfully in many problems while seemingly defying the problems of overfitting adaboost rarely overfits in the low noise regime however we show that it clearly does so for higher noise levels central to the understanding of this fact is the margin distribution adaboost can be viewed as a constraint gradient descent in an error function with respect to the margin we find that adaboost asymptotically achieves a hard margin distribution i e the algorithm concentrates its resources on a few hard to learn patterns that are interestingly very similar to support vectors a hard margin is clearly a sub optimal strategy in the noisy case and regularization in our case a mistrust in the data must be introduced in the algorithm to alleviate the distortions that single difficult patterns e g outliers can cause to the margin distribution we propose several regularization methods and generalizations of the original adaboost algorithm to achieve a soft margin in particular we suggest 1 regularized adaboost reg where the gradient decent is done directly with respect to the soft margin and 2 regularized linear and quadratic programming lp qp adaboost where the soft margin is attained by introducing slack variables extensive simulations demonstrate that the proposed regularized adaboost type algorithms are useful and yield competitive results for noisy data xgobi and xplore meet virgis in this paper we report on a linked environment of the three programs xgobi xplore and virgis while xgobi and xplore are statistical packages that focus on dynamic statistical graphics and provide analytical statistical features respectively virgis is a 3d virtual reality geographic information system gis that allows real time access to and visualization of geographic data the xgobi xplore virgis environment is based on the remote procedure call rpc technology previously developed for the arcview xgobi xplore environment it allows linked brushing and the exchange of data and commands completely transparent to the user 1 introduction in our previous work we developed an open software system consisting of the geographic information system gis arcview the dynamic statistical graphics program xgobi and the statistical computing environment xplore symanzik kotter schmelzer klinke cook swayne 1998 symanzik cook klinke lewin 1998 in this current projec how to avoid building datablades that know the value of everything and the cost of nothing the object relational database management system ordbms offers many potential benefits for scientific multimedia and financial applications however work remains in the integration of domain specific class libraries data cartridges extenders datablades into ordbms query processing a major problem is that the standard mechanisms for query selectivity estimation taken from relational database systems rely on properties specific to the standard data types creation of new mechanisms remains extremely difficult because the software interfaces provided by vendors are relatively low level in this paper we discuss extensions of the generalized search tree or gist to support a higher level but less type specific approach specifically we discuss the computation of selectivity estimates with confidence intervals using a variety of index based approaches and present results from an experimental comparison of these methods with several estimators from the literature 1 intro classification on pairwise proximity data we investigate the problem of learning a classification task on data represented in terms of their pairwise proximities this representation does not refer to an explicit feature representation of the data items and is thus more general than the standard approach of using euclidean feature vectors from which pairwise proximities can always be calculated our first approach is based on a combined linear embedding and classification procedure resulting in an extension of the optimal hyperplane algorithm to pseudo euclidean data as an alternative we present another approach based on a linear threshold model in the proximity values themselves which is optimized using structural risk minimization we show that prior knowledge about the problem can be incorporated by the choice of distance measures and examine different metrics w r t their generalization finally the algorithms are successfully applied to protein structure data and to data from the cat s cerebral cortex they show bette supporting dialogue inferencing in conversational case based reasoning dialogue inferencing is the knowledge intensive process of inferring aspects of a user s problem from its partial description conversational case based reasoning ccbr systems which interactively and incrementally elicit a user s problem description suffer from poor retrieval efficiency i e they prompt the user with questions that the user has already implicitly answered unless they perform dialogue inferencing the standard method for dialogue inferencing in ccbr systems requires library designers to supply explicit inferencing rules this approach is problematic e g maintenance is difficult we introduce an alternative approach in which the ccbr system guides the library designer in building a domain model this model and the partial problem description are then given to a query retrieval system parka db to infer any implied answers during a conversation in an initial empirical evaluation in the nacodae ccbr tool our approach improved retrieval efficiency without sa user centered design and evaluation of a real time battlefield visualization virtual environment the ever increasing power of computers and hardware rendering systems has to date primarily motivated the creation of visually rich and perceptually realistic virtual environment ve applications comparatively very little effort has been expended on the user interaction components of ves as a result ve user interfaces are often poorly designed and are rarely evaluated with users although usability engineering is a newly emerging facet of ve development user centered design and usability evaluation in ves as a practice still lags far behind what is needed this paper presents a structured iterative approach for the user centered design and evaluation of ve user interaction this approach consists of the iterative use of expert heuristic evaluation followed by formative usability evaluation followed by summative evaluation we describe our application of this approach to a real world ve for battlefield visualization describe the resulting series of design iterations and present evidence that this approach provides a cost effective strategy for assessing and iteratively improving user interaction design in ves this paper is among the first to report applying an iterative structured user centered design and evaluation approach to ve user interaction design agent based programming language for multi agent teams this report specifies a programming language for multi agent teams the language aims at providing an abstract level approach to the programming of teams composed of either software or hardware agents e g robots encapsulating the lower level implementation details e g graphical primitives for icon animation robot motion primitives and providing an abstraction level appropriate for multi agent systems the overall architecture of the multi agent team the language specifications and an example of application to robotic soccer are included keywords cooperative robotics multi agent systems distributed artificial intelligence robotic soccer tactics strategy world state world relational rules behavior11 behavior12 behavior1m behaviorn1 behaviorn2 behaviornm act sense act sense negotiation requests strategy prescription organizational state machine behavior prescription temporary behavior modification world team state behavior ij behavior i of agent j behavior the open agent architecture a framework for building distributed software systems the open agent architecture oaa developed and used for several years at sri international makes it possible for software services to be provided through the cooperative efforts of distributed collections of autonomous agents communication and cooperation between agents are brokered by one or more facilitators which are responsible for matching requests from users and agents with descriptions of the capabilities of other agents thus it is not generally required that a user or agent know the identities locations or number of other agents involved in satisfying a request oaa is structured so as to minimize the effort involved in creating new agents and wrapping legacy applications written in various languages and operating on various platforms to encourage the reuse of existing agents and to allow for dynamism and flexibility in the makeup of agent communities distinguishing features of oaa as compared with related work include extreme flexibility in using facilitator b categorisation by context assistance in retrieving of documents on the world wide web is provided either by search engines through keyword based queries or by catalogues which organise documents into hierarchical collections maintaining catalogues manually is becoming increasingly difficult due to the sheer amount of material and therefore it will be necessary to resort to techniques for automatic classification of documents classification is traditionally performed by extracting information for indexing a document from the document itself the paper describes the technique of categorisation by context which exploits the context perceivable from the structure of html documents to extract useful information for classifying the documents they refer to we present the results of experiments with a preliminary implementation of the technique 1 introduction most web search engines e g altavista tm altavista hotbot tm hotbot excite tm excite perform search based on the content of docume answering queries by semantic caches there has been growing interest in semantic query caches to aid in query evaluation semantic caches are simply the results of previously asked queries or selected relational information chosen by an evaluation strategy that have been cached locally for complex environments such as distributed heterogeneous databases and data warehousing the use of semantic caches promises to help optimize query evaluation increase turnaround for users and reduce network load and other resource usage we present a general logical framework for semantic caches we consider the use of all relational operations across the caches for answering queries and we consider the various ways to answer and to partially answer a query by cache we address when answers are in cache when answers in cache can be recovered and the notions of semantic overlaps semantic independence and semantic query remainder while there has been much work relevant to the use of semantic caches no one has addressed in conjunction the issues pertinent to the effective use of semantic caches to evaluate queries in some cases this is due to overly simplified assumptions and in other cases to the lack of a formal framework we attempt to establish some of that framework here within that framework we illustrate the issues involved in using semantic caches for query evaluation we show various applications for semantic caches and relate the work to relevant areas 1 non standard crossover for a standard representation commonality based feature subset selection the commonality based crossover framework has been presented as a general model for designing problem specific operators following this model the common features random sample climbing operator has been developed for feature subset selection a binary string optimization problem although this problem should be an ideal application for genetic algorithms with standard crossover operators experiments show that the new operator can find better feature subsets for classifier training 1 introduction a classification system is used to predict the decision class of an object based on its features when training a classifier it is beneficial to use only the features relevant to prediction accuracy and to ignore the irrelevant features koh95 the benefit arises from an increase in the signalto noise ratio of the data and a reduction in the time required to train the classifier thus the objective of feature subset selection is to identify the most relevant features feature sub distributed scheduling to support a call centre a co operative multi agent approach this paper introduces a multi agent system architecture to increase the value of 24 hour a day call centre service this system supports call centres in making appointments with clients on the basis of knowledge of employees and their schedules relevant activities of employees are scheduled for employees in preparation of such appointments the multi agent system architecture is based on principled design using the compositional development method for multi agent systems desire to schedule procedures in which more than one employee is involved each employee is represented by its own personal assistant agent and a work manager agent co ordinates the schedules of the personal assistant agents and clients through the call centre the multi agent system architecture has been applied to the banking domain in co operation with and partially funded by the rabobank 1 introduction over the past few years more and more companies and organisations have become aware of the potential of a intelligent gradient based search of incompletely defined design spaces gradient based numerical optimization of complex engineering designs offers the promise of rapidly producing better designs however such methods generally assume that the objective function and constraint functions are continuous smooth and defined everywhere unfortunately realistic simulators tend to violate these assumptions we present a rule based technique for intelligently computing gradients in the presence of such pathologies in the simulators and show how this gradient computation method can be used as part of a gradient based numerical optimization system we tested the resulting system in the domain of conceptual design of supersonic transport aircraft and found that using rule based gradients can decrease the cost of design space search by one or more orders of magnitude keywords optimization gradients sequential quadratic programming rule based systems 1 introduction automated search of a space of candidate designs seems an attractive way to improve the tr on the correspondence between neural folding architectures and tree automata the folding architecture together with adequate supervised training algorithms is a special recurrent neural network model designed to solve inductive inference tasks on structured domains recently the generic architecture has been proven as a universal approximator of mappings from rooted labeled ordered trees to real vector spaces in this article we explore formal correspondences to the automata language theory in order to characterize the computational power representational capabilities of different instances of the generic folding architecture as the main result we prove that simple instances of the folding architecture have the computational power of at least the class of deterministic bottom up tree automata it is shown how architectural constraints like the number of layers the type of the activation functions first order vs higher order and the transfer functions threshold vs sigmoid influence the representational capabilities all proofs are carried out in a c distributed safety controllers for web services we show how to use high level synchronization constraints written in a version of monadic second order logic on finite strings to synthesize safety controllers for interactive web services we improve on the na ve runtime model to avoid state space explosions and to increase the flow capacities of services equal time for data on the internet with websemantics many collections of scientific data in particular disciplines are available today around the world much of this data conforms to some agreed upon standard for data exchange i e a standard schema and its semantics however sharing this data among a global community of users is still difficult because of a lack of standards for the following necessary functions i data providers need a standard for describing or publishing available sources of data ii data administrators need a standard for discovering the published data and iii users need a standard for accessing this discovered data this paper describes a prototype implementation of a system websemantics that accomplishes the above tasks we describe an architecture and protocols for the publication discovery and access to scientific data we define a language for discovering sources and querying the data in these sources and we provide a formal semantics for this language 1 introduction recently many standardized optimizing object queries using an effective calculus object oriented databases oodbs provide powerful data abstractions and modeling facilities but they generally lack a suitable framework for query processing and optimization one of the key factors for oodb systems to successfully compete with relational systems as well as to meet the performance requirements of many non traditional applications is the development of an effective query optimizer we propose an effective framework with a solid theoretical basis for optimizing oodb query languages our calculus called the monoid comprehension calculus captures most features of odmg oql and is a good basis for expressing various optimization algorithms concisely this paper concentrates on query unnesting an optimization that even though improves performance considerably is not treated properly if at all by most oodb systems our framework generalizes many unnesting techniques proposed recently in the literature and is capable of removing any form of query nesting using a very si saccadic search with gabor features applied to eye detection and real time head tracking the gabor decomposition is a ubiquitous tool in computer vision nevertheless it is generally considered computationally demanding for active vision applications we suggest an attention driven approach to feature detection inspired by the human saccadic system a dramatic speedup is achieved by computing the gabor decomposition only on the points of a sparse retinotopic grid an off line eye detection application and a real time head localisation and tracking system are presented the real time system features a novel eyeball mounted camera designed to simulate the dynamic performance of the human eye and is to the best of our knowledge the first example of active vision system based on the gabor decomposition reasoning with examples propositional formulae and database dependencies for humans looking at how concrete examples behave is an intuitive way of deriving conclusions the drawback with this method is that it does not necessarily give the correct results however under certain conditions example based deduction can be used to obtain a correct and complete inference procedure this is the case for boolean formulae reasoning with models and for certain types of database integrity constraints the use of armstrong relations we show that these approaches are closely related and use the relationship to prove new results about the existence and sizes of armstrong relations for boolean dependencies furthermore we exhibit close relations between the questions of finding keys in relational databases and that of finding abductive explanations further applications of the correspondence between these two approaches are also discussed 1 introduction one of the major tasks in database systems as well as artificial intelligence systems is to express some know hicap an interactive case based planning architecture and its application to noncombatant evacuation operations this paper describes hicap hierarchical interactive case based architecture for planning a general purpose planning architecture that we have developed and applied to assist military commanders and their staff with planning neos noncombatant evacuation operations hicap integrates a hierarchical task editor hte with a conversational case based planning tool nacodae htn in this application hte maintains an agenda of tactical planning tasks that according to the guidelines indicated by military doctrine must be addressed in a neo plan it also supports several bookkeeping tasks which are crucial for large scale planning tasks that differ greatly among different neo operations military planning personnel select a task to decompose from hte and then use nacodae htn to interactively refine it into an operational plan by selecting and applying cases which represent task decompositions from previous neo operations thus hicap helps commanders by using previous experience to fo an algorithm to evaluate quantified boolean formulae and its experimental evaluation the high computational complexity of advanced reasoning tasks such as reasoning about knowledge and planning calls for efficient and reliable algorithms for reasoning problems harder than np in this paper we propose evaluate an algorithm for evaluating quantified boolean formulae a language that extends propositional logic in a way such that many advanced forms of propositional reasoning e g circumscription can be easily formulated as evaluation of a qbf algorithms for evaluation of qbfs are suitable for the experimental analysis on a wide range of complexity classes a property not easily found in other formalisms evaluate is based on a generalization of the davis putnam procedure for sat and is guaranteed to work in polynomial space before presenting the algorithm we discuss several abstract properties of qbfs that we singled out to make it more efficient we also discuss various options that were investigated about heuristics and data structures and report the main res mixtures of probabilistic principal component analysers principal component analysis pca is one of the most popular techniques for processing compressing and visualising data although its effectiveness is limited by its global linearity while nonlinear variants of pca have been proposed an alternative paradigm is to capture data complexity by a combination of local linear pca projections however conventional pca does not correspond to a probability density and so there is no unique way to combine pca models previous attempts to formulate mixture models for pca have therefore to some extent been ad hoc in this paper pca is formulated within a maximum likelihood framework based on a specific form of gaussian latent variable model this leads to a well defined mixture model for probabilistic principal component analysers whose parameters can be determined using an em algorithm we discuss the advantages of this model in the context of clustering density modelling and local dimensionality reduction and we demonstrate its applicatio a geometric framework for specifying spatiotemporal objects we present a framework for specifying spatiotemporal objects using spatial and temporal objects and a geometric transformation we define a number of classes of spatiotemporal objects and study their closure properties 1 introduction many natural or man made phenomena have both a spatial and a temporal extent consider for example a forest fire or property histories in a city to store information about such phenomena in a database one needs appropriate data modeling constructs we claim that a new concept spatiotemporal object is necessary in this paper we introduce a very general framework for specifying spatiotemporal objects to define a spatiotemporal object we need a spatial object a temporal object and a continuous geometric transformation specified using a parametric representation that determines the image of the spatial object at different time instants belonging to the temporal object in this framework a number of classes of spatiotemporal objects arise quite gradient based learning applied to document recognition multilayer neural networks trained with the back propagation algorithm constitute the best example of a successful gradientbased learning technique given an appropriate network architecture gradient based learning algorithms can be used to synthesize a complex decision surface that can classify high dimensional patterns such as handwritten characters with minimal preprocessing this paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task convolutional neural networks which are specifically designed to deal with the variability of two dimensional 2 d shapes are shown to outperform all other techniques real life document recognition systems are composed of multiple modules including field extraction segmentation recognition and language modeling a new learning paradigm called graph transformer networks gtn s allows such multimodule systems to be trained globally using gradient based methods so as to minimize an overall performance measure two systems for online handwriting recognition are described experiments demonstrate the advantage of global training and the flexibility of graph transformer networks a graph transformer network for reading a bank check is also described it uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks it is deployed commercially and reads several million checks per day modeling emotion based decision making this paper presents a computational approach to emotionbased decision making that models important aspects of emotional processing and integrates these with other models of perception motivation behavior and motor control a particular emphasis is placed on using some of the mechanisms of emotions as building blocks for the acquisition of emotional memories that serve as biasing signals during the process of making decisions and selecting actions we have successfully followed this approach to develop and control several different autonomous agents including both synthetic agents and physical robots introduction most theories of human reasoning and decision making fall between two different positions the first one argues that we make decisions in a way similar to that of solving problems in formal logic according to this view when faced with a problem we form a list of all different options and their possible outcomes and then we use logic in its best sense to perform a cos extending the odmg object model with time although many temporal extensions of the relational data model have been proposed there is no comparable amount of work in the context of object oriented data models moreover extensions to the relational model have been proposed in the framework of sql standards whereas no attempts have been made to extend the standard for object oriented databases defined by odmg this paper presents t odmg a temporal extension of the odmg 93 standard data model the main contributions of this work are thus the formalization of the odmg standard data model and its extension with time another contribution of this work is the investigation on a formal basis of the main issues arising from the introduction of time in an object oriented model point versus interval based temporal data models the association of timestamps with various data items such as tuples or attribute values is fundamental to the management of time varying information using intervals in timestamps as do most data models leaves a data model with a variety of choices for giving a meaning to timestamps specifically some such data models claim to be point based while other data models claim to be interval based the meaning chosen for timestamps is important it has a pervasive effect on most aspects of a data model including database design a variety of query language properties and query processing techniques e g the availability of query optimization opportunities this paper precisely defines the notions of point based and interval based temporal data models thus providing a new formal basis for characterizing temporal data models and obtaining new insights into the properties of their query languages queries in point based models treat snapshot equivalent argument relations identically mirror a state conscious concurrency control protocol for replicated real time databases data replication is one of the main techniques by which database systems can hope to meet the stringent temporal constraints of current time critical applications especially web based directory and electronic commerce services a pre requisite for realizing the benefits of replication however is the development of high performance concurrency control mechanisms we present in this paper mirror managing isolation in replicated realtime object repositories a concurrency control protocol specifically designed for firm deadline applications operating on replicated real time databases mirror augments the optimistic two phase locking o2pl algorithm developed for non real time databases with a novel and simple to implement state based conflict resolution mechanism to fine tune real time performance using a detailed simulation model we compare mir ror s performance against the real time versions of a representative set of classical protocols for a range of transaction workloads and system configurations our performance studies show that a the relative performance characteristics of replica concurrency control algorithms in the real time environment could be significantly different from their performance in a traditional non real time database system b mirror provides the best performance in both fully and partially replicated environments for real time applications with low to moderate update frequencies and c mirror s conflict resolution mechanism works almost as well as more sophisticated and difficult to implement strategies content based book recommending using learning for text categorization recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user s likes and dislikes most existing recommender systems use collaborative filtering methods that base recommendations on other users preferences by contrast content based methods use information about an item itself to make suggestions this approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations we describe a content based book recommending system that utilizes information extraction and a machine learning algorithm for text categorization initial experimental results demonstrate that this approach can produce accurate recommendations keywords recommender systems information filtering machine learning text categorization introduction there is a growing interest in recommender systems that suggest music films books and othe optimizing queries with object updates object oriented databases oodbs provide powerful data abstractions and modeling facilities but they usually lack a suitable framework for query processing and optimization even though there is an increasing number of recent proposals on oodb query optimization only few of them are actually focused on query optimization in the presence of object identity and destructive updates features often supported by most realistic oodb languages this paper presents a formal framework for optimizing object oriented queries in the presence of side effects these queries may contain object updates at any place and in any form we present a language extension to the monoid comprehension calculus to express these object oriented features and we give a formal meaning to these extensions our method is based on denotational semantics which is often used to give a formal meaning to imperative programming languages the semantics of our language extensions is expressed in terms of our monoid calculu program comprehension in multi language systems this paper presents an approach to program comprehension in multi language systems such systems are characterized by a high amount of source codes in various languages for programming database definition and job control coping with those systems requires the references crossing the language boundaries to be analysed using the eer gral approach to graph based conceptual modeling models representing relevant aspects of single language are built and integrated into a common conceptual model since conceptual modeling focusses on specific problems the integrated model presented here is especially tailored to multi language aspects software systems are parsed and represented according to this conceptual model and queried by using a powerful graph query mechanism this allows multi language cross references to be easily retrieved the multi language conceptual model and the query facilities have been developed in cooperation with the maintenance programmers at an insurance company w process oriented estimation of generalization error methods to avoid overfitting fall into two broad categories data oriented using separate data for validation and representation oriented penalizing complexity in the model both have limitations that are hard to overcome we argue that fully adequate model evaluation is only possible if the search process by which models are obtained is also taken into account to this end we recently proposed a method for process oriented evaluation poe and successfully applied it to rule induction domingos 1998b however for the sake of simplicity this treatment made a number of rather artificial assumptions in this paper the assumptions are removed and a simple formula for error estimation is obtained empirical trials show the new better founded form of poe to be as accurate as the previous one while further reducing theory sizes 1 introduction overfitting avoidance is a central problem in machine learning if a learner is su ciently powerful whatever repre query optimization in the presence of limited access patterns 1 introduction the goal of a query optimizer of a database system is to translate a declarative query expressed on a logical schema into an imperative query execution plan that accesses the physical storage of the data and applies a sequence of relational operators in building query execution plans traditional relational query optimizers try to find the most efficient method for accessing the necessary data when possible a query optimizer will use auxiliary data structures such as an index on a file in order to efficiently retrieve a certain set of tuples in a relation however when such structures do not exist or are not useful for the given query the alternative of scanning the entire relation always exists the existence of the fall back option to perform a complete scan is an important assumption in traditional query optimization several recent query processing applications have the common characteristic that it is not always possible to perform complete scans on the data instead the query optimization problem is complicated by the fact that there are only limited access patterns to the data one such sic satisfiability checking for integrity constraints sic is an interactive prototype to assist in the design of finitely satisfiable integrity constraints thus sic addresses the constraint satisfiability problem during the schema design phase of a database sic combines two systems a reasoning component and an interactive visual interface this paper outlines the functionality of both components and the theoretical background and implementation aspects of the reasoning component towards combining fuzzy and logic programming techniques many problems from ai have been successfully solved using fuzzy techniques on the other hand there are many other ai problems in which logic programming lp techniques have been very useful since we have two successful techniques why not combine them a wearable spatial conferencing space wearable computers provide constant access to computing and communications resources in this paper we describe how the computing power of wearables can be used to provide spatialized 3d graphics and audio cues to aid communication the result is a wearable augmented reality communication space with audio enabled avatars of the remote collaborators surrounding the user the user can use natural head motions to attend to the remote collaborators can communicate freely while being aware of other side conversations and can move through the communication space in this way the conferencing space can support dozens of simultaneous users informal user studies suggest that wearable communication spaces may offer several advantages both through the increase in the amount of information it is possible to access and the naturalness of the interface 1 introduction one of the broad trends emerging in human computer interaction is the increasing portability of computing and communication fac representing and querying changes in semistructured data semistructured data may be irregular and incomplete and does not necessarily conform to a fixed schema as with structured data it is often desirable to maintain a history of changes to data and to query over both the data and the changes representing and querying changes in semistructured data is more difficult than in structured data due to the irregularity and lack of schema we present a model for representing changes in semistructured data and a language for querying over these changes we discuss implementation strategies for our model and query language we also describe the design and implementation of a query subscription service that permits standing queries over changes in semistructured information sources 1 introduction semistructured data is data that has some structure but it may be irregular and incomplete and does not necessarily conform to a fixed schema e g html documents recently there has been increased interest in data models and query languages for s the dedale system for complex spatial queries this paper presents dedale a spatial database system intended to overcome some limitations of current systems by providing an abstract and non specialized data model and query language for the representation and manipulation of spatial objects dedale relies on a logical model based on linear constraints which generalizes the constraint database model of kkr90 while in the classical constraint model spatial data is always decomposed into its convex components in dedale holes are allowed to fit the need of practical applications the logical representation of spatial data although slightly more costly in memory has the advantage of simplifying the algorithms dedale relies on nested relations in which all sorts of data thematic spatial etc are stored in a uniform fashion this new data model supports declarative query languages which allow an intuitive and efficient manipulation of spatial objects their formal foundation constitutes a basis for practical query optimizati a cubist approach to object recognition we describe an appearance based object recognition system using a keyed multi level context representation reminiscent of certain aspects of cubist art specifically we utilize distinctive intermediatelevel features in this case automatically extracted 2 d boundary fragments as keys which are then verified within a local context and assembled within a loose global context to evoke an overall percept this system demonstrates good recognition of a variety of 3d shapes ranging from sports cars and fighter planes to snakes and lizards with full orthographic invariance we report the results of large scale tests involving over 2000 separate test images that evaluate performance with increasing number of items in the database in the presence of clutter background change and occlusion and also the results of some generic classification experiments where the system is tested on objects never previously seen or modeled to our knowledge the results we report are the best in the l large scale terrain visualization using the restricted quadtree triangulation real time rendering of triangulated surfaces has attracted growing interest in the last few years however interactive visualization of very large scale grid digital elevation models is still a hard problem the graphics load must be controlled by an adaptive surface triangulation and by taking advantage of different levels of detail furthermore the management of the visible scene requires efficient access to the terrain database we describe a all in one visualization system which integrates adaptive triangulation dynamic scene management and spatial data handling the triangulation model is based on the restricted quadtree triangulation furthermore we present new algorithms of the restricted quadtree triangulation these include among others exact error approximation progressive meshing performance enhancements and spatial access keywords algorithms computer graphics virtual reality triangulated surfaces terrain visualization terascale visualization 1 introduction in a data mining framework for building intrusion detection models there is often the need to update an installed intrusion detection system ids due to new attack methods or upgraded computing environments since many current idss are constructed by manual encoding of expert knowledge changes to idss are expensive and slow in this paper we describe a data mining framework for adaptively building intrusion detection id models the central idea is to utilize auditing programs to extract an extensive set of features that describe each network connection or host session and apply data mining programs to learn rules that accurately capture the behavior of intrusions and normal activities these rules can then be used for misuse detection and anomaly detection new detection models are incorporated into an existing ids through a meta learning or co operative learning process which produces a meta detection model that combines evidence from multiple models we discuss the strengths of our data mining programs namely classification meta learning multi agent reinforcement learning weighting and partitioning this paper addresses weighting and partitioning in complex reinforcement learning tasks with the aim of facilitating learning the paper presents some ideas regarding weighting of multiple agents and extends them into partitioning an input state space into multiple regions with differential weighting in these regions to exploit differential characteristics of regions and differential characteristics of agents to reduce the learning complexity of agents and their function approximators and thus to facilitate the learning overall it analyzes in reinforcement learning tasks different ways of partitioning a task and using agents selectively based on partitioning based on the analysis some heuristic methods are described and experimentally tested we find that some off line heuristic methods performed the best significantly better than single agent models keywords weighting averaging neural networks partitioning gating reinforcement learning 1 introduction multiple ag systematic change management in dimensional data warehousing with the widespread and increasing use of data warehousing in industry the design of effective data warehouses and their maintenance has become a focus of attention independently of this the area of temporal databases has been an active area of research for well beyond a decade this article identifies shortcomings of so called star schemas which are widely used in industrial warehousing in their ability to handle change and subsequently studies the application of temporal techniques for solving these shortcomings star schemas represent a new approach to database design and have gained widespread popularity in data warehousing but while they have many attractive properties star schemas do not contend well with so called slowly changing dimensions and with state oriented data we study the use of so called temporal star schemas that may provide a solution to the identified problems while not fundamentally changing the database design approach more specifically we study the rel multi layer incremental induction this paper describes a multi layer incremental induction algorithm mlii which is linked to an existing nonincremental induction algorithm to learn incrementally from noisy data mlii makes use of three operations data partitioning generalization and reduction generalization can either learn a set of rules from a sub set of examples or refine a previous set of rules the latter is achieved through a redescription operation called reduction from a set of examples and a set of rules we derive a new set of examples describing the behaviour of the rule set new rules are extracted from these behavioral examples and these rules can be seen as meta rules as they control previous rules in order to improve their predictive accuracy experimental results show that mlii achieves significant improvement on the existing nonincremental algorithm hcv used for experiments in this paper in terms of rule accuracy 1 introduction existing machine learning algorithms can be generally distin latent semantic indexing a probabilistic analysis latent semantic indexing lsi is an information retrieval technique based on the spectral analysis of the term document matrix whose empirical success had heretofore been without rigorous prediction and explanation we prove that under certain conditions lsi does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance we also propose the technique of random projection as a way of speeding up lsi we complement our theorems with encouraging experimental results we also argue that our results may be viewed in a more general framework as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering computer science division u c berkeley berkeley ca 94720 email christos cs berkeley edu y ibm almaden research center 650 harry road san jose ca 95120 email pragh almaden ibm com z computer science department meiji university tokyo japan email htamaki cs meiji facial feature detection by saccadic exploration of the gabor decomposition the gabor decomposition is a ubiquitous tool in computer vision nevertheless it is generally considered computationally demanding for active vision applications we suggest an attention driven approach to feature detection inspired by the human saccadic system a dramatic speedup is achieved by computing the gabor decomposition only on the points of a sparse retinotopic grid an application to eye detection is presented also a real time head detection and tracking system based on our approach is briefly discussed the system features a novel eyeball mounted camera designed to mimic the dynamic performance of the human eye and is to the best of our knowledge the first example of active vision system based on the gabor decomposition adapting hidden markov models for asl recognition by using three dimensional computer vision methods we present an approach to continuous american sign language asl recognition which uses as input three dimensional data of arm motions we use computer vision methods for three dimensional object shape and motion parameter extraction and an ascension technologies flock of birds interchangeably to obtain accurate three dimensional movement parameters of asl sentences selected from a 53 sign vocabulary and a widely varied sentence structure these parameters are used as features for hidden markov models hmms to address coarticulation e ects and improve our recognition results we experimented with two di erent approaches the first consists of training context dependent hmms and is inspired by speech recognition systems the second consists of modeling transient movements between signs and is inspired by the characteristics of asl phonology our experiments verified that the second approach yields better recognition results 1 introduction sign language and gesture recognition h machine learning for intelligent systems recent research in machine learning has focused on supervised induction for simple classification and reinforcement learning for simple reactive behaviors in the process the field has become disconnected from ai s original goal of creating complete intelligent agents in this paper i review recent work on machine learning for planning language vision and other topics that runs counter to this trend and thus holds interest for the broader ai research community i also suggest some steps to encourage further research along these lines introduction a central goal of artificial intelligence has long been to construct a complete intelligent agent that can perceive its environment generate plans execute those plans and communicate with other agents the pursuit of this dream naturally led many researchers to focus on the component tasks of perception planning control and natural language or on generic issues that cut across these tasks such as representation and search over constructive theory refinement in knowledge based neural networks knowledge based artificial neural networks offer an approach for connectionist theory refinement we present an algorithm for refining and extending the domain theory incorporated in a knowledge based neural network using constructive neural network learning algorithms the initial domain theory comprising of propositional rules is translated into a knowledge based network of threshold logic units tlu the domain theory is modified by dynamically adding neurons to the existing network a constructive neural network learning algorithm is used to add and train these additional neurons using a sequence of labeled examples we propose a novel hybrid constructive learning algorithm based on the tiling and pyramid constructive learning algorithms that allows knowledge based neural network to handle patterns with continuous valued attributes results of experiments on two non trivial tasks the ribosome binding site prediction and the financial advisor show that our algorithm compares favo using knowledge containers to model a framework for learning adaptation knowledge in this paper we present a framework for learning adaptation knowledge which knowledge light approaches for case based reasoning cbr systems knowledge light means that these approaches use already acquired knowledge inside the cbr system therefor we describe the sources of knowledge inside a cbr system along the different knowledge containers after that we present our framework in terms of these knowledge containers further we apply our framework in a case study to one knowledge light approach for learning adaptation knowledge after that we point on some issues which should be addressed during the design or the use of such algorithms for learning adaptation knowledge from our point of view many of these issues should be the topic of further research finally we close with a short discussion and an outlook to further work a runtime system for interactive web services interactive web services are increasingly replacing traditional static web pages producing web services seems to require a tremendous amount of laborious lowlevel coding due to the primitive nature of cgi programming we present ideas for an improved runtime system for interactive web services built on top of cgi running on virtually every combination of browser and http cgi server the runtime system has been implemented and used extensively in bigwig a tool for producing interactive web services keywords cgi interactive web service web document management runtime system session model 1 introduction an interactive web service consists of a global shared state typically a database and a number of distinct sessions that each contain some local private state and a sequential imperative action a web client may invoke an individual thread of one of the given session kinds the execution of this thread may interact with the client and inspect or modify the global state one support vector machine reference manual this document will describe these programs to find out more about svms see the bibliography we will not describe how svms work here the first program we will describe is the paragen program as it specifies all parameters needed for the svm 3 paragen when using the support vector machine for any given task it is always necessary to specify a set of parameters these parameters include information such as whether you are interested in pattern recognition or regression estimation what kernel you are using what scaling is to be done on the data etc paragen generates parameter files used by the svm program if no file was generated the user will be asked interactively experiments on automatic web page categorization for ir system this paper describes keyword based web page categorization our goal is to embed our categorization technique into information retrieval ir systems to facilitate the end users search task in such systems search results must be categorized faster while keeping accuracy high our categorization system uses a knowledge base kb to assign categories to web pages the kb contains a set of characteristic keywords with weights by category and is automatically generated from training texts with the keyword based approach the algorithms to extract keywords and assign weights to them should be considered because the algorithms affect strongly both categorization accuracy and processing speed furthermore we must take two characteristics of web pages into account 1 the text length is very variable which makes it harder to use statistics such as word frequency to calculate keyword weights and 2 a huge number of distinct words are used which makes the kb bigger and therefore pro computational logic and machine learning a roadmap for inductive logic programming computational logic has already significantly influenced symbolic machine learning through the field of inductive logic programming ilp which is concerned with the induction of logic programs from examples and background knowledge in ilp the shift of attention from program synthesis to knowledge discovery resulted in advanced techniques that are practically applicable for discovering knowledge in relational databases machine learning and ilp in particular has the potential to influence computational logic by providing an application area full of industrially significant problems thus providing a challenge for other techniques in computational logic this paper gives a brief introduction to ilp presents state of the art ilp techniques for relational knowledge discovery as well as some research and organizational directions for further developments in this area 1 introduction inductive logic programming ilp 35 39 29 is a research area that has its backgrounds in induct clustering categorical data an approach based on dynamical systems we describe a novel approach for clustering collections of sets and its application to the analysis and mining of categorical data by categorical data we mean tables with fields that cannot be naturally ordered by a metric e g the names of producers of automobiles or the names of products offered by a manufacturer our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table this facilitates a type of similarity measure arising from the cooccurrence of values in the dataset our techniques can be studied analytically in terms of certain types of non linear dynamical systems we discuss experiments on a variety of tables of synthetic and real data we find that our iterative methods converge quickly to prominently correlated values of various categorical fields 1 introduction much of the data in databases is categorical fields in tables whose attributes cannot naturally be ordered as numerical values can the pro case based learning for knowledge based design support we present a general approach to combine methods of interactive knowledge acquisition with methods for machine learning the approach has been developed in order to deliver knowledge required by support systems for design tasks learning rests upon a knowledge representation scheme for cases that distinguishes between knowledge needed for subgoaling and knowledge needed for design we employ traces i e protocols of the user s actions when tackling design tasks as the initial input for incremental knowledge acquisition this allows to learn task structures to be used for subgoaling and case bases plus similarity relations applicable to particular case bases 1 introduction integrating incremental learning into a knowledge based systems seems to be a promising way to lessen the burden of knowledge elicitation to system development 9 the goal of this paper is to point out how learning can be used in an interactive design support system that uses cbr 8 as the main problem solvin routing through networks with hierarchical topology aggregation abstract in the future global networks will consist of a hierarchy of subnetworks called domains for reasons of both scalability and security domains will not reveal details of their internal structure to outside nodes instead these domains will advertise only a summary or aggregated view of their internal structure e g as proposed by the atm pnni standard this work compares by simulation the performance of several different aggregation schemes in terms of network throughput the fraction of attempted connections that are realized and network control load the average number of crankbacks per realized connection our main results are ffl minimum spanning tree is a good aggregation scheme ffl exponential link cost functions perform better than min hop routing ffl our suggested logarithmic update scheme that determine when re aggregation should be computed can significantly reduce the computational overhead due to re aggregation with a negligible decrease in performance 1 learning comprehensible descriptions of multivariate time series supervised classification is one of the most active areas of machine learning research most work has focused on classification in static domains where an instantaneous snapshot of attributes is meaningful in many domains attributes are not static in fact it is the way they vary temporally that can make classification possible examples of such domains include speech recognition gesture recognition and electrocardiograph classification while it is possible to use ad hoc domain specific techniques for attening the time series to a learner friendly representation this fails to take into account both the special problems and special heuristics applicable to temporal data and often results in unreadable concept descriptions though traditional time series techniques can sometimes produce accurate classi ers few can provide comprehensible descriptions we propose a general architecture for classification and description of multivariate time series it employs event primitives to ana an empirical comparison of decision trees and other classification methods twenty two decision tree nine statistical and two neural network classifiers are compared on thirtytwo datasets in terms of classification error rate computational time and in the case of trees number of terminal nodes it is found that the average error rates for a majority of the classifiers are not statistically significant but the computational times of the classifiers differ over a wide range the statistical polyclass classifier based on a logistic regression spline algorithm has the lowest average error rate however it is also one of the most computationally intensive the classifier based on standard polytomous logistic regression and a decision tree classifier using the quest algorithm with linear splits have the second lowest average error rates and are about 50 times faster than polyclass among decision tree classifiers with univariate splits the classifiers based on the c4 5 ind cart and quest algorithms have the best combination of error rate and speed althoug a framework for workflow management systems based on objects rules and roles the goal of this paper is to present an approach for the development of workflow management systems supporting both reusability and adaptability i e customization due to frequently changing requirements in an organization the principal contribution is the introduction of an object oriented application framework for constructing such workflow management systems balancing between reusability and adaptability the underlying techniques are an object oriented workflow model object evolution via an integrated role model and the support of business policies via an integrated rule model categories and subject descriptors h 4 1 office automation workflow management d 2 11 software architectures domain specific architectures general terms design additional key words and phrases object oriented frameworks event condition action rule role modeling context dependent behavior introduction this paper examines the application of the framework idea to the development of workflo logic based user defined aggregates for the next generation of database systems this paper we provide logic based foundations for the extended aggregate constructs required by advanced database applications in particular we focus on data mining applications and show that they require user defined aggregates extended with early returns thus we propose a simple formalization of extended user defined aggregates using the nondeterministic construct of choice we obtain programs that have a formal semantics based on the concept of total stable models but are also amenable to efficient implementation our formalization leads to a simple syntactic characterization of user defined aggregates that are monotone with respect to set containment therefore these aggregates can be freely used in recursive programs and the fixpoints for such programs can be computed efficiently using the standard techniques of deductive databases we describe the many new applications of user defined aggregates and their implementation for the logical data language ldl finally we discuss the transfer of this technology to sql databases 1 introduction learning situation specific control in multi agent systems the work presented in this thesis deals with techniques to improve problem solving control skills of cooperative agents through machine learning in a multi agent system the local problem solving control of an agent can interact in complex and intricate ways with the problem solving control of other agents in such systems an agent cannot make effective control decisions based purely on its local problem solving state effective cooperation requires that the global problem solving state influence the local control decisions made by an agent we call such an influence cooperative control an agent with a purely local view of the problem solving situation cannot learn path constraints on semistructured and structured data we present a class of path constraints of interest in connection with both structured and semi structured databases and investigate their associated implication problems these path constraints are capable of expressing natural integrity constraints that are not only a fundamental part of the semantics of the data but are also important in query optimization we show that in semistructured databases despite the simple syntax of the constraints their associated implication problem is r e complete and finite implication problem is co r e complete however we establish the decidability of the implication problems for several fragments of the path constraint language and demonstrate that these fragments suffice to express important semantic information such as inverse relationships and local database constraints commonly found in object oriented databases we also show that in the presence of types the analysis of path constraint implication becomes more delicate we demonstrate so domain specific keyphrase extraction keyphrases are an important means of document summarization clustering and topic search only a small minority of documents have author assigned keyphrases and manually assigning keyphrases to existing documents is very laborious therefore it is highly desirable to automate the keyphrase extraction process this paper shows that a simple procedure for keyphrase extraction based on the naivebayes learning scheme performs comparably to the state of the art it goes on to explain how this procedure s performance can be boosted by automatically tailoring the extraction process to the particular document collection at hand results on a large collection of technical reports in computer science show that the quality of the extracted keyphrases improves signi cantly when domain speci c information is exploited 1 introduction keyphrases give a high level description of a document s contents that is intended to make it easy for prospective readers to decide whether or no a case based reasoning approach to learning control the paper describes an application of a case based reasoning system ta3 to a control task in robotics it differs from previous methods in its approach to relevancy based retrieval which allows for greater flexibility and for improved accuracy in performance most successful robotic manipulators are precise fast smooth and have high repeatability their major drawback is their tendency to have only a limited repertoire of movements that can only be extended by costly inverse kinematic calculations or direct teaching our approach also starts with a limited repertoire of movements but can incrementally add new positions and thus allows for higher flexibility the proposed architecture is experimentally evaluated on two real world domains albeit simple and the results are compared to other machine learning algorithms applied to the same problem keywords learning control systems case based reasoning flexible manufacturing inverse kinematic task this research was supported by towards flexible multi agent decision making under time pressure abstract autonomous agents need considerable computational resources to perform rational decision making these demands are even more severe when other agents are present in the environment in these settings the quality of an agent s alternative behaviors depends not only on the state of the environment but also on the actions of other agents which in turn depend on the others beliefs about the world their preferences and further on the other agents beliefs about others and so on the complexity becomes prohibitive when large number of agents are present and when decisions have to be made under time pressure in this paper we investigate strategies intended to tame the computational burden by using off line computation in conjunction with on line reasoning we investigate two approaches first we use rules compiled off line to constrain alternative actions considered during on line reasoning this method minimizes overhead but is not sensitive to changes in realtime demands of the situation at hand second we use performance profiles computed off line and the notion of urgency i e the value of time computed on line to choose the amount of information to be included during on line deliberation this method can adjust to various levels of real time demands but incurs some overhead associated with iterative deepening we test our framework with experiments in a simulated anti air defense domain the experiments show that both procedures are effective in reducing computation time while offering good performance under time pressure an evolutionary approach to case adaptation we present a case adaptation method that employs ideas from the field of genetic algorithms two types of adaptations case combination and case mutation are used to evolve variations on the contents of retrieved cases until a satisfactory solution is found for a new specified problem a solution is satisfactory if it matches the specified requirements and does not violate any constraints imposed by the domain of applicability we have implemented our ideas in a computational system called gencad applied to the layout design of residences such that they conform to the principles of feng shui the chinese art of placement this implementation allows us to evaluate the use of ga s for case adaptation in cbr experimental results show the role of representation and constraints 1 introduction many different methods have been proposed for performing the task of case adaptation in cbr they have been surveyed in several publications including 1 2 and 3 different approaches ma optimization of user defined functions in distributed object relational dbms full support of parallelism in object relational database systems ordbmss is desired the parallelization techniques developed for relational database systems are not adequate for ordbms because of the introduction of complex abstract data types and operations on ordered domains in this paper we consider a data stream paradigm and develop a query parallelization framework that exploits characteristics of user defined functions in a ordbms during query optimization by introducing the concept of windows and abstract data type orderings we develop a novel approach for parallelizing user defined functions in a distributed ordbms environment the implementation issues in providing query services in ordered domains are also discussed 1 introduction as time goes by more and more database vendors agree that object relational dbmss ordbmss are the future 23 though full support of parallel ordbms is expected the techniques used to parallelize relational database systems are not ad design issues for mixed initiative agent systems this paper addresses the effect of mixed initiative systems on multiagent systems design a mixed initiative system is one in which humans interact directly with software agents in a collaborative approach to problem solving there are two main levels at which multiagent systems are designed the domain level and the individual agent level at the domain level there are few unique challenges to mixedinitiative system design however at the individual agent level the agent itself must be designed to interact with the human and the agent system integrating the two into a single system introduction much of the current research related to intelligent agents has focused on the capabilities and structure of individual agents however in order to solve complex problems these agents must work cooperatively with other agents in a heterogeneous environment this is the domain of multiagent systems in multiagent systems we are interested in the coordinated behavior of a system of indiv supporting conflict management in cooperative design teams the design of complex artifacts has increasingly become a cooperative process with the detection and resolution of conflicts between design agents playing a central role effective tools for supporting the conflict management process however are still lacking this paper describes a system called dcss the design collaboration support system developed to meet this challenge in design teams with both human and machine based agents every design agent is provided with an assistant that provides domain independent conflict detection classification and resolution expertise the design agents provide the domainspecific expertise needed to instantiate this general expertise including the rationale for their actions as a part of their design activities dcss has been used successfully to support the cooperative design of local area networks by human and machine based designers this paper includes a description of dcss s underlying model and implementation examples of its operation leveraging mediator cost models with heterogeneous data sources distributed systems require declarative access to diverse data sources of information one approach to solving this heterogeneous distributed database problem is based on mediator architectures in these architectures mediators accept queries from users process them with respect to wrappers and return answers wrapper provide access to underlying data sources to efficiently process queries the mediator must optimize the plan used for processing the query in classical databases cost estimate based query optimization is an effective method for optimization in a heterogeneous distributed databases cost estimate based query optimization is difficult to achieve because the underlying data sources do not export cost information this paper describes a new method that permits the wrapper programmer to export cost estimates cost estimate formulas and statistics for the wrapper programmer to describe all cost estimates may be impossible due to lack of information or burdensome due xm2vtsdb the extended m2vts database in this paper we describe the acquisition and content of a large multi modal database intended for training and testing of multi modal verification systems the xm2vtsdb database offers synchronised video and speech data as well as image sequences allowing multiple views of the face it consists of digital video recordings taken of 295 hundred subjects at one month intervals taken over a period of five months we also describe a protocol for evaluating verification algorithms on the database the database has been made available to anyone on request to the university of surrey through http www ee surrey ac uk research vssp xm2vtsdb backtracking algorithms for disjunctions of temporal constraints we extend the framework of simple temporal problems studied originally by dechter meiri and pearl to consider constraints of the form x1 gamma y1 r1 xn gamma yn rn where x1 xn y1 yn are variables ranging over the real numbers r1 rn are real constants and n 1 we have implemented four progressively more efficient algorithms for the consistency checking problem for this class of temporal constraints we have partially ordered those algorithms according to the number of visited search nodes and the number of performed consistency checks finally we have carried out a series of experimental results on the location of the hard region the results show that hard problems occur at a critical value of the ratio of disjunctions to variables this value is between 6 and 7 introduction reasoning with temporal constraints has been a hot research topic for the last fifteen years the importance of this problem has been demonstrated in many areas of artifici logic programs for intelligent web search we present a general framework for the information extraction from web pages based on a special wrapper language called token templates by using token templates in conjunction with logic programs we are able to reason about web page contents search and collect facts and derive new facts from various web pages we give a formal definition for the semantics of logic programs extended by token templates and define a general answer completecalculus for these extendedprograms these methodsand techniquesare used to build intelligent mediators and web information systems keywords information extraction template based wrappers mediators logic programming theory reasoning deductive web databases softbots logic robots 1 introduction in the last few years it became appearant that there is an increasing need for more intelligent world wide web information systems the existing information systems are mainly document search engines e g alta vista yahoo webcrawler based on in results and challenges in web search evaluation a frozen 18 5 million page snapshot of part of the web has been created to enable and encourage meaningful and reproducible evaluation of web search systems and techniques this collection is being used in an evaluation framework within the text retrieval conference trec and will hopefully provide convincing answers to questions such as can link information result in better rankings do longer queries result in better answers and do trec systems work well on web data the snapshot and associated evaluation methods are described and an invitation is extended to participate preliminary results are presented for an effectivess comparison of six trec systems working on the snapshot collection against five well known web search systems working over the current web these suggest that the standard of document rankings produced by public web search engines is by no means state of the art 1999 published by elsevier science b v all rights reserved keywords evaluation search emediator a next generation electronic commerce server this paper presents emediator an electronic commerce server prototype that demonstrates ways in which algorithmic support and game theoretic incentive engineering can jointly improve the efficiency of ecommerce eauctionhouse the configurable auction server includes a variety of generalized combinatorial auctions and exchanges pricing schemes bidding languages mobile agents and user support for choosing an auction type we introduce two new logical bidding languages for combinatorial markets the xor bidding language and the or of xors bidding language unlike the traditional or bidding language these are fully expressive they therefore enable the use of the clarke groves pricing mechanism for motivating the bidders to bid truthfully eauctionhouse also supports supply demand curve bidding ecommitter the leveled commitment contract optimizer determines the optimal contract price and decommitting penalties for a variety of leveled commitment contracting mechanisms taking into account that rational agents will decommit strategically in nash equilibrium it also determines the optimal decommitting strategies for any given leveled commitment contract eexchangehouse the safe exchange planner enables unenforced anonymous exchanges by dividing the exchange into chunks and sequencing those chunks to be delivered safely in alternation between the buyer and the seller first order queries on finite structures over the reals we investigate properties of finite relational structures over the reals expressed by first order sentences whose predicates are the relations of the structure plus arbitrary polynomial inequalities and whose quantifiers can range over the whole set of reals in constraint programming terminology this corresponds to boolean real polynomial constraint queries on finite structures the fact that quantifiers range over all reals seems crucial however we observe that each sentence in the first order theory of the reals can be evaluated by letting each quantifier range over only a finite set of real numbers without changing its truth value inspired by this observation we then show that when all polynomials used are linear each query can be expressed uniformly on all finite structures by a sentence of which the quantifiers range only over the finite domain of the structure in other words linear constraint programming on finite structures can be reduced to ordinary query evaluation as usual in finite model theory and databases moreover if only generic queries are taken into consideration we show that this can be reduced even further by proving that such dept math computer sci university of antwerp uia universiteitsplein 1 b 2610 antwerp belgium e mail pareda uia ac be y dept wni university of limburg luc b 3590 diepenbeek belgium e mail vdbuss luc ac be z computer science department indiana university bloomington in 47405 4101 usa e mail vgucht cs indiana edu 1 queries can be expressed by sentences using as polynomial inequalities only those of the simple form x y 1 knowledge management through ontologies most enterprises agree that knowledge is an essential asset for success and survival on a increasingly competitive and global market this awareness is one of the main reasons for the exponential growth of knowledge management in the past decade our approach to knowledge management is based on ontologies and makes knowledge assets intelligently accessible to people in organizations most company vital knowledge resides in the heads of people and thus successful knowledge management does not only consider technical aspects but also social ones in this paper we describe an approach to intelligent knowledge management that explicitly takes into account the social issues involved the proof of concept is given by a large scale initiative involving knowledge management of a virtual organization 1 introduction according to information week angus et al 1998 the business problem that knowledge management is designed to solve is that knowledge acquired through experience doesn t ge trips an integrated intelligent problem solving assistant we discuss what constitutes an integrated system in ai and why ai researchers should be interested in building and studying them taking integrated systems to be ones that integrate a variety of components in order to perform some task from start to finish we believe that such systems a allow us to better ground our theoretical work in actual tasks and b provide an opportunity for much needed evaluation based on task performance we describe one particular integrated system we have developed that supports spoken language dialogue to collaboratively solve planning problems we discuss how the integrated system provides key advantages for helping both our work in natural language dialogue processing and in interactive planning and problem solving and consider the opportunities such an approach affords for the future content areas ai systems natural language understanding planning and control problem solving user interfaces introduction it is an interesting time to be an a ontobroker the very high idea the world wide web www is currently one of the most important electronic information sources however its query interfaces and the provided reasoning services are rather limited ontobroker consists of a number of languages and tools that enhance query access and inference service of the www the technique is based on the use of ontologies ontologies are applied to annotate web documents and to provide query access and inference service that deal with the semantics of the presented information in consequence intelligent brokering services for web documents can be achieved without requiring to change the semiformal nature of web documents introduction the world wide web www contains huge amounts of knowledge about almost all subjects you can think of html documents enriched by multi media applications provide knowledge in different representations i e text graphics animated pictures video sound virtual reality etc hypertext links between web documents represent r image retrieval past present and future this paper provides a comprehensive survey of the technical achievements in the research area of image retrieval especially content based image retrieval an area so active and prosperous in the past few years the survey includes 100 papers covering the research aspects of image feature representation and extraction multi dimensional indexing and system design three of the fundamental bases of content based image retrieval furthermore based on the state of the art technology available now and the demand from real world applications open research issues are identified and future promising research directions are suggested 1 introduction recent years have seen a rapid increase of the size of digital image collections everyday both military and civilian equipment generates giga bytes of images huge amount of information is out there however we can not access to or make use of the information unless it is organized so as to allow efficient browsing searching and retriev netview a framework for integration of large scale distributed visual databases many visual databases including image and video databases are being designed in various locations the integration of such databases enables users to access data across the world in a transparent manner in this article we present a system framework termed netview which supports global content based query access to various visual databases over the internet an integrated metaserver is designed to facilitate such access the metaserver contains three main components the metadatabase the metasearch agent and the query manager the metadatabase is organized to include the metadata about individual visual databases which reflect the semantics of each visual database the query manager extracts heterogeneous features such as text texture and color in the query for suitable matching of the metadata the metasearch agent derives a list of relevant database sites to the given query by matching their feature content with the metadata such a capability can significantly reduce the a intelligent diagnosis systems this paper examines and compares several different approaches to the design of intelligent systems for diagnosis and advising applications these include expert systems or knowledge based systems case based reasoning systems truth or reason maintenance systems statistical pattern classification systems decision trees and artificial neural networks or connectionist systems the key aspects of each approach are demonstrated through the design of a system for a simple automobile fault diagnosis task the paper also discusses the domain characteristics and design and performance requirements that influence the choice of a specific technique or a combination of techniques for a given application 1 introduction the last few decades have seen a proliferation of intelligent systems for diagnosis advising forecasting and related applications dean et al 1995 durkin 1994 ginsberg 1993 luger stubblefield 1993 puppe 1993 rich knight 1991 russell norvig 1995 ste array based evaluation of multi dimensional queries in object relational database systems since multi dimensional arrays are a natural data structure for supporting multi dimensional queries and object relational database systems support multi dimensional array adts it is natural to ask if a multi dimensional array based adt can be used to improve o r dbms performance on multi dimensional queries as an initial step toward answering this question we have implemented a multi dimensional array in the paradise objectrelational dbms in this paper we describe the implementation of this compressed array adt and explore its performance for queries including star join consolidations and selections we show that in many cases the array adt can provide significantly higher performance than can be obtained by applying techniques such as bitmap indices and star join algorithms to relational tables 1 introduction multi dimensional data analysis has been around for at least twenty years or95 but has recently taken the spotlight in the context of olap on line analytical process evaluation of eye gaze interaction eye gaze interaction can provide a convenient and natural addition to user computer dialogues we have previously reported on our interaction techniques using eye gaze 10 while our techniques seemed useful in demonstration we now investigate their strengths and weaknesses in a controlled setting in this paper we present two experiments that compare an interaction technique we developed for object selection based on a where a person is looking with the most commonly used selection method using a mouse we find that our eye gaze interaction technique is faster than selection with a mouse the results show that our algorithm which makes use of knowledge about how the eyes behave preserves the natural quickness of the eye eye gaze interaction is a reasonable addition to computer interaction and is convenient in situations where it is important to use the hands for other tasks it is particularly beneficial for the larger screen workspaces and virtual environments of the future and it will become increasingly practical as eye tracker technology matures a framework for automated construction and transformation of case based reasoning systems case based reasoning systems have gained immense popularity over the recent years as problem solving tools most case based reasoning systems however are developed essentially from scratch using proprietary systems and applications on a limited number of platforms although methods have been proposed to describe the structure of a case based reasoner none of these have been very successful outside their application domains in this paper we rst describe common methods for automating cbr system construction we then describe a general model for common cbr implementation and describe in detail a framework of platform independent construction of systems based on this model we discuss an implementation of such a system using java and nally describe ways systems can be developed using this framework 1 introduction achieving widespread case based reasoning support for corporate memories will require exibility in integrating implementations with existing organizational i text categorization using weight adjusted k nearest neighbor classification automatic text categorization is an important task that can help people finding information on huge online resources text categorization presents unique challenges due to the large number of attributes present in the data set large number of training samples attribute dependency and multi modality of categories existing classification techniques have limited applicability in the data sets of these natures in this paper we present a weight adjusted k nearest neighbor waknn classification that learns feature weights based on a greedy hill climbing technique we also present two performance optimizations of waknn that improve the computational performance by a few orders of magnitude but do not compromise on the classification quality we experimentally evaluated waknn on 52 document data sets from a variety of domains and compared its performance against several classification algorithms such as c4 5 ripper naive bayesian pebls and vsm experimental results navigational plans for data integration we consider the problem of building data integration systems when the data sources are webs of data rather than sets of relations previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data we describe a language for modeling data sources in this new context we show that our language has the required expressive power and that minor extensions to it would make query answering intractable we provide a sound and complete algorithm for reformulating a user query into a query over the data sources and we show how to create query execution plans that both query and navigate the data sources introduction the purpose of data integration is to provide a uniform interface to a multitude of data sources data integration applications arise frequently as corporations attempt to provide their customers and employees wit summarizing text documents sentence selection and evaluation metrics human quality text summarization systems are difficult to design and even more difficult to evaluate in part because documents can differ along several dimensions such as length writing style and lexical usage nevertheless certain cues can often help suggest the selection of sentences for inclusion in a summary this paper presents our analysis of news article summaries generated by sentence selection sentences are ranked for potential inclusion in the summary using a weighted combination of statistical and linguistic features the statistical features were adapted from standard ir methods the potential linguistic ones were derived from an analysis of news wire summaries toevaluate these features we use a normalized version of precision recall curves with a baseline of random sentence selection as well as analyze the properties of such a baseline we illustrate our discussions with empirical results showing the importance of corpus dependent baseline summarization standards compression ratios and carefully crafted long queries formal analysis of models for the dynamics of trust based on experiences the aim of this paper is to analyse and formalise the dynamics of trust in the light of experiences a formal framework is introduced for the analysis and specification of models for trust evolution and trust update different properties of these models are formally defined 1 introduction trust is the attitude an agent has with respect to the dependability capabilities of some other agent maybe itself or with respect to the turn of events the agent might for example trust that the statements made by another agent are true the agent might trust the commitment of another agent with respect to a certain joint goal the agent might trust that another agent is capable of performing certain tasks the agent might trust itself to be able to perform some tasks the agent might trust that the current state of affairs will lead to a state of affairs that is agreeable to its own intentions goals commitments or desires in 1 2 the importance of the notion trust is shown for agent learning environmental features for pose estimation we present a method for learning a set of environmental features which are useful for pose estimation the landmark learning mechanism is designed to be applicable to a wide range of environments and generalized for di erent sensing modilities in the context of computer vision each landmark is detected as a local extremum of a measure of distinctiveness and represented by an appearance based encoding which is exploited for matching the set of obtained landmarks can be parameterized and then evaluated in terms of their utility for the task at hand the method is used to motivate a general approach to task oriented sensor fusion we present experimental evidence that demonstrates the utility of the method 1 introduction in this paper we develop an approach to sensorbased robot localization by learning a set of recognizable features in the robot s environment in particular we consider the problem of learning a set of image domain landmarks from a set of di erent views of a scene hunting moving targets an extension to bayesian methods in multimedia databases it has been widely recognised that the difference between the level of abstraction of the formulation of a query by example and that of the desired result usually an image with certain semantics calls for the use of learning methods that try to bridge this gap cox et al have proposed a bayesian method to learn the user s preferences during each query cox et al s system pichunter 1 is designed for optimal performance when the user is searching for a fixed target image the performance of the system was evaluated using target testing which ranks systems according to the number of interaction steps required to find the target leading to simple easily reproducible experiments there are some aspects of image retrieval however which are not captured by this measure in particular the possibility of query drift i e a moving target is completely ignored the algorithm proposed by cox et al does not cope well with a change of target at a late query stage because it is automatic landmark selection for navigation with panoramic vision the use of visual landmarks for robot navigation is a promising field it is apparent that the success of navigating by visual landmarks depends on the landmarks chosen this paper reviews a monocular camera system proposed by bianco and zelinsky 1999 which automatically selects landmarks and uses them for localisation and navigation tasks the monocular system s landmark selection policy results in a limited area in which the robot can successfully localise itself a new landmark navigation system which uses a panoramic vision system to overcome this restriction is proposed and the first steps taken in the development of the new system are reported 1 introduction visual navigation is one of the key problems in making successful autonomous robots vision as a sensor is the richest source of information about a mobile agents enviroment and as such contains information vital to solving navigation problems one limit to visual navigation is the narrow field of view offered by norma logic based genetic programming with definite clause translation grammars dctg gp is a genetic programming system that uses definite clause translation grammars a dctg is a logical version of an attribute grammar that supports the definition of context free languages and it allows semantic information associated with a language to be easily accomodated by the grammar this is useful in genetic programming for defining the interpreter of a target language or incorporating both syntactic and semantic problem specific contraints into the evolutionary search the dctg gp system improves on other grammar based gp systems by permitting non trivial semantic aspects of the language to be defined with the grammar it also automatically analyzes grammar rules in order to determine their minimal depth and termination characteristics which are required when generating random program trees of varied shapes and sizes an application using dctg gp is described 1 introduction genetic programming gp implementations have benefitted from simple program denotations incremental and interactive sequence mining the discovery of frequent sequences in temporal databases is an important data mining problem most current work assumes that the database is static and a database update requires rediscovering all the patterns by scanning the entire old and new database in this paper we propose novel techniques for maintaining sequences in the presence of a database updates and b user interaction e g modifying mining parameters this is a very challenging task since such updates can invalidate existing sequences or introduce new ones in both the above scenarios we avoid re executing the algorithm on the entire dataset thereby reducing execution time experimental results confirm that our approach results in substantial performance gains 1 introduction sequence mining is an important data mining task where one attempts to discover frequent sequences over time of attribute sets in large databases this problem was originally motivated by applications in the retailing industry including conjunctive query containment and constraint satisfaction conjunctive query containment is recognized as a fundamental problem in database query evaluation and optimization at the same time constraint satisfaction is recognized as a fundamental problem in artificial intelligence what do conjunctive query containment and constraint satisfaction have in common our main conceptual contribution in this paper is to point out that despite their very different formulation conjunctive query containment and constraint satisfaction are essentially the same problem the reason is that they can be recast as the following fundamental algebraic problem given two finite relational structures a and b is there a homomorphism h a b as formulated above the homomorphism problem is uniform in the sense that both relational structures a and b are part of the input by fixing the structure b one obtains the following non uniform problem given a finite relational structure a is there a homomorphism h a b in general non uniform tractability results do not uniformize thus it is natural to ask which tractable cases of non uniform tractability results for constraint satisfaction and conjunctive query containment do uniformize our main technical contribution in this paper is to show that several cases of tractable non uniform constraint satisfaction problems do indeed uniformize we exhibit three non uniform tractability results that uniformize and thus give rise to polynomial time solvable cases of constraint satisfaction and conjunctive query containment agent aided aircraft maintenance aircraft maintenance is performed bymechanics who are required for non standard discrepancies to consult expert engineers for repair instructions and approval in addition to their own experience these engineers rely on external information sources which are often inadequately indexed and geographically dispersed the timely retrieval of this distributed information is essencial to the engineers abilityto devise and recommend repair procedures in response to the mechanics requests this problem domain is well suited for a multi agent system it consists of distributed multi modal information whichis needed bymultiple users with diverse preferences in this paper we describe an implementation of such a system using the retsina multi agentarchitecture such an implementation reinforces the importance of multi agent systems and in particular the usefulness of the retsina infrastructure as a basis for the construction of such systems i 1 introduction agent aided embodied evolution a response to challenges in evolutionary robotics we introduce embodied evolution ee a new methodology for conducting evolutionary robotics er embodied evolution uses a population of physical robots that evolve by reproducing with one another in the task environment ee addresses several issues identified by researchers in the evolutionary robotics community as problematic for the development of er we review results from our first experiments and discuss the advantages and limitations of the ee methodology partial models of extended generalized logic programs in recent years there has been an increasing interest in extensions of the logic programming paradigm beyond the class of normal logic programs motivated by the need for a satisfactory respresentation and processing of knowledge an important problem in this area is to find an adequate declarative semantics for logic programs in the present paper a general preference criterion is proposed that selects the intended partial models of extended generalized logic programs which is a conservative extension of the stationary semantics for normal logic programs of 13 15 and generalizes the wfsx semantics of 12 the presented preference criterion defines a partial model of an extended generalized logic program as intended if it is generated by a stationary chain the gwfsx semantics is defined by the set theoretical intersection of all stationary generated models and thus generalizes the results from 9 and 1 1 introduction declarative semantics provides a mathem on2broker semantic based access to information sources at the www on2broker provides brokering services to improve access to heterogeneous distributed and semistructured information sources as they are presented in the world wide web it relies on the use of ontologies to make explicit the semantics of web pages in the paper we will discuss the general architecture and main components of on2broker and provide some application scenarios 1 introduction in the paper we describe a tool environment called on2broker 1 that processes information sources and content descriptions in html xml and rdf and that provides intelligent information retrieval query answering and maintenance support central for our approach is the use of ontologies to describe background knowledge and to make explicit the semantics of web documents ontologies have been developed in the area of knowledge based systems for structuring and reusing large bodies of knowledge cf cyc lenat 1995 ka 2 benjamins et al 1998 ontologies are consensual and formal specificat spatial agents implemented in a logical expressible language in this paper we present a multi layered architecture for spatial and temporal agents the focus is laid on the declarativity of the approach which makes agent scripts expressive and well understandable they can be realized as constraint logic programs the logical description language is able to express actions or plans for one and more autonomous and cooperating agents for the robocup simulator league the system architecture hosts constraint technology for qualitative spatial reasoning but quantitative data is taken into account too the basic hardware layer processes the agent s sensor information an interface transfers this lowlevel data into a logical representation it provides facilities to access the preprocessed data and supplies several basic skills the second layer performs qualitative spatial reasoning on top of this the third layer enables more complex skills such as passing offside detection etc at last the fourth layer establishes acting as a team both by emergent and explicit cooperation logic and deduction provide a clean means to specify and also to implement teamwork behavior 1 imitation and mechanisms of joint attention a developmental structure for building social skills on a humanoid robot abstract adults are extremely adept at recognizing social cues such as eye direction or pointing gestures that establish the basis of joint attention these skills serve as the developmental basis for more complex forms of metaphor and analogy by allowing an infant to ground shared experiences and by assisting in the development of more complex communication skills in this chapter we review some of the evidence for the developmental course of these joint attention skills from developmental psychology from disorders of social development such as autism and from the evolutionary development of these social skills we also describe an on going research program aimed at testing existing models of joint attention development by building a human like robot which communicates naturally with humans using joint attention our group has constructed an upper torso humanoid robot called cog in part to investigate how to build intelligent robotic systems by following a developmental progression of skills similar to that observed in human development just as a child learns social skills and conventions through interactions with its parents our robot will learn to interact with people using natural social communication we further consider the critical role that imitation plays in bootstrapping a system from simple visual behaviors to more complex social skills we will present data from a face and eye finding system that serves as the basis of this developmental chain and an example of how this system can imitate the head movements of an individual 1 an efficient index structure for oid indexing in parallel temporal object oriented database systems in an object oriented database system based on logical oids an oid index oidx is necessary to convert from logical oid to physical location in a temporal objectoriented database system toodb this oidx also contains the timestamps of the object versions we have in a previous paper studied oidx performance with a relatively simple index the studies have shown that oidx maintenance can be quite costly especially objects updates because in a temporal oodb the oidx needs to be updated every time an object is updated this has convinced us that a new index structure particularly suitable to toodb requirements is necessary in this report we describe an efficient oid index structure for toodbs which we call the vagabond temporal oid index vtoidx the main goals of the vtoidx is 1 support for temporal data while still having index performance close to a non temporal one version database system 2 efficient object relational operation and 3 easy tertiary storage migrati analysis on a mobile agent based algorithm for network management recent advance in the agent technology has brought in a new method for network routing the ant routing algorithm although its effectiveness and efficiency have been demonstrated and reported in the literature its properties have not yet been well studied this paper will present some preliminary analysis of this algorithm in regard to its population growing property and jumping behavior for both synchronized and asynchronized networks we have shown that the expected number of agents in a node is no more than 1 max i fj omega i jg km where j omega i j is the number of neighbor hosts of the i th host k is the number of agents generated per request and m is the average number of requests it is shown that under a mild condition for all p 1 max i fj omega i jg km the probability of the number of agents in a node exceeding p is less than r 1 p p x dx where p x is a normal distributed function with mean and variance given by 1 max i fj omega i jg km and km nonmonotonic inheritance in object oriented deductive database languages deductive object oriented frameworks integrate logic rules and inheritance there specific problems arise due to the combination of deduction and inheritance a deduction can take place depending on inherited facts thus raising indirect conflicts and b also the class hierarchy and membership is subject to deduction from this point of view we investigate the application of the extension semantics of default logic to deductive object oriented database languages by restricting the problem to horn programs and a special type of defaults tailored to the semantics of inheritance a forwardchaining construction of extensions is possible this construction is compared with a solution as implemented in the f logic system florid which is based on a combination of classical deductive fixpoints and an inheritance trigger mechanism the paper is a condensed version of mk98 1 introduction in deductive object oriented database languages a class hierarchy and nonmonotonic inheritance i interaction between path and type constraints xml 7 which is emerging as an important standard for data exchange on the world wide web highlights the importance of semistructured data although the xml standard itself does not require any schema or type system a number of proposals 6 17 19 have been developed that roughly correspond to data definition languages these allow one to constrain the structure of xml data by imposing a schema on it these and other proposals also advocate the need for integrity constraints another form of constraints that should for example be capable of expressing inclusion constraints and inverse relationships the latter have recently been studied as path constraints in the context of semistructured data 4 9 it is likely that future xml proposals will involve both forms of constraints and it is therefore appropriate to understand the interaction between them this paper investigates that interaction in particular it studies constraint implication problems which are important both i inductive bias in case based reasoning systems in order to learn more about the behaviour of case based reasoners as learning systems we formalise a simple case based learner as a pac learning algorithm using the case based representation hcb oei we first consider a naive case based learning algorithm cb1 oeh which learns by collecting all available cases into the case base and which calculates similarity by counting the number of features on which two problem descriptions agree we present results concerning the consistency of this learning algorithm and give some partial results regarding its sample complexity we are able to characterise cb1 oeh as a weak but general learning algorithm we then consider how the sample complexity of case based learning can be reduced for specific classes of target concept by the application of inductive bias or prior knowledge of the class of target concepts following recent work demonstrating how case based learning can be improved by choosing a similarity measure appropriate to t an ejection chain approach for the generalized assignment problem this paper we propose an ejection chain approach under the framework of tabu search ts for the generalized assignment problem gap which is known to be np hard sahni and gonzalez 1976 gap seeks a minimum cost assignment of n jobs to m agents subject to a resource constraint for each agent among various heuristic algorithms developed for gap are a combination of the greedy method and local search by martello and toth 1981 1990 a tabu search and simulated annealing approach by osman 1995 a genetic algorithm by chu and beasley 1997 vds methods by amini and racer 1995 and racer and amini 1994 a tabu search approach by laguna et al 1995 which is proposed for a generalization of gap a set partitioning heuristic by cattrysse et al 1994 a relaxation heuristic by lorena and narciso 1996 a grasp and max min ant system combined with local search and tabu search by lourenco and serra 1998 a linear relaxation heuristic by trick 1992 and so on many exact algorithms have also been proposed e g nauss 2003 savelsbergh 1997 a simpler version of an ejection chain approach has also been proposed for the gap in laguna et al 1995 our ejection chain is based on the idea described in glover 1997 detection of heterogeneities in a multiple text database environment as the number of text retrieval systems search engines grows rapidly on the world wide web there is an increasing need to build search brokers metasearch engines on top of them often the task of building an effective and efficient metasearch engine is hindered by the heterogeneities among the underlying local search engines in this paper we first analyze the impact of various heterogeneities on building a metasearch engine we then present some techniques that can be used to detect the most prominent heterogeneities among multiple search engines applications of utilizing the detected heterogeneities in building better metasearch engines will be provided view disassembly we explore a new form of view rewrite called view disassembly the objective is to rewrite views in order to remove certain sub views or unfoldings of the view this becomes pertinent for complex views which may defined over other views and which may involve union such complex views arise necessarily in environments as data warehousing and mediation over heterogeneous databases view disassembly can be used for view and query optimization preserving data security making use of cached queries and materialized views and view maintenance we provide computational complexity results of view disassembly we show that the optimal rewrites for disassembled views is at least np hard however we provide good news too we provide an approximation algorithm that has much better run time behavior we show a pertinent class of unfoldings for which their removal always results in a simpler disassembled view than the view itself we also show the complexity to determine when a collection state based shoslif for indoor visual navigation in this paper we investigate vision based navigation using the self organizing hierarchical optimal subspace learning and inference framework shoslif that incorporates states and a visual attention mechanism the problem is formulated as an observation driven markov model odmm which is realized through recursive partitioning regression a stochastic recursive partition tree srpt which maps an preprocessed current input raw image and the previous state into the current state and the next control signal is used for efficient recursive partitioning regression the srpt learns incrementally each learning sample is learned or rejected onthe fly the proposed scheme has been successfully applied to indoor navigation keywords vision based navigation incremental learning eigen subspace method content based retrieval observation driven markov model nearest neighbor regression 1 1 introduction much progress has been made in autonomous navigation of mobile robots both ind improving the search on the internet by using wordnet and lexical operators a vast amount of information is available on the internet and naturally many information gathering tools have been developed search engines with dijerent characteristics such as alta vista lycos infoseek and others are available however there are inherent difficulties associated with the task of retrieving information on the internet 1 the web information is diverse and highly unstructured 2 the size of information is large and it grows at an exponential rate while these two issues are profound and require long term solutions still it is possible to develop software around the search engines to improve the quality of the information retrieved in this paper we present a natural language interface system to a search engine the search improvement achieved by our system is based on 1 a query extension using wordnet and 2 the use of new lexical operators that replace the classical boolean opera tors used by current search engines several tests have been performed using the tipster topics collection provided at the 6lb text retrieval conference trec 6 the results obtained are presented and discussed a bayesian computer vision system for modeling human interactions abstract we describe a real time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task 1 the system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction examples of interesting interaction behaviors include following another person altering one s path to meet another and so forth our system combines top down with bottom up information in a closed feedback loop with both components employing a statistical bayesian approach 2 we propose and compare two different state based learning architectures namely hmms and chmms for modeling behaviors and interactions the chmm model is shown to work much more efficiently and accurately finally to deal with the problem of limited training data a synthetic alife style training system is used to develop flexible prior models for recognizing human interactions we demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training index terms visual surveillance people detection tracking human behavior recognition hidden markov models 1 text classification from labeled and unlabeled documents using em this paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents this is important because in many text classification problems obtaining training labels is expensive while large quantities of unlabeled documents are readily available we introduce an algorithm for learning from labeled and unlabeled documents based on the combination of expectation maximization em and a naive bayes classifier the algorithm first trains a classifier using the available labeled documents and probabilistically labels the unlabeled documents it then trains a new classifier using the labels for all the documents and iterates to convergence this basic em procedure works well when the data conform to the generative assumptions of the model however these assumptions are often violated in practice and poor performance can result we present two extensions to the algorithm that improve classification accuracy under these conditions 1 a weighting factor to modulate the contribution of the unlabeled data and 2 the use of multiple mixture components per class experimental results obtained using text from three different real world tasks show that the use of unlabeled data reduces classification error by up to 30 machine learning based user modeling for www search the world wide web www offers a huge number of documents which deal with information concerning nearly any topic thus search engines and meta search engines currently are the key to finding information search engines with crawler based indexes vary in recall and offer a very bad precision meta search engines try to overcome these lacks by simple methods for information extraction information filtering and integration of heterogenous information resources only few search engines employ intelligent techniques in order to increase precision recently user modeling techniques have become more and more popular since they proved to be a useful means for user centered information filtering and presentation many personal agent based system for web browsing are currently developed it is a straightforward idea to incorporate the idea of machine learning based user modeling see 17 methods into web search services we propose an abstract prototype which is being developed at the uni the complexity of revising logic programs a rule based program will return a set of answers to each query an impure program which includes the prolog cut and not delta operators can return different answers if its rules are re ordered there are also many reasoning systems that return only the first answer found for each query these first answers too depend on the rule order even in pure rule based systems a theory revision algorithm seeking a revised rule base whose expected accuracy over the distribution of queries is optimal should therefore consider modifying the order of the rules this paper first shows that a polynomial number of training labeled queries each a query paired with its correct answer provides the distribution information necessary to identify the optimal ordering it then proves however that the task of determining which ordering is optimal once given this distributional information is intractable even in trivial situations e g even if each query is an atomic literal we are learning to classify text from labeled and unlabeled documents in many important text classification problems acquiring class labels for training documents is costly while gathering large quantities of unlabeled data is cheap this paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents we present a theoretical argument showing that under common assumptions unlabeled data contain information about the target function we then introduce an algorithm for learning from labeled and unlabeled text based on the combination of expectation maximization with a naive bayes classifier the algorithm first trains a classifier using the available labeled documents and probabilistically labels the unlabeled documents it then trains a new classifier using the labels for all the documents and iterates to convergence experimental results obtained using text from three different realworld tasks show that the use of unlabeled data reduces classification error by up to 33 intelligent information access in the web ml based user modeling for high precision meta search it is a well known fact that high precision search for documents concerning a certain topic in the world wide web www is a tough problem index based search engines vary in recall with a coverage of at most 30 of the web and offer a very bad precision by simple keyword search meta search engines provide a specialised monolithic architecture for information extraction and integration of heterogenous information resources which ensures a bigger recall and may yield a better precision few search engines as e g huskysearch employ intelligent techniques in order to increase precision on the other hand many personalized agents for web browsing are currently developed it is a straightforward idea to incorporate the idea of user modeling with machine learning ml methods into web search services we propose an abstract prototype oyster which makes use of machine learning based user modeling and which for reasons of robustness performance and quality is realised as a mult tempos a temporal database model seamlessly extending odmg this paper presents tempos a set of models and languages intended to seamlessly extend the odmg object database standard with temporal functionalities the proposed models exploit object oriented technology to meet some important yet traditionally neglected design criteria related to legacy code migration and representation independence tempos has been fully formalized both at the syntactical and the semantical level and implemented on top of the o 2 dbms its suitability in regard to applications requirements has been validated through concrete case studies from various contexts keywords temporal databases temporal data models temporal query languages time representation upward compatibility object oriented databases odmg r esum e ce document pr esente tempos un ensemble de mod eles et de langages con cus pour etendre le standard pour bases de donn ees a objets odmg par des fonctionnalit es temporelles les mod eles d ecrits exploitent les possibilit es de la tech autonomy based multi agent systems statistical issues this paper describes an autonomy based multiagent system and its application to simulations in the framework of collective robotics experimental results measured for a particular task namely object regrouping have two outcomes first they show that a form of implicit cooperation takes place in the system despite the absence of explicit cooperation protocol but just in virtue of the design of the system second they indicate that it is possible to effect the way the task is achieved by appropriately introducing obstacles that modify the environment of the agents a preliminary partial characterization of the concept of emergence is proposed and its illustration to our experimental system is given introduction our work fits in the framework of bottom up artificial intelligence brooks 1986 brooks 1991 and more particularly in that of autonomous agents pfeifer 1995 we are concerned with collective phenomena and their issues and more precisely the way to carry out solution a representation independent temporal extension of odmg s object query language tempos is a set of models providing a framework for extending database systems with temporal functionalities based on this framework an extension of the odmg s object database standard has been defined this extension includes a hierarchy of abstract datatypes for managing temporal values and histories as well as temporal extensions of odmg s object model schema definition language and query language this paper focuses on the latter namely tempoql with respect to related proposals the main originality of tempoql is that it allows to manipulate histories regardless of their representations by composition of functional constructs thereby the abstraction principle of object orientation is fulfilled and the functional nature of oql is enforced in fact tempoql goes further in preserving oql s structure by generalizing most standard oql constructs to deal with histories the overall proposal has been fully formalized both at the syntactical and the semantical level and impleme composition of services with mobile code mobile code is slowly gaining acceptance but it is still not clear where it is really useful if not used judiciously it may incur greater complexity of programming and degradation of performances the goal of this paper is to show that mobile code is particularly well suited as a glue for the composition of immobile services where flexibility and extensibility are necessary to support our claim we describe two services and one application that have been programmed with mobile code in the context of active networking we study the impact on the flexibility complexity and performances of the resulting systems we observe positive effects on flexibility and complexity and acceptable performance penalties 1 introduction the main idea of active networking is to make mobile code a core functionality of the network traditionally network elements like routers only transport packets and do not try to decode or modify the enclosed payload in an active network the nodes can perform cus collaborative maintenance this paper we examine a classical ai problem knowledge maintenance and propose an innovative solution collaborative maintenance that has been inspired by the recommendation technique of wrapper induction efficiency and expressiveness extended abstract recently many systems have been built that automatically interact with internet information resources however these resources are usually formatted for use by people e g the relevant content is embedded in html pages wrappers are often used to extract a resource s content but hand coding wrappers is tedious and error prone we advocate wrapper induction a technique for automatically constructing wrappers we have identified several wrapper classes that can be learned quickly most sites require only a handful of examples consuming a few cpu seconds of processing yet which are useful for handling numerous internet resources 70 of surveyed sites can be handled by our techniques introduction the internet presents a stunning variety of on line information resources telephone directories retail product catalogs weather forecasts and many more recently there has been much interest in systems such as software agents etzioni weld 1994 kwok weld 1996 or informati hierarchical reinforcement learning with the maxq value function decomposition this paper presents a new approach to hierarchical reinforcement learning based on decomposing the target markov decision process mdp into a hierarchy ofsmaller mdps and decomposing the value function of the target mdp into an additive combination of the value functions of the smaller mdps the decomposition known as the maxq decomposition has both a procedural semantics as a subroutine hierarchy and a declarative semantics as a representation of the value function of a hierarchical policy maxq uni es and extends previous work on hierarchical reinforcement learning by singh kaelbling and dayan and hinton it is based on the assumption that the programmer can identify useful subgoals and de ne subtasks that achieve these subgoals by de ning such subgoals the programmer constrains the set of policies that need to be considered during reinforcement learning the maxq value function decomposition can represent the value function of any policy that is consistent with the given hierarchy the decomposition also creates opportunities to exploit state abstractions so that individual mdps within the hierarchy can ignore large parts of the state space this is important for the practical application of the evolutionary design and multi objective optimisation in this paper we explore established methods for optimising multi objective functions whilst addressing the problem of preliminary design methods from the literature are investigated and new ones introduced all methods are evaluated within a collaborative project for whole system airframe design and the basic problems and difficulties of preliminary design methodology are discussed cvetkovic parmee and webb 1998 our genetic algorithm is expanded to integrate different methods for optimising multi objective functions all presented methods are also analysed in the context of whole system design discussing their advantages and disadvantages the problem of qualitative versus quantitative characterisation of relative importance of objectives such as objective a is much more important then objective b in multi objective optimisation framework is also addressed and some relationships with fuzzy preferences fodor and roubens 1994 and preference ordering established several solving stabilization problems in case based knowledge acquisition case based reasoning is widely deemed an important methodology towards alleviating the bottleneck of knowledge acquisition the key idea is to collect cases representing a human s or a system s experience directly rather than trying to construct generalizations episodic knowledge accumulated this way may be used flexibly for different purposes by determining similarities between formerly solved problems and current situations under investigation but the flexibility of case based reasoning brings with it a number of disadvantages one crucial difficulty is that every new experience might seem worth to be memorized as a result a case based reasoning system may substantially suffer from collecting a huge amount of garbage without being able to separate the chaff from the wheat this paper presents a case study in case based learning some target concept has to be learned by collecting cases and tuning similarity concepts it is extremely difficult to avoid collecting a huge amount of maintaining temporal views over non temporal information sources for data warehousing an important use of data warehousing is to provide temporal views over the history of source data that may itself be non temporal while recent work in view maintenance is applicable to data warehousing only non temporal views have been considered in this paper we introduce a framework for maintaining temporal views over non temporal information sources in a data warehousing environment we describe an architecture for the temporal data warehouse that automatically maintains temporal views over non temporal source relations and allows users to ask temporal queries using these views because of the dimension of time a materialized temporal view may need to be updated not only when source relations change but also as time advances we present incremental techniques to maintain temporal views for both cases and outline the implementation of our approach in the whips warehousing prototype at stanford 1 introduction a data warehouse is a repository for efficient querying ontobroker how to enable intelligent access to the www the world wide web www is currently one of the most important electronic information sources however its query interfaces and the provided reasoning services are rather limited ontobroker consists of a number of languages and tools that enhance query access and inference service in the www it provides languages to annotate web documents with ontological information to represent ontologies and to formulate queries the tool set of ontobroker allows us to access information and knowledge from the web and to infer new knowledge with an inference engine based on techniques from logic programming this article provides several examples that illustrate these languages and tools and the kind of service that is provided we also discuss the bottlenecks of our approach that stem from the fact that the applicability of ontobroker requires two time consuming activities 1 developing shared ontologies that reflect the consensus of a group of web users and 2 annotating we a social semantics for agent communication languages the ability to communicate is one of the salient properties of agents although a number of agent communication languages acls have been developed obtaining a suitable formal semantics for acls remains one of the greatest challenges of multiagent systems theory previous semantics have largely been mentalistic in their orientation and are based solely on the beliefs and intentions of the participating agents such semantics are not suitable for most multiagent applications which involve autonomous and heterogeneous agents whose beliefs and intentions cannot be uniformly determined accordingly we present a social semantics for acls that gives primacy to the interactions among the agents our semantics is based on social commitments and is developed in temporal logic this semantics because of its public orientation is essential to providing a rigorous basis for multiagent protocols 1 introduction interaction among agents is the distinguishing property of multia a semantics of contrast and information structure for specifying intonation in spoken language generation in this dissertation i present a model for the determination of intonation contours from context and provide two implemented systems which apply this theory to the problem of generating spoken language with appropriate intonation from high level semantic representations the theory and implementations presented here are based on an information structure framework that mediates between intonation and discourse and encodes the proper level of semantic information to account for both contextually bound accentuation patterns and intonational phrasing the structural similarities among these linguistic levels of representation are the basis for selecting combinatory categorial grammar ccg steedman 1985 1990a as the model for spoken language production this model licenses congruent syntactic prosodic and information structural constituents and consequently represents a simpli cation over models of prosody developed in syntactically more traditional frameworks the previous mention heu query optimization in kess an ontology based kbms this paper presents an approach for the implementation of query optimization techniques in kess the knowledge enhanced sql server kess is a knowledge database management system kbms that uses a semantic ontology based data model we have classified our query optimization techniques in two different categories 1 semantic based and 2 data access path related these techniques use a compiler optimization approach to simplify query predicates and use caching to optimize memory hierarchy performance for the evaluation of ontology related predicates this work also presents the results of the implementation of such optimizations in kess in the form of a performance analysis 1 introduction this paper presents an approach for the implementation of query optimization techniques in kess the knowledge enhanced sql server its main novel feature when compared to conventional sql servers is the capability to store knowledge and to use it in a manner that is analogous to deducti evolution of a central pattern generator for the swimming and trotting gaits of the salamander this paper presents the development of neural controllers for the swimming and the trotting of a salamanderlike animat using a genetic algorithm ga we extend a connectionist model of the central pattern generator cpg controlling the swimming of a lamprey 1 to control the locomotion of a 2d mechanical simulation of a salamander we study in particular what kind of neural connectivity can produce the traveling undulation of the trunk during swimming and the standing s wave undulation during trotting using a ga we evolve neural controllers which are made of two segmental oscillators controlling the fore and hindlimb muscles which project to a lamprey like cpg for the trunk muscles cpgs are successfully evolved which exhibit the swimming gait when external excitatory drive is applied only to the trunk cpg and the trotting gait when external drive is applied to both the limb and the trunk cpgs for both types of gaits the speed of locomotion can be varied with the amplitude o incremental maintenance for materialized views over semistructured data semistructured data is not strictly typed like relational or object oriented data and may be irregular or incomplete it often arises in practice e g when heterogeneous data sources are integrated or data is taken from the world wide web views over semistructured data can be used to filter the data and to restructure or provide structure to it to achieve fast query response time these views are often materialized this paper studies incremental maintenance techniques for materialized views over semistructured data we use the graph based data model oem and the query language lorel developed at stanford as the framework for our work we propose a new algorithm that produces a set of queries that compute the changes to the view based upon a change to the source we develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view we show that for nearly all types of database updates it is more efficient to a digital libraries and autonomous citation indexing the world wide web is revolutionizing the way that researchers access scientific information articles are increasingly being made available on the homepages of authors or institutions at journal web sites or in online archives however scientific information on the web is largely disorganized this article introduces the creation of digital libraries incorporating autonomous citation indexing aci aci autonomously creates citation indices similar to the science citation index r an aci system autonomously locates articles extracts citations identifies identical citations that occur in different formats and identifies the context of citations in the body of articles aci can organize the literature and provide most of the advantages of traditional citation indices such as literature search using citation links and the evaluation of articles based on citation statistics furthermore aci can provide significant advantages over traditional citation indices no manual effort is required for indexing which should result in a reduction in cost and an increase in the availability of citation indices an aci system can also provide more comprehensive and up to date indices of the literature by indexing articles on the web technical reports conference papers etc furthermore aci makes it easy to browse the context of citations to given articles allowing researchers to quickly and easily see what subsequent researchers have said about a given article digital libraries incorporating aci may significantly improve scientific dissemination and feedback methodologies for pvc configuration in heterogeneous atm environments using intelligent mobile agents traditionally the functionality enabling the configuration of permanent virtual connections pvcs in heterogeneous asynchronous transfer mode atm environments has been accomplished through the use of client server technologies however this approach suffers from a number of problems in the areas of flexibility extensibility and efficiency and as networks become increasingly complex and dynamic the need for a new paradigm has become apparent this paper is concerned with the analysis of a proposed solution wherein intelligent mobile agents are utilized to provide autonomous network configuration management functionality configuration methodologies based on a single mobile agent acting serially and multiple mobile agents acting in parallel are compared in terms of time duration and network load the use of a small expert system to provide the intelligence needed to handle error situations is investigated a test environment composed of three simulated atm switch nodes is develop autonomous evolution of gaits with the sony quadruped robot a trend in robotics is towards legged robots one of the issues with legged robots is the development of gaits typically gaits are developed manually in this paper we report our results of autonomous evolution of dynamic gaits for the sony quadruped robot fitness is determined using the robot s digital camera and infrared sensors using this system we evolve faster dynamic gaits than previously manually developed 1 introduction in this paper we present an implementation of an autonomous evolutionary algorithm ea for developing locomotion gaits all processing is handled by the robot s onboard computer and individuals are evaluated using the robot s sensors our implementation successfully evolves trot and pace gaits for our robot for which the pace gait significantly outperforms previous hand developed gaits in addition to achieving our desired goal of automatically developing gaits these results show that eas can be used on real robots to evolve non trivial behaviors a method results on reasoning about updates in transaction logic transaction logic was designed as a general logic of state change for deductive databases and logic programs it has a model theory a proof theory and its horn subset can be given a procedural interpretation previous work has demonstrated that the combination of declarative semantics and procedural interpretation turns the horn subset of transaction logic into a powerful language for logic programming with updates bk98 bk94 bk93 bk95 in this paper we focus not on the horn subset but on the full logic and we explore its potential as a formalism for reasoning about logic programs with updates we first develop a methodology for specifying properties of such programs and then provide a sound inference system for reasoning about them and conjecture a completeness result finally we illustrate the power of the inference system through a series of examples of increasing difficulty 1 introduction updates are a crucial component of any database programming language even the si heterogeneous active agents iii polynomially implementable agents in eiter subrahmanian and pick 1999 the authors have introduced techniques to build agents on top of arbitrary data structures and to agentize new existing programs they provided a series of successively more sophisticated semantics for such agent systems and showed that as these semantics become epistemically more desirable a computational price may need to be paid in this paper we identify a class of agents that are called weak regular this is done by first identifying a fragment of agent programs eiter subrahmanian and pick 1999 called weak regular agent programs wraps for short it is shown that wraps are definable via three parameters checking for a property called safety checking for a property called conflict freedom and checking for a deontic stratifiability property algorithms for each of these are developed a weak regular agent is then defined in terms of these concepts and a regular agent is one that satisfies an additional boundedness interaction between path and type constraints xml 7 which is emerging as an important standard for data exchange on the world wide web highlights the importance of semistructured data although the xml standard itself does not require any schema or type system a number of proposals 6 17 19 have been developed that roughly correspond to data definition languages these allow one to constrain the structure of xml data by imposing a schema on it these and other proposals also advocate the need for integrity constraints another form of constraints that should for example be capable of expressing inclusion constraints and inverse relationships the latter have recently been studied as path constraints in the context of semistructured data 4 9 it is likely that future xml proposals will involve both forms of constraints and it is therefore appropriate to understand the interaction between them this paper investigates that interaction in particular it studies constraint implication problems which are important both i query optimization for semistructured data using path constraints in a deterministic data model path constraints have been studied for semistructured data modeled as a rooted edge labeled directed graph 4 11 13 in this model the implication problems associated with many natural path constraints are undecidable 11 13 a variant of the graph model called the deterministic data model was recently proposed in 10 in this model data is represented as a graph with deterministic edge relations i e the edges emanating from any node in the graph have distinct labels this model is more appropriate for representing e g acedb 27 databases and web sites this paper investigates path constraints for the deterministic data model it demonstrates the application of path constraints to among others query optimization three classes of path constraints are considered the language pc introduced in 11 an extension of pc denoted by p w c by including wildcards in path expressions and a generalization of p w c denoted by p c by representing pa the cooperative problem solving process we present a model of cooperative problem solving that describes the process from its beginning with some agent recognizing the potential for cooperation with respect to one of its goals through to team action our approach is to characterize the mental states of the agents that lead them to solicit and take part in cooperative action the model is formalized by expressing it as a theory in a quantified multi modal logic keywords multi agent systems cooperation modal logic temporal logic 1 introduction agents both human and artificial can engage in many and varied types of social interaction ranging from altruistic cooperation through to open conflict however perhaps the paradigm example of social interaction is cooperative problem solving cps in which a group of autonomous agents choose to work together to achieve a common goal for example we might find a group of people working together to move a heavy object play a symphony build a house or write a jo developments in spatio temporal query languages in contrast to the field view of spatial data that basically views spatial data as a mapping from points into some features the object view clusters points by features and their values into spatial objects of type point line or region when embedding these objects into a data model such as the relational model an additional clustering according to conceptually identified objects takes place for example we could define a relation city name string center point area region that combines different features for cities in one relation an important aspect of this kind of modeling is that clustering happens on two different levels i points are grouped into spatial objects like regions and ii different attributes features are grouped into a perceived object when talking about data modeling there is no reason why this grouping should be limited to two levels for example we can consider storing regions of different population densities for each city in an attribute density num region although then the relation is not in first normal form anymore we can recover the first normal form by encapsulating the function num region in an abstract data type the important aspect is that all the required operations on such a type as well as on regions and other complex types can be defined to a large degree independently from the data model 1 the most important point about the preceding discussion is the way in which complex types can be easily the cmunited 97 robotic soccer team perception and multiagent control robotic soccer is a challenging research domain which involves multiple agents that need to collaborate in an adversarial environment to achieve specificobjectives in this paper we describe cmunited the team of small robotic agents that we developed to enter the robocup 97 competition we designed and built the robotic agents devised the appropriate vision algorithm and developed and implemented algorithms for strategic collaboration between the robots in an uncertain and dynamic environment the robots can organize themselves in formations hold specificroles and pursue their goals in game situations they have demonstrated their collaborative behaviors on multiple occasions the robots can also switch roles to maximize the overall performance of the team we present an overview of the vision processing algorithm which successfully tracks multiple moving objects and predicts trajectories the paper then focusses on the agent behaviors ranging from low level individual behaviors to coordinated strategic team behaviors context filters for document based information filtering in this paper we propose a keyphrase sense disambiguation methodology called context filters for use in keyphrase based information filtering systems a context filter finds whether an input keyphrase has occurred in the required context context filters consider various factors of ambiguity some of these factors are special to information filtering and they are handled in a structured fashion the proposed context filters are very comprehensibile context filters consider varieties of contexts which are not considered in existing word sense disambiguation methods but these are all needed for information filtering the ideas on context filters that we report in this paper form important elements of an instructible information filtering agent that we are developing 1 introduction information filtering is the process of separating out irrelevant documents from relevant ones its importance has motivated several researchers to develop software agents such as sift infoscan iagent face detection using mixtures of linear subspaces we present two methods using mixtures of linear subspaces for face detection in gray level images one method uses a mixture of factor analyzers to concurrently perform clustering and within each cluster perform local dimensionality reduction the parameters of the mixture model are estimated using an em algorithm a face is detected if the probability of an input sample is above a predefined threshold the other mixture of subspaces method uses kohonen s self organizing map for clustering and fisher linear discriminant to find the optimal projection for pattern classification and a gaussian distribution to model the class conditional density function of the projected samples for each class the parameters of the class conditional density functions are maximum likelihood estimates and the decision rule is also based on maximum likelihood a wide range of face images including ones in different poses with different expressions and under different lighting conditions are used as the training set to capture the variations of human faces our methods have been tested on three sets of 225 images which contain 871 faces experimental results on the first two datasets show that our methods perform as well as the best methods in the literature yet have fewer false detects 1 supporting trust in virtual communities at any given time the stability of a community depends on the right balance of trust and distrust trust is also the basis of economic activities making such things as credit agreements business contracts and customer confidence possible in real life we are faced with increasingly complex decisions due to the increasingly daunting range of options provided by the internet and uncertainty in the credibility of virtual entities in short information overload increased uncertainty and risk taking a prominent feature of modern living we as members of society cope with these complexities and uncertainties by relying on a vital social phenomenon trust which forms the basis of all social interaction however the ability to reason about trust is absent from the virtual medium making virtual communities of real and artificial agents fragile this is unsatisfactory as all virtual interactions are ultimately human bound therefore we need a trust model to allow artificial agents to an efficient boosting algorithm for combining preferences we study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions this problem of combining preferences arises in several applications such as that of combining the results of different search engines or the collaborativefiltering problem of ranking movies for a user based on the movie rankings provided by other users in this work we begin by presenting a formal framework for this general problem we then describe and analyze an efficient algorithm called rankboost for combining preferences based on the boosting approach to machine learning we give theoretical results describing the algorithm s behavior both on the training data and on new test data not seen during training we also describe an efficient implementation of the algorithm for a particular restricted but common case we next discuss two experiments we carried out to assess the performance of rankboost in the first experiment we used the algorithm to combine different web search strategies each of which is a query expansion for a given domain the second experiment is a collaborative filtering task for making movie recommendations learning and evaluating visual features for pose estimation we present a method for learning a set of visual landmarks which are useful for pose estimation the landmark learning mechanism is designed to be applicable to a wide range of environments and generalized for different approaches to computing a pose estimate initially each landmark is detected as a local extremum of a measure of distinctiveness and represented by a principal components encoding which is exploited for matching attributes of the observed landmarks can be parameterized using a generic parameterization method and then evaluated in terms of their utility for pose estimation we present experimental evidence that demonstrates the utility of the method 1 introduction in this paper we develop an approach to visionbased robot localization by learning a set of imagedomain landmarks in the robot s environment the landmarks are learned from a representative set of images obtained during an initial exploration of the environment no a priori assumptions are made about the advances in large margin classifiers introduction to large margin classiers alexander j smola gmd first rudower chaussee 5 12489 berlin germany smola rst gmd de http www rst gmd de smola peter bartlett australian national university rsise canberra act 0200 australia peter bartlett anu edu au http keating anu edu au people bartlett bernhard sch olkopf gmd first rudower chaussee 5 12489 berlin germany bs rst gmd de http www rst gmd de bs dale schuurmans department of computer science unversity of waterloo waterloo ontario n2l 3g1 canada dale cs uwaterloo ca http www cs uwaterloo ca dale the aim of this chapter is to provide a brief introduction to the basic concepts of large margin classiers for readers unfamiliar with the topic moreover it is aimed at establishing a common basis in terms of notation and equations upon which the subsequent chapters will bu distance browsing in spatial databases two different techniques of browsing through a collection of spatial objects stored in an r tree spatial data structure on the basis of their distances from an arbitrary spatial query object are compared the conventional approach is one that makes use of a k nearest neighbor algorithm where k is known prior to the invocation of the algorithm thus if m kneighbors are needed the k nearest neighbor algorithm needs to be reinvoked for m neighbors thereby possibly performing some redundant computations the second approach is incremental in the sense that having obtained the k nearest neighbors the k 1 st neighbor can be obtained without having to calculate the k 1nearest neighbors from scratch the incremental approach finds use when processing complex queries where one of the conditions involves spatial proximity e g the nearest city to chicago with population greater than a million in which case a query engine can make use of a pipelined strategy a general incremental nearest neighbor algorithm is presented that is applicable to a large class of hierarchical spatial data structures this algorithm is adapted to the r tree and its performance is compared to an existing k nearest neighbor algorithm for r trees 45 experiments show that the incremental nearest neighbor algorithm significantly outperforms the k nearest neighbor algorithm for distance browsing queries in a spatial database that uses the r tree as a spatial index moreover the incremental nearest neighbor algorithm also usually outperforms the k nearest neighbor algorithm when applied to the k nearest neighbor problem for the r tree although the improvement is not nearly as large as for distance browsing queries in fact we prove informally that at any step in its execution the incremental towards flexible teamwork in persistent teams extended report teamwork is a critical capability in multi agent environments many such environments mandate that the agents and agent teams must be persistent i e exist over long periods of time agents in such persistent teams are bound together by their long term common interests and goals this paper focuses on flexible teamwork in such persistent teams unfortunately while previous work has investigated flexible teamwork persistent teams remain unexplored for flexible tamwork one promising approach that has emerged is model based i e providing agents with general models of teamwork that explicitly specify their commitments in teamwork such models enable agents to autonomously reason about coordination unfortunately for persistent teams such models may lead to coordination and communication actions that while locally optimal are highly problematic for the team s long term goals we present a decisiontheoretic technique to enable persistent teams to overcome such limitations of the m using digital but physical surrogates to mediate awareness communication and privacy in media spaces digital but physical surrogates are tangible representations of remote people typically members of small intimate teams positioned within an office and under digital control surrogates selectively collect and present awareness information about the people they represent they also react to people s explicit and implicit physical actions a person s explicit acts include grasping and moving them while implicit acts include one s proximity to the surrogate by responding appropriately to these physical actions of people surrogates can control the communication capabilities of a media space in a natural way this enables the smooth transition from awareness to casual interaction while mitigating concerns about privacy keywords ubiquitous media spaces awareness casual interaction groupware cscw 1 introduction digital but physical surrogates are tangible representations of remote people typically members of small intimate teams positioned within a person s environment as the global dimensionality of face space low dimensional representations of sensory signals are key to solving many of the computational problems encountered in high level vision principal component analysis pca has been used in the past to derive such compact representations for the object class of human faces here with an interpretation of pca as a probabilistic model we employ two objective criteria to study its generalization properties in the context of large frontal pose face databases we find that the eigenfaces the eigenspectrum and the generalization depend strongly on the ensemble composition and size with statistics for populations as large as 5500 still not stationary further the assumption of mirror symmetry of the ensemble improves the quality of the results substantially in the low statistics regime and is also essential in the high statistics regime we employ a perceptual criterion and argue that even with large statistics the dimensionality of the pca subspace necessary for adequate represent coordinating mutually exclusive resources using gpgp hospital patient scheduling is an inherently distributed problem because of the way real hospitals are organized as medical procedures have become more complex and their associated tests and treatments have become interrelated the current ad hoc patient scheduling solutions have been observed to break down we propose a multi agent solution using the generalized partial global planning gpgp approach that preserves the existing human organization and authority structures while providing better system level performance increased hospital unit throughput and decreased patient stay time to do this we extend gpgp with a new coordination mechanism to handle mutually exclusive resource relationships like the other gpgp mechanisms the new mechanism can be applied to any problem with the appropriate resource relationship we evaluate this new mechanism in the context of the hospital patient scheduling problem and examine the effect of increasing interrelations between tasks performed agent oriented software engineering agent oriented techniques represent an exciting new means of analysing designing and building complex software systems they have the potential to significantly improve current practice in software engineering and to extend the range of applications that can feasibly be tackled yet to date there have been few serious attempts to cast agent systems as a software engineering paradigm this paper seeks to rectify this omission specifically it will be argued that i the conceptual apparatus of agent oriented systems is well suited to building software solutions for complex systems and ii agent oriented approaches represent a genuine advance over the current state of the art for engineering complex systems following on from this view the major issues raised by adopting an agent oriented approach to software engineering are highlighted and discussed single display groupware exploring computer support for co located collaboration this panel will explore an interaction paradigm for colocated computer based collaboration we term single display groupware sdg sdg is a class of applications that support multiple simultaneous users interacting in the same room on a single shared display with multiple inputdevices sdg are being used in various applications in the educational entertainment and research communities but many issues remain to be explored keywords computer supported cooperative work cscw computersupported collaborative learning cscl computersupported collaborative entertainment csce multiple input devices introduction single display groupware sdg is a class of applications that support multiple simultaneous users interacting in a colocated environment on a single shared display with multiple input devices 4 sdg allows users to interact more naturally and comfortably around the computer such applications take advantage of our human ability to interact and communicate in a face to combining positional information with visual media by integrating visual media with positioning information obtained with our wearable computer we create new opportunities for using visuals both in the field and at the workstation the position information we store with each visual is direction pitch roll location focal length and zoom this information allows any system to reconstruct the frustum of the visual and if height data is available to reconstruct which parts of the earth are visible in the visual this enables position based lookup and 3d mosaicing of visuals to reconstruct a 3d model 1 introduction we are interested in the association of media files with contextual information gathered by our wearable computer we have previously explored the combination of audio notes or reminders with locations 1 in this paper we turn our attention to visuals classic photographs and videos store only visual information recent advances in photo and video technology have increased the amount of information which can be st consistent query answers in inconsistent databases in this paper we consider the problem of the logical characterization of the notion of consistent answer in a relational database that may violate given integrity constraints this notion is captured in terms of the possible repaired versions of the database a method for computing consistent answers is given and its soundness and completeness for some classes of constraints and queries proved the method is based on an iterative procedure whose termination for several classes of constraints is proved as well 1 introduction integrity constraints capture an important normative aspect of every database application however it is often the case that their satisfaction cannot be guaranteed allowing for the existence of inconsistent database instances in that case it is important to know which query answers are consistent with the integrity constraints and which are not in this paper we provide a logical characterization of consistent query answers in relational databases that may agents for process coherence in virtual enterprises socom 1 on time abstract socom 2 cheap buyer seller abstract socom 3 buyer seller socom manager hoosier inc register me as buyer and seller register me as buyer and seller play seller in abstractsocom 1 yes valvano co hot air bros 8 9 concrete socom created 4 6 7 high quality roles agents directory agent id role derived 1 figure 2 instantiation of a concrete socom 68 march 1999 vol 42 no 3 communications of the acm adopt role need to initiate ask socom manager participate no no no no no no no ye s yes ye s yes yes yes ye s register socom manager suggests a socom request to create socom process request stop stop undefined stop failure instantiate and announce receipt of a request request to register condition evaluation ok find candidates ask candidates all say yes agents decision making socom manager s decision making because our agents are autonomous we must e approximating the non dominated front using the pareto archived evolution strategy we introduce a simple evolution scheme for multiobjective optimization problems called the pareto archived evolution strategy paes we argue that paes may represent the simplest possible nontrivial algorithm capable of generating diverse solutions in the pareto optimal set the algorithm in its simplest form is a 1 1 evolution strategy employing local search but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors 1 1 paes is intended to be a baseline approach against which more involved methods may be compared it may also serve well in some real world applications when local search seems superior to or competitive with population based methods we introduce 1 and variants of paes as extensions to the basic algorithm six variants of paes are compared to variants of the niched pareto genetic algorithm and the nondominated sorting genetic algorithm over a diverse suite of six test functions results are analyzed and presented using techniques that reduce the attainment surfaces generated from several optimization runs into a set of univariate distributions this allows standard statistical analysis to be carried out for comparative purposes our results provide strong evidence that paes performs consistently well on a range of multiobjective optimization tasks design to criteria scheduling real time agent control design to criteria builds custom schedules for agents that meet hard temporal constraints hard resource constraints and soft constraints stemming from soft task interactions or soft commitments made with other agents design to criteria is designed specifically for online application it copes with exponential combinatorics to produce these custom schedules in a resource bounded fashion this enables agents to respond to changes in problem solving or the environment as they arise introduction complex autonomous agents operating in open dynamic environments must be able to address deadlines and resource limitations in their problem solving this is partly due to characteristics of the environment and partly due to the complexity of the applications typically handled by software agents in our research in open environments requests for service can arrive at the local agent at any time thus making it difficult to fully plan or predict the agent s future workload in dyn case based bdi agents an effective approach for intelligent search on the world wide web we present a simple randomized algorithm which solves linear programs with n constraints and d variables in expected minfo d 2 2 d n e 2 p d ln n p d o p d ln n g time in the unit cost model where we count the number of arithmetic operations on the numbers in the input to be precise the algorithm computes the lexicographically smallest nonnegative point satisfying n given linear inequalities in d variables the expectation is over the internal randomizations work by the first author has been supported by a humboldt research fellowship work by the second and third authors has been supported by the german israeli foundation for scientific research and development g i f work by the second author has been supported by office of naval research grant n00014 90 j 1284 by national science foundation grants ccr 89 01484 and ccr 90 22103 and by grants from the u s israeli binational science foundation and the fund for basic research administered by the israeli eddies continuously adaptive query processing in large federated and shared nothing databases resources can exhibit widely fluctuating characteristics assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing as a result traditional static query optimization and execution techniques are ineffective in these environments in this paper we introduce a query processing mechanism called an eddy which continuously reorders operators in a query plan as it runs we characterize the moments of symmetry during which pipelined joins can be easily reordered and the synchronization barriers that require inputs from different sources to be coordinated by combining eddies with appropriate join algorithms we merge the optimization and execution phases of query processing allowing each tuple to have a flexible ordering of the query operators this flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm our initial implementation demonstrates prom continuous conceptual set covering learning robot operators from examples continuous conceptual set covering ccsc is an algorithm that uses engineering knowledge to learn operator effects from training examples the program produces an operator hypothesis that even in noisy and nondeterministic domains can make good quantitative predictions an empirical evaluation in the traytilting domain shows that ccsc learns faster than an alternative case based approach the best results however come from integrating ccsc and the case based approach figure 1 experimental set up 1 introduction initially the robot knows how to physically execute the tilt operator it does not however know the effects of the operator when the tray tilt operator is executed the robot tips the tray down 30 from the horizontal in the direction of tilt the new position of the puck is hard to predict because of uncertainty in the initial conditions the initial position of the puck and the tilting angle are continuous values subject to measurement error in additio the feret evaluation methodology for face recognition algorithms two of the most critical requirements in support of producing reliable face recognition systems are a large database of facial images and a testing procedure to evaluate systems the face recognition technology feret program has addressed both issues through the feret database of facial images and the establishment of the feret tests to date 14 126 images from 1199 individuals are included in the feret database which is divided into development and sequestered portions of the database in september 1996 the feret program administered the third in a series of feret face recognition tests the primary objectives of the third test were to 1 assess the state of the art 2 identify future areas of research and 3 measure algorithm performance 1 introduction over the last decade face recognition has become an active area of research in computer vision neuroscience and psychology progress has advanced to the point that face recognition systems are being demonstrated in real towards a model for spatio temporal schema selection schema versioning provides a mechanism for handling change in the structure of database systems and has been investigated widely both in the context of static and temporal databases with the growing interest in spatial and spatio temporal data as well as the mechanisms for holding such data the spatial context within which data is formatted also becomes an issue this paper presents a generalised model that accommodates schema versioning within static temporal spatial and spatio temporal relational and object oriented databases overview of datalog extensions with tuples and sets datalog with negation is the most powerful query language for relational database with a well defined declarative semantics based on the work in logic programming however datalog only allows inexpressive flat structures and cannot directly support complex values such as nested tuples and sets common in novel database applications for these reasons datalog has been extended in the past several years to incorporate tuple and set constructors in this paper we examine four different datalog extensions ldl col hilog and relationlog 1 introduction databases and logic programming are two independently developed areas in computer science database technology has evolved in order to effectively and efficiently organize manage and maintain large volumes of ever increasingly complex data reliably in various memory devices the underlying structure of databases has been the primary focus of research which leads to the development of data models the most well known and widely used da cat the copying approach to tabling the slg wam is an abstract machine that can be characterized as a sharing approach to implementing tabling the execution environments of suspended computations are interspersed in the wam stacks stacks are frozen using a set of freeze registers and the wam trail mechanism is extended so that the suspended computations can be resumed this technique has a reasonably small execution overhead but it is not easy to implement on top of an existing prolog system it is also quite difficult to understand we propose a new technique for the implementation of tabling the copying approach to tabling cat does not impose any overhead to the execution of prolog code and can be introduced into an existing prolog system orthogonally also cat is easier to understand we have implemented cat in the xsb system by taking out slg wam and adding cat we describe the additions needed for adopting cat in a wam implementation we show a case in which cat performs arbitrarily worse than slg wam but on the other hand we present empirical evidence that cat is competitive and often faster than the slg wam we also briefly discuss issues related to memory management and to the scheduling requirements for a translation between knowledge level messages and the database structure this paper discusses the requirements of a translator between queries and assertions based on database independent conceptualizations and their equivalents in the terms of the database structure to cope with a large number of independent developers of software components that will have to access these databases and who have different backgrounds and expertise this translator will allow for several conceptualizations and vocabularies to coexist it will be possible to individually customize these conceptualizations to keep them simple concise and well aligned to the conceptualizations and jargon used by the domain experts the translator will be accessed by other software components of the cristal system that have to execute complex queries like the viewpoint facility user supplied components user code and the so called icist gui 6 schema independent updates are most relevant for the user supplied components communicating agents we study the problem of endowing logic based agents that can reason about their own beliefs as well as the beliefs of other agents with communication skills we show how communication performatives from existing agent communication languages as well as their preconditions and eects can be expressed within logic based agents in terms of the agents beliefs we illustrate the resulting language for programming logic based agents by means of examples 1 introduction in an earlier paper 6 we propose an approach to logic based agents by combining the approach to agents by kowalski and sadri 11 and the approach to meta reasoning by costantini et al 5 4 similarly to kowalski and sadri s agents the agents in 6 are hybrid in that they exhibit both rational or deliberative and reactive behaviour the reasoning core of the agents is a proof procedure that combines forward and backward reasoning backward reasoning is used primarily for planning problem solving and other deliber cyber atvs dynamic and distributed reconnaissance and surveilllance using all terrain ugvs this paper describes our current effort to develop robotic vehicles for tactical distributed surveillance our research is focused on multi agent collaboration reconfigurable systems efficient perception and sensor fusion distributed command and control and task decomposition in particular this paper describes the main features and capabilities of our all terrain vehicles atvs concentrating on their autonomous navigation capabilities an instructor s assistant for team training in dynamic multi agent virtual worlds teams of people operating in highly dynamic multi agent environments must learn to deal with rapid and unpredictable turns of events simulation based training environments inhabited by synthetic agents can be effective in providing realistic but safe settings in which to develop the skills these environments require however such training environments present a problem for the instructor who must evaluate and control rapidly evolving training sessions we address the instructors problem with a pedagogical agent called the puppetmaster the puppetmaster manages a network of spy agents that report on the activities in the simulation in order to provide the instructor with an interpretation and situation specific analysis of student behavior the approach used to model student teams is to structure the state space into an abstract situation based model of behavior that supports interpretation in the face of missing information about agent s actions and goals control law design for haptic interfaces to virtual reality the goal of control law design for haptic displays is to provide a safe and stable user interface while maximizing the operator s sense of kinesthetic immersion in a virtual environment this paper outlines a control design approach which stabilizes a haptic interface when coupled to a broad class of human operators and virtual environments two port absolute stability criteria are used to develop explicit control law design bounds for two different haptic display implementations impedance display and admittance display the strengths and weaknesses of each approach are illustrated through numerical and experimental results for a three degree offreedom device the example highlights the ability of the proposed design procedure to handle some of the more difficult problems in control law synthesis for haptics including structural flexibility and non collocation of sensors and actuators the authors are with the department of electrical engineering university of washington box 352500 seattle wa 98195 2500 2 i generating executing and revising schedules for autonomous robot office couriers scheduling the tasks of an autonomous robot office courier and carrying out the scheduled tasks reliably and efficiently pose challenging problems for autonomous robot control to carry out their jobs reliably and efficiently many autonomous mobile service robots acting in human working environments have to view their jobs as everyday activity they should accomplish longterm efficiency rather than optimize problem solving episodes they should also exploit opportunities and avoid problems flexibly because often robots are forced to generate schedules based on partial information we propose to implement the controller for scheduled activity by employing concurrent reactive plans that reschedule the course of action whenever necessary and while performing their actions the plans are represented modularly and transparently to allow for easy transformation scheduling and schedule repair methods are implemented as plan transformation rules introduction to carry out their jobs reliably probabilistic object bases there are many applications where an object oriented data model is a good way of representing and querying data however current object database systems are unable to handle the case of objects whose attributes are uncertain in this paper extending previous pioneering work by kornatzky and shimony we develop an extension of the relational algebra to the case of object bases with uncertainty we propose concepts of consistency for such object bases together with an np completeness result and classes of probabilistic object bases for which consistency is polynomially checkable in addition as certain operations involve conjunctions and disjunctions of events and as the probability of conjunctive and disjunctive events depends both on the probabilities of the primitive events involved as well as on what is known if anything about the relationship between the events we show how all our algebraic operations may be performed under arbitrary probabilistic conjunction and agent based distributed planning and scheduling in global manufacturing scheduling and resource allocation problems are pervasive and important in the management of industrial and government organizations with advent of new technology and fast evolvement in industry the enterprise is gradually moving toward global manufacturing for efficient operational management and competent strategic decision making in the past two decades researchers and practitioners have been applying various techniques such as artificial intelligence optimization methodologies information system design human factors etc to design and develop planning and scheduling methodologies and systems for different applications however the inherent complexity of problems short life cycle of planner scheduler unrealistic investment of generous purpose systems and profligacy of scattering computing resource accessibility integration and re configurability become the essential factors for new planners schedulers in global manufacturing the specific objectives of this research an automatic closed loop methodology for generating character groundtruth for scanned documents abstract character groundtruth for real scanned document images is crucial for evaluating the performance of ocr systems training ocr algorithms and validating document degradation models unfortunately manual collection of accurate groundtruth for characters in a real scanned document image is not practical because i accuracy in delineating groundtruth character bounding boxes is not high enough ii it is extremely laborious and time consuming and iii the manual labor required for this task is prohibitively expensive in this paper we describe a closed loop methodology for collecting very accurate groundtruth for scanned documents we first create ideal documents using a typesetting language next we create the groundtruth for the ideal document the ideal document is then printed photocopied and then scanned a registration algorithm estimates the global geometric transformation and then performs a robust local bitmap match to register the ideal document image to the scanned document image finally groundtruth associated with the ideal document image is transformed using the estimated geometric transformation to create the groundtruth for the scanned document image this methodology is very general and can be used for creating groundtruth for documents in typeset in any language layout font and style we have demonstrated the method by generating groundtruth for english hindi and fax document images the cost of creating groundtruth using our methodology is minimal if character word or zone groundtruth is available for any real document the registration algorithm can be used to generate the corresponding groundtruth for a rescanned version of the document index terms automatic real groundtruth document image analysis ocr performance evaluation image registration geometric transformations image warping f 1 efficient and extensible algorithms for multi query optimization complex queries are becoming commonplace with the growing use of decision support systems these complex queries often have a lot of common sub expressions either within a single query or across multiple such queries run as a batch multi query optimization aims at exploiting common subexpressions to reduce evaluation cost multi query optimization has hither to been viewed as impractical since earlier algorithms were exhaustive and explore a doubly exponential search space in this paper we demonstrate that multi query optimization using heuristics is practical and provides significant benefits we propose three cost based heuristic algorithms volcano sh and volcano ru which are based on simple modifications to the volcano search strategy and a greedy heuristic our greedy heuristic incorporates novel optimizations that improve efficiency greatly our algorithms are designed to be easily added to existing optimizers we present a performance study comparing the algo progress report on the disjunctive deductive database system dlv is a deductive database system based on disjunctive logic programming which offers front ends to several advanced kr formalisms the system has been developed since the end of 1996 at technische universitat wien in an ongoing project funded by the austrian science funds fwf recent comparisons have shown that dlv is nowadays a state of the art implementation of disjunctive logic programming a major strength of dlv are its advanced knowledge modelling features its kernel language extends disjunctive logic programming by strong negation a la gelfond and lifschitz and integrity constraints furthermore front ends for the database language sql3 and for diagnostic reasoning are available suitable interfaces allow dlv users to utilize base relations which are stored in external commercial database systems this paper provides an overview of the dlv system and describes recent advances in its implementation in particular the recent implementation of incremental techniques fo xml query languages experiences and exemplars this paper identifies essential features of an xml query language by examining four existing query languages xml ql ya t l lorel and xql the first three languages come from the database community and possess striking similarities the fourth comes from the document community and lacks some key functionality of the other three a mediation infrastructure for digital library services digital library mediators allow interoperation between diverse information services in this paper we describe a flexible and dynamic mediator infrastructure that allows mediators to be composed from a set of modules blades each module implements a particular mediation function such as protocol translation query translation or result merging all the information used by the mediator including the mediator logic itself is represented by an rdf graph we illustrate our approach using a mediation scenario involving a dienst and a z39 50 server and we discuss the potential advantages and weaknesses of our framework keywords mediator wrapper interoperability component design 1 introduction heterogeneity is one of the main challenges faced by digital libraries too often documents are stored in different formats collections are searched with disparate query languages search services are accessed with incompatibleprotocols intellectual property protection and access sche indexing moving points we propose three indexing schemes for storing a set s of n points in the plane each moving along a linear trajectory so that a query of the following form can be answered quickly given a rectangle r and a real value t q report all k points of s that lie inside r at time t q we first present an indexing structure that for any given constant 0 uses o n b disk blocks where b is the block size and answers a query in o n b 1 2 k b i os it can also report all the points of s that lie inside r during a given time interval a point can be inserted or deleted or the trajectory of a point can be changed in o log 2 b n i os next we present a general approach that improves the query time if the queries arrive in chronological order by allowing the index to evolve over time we obtain a tradeoff between the query time and the number of times the index needs to be updated as the points move we also describe an indexing scheme in which the number of i os required to answer a query depends monotonically on the difference between t q and the current time finally we develop an efficient indexing scheme to answer approximate nearest neighbor queries among moving points an extended abstract of this paper appeared in the proceedings of the 19th acm sigact sigmod sigart symposium on principles of database systems y center for geometric computing department of computer science duke university box 90129 durham nc 27708 0129 pankaj cs duke edu http www cs duke edu pankaj supported in part by national science foundation grants eia 9870734 eia 9972879 and ccr 9732787 by army research of fice muri grant daah04 96 1 0013 by a sloan fellowship and by a grant from the u s israeli binational science foundation z center touchcounters designing interactive electronic labels for physical containers we present touchcounters an integrated system of electronic modules physical storage containers and shelving surfaces for the support of collaborative physical work through physical sensors and local displays touchcounters record and display usage history information upon physical storage containers thus allowing access to this information during the performance of real world tasks a distributed communications network allows this data to be exchanged with a server such that users can access this information from remote locations as well based upon prior work in ubiquitous computing and tangible interfaces touchcounters incorporate new techniques including usage history tracking for physical objects and multi display visualization this paper describes the components interactions implementation and conceptual approach of the touchcounters system keywords tangible interfaces ubiquitous computing distributed sensing visualization introduction for decades research i a business process agent the architecture of a process agent is a three layer bdi hybrid multi agent architecture these process agents are intended to deal with corporate cultural or political sensitivities as well as with corporate rules these agents adapt their behaviour on the basis of the likelihood of plan success and on estimates of the time cost and value of choosing a plan 1 introduction an intelligent multi agent system is a society of autonomous cooperating components each of which maintains an ongoing interaction with its environment intelligent agents should be autonomous cooperative and adaptive a process agent architecture is designed specifically for business process applications typically the cost of bringing a business process to its conclusion is substantially due to the cost of the human processing involved if this is so then a process management system should make its decisions in a thorough and considered way and should have no reason based on cost for not doing s more than just a pretty face affordances of embodiment prior research into embodied interface agents has found that users like them and find them engaging in this paper we argue that embodiment can serve an even stronger function if system designers use actual human conversational protocols in the design of the interface communicative behaviors such as salutations and farewells conversational turn taking with interruptions and referring to objects using pointing gestures are examples of protocols that all native speakers of a language already know how to perform and that can thus be leveraged in an intelligent interface we discuss how these protocols are integrated into rea an embodied multi modal conversational interface agent who acts as a real estate salesperson and we show why embodiment is required for their successful implementation introduction there is a qualitative difference between face to face conversation and other forms of human human communication 4 businesspeople and academics routinely travel long distances accurate estimation of the cost of spatial selections optimizing queries that involve operations on spatial data requires estimating the selectivity and cost of these operations in this paper we focus on estimating the cost of spatial selections or window queries where the query windows and data objects are general polygons cost estimation techniques previously proposed in the literature only handle rectangular query windows over rectangular data objects thus ignoring the very significant cost of exact geometry comparison the refinement step in a filter and refine query processing strategy the cost of the exact geometry comparison depends on the selectivity of the filtering step and the average number of vertices in the candidate objects identified by this step in this paper we introduce a new type of histogram for spatial data that captures the complexity and size of the spatial objects as well as their location capturing these attributes makes this type of histogram useful for accurate estimation as we experimentally demonstrate we also investigate sampling based estimation approaches sampling can yield better selectivity estimates than histograms for polygon data but at the high cost of performing exact geometry comparisons for all the sampled objects 1 object relational queries into multidimensional databases with the active data repository as computational power and storage capacity increase processing and analyzing large volumes of multi dimensional datasets play an increasingly important role in many domains of scientific research scientific applications that make use of very large scientific datasets have several important characteristics datasets consist of complex data and are usually multi dimensional applications usually retrieve a subset of all the data available in the dataset various application specific operations are performed on the data items retrieved such applications can be supported by object relational database management systems or dbmss in addition to providing functionality to define new complex datatypes and user defined functions an or dbms for scientific datasets should contain runtime support that will provide optimized storage for very large datasets and an execution environment for user defined functions involving expensive operations in this paper we describe an infrastructure t evolving detectors of 2d patterns on a simulated cam brain machine an evolvable hardware tool for building a 75 million neuron artificial brain this paper presents some simulation results of the evolution of 2d visual pattern recognizers to be implemented very shortly on real hardware namely the cam brain machine cbm an fpga based piece of evolvable hardware which implements a genetic algorithm ga to evolve a 3d cellular automata ca based neural network circuit module of approximately 1 000 neurons in about a second i e a complete run of a ga with 10 000s of circuit growths and performance evaluations up to 65 000 of these modules each of which is evolved with a humanly specified function can be downloaded into a large ram space and interconnected according to humanly specified artificial brain architectures this ram containing an artificial brain with up to 75 million neurons is then updated by the cbm at a rate of 130 billion ca cells per second such speeds will enable real time control of robots and hopefully the birth of a new research field that we call brain building the first su the morph node we discuss potential and limitations of a morph node inspired by the corresponding construct in java3d a morph node in java3d interpolates vertex attributes among several homeomorphic geometries this node is a promising candidate for the delivery of 3d animation in a very compact form we review the state of the art in web 3d techniques allowing for the possibility of interpolating among several geometries this review leads to a simple extension for vrml 97 as well as a recommendation for necessary changes in java3d furthermore we discuss various optimization issues for morph nodes cr categories and subject descriptors i 3 6 computer graphics methodology and techniques standards vrml i 3 7 computer graphics three dimensional graphics and realism animation i 3 8 computer graphics applications additional keywords animation avatars morphing virtual humans vrml introduction animation of three dimensional shapes involves the change of vertex attributes over rol2 a real deductive object oriented database language rol is a strongly typed deductive object oriented database language it integrates many important features of deductive databases and object oriented databases however it is only a structurally objectoriented language in this paper we describe our extension of rol called rol2 rol2 keeps all the important features of rol in addition it incorporates important behaviorally object oriented features such as rulebased methods and encapsulation so that it is a real deductive objectoriented database language it supports object identity complex objects class hierarchy methods non monotonic multiple structural and behavioral inheritance with overriding and blocking 1 introduction in the past decade a number of deductive object oriented database languages have been proposed such as o logic 21 revised o logic 15 iql 1 logres 7 datalog meth 2 coral 26 gulog 10 rock roll 3 flogic 14 and rol 18 19 however most of them are only structural on constructing the right sort of cbr implementation case based reasoning implementations as currently constructed tend to t three general models characterized by implementation constraints task based task alone enterprise integrating databases and web based integrating web representations these implementations represent the targets for automatic system construction and it is important to understand the strengths of each how they are built and how one may be constructed by transforming another this paper describes a framework that relates the three types of cbr implementation discusses their typical strengths and weaknesses and describes practical methods for automating the construction of new cbr systems by transforming and synthesizing existing resources 1 introduction cbr systems as currently constructed tend to t three general implementation models de ned by broad implementation constraints on representation and process traditionally task based implementations have addressed system goals bas a process oriented heuristic for model selection current methods to avoid overfitting are either data oriented using separate data for validation or representation oriented penalizing complexity in the model this paper proposes process oriented evaluation where a model s expected generalization error is computed as a function of the search process that led to it the paper develops the necessary theoretical framework and applies it to one type of learning rule induction a process oriented version of the cn2 rule learner is empirically compared with the default cn2 the process oriented version is more accurate in a large majority of the datasets with high significance and also produces simpler models experiments in artificial domains suggest that processoriented evaluation is particularly useful in high dimensional domains 1 introduction overfitting avoidance is often considered the central problem of machine learning e g cheeseman oldford 1994 if a learner is sufficiently powerful it must guard against selec using probabilistic relational models for collaborative filtering recent projects in collaborative filtering and information filtering address the task of inferring user preference relationships for products or information the data on which these inferences are based typically consists of pairs of people and items the items may be information sources such as web pages or newspaper articles or products such as books software movies or cds we are interested in making recommendations or predictions traditional approaches to the problem derive from classical algorithms in statistical pattern recognition and machine learning the majority of these approaches assume a flat data representation for each object and focus on a single dyadic relationship between the objects in this paper we examine a richer model that allows us to reason about many different relations at the same time we build on the recent work on probabilistic relational models prms and describe how prms can be applied to the task of collaborative filtering prms allow us to represent uncertainty about the existence of relationships in the model and allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects 1 an overview of the tatami project this paper describes the tatami project at ucsd which is developing a system to support distributed cooperative software development over the web and in particular the validation of concurrent distributed software the main components of our current prototype are a proof assistant a generator for documentation websites a database an equational proof engine and a communication protocol to support distributed cooperative work we believe behavioral specification and verification are important for software development and for this purpose we use first order hidden logic with equational atoms the paper also briefly describes some novel user interface design methods that have been developed and applied in the project hierarchical models for screening of iron deficiency anemia we investigate the problem of classifying individuals based on estimated density functions for each individual the problem is similar to conventional classification in that there is labelled training data but different in that the underlying measurements are not feature vectors but histograms or density estimates we describe a general framework based on probabilistic hierarchical models for modelling such data and illustrate how the model lends itself to classification we contrast this approach with two other alternatives 1 directly defining distance between densities using a cross entropy distance measure and 2 using parameters of the estimated densities as feature vectors for a standard discriminative classification framework we evaluate all three methods on a realworld medical diagnosis problem the hierarchical modeling and density distance approaches are most accurate yielding crossvalidated error rates in the range of 1 to 2 we conclude by discussing the relative me on the scalability of simple genetic algorithms scalable evolutionary computation has become an intensively studied research topic in recent years the issue of scalability is predominant in any eld of algorithmic design but it became particularly relevant for the design of competent genetic algorithms once the scalability problems of simple genetic algorithms were understood here we present some of the work that has aided in getting a clear insight in the scalability problems of simple genetic algorithms particularly we discuss the important issue of building block mixing and show how the need for mixing places a boundary in the ga parameter space that together with the boundary from the schema theorem delimits the region where the ga converges reliably to the optimum of problems of bounded diculty this region or sweet spot as it has been called shrinks unfortunately very rapidly with increasing problem size unless the building blocks are tightly linked in the problem coding structure in addition we look how straightforw active user interfaces the current state of user interfaces for large information spaces imposes an unmanageable cognitive burden upon the user determining how to get the right information into the right form with the right tool at the right time has become a monumental task while advances in graphical user interfaces can partially address this problem the basic problem of information overload and under load can not be solely addressed through development of a better direct manipulation interface to the information space we survey the state of the art in two research fields interface agents and user modeling that address the problem of information overload and under load first we take a historical look at how the fields of human computer interaction and artificial intelligence have viewed interface agent research interface agents address the problem of increasing task load by serving as either an assistant or associate extracting and analyzing relevant information providing informatio hybrid neural systems this chapter provides an introduction to the eld of hybrid neural systems hybrid neural systems are computational systems which are based mainly on articial neural networks but also allow a symbolic interpretation or interaction with symbolic components in this overview we will describe recent results of hybrid neural systems we will give a brief overview of the main methods used outline the work that is presented here and provide additional references we will also highlight some important general issues and trends discovery of similarity computations of search engines two typical situations in which it is of practical interest to determine the similarities of text documents to a query due to a search engine are 1 a global search engine constructed on top of a group of local search engines wishes to retrieve the set of local documents globally most similar to a given query and 2 an organization wants to compare the retrieval performance of search engines the dot product function is a widely used similarity function for a search engine using such a function we can determine its similarity computations if how the search engine sets the weights of terms is known which is usually not the case in this paper techniques are presented to discover certain mathematical expressions of these formulas and the values of embedded constants when the dot product similarity function is used preliminary results from experiments on the webcrawler search engine are given to illustrate our techniques 1 categories and subject descriptors h 3 information audio visual speaker detection using dynamic bayesian networks the development of human computer interfaces poses a challenging problem actions and intentions of different users have to be inferred from sequences of noisy and ambiguous sensory data temporal fusion of multiple sensors can be efficiently formulated using dynamic bayesian networks dbns dbn framework allows the power of statistical inference and learning to be combined with contextual knowledge of the problem we demonstrate the use of dbns in tackling the problem of audio visual speaker detection off the shelf visual and audio sensors face skin texture mouth motion and silence detectors are optimally fused along with contextual information in a dbn architecture that infers instances when an individual is speaking results obtained in the setup of an actual human machine interaction system genie casino kiosk demonstrate superiority of our approach over that of static context free fusion architecture 1 introduction advanced human computer interfaces increasingly r the persistent cache improving oid indexing in temporal object oriented database systems in a temporal oodb an oid index oidx is needed to map from oid to the physical location of the object in a transaction time temporal oodb the oidx should also index the object versions in this case the index entries which we call object descriptors od also include the commit timestamp of the transaction that created the object version the oidx in a non temporal oodb only needs to be updated when an object is created but in a temporal oodb the oidx have to be updated every time an object is updated we have in a previous study shown that this can be a potential bottleneck and in this report we present the persistent cache pcache a novel approach which reduces the index update and lookup costs in temporal oodbs in this report we develop a cost model for the pcache and use this to show that the use of a pcache can reduce the average access cost to only a fraction of the cost when not using the pcache even though the primary context of this report is oid indexing in an application independent intelligent user support system exploiting action sequence based user modelling many software systems usability suffers from their complexity usually caused by the market driven trend to bundle a huge amount of features which are supposed to increase the product s attractiveness this attempt however more often than not leads to software with poor usability characteristics therefore requiring an extensive amount of initial effort for the users to become familiar with the system one way to overcome this problem is by providing user adapted usage support in this paper we present an experimental system for intelligent user support which has been developed under the aspect of portability focusing on this goal the system supports a variety of user and task modeling approaches and is independent from the hosting software application environment thus being ready to integrate with existing and new applications the different user modeling approaches have been empirically evaluated and compared in a medical software application embedding our syste patterns of search analyzing and modeling web query refinement we discuss the construction of probabilistic models centering on temporal patterns of query refinement our analyses are derived from a large corpus of web search queries extracted from server logs recorded by a popular internet search service we frame the modeling task in terms of pursuing an understanding of probabilistic relationships among temporal patterns of activity informational goals and classes of query refinement we construct bayesian networks that predict search behavior with a focus on the progression of queries over time we review a methodology for abstracting and tagging user queries after presenting key statistics on query length query frequency and informational goals we describe user models that capture the dynamics of query refinement 1 introduction the evolution of the world wide web has provided rich opportunities for gathering and analyzing anonymous log data generated by user interactions with network based services web based search engine real time scheduling for distributed agents providing an environment for a software agent to execute is very similar to building an operating system for the execution of general purpose applications in the same fashion that an operating system provides a set of services for the execution of a user request an agent framework provides a similar set of services for the execution of agent actions such services include the ability to communicate with other agents maintaining the current state of an executing agent and selecting an execution path from a set of possible execution paths the particular focus of this paper is the study of soft real time agentscheduling in the context of a framework for the execution of intelligent software agents acharacterization of agent performance and developmentofanenvironment for testing and comparing the performance of agent activities the agent architecture used for this study decaf distributed environment centered agentframework is a software toolkit for the rapid d alerting services in a digital library environment the classical paradigm of finding information in the www by initiating retrieval and browsing becomes more and more ineffective other techniques have to be considered automatic delivery of contents to the user according to their needs and filtered by her profile of interests is required current implementations of such alerting services at content providers side have several drawbacks in my research project i evaluate methods and techniques for alerting services with special respect to the area of digital libraries i intend to provide a framework that supports design decisions in building alerting services depending on the infrastructure and desired system parameters 1 introduction imagine one morning you just arrive at your office and switch on your computer to have a look at the recent news in your special field of research little pictures for each topic tell you that some interesting documents arrived behind one icon you find for instance the new announcements for c topical locality in the web most web pages are linked to others with related content this idea combined with another that says that text in and possibly around html anchors describe the pages to which they point is the foundation for a usable worldwide web in this paper we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the web in particular we find that the likelihood of linked pages having similar textual content to be high the similarity of sibling pages increases when the links from the parent are close together titles descriptions and anchor text represent at least part of the target page and that anchor text may be a useful discriminator among unseen child pages these results show the foundations necessary for the success of many web systems including search engines focused crawlers linkage analyzers and intelligent web agents querying the physical world data type adt objects that are single attribute values encapsulating a collection of related data s98 note that there are natural parallels between devices and adts both adts and devices provide controlled access to encapsulated data through a well defined interface we build upon this observation by modeling each type of device in the network as an adt the public interface of the adt corresponds to the specific functions supported by the device an actual adt object in the database corresponds to a physical device in the real world let us model the database schema corresponding to the flood detection example from the introduction we consider a simplified schema that consists of the following relations rfsensors sensor x y areas name x1 y1 x2 y2 a record in the rfsensors relation has three attributes the first attribute called sensor is an adt that represents the physical rainfall sensor the actual sensor data is located on the rainfall sensor the adt experimental results with real time scheduling using decaf decaf 6 is a software toolkit for the rapid design development and execution of intelligent agents to achieve solutions in complex software systems from a research community perspective decaf provides a modular platform for evaluating and disseminating results in agent architectures including communication planning scheduling execution monitoring coordination diagnosis and learning this paper will describe a methodology and results for building and evaluating execution schedules of agent actions in the decaf architecture a brief description of decaf is provided describing how the modular design allows for testing of multiple scheduling algorithms a description of the types of agents that were used to demonstrate three particular capabilities of decaf scalability parallelism and the threaded nature of the architecture lastly experiments using different scheduling algorithms were utilized to develop an experimental platform for future research in agent scheduling f optimizing the efficiency of parameterized local search within global search a preliminary study application specific parameterized local search algorithms plsas in which optimization accuracy can be traded off with run time arise naturally in many optimization contexts we introduce a novel approach called simulated heating for systematically integrating parameterized local search into global search algorithms gsas in general and evolutionary algorithms in particular using the framework of simulated heating we investigate both static and dynamic strategies for systematically managing the trade off between plsa accuracy and optimization effort we show quantitatively that careful management of this trade off is necessary to achieve the full potential of a gsa plsa combination furthermore we provide preliminary results which demonstrate the effectiveness of our simulated heating techniques in the context of code optimization for embedded software implementation a practical problem that involves vast and complex search spaces 1 motivation for many useful optimizatio collision avoidance and resolution multiple access for multichannel wireless networks the carma ntg protocol is presented and analyzed carma ntg dynamically divides the channel into cycles of variable length each cycle consists of a contention period and a group transmission period during the contention period a station with one or more packets to send competes for the right to be added to the group of stations allowed to transmit data without collisions this is done using a collision resolution splitting algorithm based on a request to send clear to send rts cts message exchange with non persistent carrier sensing carma ntg ensures that one station is added to the group transmission period if one or more stations send requests to be added in the previous contention period the group transmission period is a variable length train of packets which are transmitted by stations that have been added to the group by successfully completing an rts cts message exchange in previous contention periods as long as a station maintains its position in the group it is able to transmit data packets without collision an upper bound is derived for the average costs of obtaining the first success in the splitting algorithm this bound is then applied to the computation of the average channel utilization in a fully connected network with a large number of stations these results indicate that collision resolution is a powerful mechanism in combination with floor acquisition and group allocation multiple access 1 on the learnability and design of output codes for multiclass problems output coding is a general framework for solving multiclass categorization problems previous research on output codes has focused on building multiclass machines given predefined output codes in this paper we discuss for the first time the problem of designing output codes for multiclass problems for the design problem of discrete codes which have been used extensively in previous works we present mostly negative results we then introduce the notion of continuous codes and cast the design problem of continuous codes as a constrained optimization problem we describe three optimization problems corresponding to three different norms of the code matrix interestingly for the l 2 norm our formalism results in a quadratic program whose dual does not depend on the length of the code a special case of our formalism provides a multiclass scheme for building support vector machines which can be solved efficiently we give a time and space efficient algorithm for solving the quadratic program we describe preliminary experiments with synthetic data show that our algorithm is often two orders of magnitude faster than standard quadratic programming packages we conclude with the generalization properties of the algorithm keywords multiclass categorization output coding svm 1 optimizing large join queries in mediation systems in data integration systems queries posed to a mediator need to be translated into a sequence of queries to the underlying data sources in a heterogeneous environment with sources of diverse and limited query capabilities not all the translations are feasible in this paper we study the problem of finding feasible and efficient query plans for mediator systems we consider conjunctive queries on mediators and model the source capabilities through attribute binding adornments we use a simple cost model that focuses on the major costs in mediation systems those involved with sending queries to sources and getting answers back under this metric we develop two algorithms for source query sequencing one based on a simple greedy strategy and another based on a partitioning scheme the first algorithm produces optimal plans in some scenarios and we show a linear bound on its worst case performance when it misses optimal plans the second algorithm generates optimal plans in more scenarios while having no bound on the margin by which it misses the optimal plans we also report on the results of the experiments that study the performance of the two algorithms mixed initiative interaction spect of effective multiagent collaboration to solve problems or perform tasks in our minimal human computer configuration such tasks could include systems designed to interact with a user to design a kitchen find the best airfare coordinate an emergency relief mission or teach the user how to use new equipment mixed initiative refers to a flexible interaction strategy where each agent can contribute to the task what it does best furthermore in the most general cases the agents roles are not determined in advance but opportunistically negotiated between them as the problem is being solved at any one time one agent might have the initiative controlling the interaction while the other works to assist it contributing to the interaction as required at other times the roles are reversed and at other times again the agents might be working independently assisting each other only when specifically asked the agents dynamically adapt their interaction st content based video indexing of tv broadcast news using hidden markov models this paper presents a new approach to content based video indexing using hidden markov models hmms in this approach one feature vector is calculated for each image of the video sequence these feature vectors are modeled and classified using hmms this approach has many advantages compared to other video indexing approaches the system has automatic learning capabilities it is trained by presenting manually indexed video sequences to improve the system we use a video model that allows the classification of complex video sequences the presented approach works three times faster than real time we tested our system on tv broadcast news the rate of 97 3 correctly classified frames shows the efficiency of our system 1 introduction the increasing amount of digital video in multimedia databases results in a demand for techniques for automatic content based access to video data in the last years there have been many different approaches to content based video indexing a rough data driven theory refinement using kbdistal knowledge based artificial neural networks offer an attractive approach to extending or modifying incomplete knowledge bases or domain theories through a process of data driven theory refinement we present an efficient algorithm for data driven knowledge discovery and theory refinement using distal a novel inter pattern distance based polynomial time constructive neural network learning algorithm the initial domain theory comprising of propositional rules is translated into a knowledge based network the domain theory is modified using distal which adds new neurons to the existing network as needed to reduce classification errors associated with the incomplete domain theory on labeled training examples the proposed algorithm is capable of handling patterns represented using binary nominal as well as numeric real valued attributes results of experiments on several datasets for financial advisor and the human genome project indicate that the performance of the proposed algorithm compares quite favorably with other algorithms for connectionist theory refinement including those that require substantially more computational resources both in terms of generalization accuracy and network size extending classical logic with inductive definitions the goal of this paper is to extend classical logic with a generalized notion of inductive definition supporting positive and negative induction to investigate the properties of this logic its relationships to other logics in the area of non monotonic reasoning logic programming and deductive databases and to show its application for knowledge representation by giving a typology of definitional knowledge towards a highly scalable metasearch engine the world wide web has been expanding in a very fast rate the coverage of the web by each of the major search engines has been steadily decreasing despite their effort to index more web pages worse yet as these search engines get larger higher percentages of their indexed information are becoming obsolete more and more people are having doubt about the scalability of centralized search engine technology a more scalable alternative to search the web is the metasearch engine approach a metasearch engine can be considered as an interface on top of multiple local search engines to provide uniform access to many local search engines database selection is one of the main challenges in building a large scale metasearch engine the problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query in order to enable accurate selection metadata that reflect the content of each search engine need to be co beyond euclidean eigenspaces bayesian matching for visual recognition we propose a novel technique for direct visual matching of images for the purposes of face recognition and database search speci cally we argue in favor of a probabilistic measure of similarity in contrast to simpler methods which are based on standard euclidean l2 norms e g template matching or subspace restricted norms e g eigenspace matching the proposed similarity measure is based on a bayesian analysis of image di erences we model twomutually exclusive classes of variation between two facial images intra personal variations in appearance of the same individual due to di erent expressions or lighting and extra personal variations in appearance due to a di erence in identity the high dimensional probability density functions for each respective class are then obtained from training data using an eigenspace density estimation technique and subsequently used to compute a similarity measure based on the a posteriori probability of membership in the intra personal class which is used to rank matches in the database the performance advantage of this probabilistic matching technique over standard euclidean nearest neighbor eigenspace matching is demonstrated using results from arpa s 1996 feret face recognition competition in which this algorithm was found to be the top performer tip a temporal extension to informix commercial relational database systems today provide only limited temporal support to address the needs of applications requiring rich temporal data and queries we have built tip temporal information processor a temporal extension to the informix database system based on its datablade technology our tip datablade extends informix with a rich set of datatypes and routines that facilitate temporal modeling and querying tip provides both c and java libraries for client applications to access a tipenabled database and provides end users with a gui interface for querying and browsing temporal data 1 introduction our research in temporal data warehouses 9 10 has led us to require a relational database system with full sql as well as rich temporal support in order to experiment with our temporal view maintenance techniques most commercial relational database systems support only a date type or its variants an attribute of type date can be used to timestamp a tuple with probabilistic deduction with conditional constraints over basic events we study the problem of probabilistic deduction with conditional constraints over basic events we show that globally complete probabilistic deduction with conditional constraints over basic events is np hard we then concentrate on the special case of probabilistic deduction in conditional constraint trees we elaborate very efficient techniques for globally complete probabilistic deduction in detail for conditional constraint trees with point probabilities we present a local approach to globally complete probabilistic deduction which runs in linear time in the size of the conditional constraint trees for conditional constraint trees with interval probabilities we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs we show how these nonlinear programs can be transformed into equivalent linear programs which are solvable in polynomial time in the size of the conditional constraint trees 1 introduction dealing wit temporal dependencies generalized for spatial and other dimensions recently there has been a lot of interest in temporal granularity and its applications in temporal dependency theory and data mining generalization hierarchies used in multi dimensional databases and olap serve a role similar to that of time granularity in temporal databases but they also apply to non temporal dimensions like space in this paper we first generalize temporal functional dependencies for non temporal dimensions which leads to the notion of roll up dependency rud we show the applicability of ruds in conceptual modeling and data mining we then indicate that the notion of time granularity used in temporal databases is generally more expressive than the generalization hierarchies in multi dimensional databases and show how this surplus expressiveness can be introduced in non temporal dimensions which leads to the formalism of rud with negation rud a complete axiomatization for reasoning about rud is given 1 introduction generalization hierarchi the case against accuracy estimation for comparing induction algorithms we analyze critically the use of classification accuracy to compare classifiers on natural data sets providing a thorough investigation using roc analysis standard machine learning algorithms and standard benchmark data sets the results raise serious concerns about the use of accuracy for comparing classifiers and draw into question the conclusions that can be drawn from such studies in the course of the presentation we describe and demonstrate what we believe to be the proper use of roc analysis for comparative studies in machine learning research we argue that this methodology is preferable both for making practical choices and for drawing scientific conclusions 1 introduction substantial research has been devoted to the development and analysis of algorithms for building classifiers and a necessary part of this research involves comparing induction algorithms a common methodology for such evaluations is to perform statistical comparisons of the accuracies of learned class spatio temporal data types an approach to modeling and querying moving objects in databases spatio temporal databases deal with geometries changing over time in general geometries cannot only change in discrete steps but continuously and we are talking about moving objects if only the position in space of an object is relevant then moving point is a basic abstraction if also the extent is of interest then the moving region abstraction captures moving as well as growing or shrinking regions we propose a new line of research where moving points and moving regions are viewed as three dimensional 2d space time or higher dimensional entities whose structure and behavior is captured by modeling them as abstract data types such types can be integrated as base attribute data types into relational object oriented or other dbms data models they can be implemented as data blades cartridges etc for extensible dbmss we expect these spatio temporal data types to play a similarly fundamental role for spatio temporal databases as spatial data types have played for spatial databases the paper explains the approach and discusses several fundamental issues and questions related to it that need to be clarified before delving into specific designs of spatio temporal algebras computing geographical scopes of web resources many information resources on the web are relevant primarily to limited geographical communities for instance web sites containing information on restaurants theaters and apartment rentals are relevant primarily to web users in geographical proximity to these locations in contrast other information resources are relevant to a broader geographical community for instance an on line newspaper may be relevant to users across the united states unfortunately most current web search engines largely ignore the geographical scope of web resources in this paper we introduce techniques for automatically computing the geographical scope of web resources based on the textual content of the resources as well as on the geographical distribution of hyperlinks to them we report an extensive experimental evaluation of our strategies using real web data finally we describe a geographically aware search engine that we have built using our techniques for determining the geographical scope of web resources 1 meeting plan recognition requirements for real time air mission simulations in this paper the potential synergy between instancebased pattern recognition and means end possible world reasoning is explored for supporting plan recognition in multi aeroplane air mission simulations a combination of graph matching induction probabilistic principles and dynamic programming are applied to traces of aeroplane behaviour during flight manoeuvres these satisfy the real time constraints of the simulation this enables the agents to recognise what other agents are doing and to abstract about their activity at the instrumentation level a means end reasoning model is then used to deliberate about and invoke standard operating procedures based on recognised activity the reasoning model constrains the recognition process by framing queries according to what a pilot would expect during the execution of the current plan s results from experiments involving the dmars procedural reasoning system and the claret pattern matching and induction system are described for reasoning with inconsistency in structured text reasoning with inconsistency involves some compromise on classical logic there is a range of proposals for logics called paraconsistent logics for reasoning with inconsistency each with pros and cons selecting an appropriate paraconsistent logic for an application depends on the requirements of the application here we review paraconsistent logics for the potentially significant application area of technology for structured text structured text is a general concept that is implicit in a variety of approaches to handling information syntactically an item of structured text is a number of grammatically simple phrases together with a semantic label for each phrase items of structured text may be nested within larger items of structured text the semantic labels in a structured text are meant to parameterize a stereotypical situation and so a particular item of structured text is an instance of that stereotypical situation much information is potentially available as st approximate query translation across heterogeneous information sources extended version in this paper we present a mechanism for approximately translating boolean query constraints across heterogeneous information sources achieving the best translation is challenging because sources support different constraints for formulating queries and often these constraints cannot be precisely translated for instance a query score 8 might be perfectly translated as rating 0 8 at some site but can only be approximated as grade a at another unlike other work our general framework adopts a customizable closeness metric for the translation that combines both precision and recall our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts as the basis we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units our algorithm then translates complex queries by rewriting them in terms of the semantic units we show that un principles for a usability oriented pattern language high level usability principles e g familiarity are difficult to apply to specific projects and style guides providing more detailed instructions are often misinterpreted and inaccessible an approach to usability based on design patterns enables designers to learn how certain recurring problems can be solved according to high level principles this paper summarises a review of the desirable properties advocated by five popular style guides and discusses how this list has been modified to provide an underlying philosophy which is appropriate for a usability oriented pattern language a sample pattern which exemplifies this philosophy involving iteration through selectable objects is described keywords usability engineering design techniques style guides 1 introduction there has been considerable discussion about how to reconcile the gaps between software engineering se and human computer interaction hci one of the primary ways to smoothly integrate the disciplines offering a precision performance tradeoff for aggregation queries over replicated data strict consistency of replicated data is infeasible or not required by many distributed applications so current systems often permit stale replication inwhich cached copies of data values are allowed to become out of date queries over cached data return an answer quickly but the stale answer may be unboundedly imprecise alternatively queries over remote master data return a precise answer but with potentially poor performance to bridge the gap between these two extremes we propose a new class of replication systems called trapp tradeoff in replication precision and performance trapp systems give each user fine grained control over the tradeoff between precision and performance caches store ranges that are guaranteed to bound the current data values instead of storing stale exact values users supply a quantitative precision constraint along with each query to answer a query trapp systems automatically select a combination of locally cached bounds and exact master data stored remotely to deliver a bounded answer consisting of a range that is no wider than the specified precision constraint that is guaranteed to contain the precise answer and that is computed as quickly as possible this paper defines the architecture of trapp replication systems and covers some mechanics of caching data ranges it then focuses on queries with aggregation presenting optimization algorithms for answering queries with precision constraints and reporting on performance experiments that demonstrate the fine grained control of the precision performance tradeoff offered by trapp systems erratic fudgets a semantic theory for an embedded coordination language the powerful abstraction mechanisms of functional programming languages provide the means to develop domain specific programming languages within the language itself typically this is realised by designing a set of combinators higher order reusable programs for an application area and by constructing individual applications by combining and coordinating individual combinators this paper is concerned with a successful example of such an embedded programming language namely fudgets a library of combinators for building graphical user interfaces in the lazy functional language haskell the fudget library has been used to build a number of substantial applications including a web browser and a proof editor interface to a proof checker for constructive type theory this paper develops a semantic theory for the non deterministic stream processors that are at the heart of the fudget concept the interaction of two features of stream processors makes the development of such a semantic theory problematic the sharing of computation provided by the lazy evaluation mechanism of the underlying host language and the addition of non deterministic choice needed to handle the natural concurrency that reactive applications entail we demonstrate that this combination of features in a higher order functional language can be tamed to provide a tractable semantic theory and induction principles suitable for reasoning about contextual equivalence of fudgets relational learning techniques for natural language information extraction the recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information one type of processing appropriate for many tasks is information extraction a type of text skimming that retrieves specific types of information from text although information extraction systems have existed for two decades these systems have generally been built by hand and contain domain specific information making them difficult to port to other domains a few researchers have begun to apply machine learning to information extraction tasks but most of this work has involved applying learning to pieces of a much larger system this paper presents a novel rule representation specific to natural language and a learning system rapier which learns information extraction rules rapier takes pairs of documents and filled templates indicating the information to be ext support vector machines for face authentication the paper studies support vector machines svms in the context of face authentication our study supports the hypothesis that the svm approach is able to extract the relevant discriminatory information from the training data we believe this is the main reason for its superior performance over benchmark methods when the representation space already captures and emphasises the discriminatory information content as in the case of fisherfaces svms loose their superiority svms can also cope with illumination changes provided these are adequately represented in the training data however on data which has been sanitised by feature extraction fisherfaces and or normalisation svms can get over trained resulting in the loss of the ability to generalise svms involve many parameters and can employ different kernels this makes the optimisation space rather extensive without the guarantee that it has been fully explored to find the best solution 1 introduction verificat an open framework for distributed multimedia retrieval this article describes a framework for distributed multimedia retrieval which permits the connection of compliant user interfaces with a variety of multimedia retrieval engines via an open communication protocol mrml multi media retrieval markup language it allows the choice of image collection feature set and query algorithm during run time permitting multiple users to query a system adapted to their needs using the query paradigm adapted to their problem such as query by example qbe browsing queries or query by annotation user interaction is implemented over several levels and in diverse ways relevance feedback is implemented using positive and negative example images that can be used for a best match qbe query in contrast browsing methods try to approach the searched image by giving overviews of the entire collection and by successive renement in addition to these query methods long term o line learning is implemented it allows feature preferences per designing synchronous user interface for collaborative applications synchronous user interface is a medium where all objects being shared on it can be viewed indifferently from the geographical location and its users can interact with each other in real time designing such an interface for users working collaboratively requires to deal with a number of issues herein our concerns lies on the design of control component of human computer interaction hci and corresponding user interface ui software that implements it we make use of our approach to interactive system development based on the mpx mapping from pan protagonist action notation into xchart extended statechart and illustrate it by presenting the case study of a collaborative application keywords pan mpx hci design ui software design 1 introduction to survive human beings need to organize themselves into a society differently from other animals that are able to live separately in reasonable manner human beings are endowed with physical and cognitive abilities ne process and agent based modelling techniques for dialogue systems and virtual environments this text presents results of ongoing research which is aimed at developing a framework for developing multimodal natural language dialogue systems operating within virtual environments the aspects of multimodality and presence in a virtual environment are chosen as the main focus of this research it may be argued that specification techniques would form the basis of such a framework therefore a general overview and evaluation is given of existing specification techniques for interactive systems based on both literature and previous research results this includes the object oriented model process algebras interactor models and agent systems agent systems are further subdivided into intentional logics production rule systems agent communication languages agent platforms and agent architectures a new agent system is proposed which is based on update notification mechanisms as found in interactor models and the facilitator function as found in some agent platfo document classification with unsupervised artificial neural networks text collections may be regarded as an almost perfect application arena for unsupervised neural networks this is because many operations computers have to perform on text documents are classification tasks based on noisy patterns in particular we rely on self organizing maps which produce a map of the document space after their training process from geography however it is known that maps are not always the best way to represent information spaces for most applications it is better to provide a hierarchical view of the underlying data collection in form of an atlas where starting from a map representing the complete data collection different regions are shown at finer levels of granularity using an atlas the user can easily zoom into regions of particular interest while still having general maps for overall orientation we show that a similar display can be obtained by using hierarchical feature maps to represent the contents of a document archive these neural networks have layerd architecture where each layer consists of a number of individual self organizing maps by this the contents of the text archive may be represented at arbitrary detail while still having the general maps available for global orientation use case maps as a feature description notation we propose use case maps ucms as a notation for describing features ucms capture functional requirements in terms of causal scenarios bound to underlying abstract components this particular view proved very useful in the description of a wide range of reactive and telecommunications systems this paper presents some of the most interesting constructs and benefits of the notation in relation to a question on a user requirements notation recently approved by itu t study group 10 which will lead to a new recommendation by 2003 tool support current research on ucms and related notations are also discussed 1 introduction the modeling of reactive systems requires an early emphasis on behavioral aspects such as interactions between the system and the external world including the users on the cause to e ect relationships among these interactions and on intermediate activities performed by the system scenarios are particularly good at representing such aspects so that various optimising propositional modal satisfiability for description logic subsumption effective optimisation techniques can make a dramatic difference in the performance of knowledge representation systems based on expressive description logics because of the correspondence between description logics and propositional modal logic many of these techniques carry over into propositional modal logic satisfiability checking currently implemented representation systems that employ these techniques such as fact and dlp make effective satisfiable checkers for various propositional modal logics 1 introduction description logics are a logical formalism for the representation of knowledge about individuals and descriptions of individuals description logics represent and reason with descriptions similar to all people whose friends are both doctors and lawyers or all people whose children are doctors or lawyers or who have a child who has a spouse the computations performed by systems that implement description logics are based around determining whether one descriptio managing historical semistructured data this article appeared in 6 macroscopic observation of multi robot behavior this paper presents a new approach to the observation and the control of the behavior of multiple autonomous robots it is the microscopic observation expressed by dynamic equations that has been commonly employed to observe the multi robot behavior however the approach has the difficulties in estimating the behavior and the mutual interactions of robots furthermore it is hard to realize the system by checking up all the factors of the system it seems that a macroscopic observation defined by state equations is efficient for recognizing the multiple robots behavior therefore we would like to propose a quantitative observation approach this attempt means the application of the thermodynamic macroscopic state values to the multi robot systems the advantage of this approach is that it enables us to observe the behavior of autonomous robots in real world by mapping the characteristic values of them in another conceptual state space first we discuss the implication of applying a foundation for representing and querying moving objects spatio temporal databases deal with geometries changing over time the goal of our work is to provide a dbms data model and query language capable of handling such time dependent geometries including those changing continuously which describe moving objects two fundamental abstractions are moving point and moving region describing objects for which only the time dependent position or position and extent are of interest respectively we propose to represent such time dependent geometries as attribute data types with suitable operations that is to provide an abstract data type extension to a dbms data model and query language this paper presents a design of such a system of abstract data types it turns out that besides the main types of interest moving point and moving region a relatively large number of auxiliary data types is needed for example one needs a line type to represent the projection of a moving point into the plane or a moving real to represent the time dependent distance of two moving points it then becomes crucial to achieve i orthogonality in the design of the type system i e type constructors can be applied uniformly ii genericity designing storyrooms interactive storytelling spaces for children limited access to space costly props and complicated authoring technologies are among the many reasons why children can rarely enjoy the experience of authoring roomsized interactive stories typically in these kinds of environments children are restricted to being story participants rather than story authors therefore we have begun the development of storyrooms room sized immersive storytelling experiences for children with the use of low tech and high tech storytelling elements children can author physical storytelling experiences to share with other children in the paper that follows we will describe our design philosophy design process with children the current technology implementation and example storyrooms keywords augmented environments storytelling children educational applications participatory design cooperative inquiry introduction a child sits in a playroom she tells a story to her dolls about her family another child sits at the dinner table wit learning to recognize objects a learning account for the problem of object recognition is developed within the pac probably approximately correct model of learnability the proposed approach makes no assumptions on the distribution of the observed objects but quantifies success relative to its past experience most importantly the success of learning an object representation is naturally tied to the ability to represent it as a function of some intermediate representations extracted from the image we evaluate this approach in a large scale experimental study in which the snow learning architecture is used to learn representations for the 100 objects in the columbia object image database coil 100 the snow based method is shown to outperform other methods in terms of recognition rates its performance degrades gracefully when the training data contains fewer views and in the presence of occlusion noise experiences using case based reasoning to predict software project effort this paper explores some of the practical issues associated with the use of case based reasoning cbr or estimation by analogy we note that different research teams have reported widely differing results with this technology whilst we accept that underlying characteristics of the datasets being used play a major role we also argue that configuring a cbr system can also have an impact we examine the impact of the choice of number of analogies when making predictions we also look at different adaptation strategies our analysis is based on a dataset of software projects collected by a canadian software house our results show that choosing analogies is important but adaptation strategy appears to be less so these findings must be tempered however with the finding that it was difficult to show statistical significance for smaller datasets even when the accuracy indicators differed quite substantially for this reason we urge some degree of caution when comparing competing predicti assessment methods for information quality criteria information quality iq is one of the most important aspects of information integration on the internet many projects realize and address this fact by gathering and classifying iq criteria hardly ever do the projects address the immense difficulty of assessing scores for the criteria this task must precede any usage of criteria for qualifying and integrating information after reviewing previous attempts to classify iq criteria in this paper we also classify criteria but in a new assessment oriented way we identify three sources for iq scores and thus three iq criterion classes each with different general assessment possibilities additionally for each criterion we give detailed assessment methods finally we consider confidence measures for these methods confidence expresses the accuracy lastingness and credibility of the individual assessment methods 1 introduction low information quality is one of the most pressing problems for consume rs of information that is di task oriented collaboration with embodied agents in virtual worlds we are working toward animated agents that can collaborate with human students in virtual worlds the agent s objective is to help students learn to perform physical procedural tasks such as operating and maintaining equipment like most of the previous research on task oriented dialogues the agent computer serves as an expert that can provide guidance to a human novice research on such dialogues dates back more than twenty years deutsch 1974 and the subject remains an active research area allen et al 1996 lochbaum 1994 walker 1996 however most of that research has focused solely on verbal dialogues even though the earliest studies clearly showed the ubiquity of nonverbal communication in human task oriented dialogues deutsch 1974 to allow a wider variety of interactions among agents and human students we use virtual reality durlach and mavor 1995 agents and students cohabit a threedimensional interactive simulated mock up of the student cmunited 97 robocup 97 small robot world champion team robotic soccer is a challenging research domain which involves multiple agents that need to collaborate in an adversarial environment to achieve specificobjectives in this paper we describe cmunited the team of small robotic agents that we developed to enter the robocup 97 competition we designed and built the robotic agents devised the appropriate vision algorithm and developed and implemented algorithms for strategic collaboration between the robots in an uncertain and dynamic environment the robots can organize themselves in formations hold specific roles and pursue their goals in game situations they have demonstrated their collaborative behaviors on multiple occasions we present an overview of the vision processing algorithm which successfully tracks multiple moving objects and predicts trajectories the paper then focusses on the agent behaviors ranging from low level individual behaviors to coordinated strategic team behaviors cmunited won the robocup 97 small robot competition at ijcai 97 in nagoya japan the adaptive agent architecture achieving faulttolerance using persistent broker teams brokers are used in many multi agent systems for locating agents for routing and sharing information for managing the system and for legal purposes as independent third parties however these multi agent systems can be incapacitated and rendered non functional when the brokers become inaccessible due to failures such as machine crashes network breakdowns and process failures that can occur in any distributed software system we propose that the theory of teamwork can be used to create robust brokered architectures that can recover from broker failures and we present the adaptive agent architecture aaa to show the feasibility of this approach the aaa brokers form a team with a joint commitment to serve any agent that registers with the broker team as long as the agent remains registered with the team this commitment enables the brokers to substitute for each other when needed a multiagent system based on the aaa can continue to work despite broker failures as long an object oriented multidimensional data model for olap online analytical processing olap data is frequently organized in the form of multidimensional data cubes each of which is used to examine a set of data values called measures associated with multiple dimensions and their multiple levels in this paper we first propose a conceptual multidimensional data model which is able to represent and capture natural hierarchical relationships among members within a dimension as well as the relationships between dimension members and measure data values hereafter dimensions and data cubes with their operators are formally introduced afterward we use uml unified modeling language to model the conceptual multidimensional model in the context of object oriented databases 1 introduction data warehouses and olap are essential elements of decision support 5 they enable business decision makers to creatively approach analyze and understand business problems 16 while data warehouses are built to store very large amounts of integrat combining multiple learning strategies for effective cross validation parameter tuning through cross validation becomes very difficult when the validation set contains no or only a few examples of the classes in the evaluation set we address this open challenge by using a combination of classifiers with different performance characteristics to effectively reduce the performance variance on average of the overall system across all classes including those not seen before this approach allows us to tune the combination system on available but less representative validation data and obtain smaller performance degradation of this system on the evaluation data than using a single method classifier alone we tested this approach by applying k nearest neighbor rocchio and language modeling classifiers and their combination to the event tracking problem in the topic detection and tracking tdt domain where new classes events are created constantly over time and representative validation sets for new classes are often difficult to ob rigid and articulated motion seen with an uncalibrated stereo rig this paper establishes a link between uncalibrated stereo vision and the motion of rigid and articulated bodies the variation in the projective reconstruction of a dynamic scene over time allows an uncalibrated stereo rig to be used as a faithful motion capturing device we introduce an original theoretical framework projective kinematics which allows rigid and articulated motion to be represented within the transformation group of projective space corresponding projective velocities are defined in the tangent space most importantly these projective motions inherit the lie group structure of the displacement group these theoretical results lead immediately to nonmetric formulations of visual servoing tracking motion capturing and motion synthesis systems that no longer require the metric geometry of a stereo camera or of the articulated body to be known we report on such a nonmetric formulation of a visual servoing system and present simulated experimental results 1 in subsumption for xml types xml data is often used validated stored queried etc with respect to different types understanding the relationship between these types can provide important information for manipulating this data we propose a notion of subsumption for xml to capture such relationships subsumption relies on a syntactic mapping between types and can be used for facilitating validation and query processing we study the properties of subsumption in particular the notion of the greatest lower bound of two schemas and show how this can be used as a guide for selecting a storage structure while less powerful than inclusion subsumption generalizes several other mechanisms for reusing types notably extension and refinement from xml schema and subtyping 1 introduction xml 5 is a data format for web applications as opposed to e g relational databases xml documents do not have to be created and used with respect to a fixed existing schema this is particularly useful in web ap scalable maintenance in distributed data warehousing environment the maintenance of data warehouses is becoming an increasingly important topic due to the growing use derivation and integration of digital information most previous work has dealt with one centralized data warehouse dw only in this paper we now focus on environments with multiple data warehouses that are possibly derived from other data warehouses in such a large scale environment data updates from base sources may arrive in individual data warehouses in different orders thus resulting in inconsistent data warehouse extents we propose a registry based solution strategy that addresses this problem by employing a registry agent responsible for establishing one unique order for the propagation of updates from the base sources to the data warehouses with this solution individual data warehouse managers can still maintain their respective extents autonomously and independently from each other thus allowing them to apply any of the existing incremental maintenance algo scalable information organization we present three scalable extensions of the star algorithm for information organization that use sampling the star algorithm organizes a document collection into clusters that are naturally induced by the topic structure of collection via a computationally efficient cover by dense subgraphs we also provide supporting data from extensive experiments 1 introduction our goal is to develop a completely automated information organization system for digital libraries automated tools for librarians to classify this information automatic tools to create reference pointers into such collections and automated tools that allow users to locate information effectively we focus on static and dynamic digital collections of unstructured text we consider the problem of determining the topic structure of text data without a priori knowledge of the number of topics in the data or any other information about their composition we assume that the collections may be static for example digi efficient and effective metasearch for text databases incorporating linkages among documents linkages among documents have a significant impact on the importance of documents as it can be argued that important documents are pointed to by many documents or by other important documents metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources text databases in a large scale metasearch engine the contents of each local database is represented by a representative each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search in previous works the linkage information between documents has not been utilized in determining the appropriate databases to search in this paper such information is employed to determine the degree of relevance of a document with respect to a given query this information is then stored in each database representative to facilitate the selection of databases for each given query we establish a necessary an audio signal classification audio signal classification asc consists of extracting relevant features from a sound and of using these features to identify into which of a set of classes the sound is most likely to fit the feature extraction and grouping algorithms used can be quite diverse depending on the classification domain of the application this paper presents background necessary to understand the general research domain of asc including signal processing spectral analysis psychoacoustics and auditory scene analysis also presented are the basic elements of classification systems perceptual and physical features are discussed as well as clustering algorithms and analysis duration neural nets and hidden markov models are discussed as they relate to asc these techniques are presented with an overview of the current state of the asc research literature 1 introduction this paper will present a review of the state of the current research literature pertaining to audio signal classifiation asc problem solving in id logic with aggregates some experiments the goal of the lp project at the k u leuven is to design an expressive logic suitable for declarative knowledge representation and to develop intelligent systems based on logic programming technology for solving computational problems using the declarative specications the id logic is an integration of typed classical logic and a denition logic dierent abductive solvers for this language are being developed this paper is a report of the integration of high order aggregates into id logic and the consequences on the solver sldnfa transformation based bottom up computation of the well founded model we present a bottom up algorithm for the computation of the well founded model of non disjunctive logic programs our method is based on the notion of conditional facts and elementary program transformations studied by bras and dix bd94 bd97 bd98b however their residual program can grow to exponential size whereas for function free programs our program remainder is always polynomial in the size of the extensional database edb our approach is also closely related to the alternating fixpoint procedure vg89 vg93 however the alternating fixpoint procedure is known to have inefficiencies due to redundant recomputations of possible facts by using conditional facts that can be deleted directly instead of recomputing the complement our approach is guaranteed to need not more work than the alternating fixpoint procedure and to be much more efficient in many cases the magic set transformation as a method for bottom up query answering is known to have problems with undefined magic facts in the context of the well founded semantics however our approach treats magic set transformed programs correctly i e it always computes a relevant part of the well founded model of the original program we show that our approach is guaranteed to need not more work than the well founded magic sets method kss95 or the magic alternating fixpoint procedure mor96 and is much more efficient for many programs again the use of conditional facts gives valuable information to select the right magic facts to use and to avoid redundant recomputations towards detection of human motion detecting humans in images is a useful application of computer vision loose and textured clothing occlusion and scene clutter make it a difficult problem because bottom up segmentation and grouping do not always work we address the problem of detecting humans from their motion pattern in monocular image sequences extraneous motions and occlusion may be present we assume that we may not rely on segmentation nor grouping and that the vision front end is limited to observing the motion of key points and textured patches in between pairs of frames we do not assume that we are able to track features for more than two frames our method is based on learning an approximate probabilistic model of the joint position and velocity of different body features detection is performed by hypothesis testing on the maximum a posteriori estimate of the pose and motion of the body our experiments on a dozen of walking sequences indicate that our algorithm is accurate and efficient exploring auction based leveled commitment contracting part iii vickrey type auctioning a key problem addressed in the area of multiagent systems is the automated assignment of multiple tasks to executing agents the automation of multiagent task assignment requires that the individual agents i use a common protocol that prescribes how they have to interact in order to come to an agreement and ii fix their final agreement in a contract that specifies the commitments resulting from the assignment on which they agreed this report describes a novel approach to automated task assignment in multiagent systems that is based on an auction based protocol and on leveled commitment contracting this approach is applicable in a broad range of realistic scenarios in which knowledge intensive negotiation among agents is not feasible and in which future environmental changes may require agents to breach their contracts 1 introduction the area of multiagent systems e g 5 8 16 which is concerned with systems composed of technical entities called agents that in data mining on an oltp system nearly for free this paper proposes a scheme for scheduling disk requests that takes advantage of the ability of high level functions to operate directly at individual disk drives we show that such a scheme makes it possible to support a data mining workload on an oltp system almost for free there is only a small impact on the throughput and response time of the existing workload specifically we show that an oltp system has the disk resources to consistently provide one third of its sequential bandwidth to a background data mining task with close to zero impact on oltp throughput and response time at high transaction loads at low transaction loads we show much lower impact than observed in previous work this means that a production oltp system can be used for data mining tasks without the expense of a second dedicated system our scheme takes advantage of close interaction with the on disk scheduler by reading blocks for the data mining workload as the disk head passes over them while satisfying demand blocks from the oltp request stream we show that this scheme provides a consistent level of throughput for the background workload even at very high foreground loads such a scheme is of most benefit in combination with an active disk environment that allows the background data mining application to also take advantage of the processing power and memory available directly on the disk drives this research was sponsored by darpa ito through arpa order d306 and issued building a large location table to find replicas of physics objects the problem of building a large location table for physics objects occurs within a number of planned physics data management systems like those that control reclustering and wide area replication to satisfy their e ciency goals these systems have to make local or remote replicas of individual physics objects which contain raw or reconstructed data for a single event rather than replicas of large run or ntuple files this replication implies the use of a table to resolve the logical location independent object descriptor into a physical location where an object replica can be found for modern physics experiments the table needs to scale to at least some 10 10 objects we argue that such a table can be e ciently implemented by limiting the freedom of lookup operations and by exploiting some specific properties of the physics data model one specific viable implementation is discussed key words object location table object oriented databases object clustering object re cl centroid based document classification analysis experimental results in this paper we present a simple linear time centroid based document classification algorithm that despite its simplicity and robust performance has not been extensively studied and analyzed our experiments show that this centroid based classifier consistently and substantially outperforms other algorithms such as naive bayesian k nearest neighbors and c4 5 on a wide range of datasets our analysis shows that the similarity measure used by the centroidbased scheme allows it to classify a new document based on how closely its behavior matches the behavior of the documents belonging to different classes this matching allows it to dynamically adjust for classes with different densities and accounts for dependencies between the terms in the different classes 1 introduction we have seen a tremendous growth in the volume of online text documents available on the internet digital libraries news sources and company wide intranets it has been forecasted that these docu integrating keyword search into xml query processing due to the popularity of the xml data format several query languages for xml have been proposed specially devised to handle data whose structure is unknown loose or absent while these languages are rich enough to allow for querying the content and structure of an xml document a varying or unknown structure can make formulating queries a very difficult task we propose an extension to xml query languages that enables keyword search at the granularity of xml elements that helps novice users formulate queries and also yields new optimization opportunities for the query processor we present an implementation of this extension on top of a commercial rdbms we then discuss implementation choices and performance results keywords xml query processing full text index 1 introduction there is no doubt that xml is rapidly becoming one of the most important data formats it is already used for scientific data e g dna sequences in linguistics e g the treebank database at the u organisational abstractions for the analysis and design of multi agent systems abstract the architecture of a multi agent system can naturally be viewed as a computational organisation for this reason we believe organisational abstractions should play a central role in the analysis and design of such systems to this end the concepts of agent roles and role models are increasingly being used to specify and design multi agent systems however this is not the full picture in this paper we introduce three additional organisational concepts organisational rules organisational structures and organisational patterns that we believe are necessary for the complete specification of computational organisations we view the introduction of these concepts as a step towards a comprehensive methodology for agent oriented systems 1 a multi version approach to conflict resolution in distributed groupware systems groupware systems are a special class of distributed computing systems which support human computer human interaction real time collaborative graphics editors allow a group of users to view and edit the same graphics document at the same time from geographically dispersed sites connected by communication networks resolving conflict access to shared objects is one of the core issues in the design of this type of systems this paper proposes a novel distributed multi version approach to conflict resolution this approach aims to preserve the work concurrently produced by multiple users in the face of conflicts and to minimize the number of object versions for accommodating combined effects of conflicting and compatible operations major technical contributions of this work include a formal specification of a unique combined effect for any group of conflicting and compatible operations a distributed algorithm for incremental creation of multiple object versions and a consistent objec non supervised sensory motor agents learning this text discusses a proposal for creation and destruction of neurons based on the sensory motor activity this model called sensory motor schema is used to define a sensory motor agent as a collection of activity schemata the activity schema permits a useful distribution of neurons in a conceptual space creating concepts based on action and sensation such approach is inspired in the theory of the swiss psychologist and epistemologist jean piaget and intends to make explicit the account of the processes of continuous interaction between sensory motor agents and their environments when agents are producing cognitive structures 1 introduction the notion of an autonomous agent plays a central role in contemporaneous research on artificial intelligence 3 cognitive agents are based on symbolic processing mechanisms reactive agents are based on alternative computational mechanisms like neural networks analogic processing etc the alternative approach using autonomous agents b leveled commitment contracts and strategic breach in automated negotiation systems for self interested agents contracts have traditionallybeen binding theydo not accommodate future events contingency contracts address this but are often impractical as an alternative we propose le eled commitment contracts the level of commitment is set bybreach penalties to be freed from the contract an agent simply pays the penalty to the other party a self interested agent will be reluctant to breach because the other partymight breach in which case the former agent is freed from the contract does not incur a penalty and collects a penalty from the breacher we show that despite such strategic breach leveled commitment increases the expected payoff to both contract parties and can enable deals that are impossible under full commitment asymmetric beliefs are also discussed different decommitting mechanisms are introduced and compared practical prescriptions for market designers are provided a contract optimizer is provided on the web journal ofeconomic literature essex wizards 99 team description this paper describes the essex wizards team participated in the robocup 99 simulator league it is mainly concentrated on a multi threaded implementation of simulated soccer agents to achieve real time performance simulated robot agents work at three distinct phases sensing thinking and acting posix threads are adopted to implement them concurrently the issues of decision making and co operation are also addressed 1 introduction in the robocup simulator environment the response time of a soccer agent becomes significantly important since the soccer server operates with 100ms cycles for executing actions and 150ms cycles for providing sensory data 12 moreover auditory sensory data can be received at random intervals it is vital that each agent has bounded response times if an action is not generated within 100ms then the agent will stay idle for that cycle and enemy agents that did act might gain an advantage on the other hand if more than one action is generat multi modal identity verification using support vector machines svm the contribution of this paper is twofold 1 to formulate a decision fusion problem encountered in the design of a multi modal identity verification system as a particular classification problem 2 to propose to solve this problem by a support vector machine svm the multi modal identity verification system under consideration is built of d modalities in parallel each one delivering as output a scalar number called score stating how well the claimed identity is verified a fusion module receiving as input the d scores has to take a binary decision accept or reject identity this fusion problem has been solved using support vector machines the performances of this fusion module have been evaluated and compared with other proposed methods on a multimodal database containing both vocal and visual modalities keywords decision fusion support vector machine multi modal identity verification 1 introduction automatic identification verification is rapidly becoming an importa learning human arm movements by imitation evaluation of a biologically inspired connectionist architecture this paper is concerned with the evaluation of a model of human imitation of arm movements the model consists of a hierarchy of artificial neural networks which are abstractions of brain regions involved in visuo motor control these are the spinal cord the primary and pre motor cortexes m1 pm the cerebellum and the temporal cortex a biomechanical simulation is developed which models the muscles and the complete dynamics of a 37 degree of freedom humanoid input to the model are data from human arm movements recorded using video and marker based tracking systems the model s performance is evaluated for reproducing reaching movements and oscillatory movements of the two arms results show a high qualitative and quantitative agreement with human data in particular the model reproduces the well known features of reaching movements in humans namely the bell shaped curves for the velocity and quasi linear hand trajectories finally the model s performance is compar electronic books in digital libraries 1 electronic book is an application with a multimedia database of instructional resources which include hyperlinked text instructor s audio video clips slides animation still images etc as well as content based information about these data and metadata such as annotations tags and cross referencing information electronic books in the internet or on cds today are not easy to learn from we propose the use of a multimedia database of instructional resources in constructing and delivering multimedia lessons about topics in an electronic book we introduce an electronic book data model containing a topic objects and b instructional resources called instruction module objects which are multimedia presentations possibly capturing real life lectures of instructors we use the notion of topic prerequisites for topics at different detail levels to allow electronic book users to request compose multimedia lessons about topics in the electronic book we present automated construction of the best user tailored lesson as a multimedia presentation 1 towards automating the evolution of linguistic competence in artificial agents the goal of this research is to understand and automate the mechanisms by which language can emerge among artificial knowledge based and rational agents we use the paradigm of rationality defined by decision theory and employ the formal model of negotiation studied in game theory to allow the emergence and enrichment of an agent communication language 1 introduction the aim of our research is to understand and automate the mechanisms by which language can emerge among artificial knowledge based and rational agents our ultimate goal is to be able to design and implement agents that upon encountering other agent s with which they do not share an agent communication language are able to initiate creation of and further able to evolve and enrich a mutually understandable agent communication language acl this paper outlines the overall approach we are taking and identifies some of the basic concepts and tools that we think are necessary to accomplish our goal first the a database replication using epidemic communication there is a growing interest in asynchronous replica management protocols in which database transactions are executed locally and their effects are incorporated asynchronously on remote database copies in this paper we investigate an epidemic update protocol that guarantees consistency and serializability in spite of a write anywhere capability and conduct simulation experiments to evaluate this protocol our results indicate that this epidemic approach is indeed a viable alternative to eager update protocols for a distributed database environment where serializability is needed 1 introduction data replication in distributed databases is an important problem that has been investigated extensively in spite of numerous proposals the solution to efficient access of replicated data remains elusive data replication has long been touted as a technique for improved performance and high reliability in distributed databases unfortunately data replication has not delivered on a neural network diagnosis model without disorder independence assumption generally the disorders in a neural network diagnosis model are assumed independent each other in this paper we propose a neural network model for diagnostic problem solving where the disorder independence assumption is no longer necessary firstly we characterize the diagnostic tasks and the causal network which is used to represent the diagnostic problem then we describe the neural network diagnosis model finally some experiment results will be given 1 introduction finding explanations for a given set of events is an important aspect of general intelligent behaviour the process of finding the best explanation was defined as abduction by the philosopher c s peirce 8 diagnosis is a typical abductive problem for a set of manifestations observations the diagnostic inference is to find the most plausible faults or disorders which can explain the manifestations observed in general an individual fault or disorder can explain only a portion of the manifestations ther design issues for mixed initiative agent systems this paper addresses the effect of mixed initiative systems on multiagent systems design a mixed initiative system is one in which humans interact directly with software agents in a collaborative approach to problem solving there are two main levels at which multiagent systems are designed the domain level and the individual agent level at the domain level there are few unique challenges to mixedinitiative system design however at the individual agent level the agent itself must be designed to interact with the human and the agent system integrating the two into a single system introduction much of the current research related to intelligent agents has focused on the capabilities and structure of individual agents however in order to solve complex problems these agents must work cooperatively with other agents in a heterogeneous environment this is the domain of multiagent systems in multiagent systems we are interested in the coordinated behavior of a system of indiv an open framework for distributed multimedia retrieval this article describes a framework for distributed multimedia retrieval which permits the connection of compliant user interfaces with a variety of multimedia retrieval engines via an open communication protocol mrml multi media retrieval markup language it allows the choice of image collection feature set and query algorithm during run time permitting multiple users to query a system adapted to their needs using the query paradigm adapted to their problem such as query by example qbe browsing queries or query by annotation user interaction is implemented over several levels and in diverse ways relevance feedback is implemented using positive and negative example images that can be used for a best match qbe query in contrast browsing methods try to approach the searched image by giving overviews of the entire collection and by successive renement in addition to these query methods long term o line learning is implemented it allows feature preferences per is paper safer the role of paper flight strips in air traffic control air traffic control is a complex safety critical activity with well established and successful work practices yet many attempts to automate the existing system have failed because controllers remain attached to a key work artifact the paper flight strip this article describes a four month intensive study of a team of paris en route controllers in order to understand their use of paper flight strips the article also describes a comparison study of eight different control rooms in france and the netherlands our observations have convinced us that we do not know enough to simply get rid of paper strips nor can we easily replace the physical interaction between controllers and paper strips these observations highlight the benefits of strips including qualities difficult to quantify and replicate in new computer systems current thinking offers two basic alternatives maintaining the existing strips without computer support and bearing the financial cost of limiting the air traff a behaviour based approach to position selection for simulated soccer agents selecting an optimal position for each soccer robot to move to in a robot football game is a challenging and complex task since the environment and robot motion are so dynamic and unpredictable this paper provides an overview of behaviour based position selection schemes used by essex wizards 99 simulated soccer team a third place in robocup 99 simulator league at stockholm the focus concentrates on how each position selection behaviour is selected for individual robot agents the factors that need to be considered and the architecture used to implement such position selection are also described finally the team performance at robocup 99 is examined and future extensions are proposed 1 introduction the robot world cup initiative robocup is an attempt to foster ai and intelligent robotics research by providing a uniform task the game of soccer 4 some of the software technologies include design principles of autonomous agents multi agent collaboration strategy acquisitio multi robot learning in a cooperative observation task an important need in multi robot systems is the development of mechanisms that enable robot teams to autonomously generate cooperative behaviors this paper rst briey presents the cooperative multi robot observation of multiple moving targets cmommt application as a rich domain for studying the issues of multi robot learning of new behaviors we discuss the results of our handgenerated algorithm for cmommt and then describe our research in generating multi robot learning techniques for the cmommt application comparing the results to the hand generated solutions our results show that while the learning approach performs better than random naive approaches much room still remains to match the results obtained from the hand generated approach the ultimate goal of this research is to develop techniques for multi robot learning and adaptation that will generalize to cooperative robot applications in many domains thus facilitating the practical use of multi robot teams in a wid implicit interest indicators recommender systems provide personalized suggestions about items that users will find interesting typically recommender systems require a user interface that can intelligently determine the interest of a user and use this information to make suggestions the common solution explicit ratings where users tell the system what they think about a piece of information is well understood and fairly precise however having to stop to enter explicit ratings can alter normal patterns of browsing and reading a more intelligent method is to use implicit ratings where a rating is obtained by a method other than obtaining it directly from the user these implicit interest indicators have obvious advantages including removing the cost of the user rating and that every user interaction with the system can contribute to an implicit rating current recommender systems mostly do not use implicit ratings nor is the ability of implicit ratings to predict actual user interest well unders incremental computation and maintenance of temporal aggregates abstract we consider the problems of computing aggregation queries in temporal databases and of maintaining materialized temporal aggregate views efficiently the latter problem is particularly challenging since a single data update can cause aggregate results to change over the entire time line we introduce a new index structure called the sb tree which incorporates features from both segment trees and b trees sb trees support fast lookup of aggregate results based on time and can be maintained efficiently when the data change we extend the basicsb tree index to handle cumulative also called moving window aggregates considering separately cases when the window size is or is not fixed in advance for materialized aggregate views in a temporal database or warehouse we propose building and maintaining sb tree indices instead of the views themselves declarative semantics of belief queries in mls deductive databases extended abstract a logic based language called multilog for multi level secure relational databases has recently been proposed it has been shown that multilog is capable of capturing the notion of user belief of ltering unwanted and useless information in its proof theory additionally it can guard against a previously unknown security breach the so called surprise stories in this paper we outline a possible approach to a declarative characterization of belief queries in multilog in a very informal manner we show that for simple programs with belief queries the semantics is rather straight forward semantics for the general horn programs may be developed based on the understanding of the model theoretic characterization of belief queries developed in this paper keywords multi level security belief queries declarative semantics completeness introduction in a recent research jukic and vrbsky 8 demonstrate that users in the relational mls model potentially have a cluttered view appearance based place recognition for topological localization this paper presents a new appearance based place recognition system for topological localization the method uses a panoramic vision system to sense the environment color images are classified in real time based on nearest neighbor learning image histogram matching and a simple voting scheme the system has been evaluated with eight cross sequence tests in four unmodified environments three indoors and one outdoors in all eight cases the system successfully tracked the mobile robot s position the system correctly classified between 87 and 98 of the input color images for the remaining images the system was either momentarily confused or uncertain but never classified an image incorrectly 1 introduction localization is a fundamental problem in mobile robotics most mobile robots must be able to locate themselves in their environment in order to accomplish their tasks since mobile robot localization is a prerequisite for most applications research has been very active i maximizing coverage of mediated web queries over the web mediators are built on large collections of sources to provide integrated access to web content e g meta search engines in order to minimize the expense of visiting a large number of sources mediators need to choose a subset of sources to contact when processing queries as fewer sources participate in processing a mediated query the coverage of the query goes down in this paper we study this trade off and develop techniques for mediators to maximize the coverage for their queries while at the same time visiting a subset of their sources we formalize the problem study its complexity propose algorithms to solve it and analyze the theoretical performance guarantees of the algorithms we also study the performance of our algorithms through simulation experiments 1 introduction web sources often provide limited information coverage for instance one type of information source is search engines such as lycos 27 northern light 29 and yahoo 30 long term learning from user behavior in content based image retrieval this article describes an algorithm for obtaining knowledge about the importance of features from analyzing user log les of a content based image retrieval system cbirs the user log les from the usage of the viper web demonstration system are analyzed over a period of four months within this period about 3500 accesses to the system were made with almost 800 multiple image queries all the actions of the users were logged in a le the analysis only includes multiple image queries of the system with positive and or negative input images because only multiple image queries contain enough information for the method described features frequently present in images marked together positively in the same query step get a higher weighting whereas features present in one image marked positively and another image marked negatively in the same step get a lower weighting the viper system oers a very large number of simple features this allows the creation of exible feature reclustering of hep data in object oriented databases the large hadron collider lhc build at cern will enter operation in 2005 the experiments at the lhc will generate some 5 pb of data per year which are stored in an odbms a good object clustering on the disk drives will be critical to achieve a high data throughput required by future analysis scenarios this paper presents a new reclustering algorithm for hep data that maximizes the read transfer rate for objects contained in multiple overlapping collections it works by decomposing the stored objects into a number of chunks and rearranging them by means of heuristics solving the traveling salesman problem with hamming distance furthermore experimental results of a prototype are presented keywords object oriented databases scientific databases object clustering query optimisation 1 introduction the atlas experiment 1 at cern due to take data in the year 2005 will store approximately 1 pb 10 15 bytes of data per year data taking is expected to last 15 or more yea evaluation challenges for a federation of heterogeneous information providers the case of nasa s earth science information partnerships nasa s earth science information partnership federation is an experiment funded to assess the ability of a group of widely heterogeneous earth science data or service providers to self organize and provide improved and cheaper access to an expanding earth science user community as it is organizing itself the federation is mandated to set in place an evaluation methodology and collect metrics reflecting the health and benefits of the federation this paper describes the challenges of organizing such a federated partnership self evaluation and discusses the issues encountered during the metrics definition phase of the early data collection keyword metrics quantitative evaluation qualitative evaluation earth science 1 introduction beside the obvious need to evaluate any experiment to measure its positive and negative impact the impact of the government performance and results act gpra is slowly changing the way federal projects are being conducted quantitative and qualitative learning motor skills by imitation a biologically inspired robotic model this article presents a biologically inspired model for motor skills imitation the model is composed of modules whose functinalities are inspired by corresponding brain regions responsible for the control of movement in primates these modules are high level abstractions of the spinal cord the primary and premotor cortexes m1 and pm the cerebellum and the temporal cortex each module is modeled at a connectionist level neurons in pm respond both to visual observation of movements and to corresponding motor commands produced by the cerebellum as such they give an abstract representation of mirror neurons learning of new combinations of movements is done in pm and in the cerebellum premotor cortexes and cerebellum are modeled by the drama neural architecture which allows learning of times series and of spatio temporal invariance in multimodal inputs the model is implemented in a mechanical simulation of two humanoid avatars the imitator and the imitatee three types of sequences learning are presented 1 learning of repetitive patterns of arm and leg movements 2 learning of oscillatory movements of shoulders and elbows using video data of a human demonstration 3 learning of precise movements of the extremities for grasp and reach a general learning approach to multisensor based control using statistic indices we propose a concept for integrating multiple sensors in real time robot control to increase the controller robustness under diverse uncertainties the robot systematically generates series of sensor data as robot state while memorising the corresponding motion parameters from the collection of multi sensor trajectories statistical indices like principal components for each sensor type can be extracted if the sensor data are preselected as output relevant these principal components can be used very efficiently to approximately represent the original perception scenarios after this dimension reduction procedure a non linear fuzzy controller e g a b spline type can be trained to map the subspace projection into the robot control parameters we apply the approach to a real robot system with two arms and multiple vision and force torque sensors these external sensors are used simultaneously to control the robot arm performing insertion and screwing operations the successful experiments show that the robustness as well as the precision of robot control can be enhanced by integrating multiple additional sensors using this concept 1 naive bayes and exemplar based approaches to word sense disambiguation revisited this paper describes an experimental comparison between two standard supervised learning methods namely naive bayes and exemplar based classification on the word sense disambiguation wsd problem the aim of the work is twofold firstly it attempts to contribute to clarify some confusing information about the comparison between both methods appearing in the related literature in doing so several directions have been explored including testing several modifications of the basic learning algorithms and varying the feature space secondly an improvement of both algorithms is proposed in order to deal with large attribute sets this modification which basically consists in using only the positive information appearing in the examples allows to improve greatly the efficiency of the methods with no loss in accuracy the experiments have been performed on the largest sense tagged corpus available containing the most frequent and ambiguous english words results show that the exemplar based approach to wsd is generally superior to the bayesian approach especially when a specific metric for dealing with symbolic attributes is used distance metrics and indexing strategies for a digital library of popular music introduction digital libraries until now could hardly be described as popular they tend to be based on esoteric scholarly sources close to the interests of digital library researchers themselves we are developing a digital library containing the quintessence of popular culture music the principal mode of searching this library will be by sung query the system should be able to answer the kinds of queries that shop assistants in music stores deal with every day where a customer can sing a tune but can t remember the title or artist the operation of our musical digital library is sketched in figure 1 at the time of collection creation midi files are gathered from the internet and indexed based on their note sequences at retrieval time a user s sung query is transformed from a waveform to a sequence of pitch duration events by a pitch tracker we are using an off the shelf pitch tracker pitch tracking is beyond the scope of the current project pot observer an approach for query processing in global information systems based on interoperation across pre existing ontologies there has been an explosion in the types availability and volume of data accessible in an information system thanks to the world wide web the web and related inter networking technologies in this environment there is a critical need to replace or complement earlier database integration approaches and current browsing and keyword based techniques with concept based approaches ontologies are increasingly becoming accepted as an important part of any concept or semantics based solution and there is increasing realization that any viable solution will need to support multiple ontologies that may be independently developed and managed in particular we consider the use of concepts from pre existing real world domain ontologies for describing the content of the underlying data repositories the most challenging issue in this approach is that of vocabulary sharing which involves dealing with the use of different terms or concepts to describe similar information in this paper we about knowledge discovery in texts a definition and an example this paper claims that knowledge discovery in texts kdt is a new scientific topic stemming from knowledge discovery in databases kdd both of them relying on a rather specific definition of what knowledge is it has to be as it is often said understandable and useful and even surprising we shall rapidly explore this definition of knowledge and apply it to a definition of what kdt is we shall then illustrate on a detailed example what kind of new results kdt can bring showing their interest and that they have little to do with the results of the long existing natural language processing nlp research field discovering comprehensible classification rules with a genetic algorithm this work presents a classification algorithm based on genetic algorithms gas that discovers comprehensible if then rules in the spirit of data mining the proposed ga has a flexible chromosome encoding where each chromosome corresponds to a classification rule although the number of genes genotype is fixed the number of rule conditions phenotype is variable the ga also has specific mutation operators for this chromosome encoding the algorithm was evaluated on two public domain realworld data sets on the medical domains of dermatology and breast cancer 1 introduction this work presents a system based on genetic algorithms gas to perform the task of classification the system is evaluated in two medical domains diagnosis of dermatological diseases and prediction of recurrence of breast cancer the use of gas in classification is an attempt to effectively exploit the large search space usually associated with classification tasks the ga presented here was designed ac projective translations and affine stereo calibration this paper investigates the structure of projective translations rigid translations expressed as homographies in projective space a seven parameter representation is proposed which explicitly represents the geometric entities constraining and defining the translation a practical algebraic method for estimating these parameters is developed it provides affine calibration of a stereo rig determines the translation axis and allows projective translations to be composed the practical effectiveness of the calibration is evaluated on synthetic and real image data 1 introduction the recovery of structure and motion is a basic problem in machine vision the difficulties increase when uncalibrated cameras are considered since in this case metric information is missing nevertheless the rigidity of 3d motions imposes strong consistency constraints this allows metric information to be extracted mathematically given a set of points reconstructed in projective space the effect of the 3w model and algebra for unified data mining real data mining analysis applications call for a framework which adequately supports knowledge discovery as a multi step process where the input of one mining operation can be the output of another previous studies primarily focusing on fast computation of one specific mining task at a time ignore this vital issue motivated by this observation we develop a unified model supporting all major mining and analysis tasks our model consists of three distinct worlds corresponding to intensional and extensional dimensions and to data sets the notion of dimension is a centerpiece of the model equipped with hierarchies dimensions integrate the output of seemingly dissimilar mining and analysis operations in a clean manner we propose an algebra called the dimension algebra for manipulating intensional dimensions as well as operators that serve as bridges between the worlds we demonstrate by examples that several real data mining processes can be captured inference and learning in hybrid bayesian networks we survey the literature on methods for inference and learning in bayesian networks composed of discrete and continuous nodes in which the continuous nodes have a multivariate gaussian distribution whose mean and variance depends on the values of the discrete nodes we also briefly consider hybrid dynamic bayesian networks an extension of switching kalman filters this report is meant to summarize what is known at a sufficient level of detail to enable someone to implement the algorithms but without dwelling on formalities 1 1 introduction we discuss bayesian networks bns jen96 in which each node is either discrete or continuous scalar or vector valued and in which the joint distribution over all the nodes is conditional gaussian cg lw89 lau92 i e for each instantiation i of the discrete nodes y the distribution over the continuous nodes x has the form f xjy i n x i sigma i where n represents a multivariate gaussian mvg or normal density note modeling spatial dependencies for mining geospatial data an introduction spatial data mining is a process to discover interesting potentially useful and high utility patterns embedded in spatial databases efficient tools for extracting information from spatial data sets can be of importance to organizations which own generate and manage large spatial data sets the current approach towards solving spatial data mining problems is to use classical data mining tools after materializing spatial relationships however the key property of spatial data is that of spatial autocorrelation like temporal data spatial data values are influenced by values in their immediate vicinity ignoring spatial autocorrelation in the modeling process leads to results which are a poor fit and unreliable in this chapter we will first review spatial statistical techniques which explictly model spatial autocorrelation second we will propose plums predicting locations using map similarity a new approach for supervised spatial data mining problems plums searches the space of solutions using a map similarity measure which is more appropriate in the context of spatial data we will show that compared to state of the art spatial statistics approaches plums achives comparable accuracy but at a fraction of the computational cost furthermore plums provides a general framework for specializing other data mining techniques for mining spatial data learning to order things wcohen schapire singer there are many applications in which it is desirable to order rather than classify instances here we consider the problem of learning how to order given feedback in the form of preference judgments i e statements to the effect that one instance should be ranked ahead of another we outline a two stage approach in which one first learns by conventional means a preference function of the form pref which indicates whether it is advisable to rank before new instances are then ordered so as to maximize agreements with the learned preference function we show that the problem of finding the ordering that agrees best with a preference function is np complete even under very restrictive assumptions nevertheless we describe a simple greedy algorithm that is guaranteed to find a good approximation we then discuss an on line learning algorithm based on the hedge algorithm for finding a good linear combination of ranking experts we use the ordering algorithm combined with the on line learning algorithm to find a combination of search experts each of which is a domain specific query expansion strategy for a www search engine and present experimental results that demonstrate the merits of our approach 1 scheduling queries on tape resident data advances in storage technology have made near line tertiary storage a viable alternative for database and data warehouse systems tertiary storage systems are employed in cases where secondary storage can not satisfy the data handling requirements or tertiary storage is more cost effective option tertiary storage devices have traditionally been used as archival storage the new application domains require on demand retrieval of data from these devices this paper investigates issues in optimizing i o time for a query whose data resides on automated tertiary storage containing multiple storage devices we model the problem as a limited storage parallel two machine flow shop scheduling problem with additional constraints given a query we establish an upper bound on the number of storage devices for an optimal i o schedule and provide experimental proof for it for queries that access small amounts of data from multiple media we derive an optimal schedule analytically for q management and query processing of one dimensional intervals with the ub tree the management and query processing of one dimensional intervals is a special case of extended object handling one dimensional intervals play an important role in temporal databases and they can also be used for fuzzy matching fuzzy logic and measuring quality classes etc most existing multidimensional access methods for extended objects do not address this special problem and most of them are main memory access methods that do not support e cient access to secondary storage the research in the application of the ub tree to extended objects is part of my doctoral work the contribution of this article is a specific solution for managing and querying one dimensional intervals with the ub tree a multidimensional extension of the classical b tree the combination of ub tree and transformation of extended objects to parameter space is an e ective solution for this specific problem keywords one dimensional intervals extended object handling point query range query spatial data dquob managing large data flows using dynamic embedded queries the dquob system satisfies client need for specific information from high volume data streams the data streams we speak of are the flow of data existing during large scale visualizations video streaming to large numbers of distributed users and high volume business transactions we introduces the notion of conceptualizing a data stream as a set of relational database tables so that a scientist can request information with an sql like query transformation or computation that often needs to be performed on the data en route can be conceptualized ascomputation performed onconsecutive views of the data with computation associated with each view the dquob system moves the query code into the data stream as a quoblet as compiled code the relational database data model has the significant advantage of presenting opportunities for efficient reoptimizations of queries and sets of queries using examples from global atmospheric modeling we illustrate the usefulness of the dquob system we carry the examples through the experiments to establish the viability of the approach for high performance computing with a baseline benchmark we define a cost metric of end to end latency that can be used to determine realistic cases where optimization should be applied finally we show that end to end latency can be controlled through a probability assigned to a query that a query will evaluate to true evolution and revolutions in ldap directory caches ldap directories have recently proliferated with the growth of the internet and are being used in a wide variety of network based applications in this paper we propose the use of generalized queries referred to as query templates obtained by generalizing individual user queries as the semantic basis for low overhead high benefit ldap directory caches for handling declarative queries we present efficient incremental algorithms that given a sequence of user queries maintain a set of potentially beneficial candidate query templates and select a subset of these candidates for admission into the directory cache a novel feature of our algorithms is their ability to deal with overlapping query templates finally we demonstrate the advantages of template caches over query caches with an experimental study based on real data and a prototype implementation of the ldap directory cache 1 introduction ldap lightweight directory access protocol network directories ha planned disconnections for mobile databases as mobility permeates todays computing environment we envision application infrastructures that will increasingly use mobile technologies traditional database applications will need to integrate mobile entities people and computers in this paper we develop a distributed database framework for mobile environments a key requirement in such an environment is to support frequent connection and disconnection of database sites 1 introduction as mobility permeates into todays computing and communication arena we envision application infrastructures that will increasingly rely on mobile technologies current mobility applications tend to have a large central server and use mobile platforms only as caching devices we want to elevate the role of mobile computers to first class entities in the sense that they allow the mobile user work update capabilities independent of a central server in such an environment several mobile computers may collectively form the entire distributed syste a scalable algorithm for answering queries using views the problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database rather than accessing the database relations the problem has received significant attention because of its relevance to a wide variety of data management problems such as data integration query optimization and the maintenance of physical data independence to date the performance of proposed algorithms has received very little attention and in particular their scale up in the presence of a large number of views is unknown we first analyze two previous algorithms the bucket algorithm and the inverse rules algorithm and show their deficiencies we then describe the minicon algorithm a novel algorithm for finding the maximally contained rewriting of a conjunctive query using a set of conjunctive views we present the first experimental study of algorithms for answering queries using views the study shows that the minicon algorithm scales up well and significantly outperforms the previous algorithms finally we describe an extension of the minicon algorithm to handle comparison predicates and show its performance experimentally thanks to daniela florescu marc friedman zack ives ioana manolescu dan weld and steve wolfman for their comments on earlier drafts of this paper this research was funded by a sloan fellowship nsf grant iis 9978567 a nsf graduate research fellowship and a lucent technologies grpw grant permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage the vldb copyright notice and the title of the publication and its date appear and notice is given tha towards automatic discovery of object categories we propose a method to learn heterogeneous models of object classes for visual recognition the training images contain a preponderance of clutter and learning is unsupervised our models represent objects as probabilistic constellations of rigid parts features the variability within a class is represented by a joint probability density function on the shape of the constellation and the appearance of the parts our method automatically identifies distinctive features in the training set the set of model parameters is then learned using expectation maximization see the companion paper 11 for details when trained on different unlabeled and unsegmented views of a class of objects each component of the mixture model can adapt to represent a subset of the views similarly different component models can also specialize on sub classes of an object class experiments on images of human heads leaves from different species of trees and motor cars demonstrate that the method works from pets to storykit creating new technology with an intergenerational design team working with children as our design partners our intergenerational design team at the university of maryland has been developing both new design methodologies and new storytelling technology for children in this paper we focus on two recent results of our efforts pets a robotic storyteller and storykit a construction kit of low tech and high tech components to build immersive storyrooms we then describe some lessons we learned introduction over the past two years our intergenerational design team at the university of maryland has been developing new design methodologies to create new storytelling technology for children this team is made of six adult researchers from computer science education art and engineering and seven children ages 7 to 11 from local elementary schools these children stay with us for a long term at least for one year the adults are undergraduate students graduate students and faculty from art education engineering and computer sc view based query processing and constraint satisfaction view based query processing requires to answer a query posed to a database only on the basis of the information on a set of views which are again queries over the same database this problem is relevant in many aspects of database management and has been addressed by means of two basic approaches namely query rewriting and query answering in the former approach one tries to compute a rewriting of the query in terms of the views whereas in the latter one aims at directly answering the query based on the view extensions we study view based query processing for the case of regular path queries which are the basic querying mechanisms for the emergent field of semistructured data based on recent results we first show that a rewriting is in general a co np function wrt to the size of view extensions hence the problem arises of characterizing which instances of the problem admit a rewriting that is ptime a second contribution of the work is to establish a tight connection between view based query answering and constraint satisfaction problems which allows us to show that the above characterization is going to be difficult as a third contribution of our work we present two methods for computing ptime rewritings of specific forms the first method which is based on the established connection with constraint satisfaction problems gives us rewritings expressed in datalog with a fixed number of variables the second method based on automata theoretic techniques gives us rewritings that are formulated as unions of conjunctive regular path queries with a fixed number of variables towards active logic programming in this paper we present the new logic programming language dali aimed at defining agents and agent systems a main design objective for dali has been that of introducing in a declarative fashion all the essential features while keeping the language as close as possible to the syntax and semantics of the plain horn clause language special atoms and rules have been introduced for representing external events to which the agent is able to respond reactivity actions reactivity and proactivity internal events previous conclusions which can trigger further activity past and present events to be aware of what has happened an extended resolution is provided so that a dali agent is able to answer queries like in the plain horn clause language but is also able to cope with the different kinds of events and exhibit a rational reactive and proactive behaviour 1 introduction in this paper we address the issue of defining a logic programming language for reac mind warping towards creating a compelling collaborative augmented reality game computer gaming offers a unique test bed and market for advanced concepts in computer science such as human computer interaction hci computer supported collaborative work cscw intelligent agents graphics and sensing technology in addition computer gaming is especially wellsuited for explorations in the relatively young fields of wearable computing and augmented reality ar this paper presents a developing multi player augmented reality game patterned as a cross between a martial arts fighting game and an agent controller as implemented using the wearable augmented reality for personal intelligent and networked gaming warping system through interactions based on gesture voice and head movement input and audio and graphical output the warping system demonstrates how computer vision techniques can be exploited for advanced intelligent interfaces keywords augmented reality wearable computing computer vision 1 introduction why games computer gaming provides user modeling for information access based on implicit feedback user modeling can be used in information filtering and retrieval systems to improve the representation of a user s information needs user models can be constructed by hand or learned automatically based on feedback provided by the user about the relevance of documents that they have examined by observing user behavior it is possible to infer implicit feedback without requiring explicit relevance judgments previous studies based on internet discussion groups usenet news have shown reading time to be a useful source of implicit feedback for predicting a user s preferences the study reported in this paper extends that work by providing framework for considering alternative sources of implicit feedback examining whether reading time is useful for predicting a user s preferences for academic and professional journal articles and exploring whether retention behavior can usefully augment the information that reading time provides two user studies were conducted in which undergradua tabling for non monotonic programming this paper we describe tabling as it is implemented in the xsb system active disks programming model algorithms and evaluation several application and technology trends indicate that it might be both pro table and feasible to move computation closer to the data that it processes in this paper we evaluate active disk architectures which integrate signi cant processing power and memory into a disk drive and allow application speci c code to be downloaded and executed on the data that is being read from written to disk the key idea is to o oad bulk of the processing to the diskresident processors and to use the host processor primarily for coordination scheduling and combination of results from individual disks to program active disks we propose a stream based programming model which allows disklets to be executed e ciently and safely simulation results for a suite of six algorithms from three application domains commercial data warehouses image processing and satellite data processing indicate that for these algorithms active disks outperform conventional disk architectures 1 introduction severa using an explicit teamwork model and learning in robocup an extended abstract stacy marsella jafar adibi yaser al onaizan ali erdem randall hill gal a kaminka zhun qiu milind tambe information sciences institute and computer science department university of southern california 4676 admiralty way marina del rey ca 90292 usa robocup sim isi edu 1 introduction the robocup research initiative has established synthetic and robotic soccer as testbeds for pursuing research challenges in artificial intelligence and robotics this extended abstract focuses on teamwork and learning two of the multiagent research challenges highlighted in robocup to address the challenge of teamwork we discuss the use of a domain independent explicit model of teamwork and an explicit representation of team plans and goals we also discuss the application of agent learning in robocup the vehicle for our research investigations in robocup is isis isi synthetic a team of synthetic soccer players that successfully participated in the simulation league of robocup 97 by win an indexed bibliography of genetic algorithms and neural networks a computer based simulation with artificial adaptive agents for predicting secondary structure from the protein hydrophobicity ffl 79 abstract appl of gas to de novo design of therapeutic peptides ffl of a poster 77 assigning a protein sequence to a three dimensional fold ffl of a poster 78 design of a three helix bundle with a native like folded state ffl of a poster 82 resolving water mediated and polar ligand recognition using gas ffl of a poster 60 actinomycin gas for docking of ffl d and deoxyguanosine molecules with comparison to the crystal structure of ffl d deoxyguanosine complex 118 adaptive a computer based simulation with artificial ffl agents for predicting secondary structure from the protein hydrophobicity abstract 75 ag 1343 molecular recognition of the inhibitor ffl by hiv 1 protease conformationally flexible docking by ep 118 agents a computer based simulation with artificial adaptive ffl for predicting secondary structure from th improved algorithms for optimal winner determination in combinatorial auctions and generalizations combinatorial auctions i e auctions where bidders can bid on com binations of items tend to lead to more e cient allocations than tra ditional auctions in multi item auctions where the agents valuations of the items are not additive however determining the winners so as to maximize revenue is np complete first existing approaches for tackling this problem are reviewed exhaustive enumeration dynamic programming approximation algorithms and restricting the allow able combinations then we present our search algorithm for optimal winner determination experiments are shown on several bid distri butions the algorithm allows combinatorial auctions to scale up to signi cantly larger numbers of items and bids than prior approaches to optimal winner determination by capitalizing on the fact that the space of bids is necessarily sparsely populated in practice the algo rithm does this by provably su cient selective generation of children in the search tree by using a secondary search for fast child genera tion by heuristics that are accurate and optimized for speed and by four methods for preprocessing the search space patent pending a highly optimized implementation of the algorithm is available for licensing both for research and commercial purposes please contact the author 1 1 towards agent oriented information systems this paper reviews several relevant agent concepts developed in the intelligent agents and multiagent systems area and makes suggestions how to extend current is technology on the basis of these concepts since it presents work in progress the 3 implementation and performance analysis of incremental equations of nested relations view materialization is an important way of improving the performance of query processing when an update occurs to the source data from which a materialized view is derived the materialized view has to be updated so that it is consistent with the source data this update process is called view maintenance the incremental method of view maintenance which computes the new view using the old view and the update to the source data is widely preferred to full view recomputation when the update is small in size the small update size becomes an important concept for measuring the cheap performance of the incremental methods in this paper we investigate what is the limit of the small update size which we call size limit for the incremental maintenance when the size of an update exceeds the limit the incremental maintenance is no longer cheaper than the view recomputation the investigation is based on incremental equations for operators in the nested relational model hierarchical memory based reinforcement learning a key challenge for reinforcement learning is scaling up to large partially observable domains in this paper we show how a hierarchy of behaviors can be used to create and select among variable length short term memories appropriate for a task at higher levels in the hierarchy the agent abstracts over lower level details and looks back over a variable number of high level decisions in time we formalize this idea in a framework called hierarchical sux memory hsm hsm uses a memory based smdp learning method to rapidly propagate delayed reward across long decision sequences we describe a detailed experimental study comparing memory vs hierarchy using the hsm framework on a realistic corridor navigation task 1 introduction reinforcement learning encompasses a class of machine learning problems in which an agent learns from experience as it interacts with its environment one fundamental challenge faced by reinforcement learning agents in real world problems is that the well founded semantics is the principle of inductive definition existing formalisations of transfinite inductive definitions in constructive mathematics are reviewed and strong correspondences with lp under least model and perfect model semantics become apparent i point to fundamental restrictions of these existing formalisations and argue that the well founded semantics wfs overcomes these problems and hence provides a superior formalisation of the principle of inductive definition the contribution of this study for lp is that it re introduces the knowledge theoretic interpretation of lp as a logic for representing definitional knowledge i point to fundamental differences between this knowledge theoretic interpretation of lp and the more commonly known interpretations of lp as default theories or auto epistemic theories the relevance is that differences in knowledge theoretic interpretation have strong impact on knowledge representation methodology and on extensions of the lp formalism for example for representing uncertainty keywo information theoretic learning this chapter seeks to extend the ubiquitous mean square error criterion mse to cost functions that include more information about the training data since the learning process ultimately should transfer the information carried in the data samples onto the system s parameters a natural goal is to find cost functions that directly manipulate information hence the name informationtheoretic learning itl in order to be useful itl should be independent of the learning machine architecture and require solely the availability of the data i e it should not require a priori assumptions about the data distributions the chapter presents our current efforts to develop itl criteria based on the integration of nonparametric density estimators with renyi s quadratic entropy definition as a motivation we start with an application of the mse to manipulate information using the nonlinear characteristics of the learning machine this section illustrates the issues faced when we attempt to use a web based information system that reasons with structured collections of text the degree to which information sources are pre processed by web based information systems varies greatly in search engines like altavista little pre processing is done while in knowledge integration systems complex site specific wrappers are used integrate different information sources into a common database representation in this paper we describe an intermediate between these two models in our system information sources are converted into a highly structured collection of small fragments of text databaselike queries to this structured collection of text fragments are approximated using a novel logic called whirl which combines inference in the style of deductive databases with ranked retrieval methods from information retrieval whirl allows queries that integrate information from multiple web sites without requiring the extraction and normalization of object identifiers that can be used as keys instead operations that in conventional databases require equality tests grounded speech communication language is grounded in sensory motor experience grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context currently machines which process text and spoken language are not grounded in human like ways instead semantic representations in machines are highly abstract and have meaning only when interpreted by humans we are interested in developing computational systems which represent words utterances and underlying concepts in terms of sensory motor experiences leading to richer levels of understanding by machines inspired by theories of infant cognition we present a computational model which learns from untranscribed multisensory input acquired words are represented in terms associations between acoustic and visual sensory experience the system has been tested in a robotic embodiment which supports interactive language learning and understanding successful learning has also been demonstrated using infant directed s how developmental psychology and robotics complement each other this paper presents two complementary ideas relating the study of human development and the construction of intelligent artifacts first the use of developmental models will be a critical requirement in the construction of robotic systems that can acquire a large repertoire of motor perceptual and cognitive capabilities second robotic systems can be used as a test bed for evaluating models of human development much in the same way that simulation studies are currently used to evaluate cognitive models to further explore these ideas two examples from the author s own work will be presented the use of developmental models of hand eye coordination to simplify the task of learning to reach for a visual target and the use of a humanoid robot to evaluate models of normal and abnormal social skill development introduction research on human development and research on the construction of intelligent artifacts can and should be complementary studies of human developm corpus based stemming using co occurrence of word variants stemming is used in many information retrieval ir systems to reduce variant word forms to common roots it is one of the simplest applications of natural language processing to ir and one of the most effective in terms of user acceptance and consistent though small retrieval improvements current stemming techniques do not however reflect the language use in specific corpora and this can lead to occasional serious retrieval failures we propose a technique for using corpus based word variant co occurrence statistics to modify or create a stemmer the experimental results generated using english newspaper and legal text and spanish text demonstrate the viability of this technique and its advantages relative to conventional approaches categories and subject descriptors h 3 1 information storage and retrieval content analysis and indexing indexing methods linguistic processing h 3 3 information storage and retrieval information search and retrieval query f reinforcement learning for a vision based mobile robot reinforcement learning systems improve behaviour based on scalar rewards from a critic in this work vision based behaviours servoing and wandering are learned through a q learning method which handles continuous states and actions there is no requirement for camera calibration an actuator model or a knowledgeable teacher learning through observing the actions of other behaviours improves learning speed experiments were performed on a mobile robot using a real time vision system 1 introduction collision free wandering and visual servoing are building blocks for purposeful robot behaviours such as foraging target pursuit and landmark based navigation visual servoing consists of moving some part of a robot to a desired position using visual feedback 15 wandering is an environment exploration behaviour 6 in this work we demonstrate real time learning of wandering and servoing on a real robot learning eliminates the calibration process and leads to flexible behaviour learning decision trees for loss minimization in multi class problems many machine learning applications require classifiers that minimize an asymmetric loss function rather than the raw misclassification rate we study methods for modifying c4 5 to incorporate arbitrary loss matrices one way to incorporate loss information into c4 5 is to manipulate the weights assigned to the examples from different classes for 2 class problems this works for any loss matrix but for k 2 classes it is not sufficient nonetheless we ask what is the set of class weights that best approximates an arbitrary k theta k loss matrix and we test and compare several methods a wrapper method and some simple heuristics the best method is a wrapper method that directly optimizes the loss using a hold out data set we define complexity measure for loss matrices and show that this measure can predict when more efficient methods will suffice and when the wrapper method must be applied 1 introduction for most of the history of machine learning research a central goal has incremental document clustering for web page classification motivated by the benefits in organizing the documents in web search engines we consider the problem of automatic web page classification we employ the clustering techniques each document is represented by a feature vector by analyzing the clusters formed by these vectors we can assign the documents within the same cluster to the same class automatically our contributions are the following 1 we propose a feature extraction mechanism which is more suitable to web page classification 2 we introduce a tree structure called the dc tree to make the clustering process incremental and less sensitive to the document insertion order 3 we show with experiments on a set of internet documents from yahoo that the proposed clustering algorithm can classify web pages effectively keywords incremental update tree document clustering web classification 0 1 introduction the popularity of the internet has caused a continuous massive increase in the amount of web pages o implicit human computer interaction through context in this paper the term implicit human computer interaction is defined it is discussed how the availability of processing power and advanced sensing technology can enable a shift in hci from explicit interaction such as direct manipulation guis towards a more implicit interaction based on situational context in the paper an algorithm that is based on a number of questions to identify applications that can facilitate implicit interaction is given an xmlbased language to describe implicit hci is proposed the language uses contextual variables that can be grouped using different types of semantics as well as actions that are called by triggers the term of perception is discussed and four basic approaches are identified that are useful when building context aware applications providing two examples a wearable context awareness component and a sensor board it is shown how sensor based perception can be implemented it is also discussed how situational context can be exploited to im min wise independent permutations we define and study the notion of min wise independent families of permutations we say that f s n is min wise independent if for any set x n and any x x when is chosen at random in f we have pr min x x 1 x in other words we require that all the elements of any fixed set x have an equal chance to become the minimum element of the image of x under our research was motivated by the fact that such a family under some relaxations is essential to the algorithm used in practice by the altavista web index software to detect and filter near duplicate documents however in the course of our investigation we have discovered interesting and challenging theoretical questions related to this concept we present the solutions to some of them and we list the rest as open problems digital src 130 lytton avenue palo alto ca 94301 usa e mail broder pa dec com computer science department stanford university ca 94305 usa e mail moses cs stan a realistic non associative logic and a possible explanations of 7 plusmn 2 law when we know the subjective probabilities degrees of belief p1 and p2 of two statements s1 and s2 and we have no information about the relationship between these statements then the probability of s1 s2 can take any value from the interval max p1 p2 gamma 1 0 min p1 p2 if we must select a single number from this interval the natural idea is to take its midpoint the corresponding and operation p1 p2 def 1 2 delta max p1 p2 gamma 1 0 min p1 p2 is not associative however since the largest possible non associativity degree j a b c gamma a b c j is equal to 1 9 this non associativity is negligible if the realistic granular degree of belief have granules of width 1 9 this may explain why humans are most comfortable with 9 items to choose from the famous 7 plus minus 2 law we also show that the use of interval computations can simplify the rather complicated proofs 1 1 in expert systems we need estimates for the degree of learning to construct knowledge bases from the world wide web the world wide web is a vast source of information accessible to computers but understandable only to humans the goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the world wide web such a knowledge base would enable much more effective retrieval of web information and promote new uses of the web to support knowledge based inference and problem solving our approach is to develop a trainable information extraction system that takes two inputs the first is an ontology that defines the classes e g company person employee product andrelations e g employed by produced by of interest when creating the knowledge base the second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations given these inputs the system learns to extract information from other pages and hyperlinks on the web this article describes our general approach several machine learning algorithms for this task and promising initial results with a prototype system that has created a knowledge base describing university people courses and research projects maude specification and programming in rewriting logic maude is a high level language and a high performance system supporting executable specification and declarative programming in rewriting logic since rewriting logic contains equational logic maude also supports equational specification and programming in its sublanguage of functional modules and theories the underlying equational logic chosen for maude is membership equational logic that has sorts subsorts operator overloading and partiality definable by membership and equality conditions rewriting logic is reflective in the sense of being able to express its own metalevel at the object level reflection is systematically exploited in maude endowing the language with powerful metaprogramming capabilities including both user definable module operations and declarative strategies to guide the deduction process this paper explains and illustrates with examples the main concepts of maude s language design including its underlying logic functional system and object oriented modules as well as parameterized modules theories and views we also explain how maude supports reflection metaprogramming and internal strategies the paper outlines the principles underlying the maude system implementation including its semicompilation techniques we conclude with some remarks about applications work on a formal environment for maude and a mobile language extension of maude learning maps for indoor mobile robot navigation autonomous robots must be able to learn and maintain models of their environments research on mobile robot navigation has produced two major paradigms for mapping indoor environments grid based and topological while grid based methods produce accurate metric maps their complexity often prohibits efficient planning and problem solving in large scale indoor environments topological maps on the other hand can be used much more efficiently yet accurate and consistent topological maps are often difficult to learn and maintain in large scale environments particularly if momentary sensor data is highly ambiguous this paper describes an approach that integrates both paradigms grid based and topological grid based maps are learned using artificial neural networks and naive bayesian integration topological maps are generated on top of the grid based maps by partitioning the latter into coherent regions by combining both paradigms the approach presented here gains advantages from both worlds accuracy consistency and efficiency the paper gives results for autonomous exploration mapping and operation of a mobile robot in populated multi room environments agent based modeling for holonic manufacturing systems with fuzzy control agent based systems technologies are of emerging interest in the specification and implementation of complex systems this article introduces the casa agent development system which seamlessly combines the bdi belief desire intention approach with the fipa agent communication language standard and an integrated specification of fuzzy controllers the behavior of agents is defined by strategies which basically correspond to extended guarded horn clauses with priorities the presented concepts are introduced by an example from computer integrated manufacturing cim the example gives the specification of a fuzzy controller for a manufacturing station in the context of a holonic manufacturing system hms 1 introduction agent based systems technologies in the sense of distributed computing is an area of emerging interest in the domain of complex systems design the agent based paradigm can be seen as a real enhancement of the objectoriented paradigm where objects become autonomous ensembles of classifiers based on approximate reducts a problem of improving rough set based expert systems by modifying a notion of reduct is discussed a notion of approximate reduct is introduced as well as some proposals of quality measure for such a reduct a complete classifying system based on approximate reducts is presented and discussed it is proved that a problem of nding optimal set of classifying agents based on approximate reducts is np hard a genetic algorithm is used to nd the suboptimal set experimental results show that the classifying system is eective and relatively fast 1 introduction rough set expert systems base on the notion of reduct 11 12 a minimal subset of attributes which is sucient to discern between objects with dierent decision values a set of short reducts can be used to generate rules 2 a problem of short reducts generation is np hard but an approximate algorithm like the genetic one described in 13 8 and implemented successfully see 10 can be used to obtain redu scalable replication in database clusters the widespread use of clusters and web farms has increased the importance of data replication in existing protocols typical distributed system solutions emphasize fault tolerance at the price of performance while database solutions emphasize performance at the price of consistency in this paper we explore the use of data replication in a cluster configuration with the objective of providing both fault tolerance and good performance without compromising consistency we do this by combining transactional concurrency control with group communication primitives in our approach transactions are executed at only one site so that not all nodes incur in the overhead of parsing optimizing and producing results to further reduce latency we use an optimistic multicast approach that overlaps transaction execution with the total order message delivery the techniques we present in the paper provide correct executions while minimizing overhead and providing higher scalability machine learning in automated text categorisation the automated categorisation or classification of texts into topical categories has a long history dating back at least to the early 60s until the late 80s the most effective approach to the problem seemed to be that of manually building automatic classifiers by means of knowledgeengineering techniques i e manually defining a set of rules encoding expert knowledge on how to classify documents under a given set of categories in the 90s with the booming production and availability of on line documents automated text categorisation has witnessed an increased and renewed interest prompted by which the machine learning paradigm to automatic classifier construction has emerged and definitely superseded the knowledge engineering approach within the machine learning paradigm a general inductive process called the learner automatically builds a classifier also called the rule or the hypothesis by learning from a set of previously classified documents the characteristics of one or more categories the advantages of this approach are a very good effectiveness a considerable savings in terms of expert manpower and domain independence in this survey we look at the main approaches that have been taken towards automatic text categorisation within the general machine learning paradigm issues pertaining to document indexing classifier construction and classifier evaluation will be discussed in detail a final section will be devoted to the techniques that have specifically been devised for an emerging application such as the automatic classification of web pages into yahoo like hierarchically structured sets of categories categories and subject descriptors h 3 1 information storage and retrieval content analysis and indexing indexing methods h 3 3 information storage and retrieval information search and retrieval information filtering h 3 3 information storage and retrieval systems and software performance evaluation efficiency and effectiveness i 2 3 artificial on the expressiveness of distributed leasing in linda like coordination languages s all local authors can be reached via e mail at the address last name cs unibo it questions and comments should be addressed to tr admin cs unibo it recent titles from the ublcs technical report series 99 2 a theory of efficiency for markovian processes m bernardo w r cleaveland february 1999 revisied march 2000 99 3 a reliable registry for the jgroup distributed object model a montresor march 1999 99 4 comparing the qos of internet audio mechanisms via formal methods a aldini m bernardo r gorrieri m roccetti march 1999 99 5 group enhanced remote method invocations a montresor r davoli o babao glu april 1999 99 6 managing complex documents over the www a case study for xml p ciancarini f vitali c mascolo april 1999 99 7 data flow hard real time programs scheduling processors and communication channels in a distributed environment r davoli f tamburini april 1999 99 8 the mps computer system simulator m morsiani r davoli apri the effect of network hierarchy structure on performance of atm pnni hierarchical routing networks deploying hierarchical routing are recursively partitioned into sub networks that do not reveal the full details of their internal structure outside their domains instead an aggregated view of certain parameters that are associated with traversal within such sub networks between their border nodes is advertised the atm pnni standard and the internet nimrod architecture both adopt this approach for routing this paper studies the effectiveness of atm hierarchical routing protocols on networks with different hierarchical structures by simulation our study shows that in general the hierarchical source routing performs well compared to the global routing strategy which imposes no hierarchy while utilizing less storage and communication overhead for certain networks and topologies the hierarchical routing performs better than the global routing different hierarchies imposed on the same topologies have significantly different performance on the throughput and routing dela obdd based universal planning for synchronized agents in non deterministic domains recently model checking representation and search techniques were shown to be efficiently applicable to planning in particular to non deterministic planning such planning approaches use ordered binary decision diagrams obdds to encode a planning domain as a non deterministic finite automaton and then apply fast algorithms from model checking to search for a solution obdds can effectively scale and can provide universal plans for complex planning domains we are particularly interested in addressing the complexities arising in non deterministic multi agent domains in this article we present umop a new universal obdd based planning framework for non deterministic multi agent domains we introduce a new planning domain description language nadl to specify non deterministic multi agent domains the language contributes the explicit definition of controllable agents and uncontrollable environment agents we describe the syntax and semantics of nadl and show how to bu learning to recognize 3d objects a learning account for the problem of object recognition is developed within the pac probably approximately correct model of learnability the key assumption underlying this work is that objects can be recognized or discriminated using simple representations in terms of syntactically simple relations over the raw image although the potential number of these simple relations could be huge only a few of them are actually present in each observed image and a fairly small number of those observed is relevant to discriminating an object we show that these properties can be exploited to yield an ecient learning approach in terms of sample and computational complexity within the pac model no assumptions are needed on the distribution of the observed objects and the learning performance is quantied relative to its past experience most importantly the success of learning an object representation is naturally tied to the ability to represent it as a function of some in a sound algorithm for region based image retrieval using an index region based image retrieval systems aim to improve the effectiveness of content based search by decomposing each image into a set of homogeneous regions thus similarity between images is assessed by computing similarity between pairs of regions and then combining the results at the image level in this paper we propose the first provably sound algorithm for performing region based similarity search when regions are accessed through an index experimental results demonstrate the effectiveness of our approach as also compared to alternative retrieval strategies 1 introduction many real world applications in the field of medicine weather prediction and communications to name a few require efficient access to image databases based on content to this end the goal of content based image retrieval cbir systems is to define a set of properties features able to effectively characterize the content of images and then to use such features during retrieval users accessing a cb on decidability of boundedness property for regular path queries the paper studies the evaluation of regular path queries on semi structured data i e path queries of the form find all objects reachable by path whose labels form a word in r where r is a regular expression we use local information expressed in the form of path constraints in the optimization of path expression queries these constraints are of the form r ae w where r is a regular language and w is a word the chromatic structure of natural scenes we applied independent component analysis ica to hyperspectral images in order to learn an ecient representation of color in natural scenes in the spectra of single pixels the algorithm found basis functions that had broadband spectra as well as basis functions that were similar to natural reectance spectra when applied to small image patches the algorithm found basis functions that were achromatic and others with overall chromatic variation along lines in color space indicating color opponency the directions of opponency were not strictly orthogonal comparison 1 with principal component analysis pca on the basis of statistical measures such as average mutual information kurtosis and entropy shows that the ica transformation results in much sparser coecients and gives higher coding eciency our ndings suggest that non orthogonal opponent encoding of photoreceptor signals leads to higher coding eciency and that ica may be used to reveal the underlying stati improving the performance of high energy physics analysis through bitmap indices bitmap indices are popular multi dimensional structures for accessing read mostly data such as data warehouse dw applications decision support systems dss and on line analytical processing olap one of their main strengths is that they provide good performance characteristics for complex adhoc and an efficient combination of multiple index in one query considerable research work has been done in the area of finite and low attribute cardinalities however additional complexity is imposed on the design of bitmap indices for high cardinality or even non discrete attributes where different optimisation techniques than the ones proposed so far have to be applied in this paper we discuss the design and implementation of bitmap indices for high energy physics hep analysis where the potential search space consists of hundreds of independent dimensions a single hep query typically covers 10 to 100 dimensions out of the whole searchs space in this context we evaluated two different bitmap encoding techniques namely equality encoding and range encoding for both methods the number of bit slices or bitmap vectors per attribute is a a central optimisation parameter the paper presents some first results for choosing the optimal number of bit slices for multi dimensional indices with attributes of different value distribution and query selectivity we believe taht this discussion is not only applicable to hep but also to dw dss and olap type problems in general multimodal system processing in mobile environments one major goal of multimodal system design is to support more robust performance than can be achieved with a unimodal recognition technology such as a spoken language system in recent years the multimodal literatures on speech and pen input and speech and lip movements have begun developing relevant performance criteria and demonstrating a reliability advantage for multimodal architectures in the present studies over 2 600 utterances processed by a multimodal pen voice system were collected during both mobile and stationary use a new data collection infrastructure was developed including instrumentation worn by the user while roaming a researcher field station and a multimodal data logger and analysis tool tailored for mobile research although speech recognition as a stand alone failed more often during mobile system use the results confirmed that a more stable multimodal architecture decreased this error rate by 19 35 furthermore these findings were replicated across different types of microphone technology in large part this performance gain was due to significant levels of mutual disambiguation in the multimodal architecture with higher levels occurring in the noisy mobile environment implications of these findings are discussed for expanding computing to support more challenging usage contexts in a robust manner learning intonation rules for concept to speech generation we aim to design and develop a concept to speech cts generation system a speech synthesis system producing speech from semantic representations by integrating language generation with speech synthesis we focus on five issues 1 how to employ newly available accurate discourse semantic and syntactic information produced by a natural language generation system to improve the quality of synthesized speech 2 how to extend an existing general purpose natural language generation system to a concept to speech generation tool 3 how to integrate rule induction into a cts architecture to separate the domain dependent prosodic rules from the prosody generation component 4 how to design a speech interface language to facilitate a more flexible and open cts architecture 5 how to build prosody models and explore their use in multimedia synchronization unlike previous cts research we utilize linguistic constraints provided by a general purpose natural language generation system webmining learning from the world wide web automated analysis of the world wide web is a new challenging area relevant in many applications e g retrieval navigation and organization of information automated information assistants and e commerce this paper discusses the use of unsupervised and supervised learning methods for user behavior modeling and content based segmentation and classification of web pages the modeling is based on independent component analysis and hierarchical probabilistic clustering techniques keywords webmining unsupervised learning hierarchical probabilistic clustering 1 negotiation protocols and dialogue games in a dynamic and open environment negotiation protocols cannot be known beforehand we propose a methodology for constructing exible negotiation protocols based on joint actions and dialogue games we view negotiation as a combination of joint actions simple dialogue games that consist of initiatives followed by responses function as recipes for joint action from which larger interactions can be constructed coherently 1 introduction agent based software engineering is an active research area one of its main challenges is to bridge theoretical models and practical applications for example there are many theoretical results on negotiation 11 but automated negotiation is still rare on the internet implementing negotiation protocols raises a number of practical questions in particular current negotiation systems are either closed or semi closed in closed and semi closed environments like in auctions there is central control over the agents that can participate in s decomposition of object oriented database schemas based on f logic we specify an advanced data model with object oriented and logic oriented features for this model we study the decomposition of a class the counterpart to the well known decomposition of a relation scheme under functional dependencies for this decomposition of a class the transformation pivoting is used pivoting transplants some attributes of the class to a newly generated class this new class is a subclass of the result class of the so called pivot attribute the pivot attribute maintains the link between the original class and the new subclass we identify the conditions for the output of pivoting being equivalent with its input additionally we show under which conditions a schema with functional dependencies can be recursively transformed into an equivalent one without nonkey functional dependencies 1 introduction the theory of database schema design aims at formally characterising good schemas and at inventing algorithms to measure and to im mining optimized support rules for numeric attributes mining association rules on large data sets has received considerable attention in recent years association rules are useful for determining correlations between attributes of a relation and have applications in marketing financial and retail sectors furthermore optimized association rules are an effective way to focus on the most interesting characteristics involving certain attributes optimized association rules are permitted to contain uninstantiated attributes and the problem is to determine instantiations such that either the support confidence or gain of the rule is maximized in this paper we generalize the optimized support association rule problem by permitting rules to contain disjunctions over uninstantiated numeric attributes our generalized association rules enable us to extract more useful information about seasonal and local patterns involving the uninstantiated attribute for rules containing a single numeric attribute we present a dynamic programming algorith integration of machine learning and knowledge acquisition introduction integration of machine learning and knowledge acquisition may be a surprising title for an ecai 94 workshop since most machine learning ml systems are dedicated to knowledge acquisition ka what could thus mean integrating ml and ka the answer lies in the difference between the approaches developed by what is referred to as ml and ka research apart from some major exceptions such as learning apprentice tools mitchell et al 1989 or libraries like machine learning toolbox mlt 1993 most ml algorithms were described without any characterization in terms of real application needs in term of what they could be effectively useful for however ml methods were applied to real world problems but few general and reusable conclusions were drawn from these knowledge acquisition experiments as ml techniques become more and more sophisticated and able to produce various forms of knowledge the number of possible applications grows lower bounds for high dimensional nearest neighbor search and related problems in spite of extensive and continuing research for various geometric search problems such as nearest neighbor search the best algorithms known have performance that degrades exponentially in the dimension this phenomenon is sometimes called the curse of dimensionality recent results 33 32 35 show that in some sense it is possible to avoid the curse of dimensionality for the approximate nearest neighbor search problem but must the exact nearest neighbor search problem suffer this curse we provide some evidence in support of the curse specifically we investigate the exact nearest neighbor search problem and the related problem of exact partial match within the asymmetric communication model first used by miltersen 38 to study data structure problems we derive non trivial asymptotic lower bounds for the exact problem that stand in contrast to known algorithms for approximate nearest neighbor search department of computer science university of toronto part of this work cabins a framework of knowledge acquisition and iterative revision for schedule improvement and reactive repair practical scheduling problems generally require allocation of resources in the presence of a large diverse and typically conflicting set of constraints and optimization criteria the ill structuredness of both the solution space and the desired objectives make scheduling problems difficult to formalize this paper describes a case based learning method for acquiring context dependent user optimization preferences and tradeoffs and using them to incrementally improve schedule quality in predictive scheduling and reactive schedule management in response to unexpected execution events the approach implemented in the cabins system uses acquired user preferences to dynamically modify search control to guide schedule improvement during iterative repair cases are exploited for 1 repair action selection 2 evaluation of intermediate repair results and 3 recovery from revision failures the method allows the system to dynamically switch between repair heuristic actions each of whi meta learning in distributed data mining systems issues and approaches data mining systems aim to discover patterns and extract useful information from facts recorded in databases a widely adopted approach to this objective is to apply various machine learning algorithms to compute descriptive models of the available data here we explore one of the main challenges in this research area the development of techniques that scale up to large and possibly physically distributed databases meta learning is a technique that seeks to compute higher level classifiers or classification models called meta classifiers that integrate in some principled fashion multiple classifiers computed separately over different databases this study describes meta learning and presents the jam system java agents for meta learning an agent based meta learning system for large scale data mining applications specifically it identifies and addresses several important desiderata for distributed data mining systems that stem from their additional complexity co inducing conceptual user models usually performance is the primary objective in systems that make use of user modeling um techniques but since machine learning ml in user modeling addresses several issues in the context of human computer interaction hci the requirements on performance are manifold thus machine learning for user modeling ml4um has to meet several demands in order to satisfy the aims of involved disciplines in this article we describe the application of a rather unusual ml method to um namely inductive logic programming ilp though not primarily associated with efficient learning methods we motivate the use of ilp by showing translucency of derived user models and the explanatory potential of such models during a user adapted filtering process in course of the oyster project our goal is to induce conceptual user models that allow for a transparent query refinement and information filtering in the domain of www meta search 1 introduction why conceptual user models dyda dynamic data warehouse maintenance in a fully concurrent environment data warehouse is an emerging technology to support high level decision making by gathering data from several distributed information sources into one repository in dynamic environments data warehouses must be maintained in order to stay consistent with the underlying sources recently proposed view maintenance algorithms tackle the problem of data warehouse maintenance under concurrent source data updates while the view synchronization is to handle non concurrent source schema changes however the concurrency between interleaved schema changes and data updates still remain unexplored problems in this paper we propose a solution framework called dyda that successfully addresses this problem the dyda framework detects concurrent schema changes by a broken query scheme and conicting concurrent data updates by a local timestamp scheme a fundamental idea of the dyda framework is the development of a two layered architecture that separates the concerns for concurrent data updates and concurrent schema changes handling without imposing any restrictions on the sourse update transactions at the lower level of the framework it employs a local compensation algorithm to handle concurrent data updates and a metadata name mapping strategy to handle concurrent source rename operations at the higher level it addresses the problem of concurrent source drop operations for the latter problem we design a strategy for the detection and correction of such concurrency and nd an executable plan for the aected updates we then develop a new view adaption algorithm called batch va for execution of such plan to incrementally adapt the view put together these algorithms are the rst to provide a complete solution to data warehouse management in a fully concurrent environment evaluation of item based top n recommendation algorithms the explosive growth of the world wide web and the emergence of e commerce has led to the development of recommender systems a personalized information filtering technology used to identify a set of n items that will be of interest to a certain user user based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems unfortunately the computational complexity of these methods grows linearly with the number of customers that in typical commercial applications can grow to be several millions to address these scalability concerns item based recommendation techniques have been developed that analyze the user item matrix to identify relations between the different items and use these relations to compute the list of recommendations in this paper we present one such class of item based recommendation algorithms that first determine the similarities between the various ite rewriting logic as a metalogical framework a metalogical framework is a logic with an associated methodology that is used to represent other logics and to reason about their metalogical properties we propose that logical frameworks can be good metalogical frameworks when their logics support reflective reasoning and their theories always have initial models we present a concrete realization of this idea in rewriting logic theories in rewriting logic always have initial models and this logic supports reflective reasoning this implies that inductive reasoning is valid when proving properties about the initial models of theories in rewriting logic and that we can use reflection to reason at the metalevel about these properties in fact we can uniformly reflect induction principles for proving metatheorems about rewriting logic theories and their parameterized extensions we show that this reflective methodology provides an effective framework for di erent non trivial kinds of formal metatheoretic reasoning one can for examp evolving user profiles to reduce internet information overload this paper discusses the use of evolving personal agent environments as a potential solution to the problem of information overload as experienced in habitual web surfing some first experimental results on evolving user profiles using speciating hybrid gas the reasoning behind them and support for their potential application in mobile wireless and location aware information devices are also presented 1 information overload in everyday life the internet user is faced with the ever increasing problem of information overload whether this occurs at home at the workplace or as will soon be happening everywhere 1 2 the overwhelming information feed that computer users face leads to anxiety strain inefficiency and finally results in uninformed or misinformed and frustrated users 3 4 continuously and increasingly internet users are confronted with laborious and difficult tasks of information filtering and or gathering which are inherently computer oriented processes debora developing an interface to support collaboration in a digital library interfaces to library systems have largely failed to represent the inherently collaborative nature of information work this paper describes how collaborative functionality is being implemented as part of the debora project to provide access to digitised renaissance documents work practices of users of renaissance documents are described and the collaborative features of the client software are outlined functionalities discussed include annotation the creation of virtual books and the inclusion of user supplied metadata 1 introduction this paper describes the development of collaborative functionality for users of digital libraries in the context of the eu telematics for libraries project debora digital access to books of the renaissance the aim of the debora project is to make renaissance books more generally available as digital resources and to examine the potential for novel collaborative functionality the collection being created within debora consists of digiti yarrow a real time client side meta search learner in this paper we report our research on building yarrow an intelligent web meta search engine the predominant feature of yarrow is that in contrast to the lack of adaptive learning features in existing metasearch engines yarrow is equipped with a practically efficient on line learning algorithm so that it is capable of helping the user to search for the desired documents with as little feedback as possible currently yarrow can query eight of the most popular search engines and is able to perform document parsing and indexing and learning in real time on client side its architecture and performance are also discussed 1 introduction as the world wide web evolves and grows so rapidly web search an interface between the human users and the vast information gold mine of the web is becoming a necessary part of people s daily life designing and implementing practically effective web search tools is a challenging task it calls for innovative methods and strategies f feature selection in web applications by roc inflections and powerset pruning coetzee compuman lawrence giles a basic problem of information processing is selecting enough features to ensure that events are accurately represented for classification problems while simultaneously minimizing storage and processing of irrelevant or marginally important features to address this problem feature selection procedures perform a search through the feature power set to find the smallest subset meeting performance requirements major restrictions of existing procedures are that they typically explicitly or implicitly assume a fixed operating point and make limited use of the statistical structure of the feature power set we present a method that combines the neyman pearson design procedure on finite data with the directed set structure of the receiver operating curves on the feature subsets to determine the maximal size of the feature subsets that can be ranked in a given problem the search can then be restricted to the smaller subsets resulting in significant reductions in computational complexity optimizing the overall receiver operating curve also allows for end users to select different operating points and cost functions to optimize the algorithm also produces a natural method of boolean representation of the minimal feature combinations that best describe the data near a given operating point these representations are especially appropriate when describing data using common text related features useful on the web such as thresholded tfidf data we show how to use these results to perform automatic boolean query modification generation for distributed databases such as niche metasearch engines 1 hierarchical discriminant regression the main motivation of this paper is to propose a new classification and regression method for challenging high dimensional data the proposed new technique casts classification problems class labels as output and regression problems numeric values as output into a unified regression problem this unified view enables classification problems to use numeric information in the output space that is available for regression problems but are traditionally not readily available for classification problems distance metric among clustered class labels for coarse and fine classifications a doubly clustered subspace based hierarchical discriminating regression hdr method is proposed in this work the major characteristics include 1 clustering is performed in both output space and input space at each internal node and thus the term doubly clustered clustering in the output space provides virtual labels for computing clusters in the input space 2 discriminants in the input spa a cognitive bias approach to feature selection and weighting for case based learners research in psychology psycholinguistics and cognitive science has discovered and examined numerous psychological constraints on human information processing short term memory limitations a focus of attention bias and a preference for the use of temporally recent information are three examples this paper shows that psychological constraints such as these can be used e ectively as domain independent sources of bias to guide feature set selection and weighting for case based learning algorithms we first show that cognitive biases can be automatically and explicitly encoded into the baseline instance representation each bias modifies the representation by changing features deleting features or modifying feature weights next we investigate the related problems of cognitive bias selection and cognitive bias interaction for the feature weighting approach in particular we compare two cross validation algorithms for bias selection that make di erent assumptions about the indep mixed depth representations for dialog processing we describe our work on developing a general purpose tutoring system that will allow students to practice their decision making skills in a number of domains the tutoring system b2 supports mixed initiative natural language interaction the natural language processing and knowledge representation components are also general purpose which leads to a tradeo between the limitations of super cial processing and syntactic representations and the di culty of deeper methods and conceptual representations students utterances may be short and ambiguous requiring extensive reasoning about the domain or the discourse model to fully resolve however full disambiguation is rarely necessary our solution is to use a mixed depth representation one that encodes syntactic and conceptual information in the same structure as a result we can use the same representation framework to produce a detailed representation of requests which tend to be well speci ed and to produce a partial representation of questions which tend to require more inference about the context moreover the representations use the same knowledge representation framework that is used to reason about discourse processing and domain information so that the system can reason with and about the utterances if necessary design and implementation of the olog deductive object oriented database management system olog is a novel deductive database system for advanced intelligent information system applications it directly supports eective storage ecient access and inference of large amount of persistent data with complex structures it provides a sql like data denition language and data manipulation language and a declarative rule based query language it combines the best of the deductive object oriented and objectrelational approaches in a uniform framework this paper describes the design and implementation of the olog system 1 introduction deductive object oriented and object relational databases are three important extensions of the traditional relational database technology deductive databases stem from the integration of logic programming and relational databases it oers representational and operational uniformity reasoning capabilities recursion declarative querying ecient secondary storage access etc however deductive databases based on relational databas query processing in relationlog relationlog is a persistent deductive database system that supports eective storage ecient access and inference of large amounts of data with complex structures in this paper we describe query processing in the relationlog system in particular we illustrate the extended semi naive and magic set techniques used in relationlog 1 introduction during the past decades the nested relational and complex object models 1 5 11 12 15 18 21 22 were developed to extend the applicability of the traditional relational model to more complex non business applications such as cad image processing and text retrieval 2 another important direction of intense research has been in using a logic programming based language datalog 8 23 as a database query language such a language provides a simple and natural way to express queries on a relational database and is more expressive than the traditional relational languages in the past several years there have been some eorts indexing moving points we propose three indexing schemes for storing a set s of n points in the plane each moving along a linear trajectory so that a query of the following form can be answered quickly given a rectangle r and a real value t q report all k points of s that lie inside r at time t q we first present an indexing structure that for any given constant 0 uses o n b disk blocks where b is the block size and answers a query in o n b 1 2 k b i os it can also report all the points of s that lie inside r during a given time interval a point can be inserted or deleted or the trajectory of a point can be changed in o log 2 b n i os next we present a general approach that improves the query time if the queries arrive in chronological order by allowing the index to evolve over time we obtain a tradeoff between the query time and the number of times the index needs to be updated as the points move we also describe an indexing scheme in which the number of i os required to answer a query depends monotonically on the difference between t q and the current time finally we develop an efficient indexing scheme to answer approximate nearest neighbor queries among moving points an extended abstract of this paper appeared in the proceedings of the 19th acm sigact sigmod sigart symposium on principles of database systems y center for geometric computing department of computer science duke university box 90129 durham nc 27708 0129 pankaj cs duke edu http www cs duke edu pankaj supported in part by national science foundation grants eia 9870734 eia 9972879 and ccr 9732787 by army research of fice muri grant daah04 96 1 0013 by a sloan fellowship and by a grant from the u s israeli binational science foundation z center a formal approach to detecting security flaws in object oriented databases this paper adopts the method based authorization model and assumes the following database management policies let m c 1 c 2 c n be in an authorization for a user u intention reconsideration in theory and practice autonomous agents operating in complex dynamic environments need the ability to integrate robust plan execution with higher level reasoning this paper describes work to combine low level navigation techniques drawn from mobile robotics with deliberation techniques drawn from intelligent agents in particular we discuss the combination of a navigation system based on fuzzy logic with a deliberator based on the belief desire intention bdi model we discuss some of the subtleties involved in this integration and illustrate it with an example 1 introduction milou the robot works in a food factory he has to regularly go and fetch two food samples potato crisps from two production lines in two different rooms a and b and take them to an electronic tester in the quality control lab milou must now plan his next delivery he decides to get the sample from a first since room a is closer than b while going there however he finds the main door to that room closed milou knows t boosting the margin a new explanation for the effectiveness of voting methods abstract one of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated hypothesis usually does not increase as its size becomes very large and often is observed to decrease even after the training error reaches zero in this paper we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label we show that techniques used in the analysis of vapnik s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error we also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples finally we compare our explanation to those based on the bias variance decomposition 1 oms java lessons learned from building a multi tier object management framework we present the object oriented multi tier application framework oms java which is independent of the underlying database management system dbms we detail the storage management component and sketch which part of the framework has to be extended when introducing a new dbms we compare versions of oms java using the persistent storage engine objectstore pse pro for java the object oriented dbms objectivity db the objectrelational dbms oracle and the proprietary dbms berkley db 1 introduction most applications create data that extends the life of an application process making it necessary that application objects can be stored in and retrieved from non volatile storage furthermore looking at pure object oriented applications i e applications developed entirely using an objectoriented language environment such as java ka96 application objects typically refer to many other application objects resulting in complex object hierarchies it is therefore crucial to find mechan query optimization for selections using bitmaps bitmaps are popular indexes for data warehouse dw applications and most database management systems offer them today this paper proposes query optimization strategies for selections using bitmaps both continuous and discrete selection criteria are considered query optimization strategies are categorized into static and dynamic static optimization strategies discussed are the optimal design of bitmaps and algorithms based on tree and logical reduction the dynamic optimization discussed is the approach of inclusion and exclusion for both bit sliced indexes and encoded bitmap indexes 1 introduction bitmap indexing has become a promising technique for query processing in dws variations of bitmap indexes include bit sliced indexes 14 3 encoded bitmap indexes ebi 18 bitmapped join indexes 13 range based bitmap indexes 20 and others 16 for query operations such as selections aggregates and joins query evaluation algorithms using bitmaps have been proposed in forming neural networks through efficient and adaptive coevolution this article demonstrates the advantages of a cooperative coevolutionary search in difficult control problems the sane system coevolves a population of neurons that cooperate to form a functioning neural network in this process neurons assume different but overlapping roles resulting in a robust encoding of control behavior sane is shown to be more efficient more adaptive and maintain higher levels of diversity than the more common network based population approaches further empirical studies illustrate the emergent neuron specializations and the different roles the neurons assume in the population 1 introduction artificial evolution has become an increasingly popular method for forming control policies in difficult decision problems grefenstette ramsey schultz 1990 moriarty miikkulainen 1996a whitley dominic das anderson 1993 such applications are very different from the function optimization tasks to which evolutionary algorithms ea have been tradition the cmunited 97 small robot team abstract robotic soccer is a challenging research domain which involves multiple agents that need to collaborate in an adversarial environment toachieve speci c objectives in this paper we describe cmunited the team of small robotic agents that we developed to enter the robocup 97 competition we designed and built the robotic agents devised the appropriate vision algorithm and developed and implemented algorithms for strategic collaboration between the robots in an uncertain and dynamic environment the robots can organize themselves in formations hold speci c roles and pursue their goals in game situations they have demonstrated their collaborative behaviors on multiple occasions the robots can also switch roles to maximize the overall performance of the team we present anoverview of the vision processing algorithm which successfully tracks multiple moving objects and predicts trajectories the paper then focusses on the agent behaviors ranging from low level individual behaviors to coordinated strategic team behaviors cmunited won the robocup 97 small robot competition at ijcai 97 in nagoya japan 1 proposal id p480 proposal for mpeg 7 image description scheme name this document although any specific ddl selected by mpeg 7 can be used to serve the same purpose as well we will demonstrate that new features can be easily accommodated using the hierarchical structures and entity relation structures exploring brick based navigation and composition in an augmented reality build it is a planning tool based on computer vision technology supporting complex planning and composition tasks a group of people seated around a table interact with objects in a virtual scene using real bricks a plan view of the scene is projected onto the table where object manipulation takes place a perspective view is projected on the wall the views are set by virtual cameras having spatial attributes like shift rotation and zoom however planar interaction with bricks provides only position and rotation information object height control is equally constrained by planar interaction the aim of this paper is to suggest methods and tools bridging the gap between planar interaction and three dimensional control to control camera attributes active objects with intelligent behaviour are introduced to control object height several real and virtual tools are suggested some of the solutions are based on metaphors like window sliding ruler and floor 1 i context awareness by analysing accelerometer data in this paper we describe continuing work being carried out as part of the bristol wearable computing initiative we are researching processing techniques for data from accelerometers which enable the wearable computer to determine the user s activity we have experimented with and review techniques already employed by others and then propose new methods for analysing the data delivered by these devices we try to minimise the number of devices needed and use a single x y accelerometer device using our techniques we have adapted our gps based tourist guide wearable computer application to include a multimedia presentation which gives the user information using different media depending on the user s activity as well as location 1 introduction and background this is a condensed version of a technical report 1 our interests in wearable computing are centred around determining the context of the user and developing applications which make use of this information we are expl comparing structures using a hopfield style neural network labeled graphs are an appropriate and popular representation of structured objects in many domains if the labels describe the properties of real world objects and their relations finding the best match between two graphs turns out to be the weakly defined np complete task of establishing a mapping between them that maps similar parts onto each other preserving as much as possible of their overall structural correspondence in this paper former approaches of structural matching and constraint relaxation by spreading activation in neural networks and the method of solving optimization tasks using hopfield style nets are combined the approximate matching task is reformulated as the minimization of a quadratic energy function the design of the approach enables the user to change the parameters and the dynamics of the net so that knowledge about matching preferences is included easily and transparently in the last section some examples demonstrate the successful application of on wrapping query languages and efficient xml integration modern applications portals e commerce digital libraries etc require integrated access to various information sources from traditional rdbms to semistructured web repositories fast deployment and low maintenance cost in a rapidly evolving environment because of its flexibility there is an increasing interest in using xml as a middleware model for such applications xml enables fast wrapping and declarative integration however query processing in xml based integration systems is still penalized by the lack of an algebra with adequate optimization properties and the difficulty to understand source query capabilities in this paper we propose an algebraic approach to support efficient query evaluation in xml integration systems we define a general purpose algebra suitable for semistructured or xml query languages we showhow this algebra can be used with appropriate type information to also wrap more structured query languages such as oql or sql finally we develop new optimizat the kraft architecture for knowledge fusion and transformation this paper describes the kraft architecture which supports the fusion of knowledge from multiple distributed heterogeneous sources the architecture uses constraints as a common knowledge interchange format expressed against a common ontology knowledge held in local sources can be tranformed into the common constraint language and fused with knowledge from other sources the fused knowledge is then used to solve some problem or deliver some information to a user problem solving in kraft typically exploits pre existing constraint solvers kraft uses an open and flexible agent architecture in which knowledge sources knowledge fusing entities and users are all represented by independent kraft agents communicating using a messaging protocol facilitator agents perform matchmaking and brokerage services between the various kinds of agent kraft is being applied to an example application in the domain of network data services design 1 introduction and motivation most report on the trec 8 experiment searching on the web and in distributed collections this paper verifies whether or not hyperlinks improve retrieval effectiveness in the second chapter we describe experiments on the ad hoc track in this case we acknowledge that currently it is becoming more and more difficult to store and manage the growing document collections within a single computer recent advances in network technology do however allow us to disseminate information sources by partitioning a single huge corpus or distributing heterogeneous collections over a local area network intranet most retrieval mechanisms currently proposed however are based on conventional ir models salton 89 and where a centralized document collection is assumed monte carlo localization efficient position estimation for mobile robots this paper presents a new algorithm for mobile robot localization called monte carlo localization mcl mcl is a version of markov localization a family of probabilistic approaches that have recently been applied with great practical success however previous approaches were either computationally cumbersome such as grid based approaches that represent the state space by high resolution 3d grids or had to resort to extremely coarse grained resolutions our approach is computationally efficient while retaining the ability to represent almost arbitrary distributions mcl applies sampling based methods for approximating probability distributions in a way that places computation where needed the number of samples is adapted on line thereby invoking large sample sets only when necessary empirical results illustrate that mcl yields improved accuracy while requiring an order of magnitude less computation when compared to previous approaches it is also much easier to implement combining statistical measures to find image text regions we present a method based on statistical properties of local image pixels for focussing attention on regions of text in arbitrary scenes where the text plane is not necessarily fronto parallel to the camera this is particularly useful for desktop or wearable computing applications the statistical measures are chosen to reveal charactersitic properties of text we combine a number of localised measures using a neural network to classify each pixel as text or non text we demonstrate our results on typical images 1 introduction to automatically enter the contents of a text document into a computer one can place it on a flatbed scanner and use state of the art optical character recognition ocr software to retrieve the characters however automatic segmentation and recognition of text in arbitrary scenes where the text may or may not be fronto parallel to the viewing plane is an area of computer vision which has not been extensively researched previously the problems involved a using the resources model in virtual environment design this paper we take a step back from the formal specification of ves to investigate where requirements and design information are located within these environments and how it can be structured and analysed more specifically we are interested in considering ves in terms of distributed cognition dc 5 7 14 20 let s browse a collaborative browsing agent web browsing like most of today s desktop applications is usually a solitary activity other forms of media such as watching television are often done by groups of people such as families or friends what would it be like to do collaborative web browsing could the computer provide assistance to group browsing by trying to help find mutual interests among the participants let s browse is an experiment in building an agent to assist a group of people in browsing by suggesting new material likely to be of common interest it is built as an extension to the single user web browsing agent letizia let s browse features automatic detection of the presence of users automated channel surfing browsing and dynamic display of the user profiles and explanation of recommendations 1999 elsevier science b v all rights reserved keywords browsing collaboration agents user profiles 1 collaborative browsing increasingly web browsing will be performed in collaborative settings on the integration of ir and databases integration of information retrieval ir in database management systems dbmss has proven di cult previous attempts to integration su ered from inherent performance problems or lacked desirable separation between logical and physical data models to overcome these problems we discuss a database approach based on structural object orientation we implement ir techniques using extensions in an object algebra called moa moa has been implemented on top of the database backend monet a state of the art highperformance database kernel with a binary relational interface our prototype implementation of the inference network retrieval model using moa and monet demonstrates the feasibility of this approach we conclude with a discussion of the advantages of our database design introduction information retrieval ir is concerned with the retrieval of usually text documents that are likely to be relevant to the user s information need as expressed by his request van rijsb object based multimedia content description schemes and applications for mpeg 7 in this paper we describe description schemes dss for image video multimedia home media and archive content proposed to the mpeg 7 standard mpeg 7 aims to create a multimedia content description standard in order to facilitate various multimedia searching and filtering applications during the design process special care was taken to provide simple but powerful structures that represent generic multimedia data we use the extensible markup language xml to illustrate and exemplify the proposed dss because of its interoperability and flexibility advantages the main components of the image video and multimedia description schemes are object feature classification object hierarchy entity relation graph code downloading multi abstraction levels and modality transcoding the home media description instantiates the former dss proposing the 6 w semantic features for objects and 1 p physical and 6 w semantic object hierarchies the archive description scheme aims to describ contractual agent societies negotiated shared context and social control in open multi agent systems information systems for supporting the fluid organizations of the 21 st century must be correspondingly open and agile able to automatically configure themselves out of heterogeneous system components accommodate the dynamic exit and entry of hitherto unknown participants and maintain system stability in the face of limited trust this paper introduces the concept of contractual agent societies cas as a metaphor for building such open information systems cas are open information systems where independently developed agents configure themselves automatically through a set of dynamically negotiated social contracts social contracts define the shared context of agent interactions including ontologies joint beliefs joint goals normative behaviors etc in addition they specify classes of associated exceptions deviations from ideal behavior together with associated prevention and resolution mechanisms a research agenda for developing the infrastructure that will enable the c a hierarchical probabilistic model for novelty detection in text topic detection and tracking tdt is a variant of classification in which the classes are not known or fixed in advance consider for example an incoming stream of news articles or email messages that are to be classified by topic new classes must be created as new topics arise the problem is a challenging one for machine learning instances of new topics must be recognized as not belonging to any of the existing classes detection and instances of old topics must be correctly classified tracking often with extremely little training data per class this paper proposes a new approach to tdt based on probabilistic generative models strong statistical techniques are used to address the many challenges hierarchical shrinkage for sparse data statistical garbage collection for new event detection clustering in time to separate the different events of a common topic and deterministic annealing for creating the hierarchy preliminary experimental results show promise keyword learning to create customized authority lists the proliferation of hypertext and the popularity of kleinberg s hits algorithm have brought about an increased interest in link analysis while hits and its older relatives from the bibliometrics provide a method for finding authoritative sources on a particular topic they do not allow individual users to inject their own opinions on what sources are authoritative this paper presents a technique for learning a user s internal model of authority we present experimental results based on cora on line index a database of approximately one million on line computer science literature references 1 introduction bibliometrics white mccain 1989 small 1973 involves studying the structure that emerges from sets of linked documents traditionally these links have taken the form of citations among journal articles although kleinberg 1997 and others e g brin page 1998 have found that they adapt well to sets of hyperlinked documents bibliometric techniques exis language models for financial news recommendation we present a unique approach to identifying news stories that influence the behavior of financial markets specifically we describe the design and implementation of aelig nalyst a system that can recommend interesting news stories stories that are likely to affect market behavior aelig nalyst operates by correlating the content of news stories with trends in financial time series we identify trends in time series using piecewise linear fitting and then assign labels to the trends according to an automated binning procedure we use language models to represent patterns of language that are highly associated with particular labeled trends aelig nalyst can then identify and recommend news stories that are highly indicative of future trends we evaluate the system in terms of its ability to recommend the stories that will a ect the behavior of the stock market we demonstrate that stories recommended by aelig nalyst could be used to profitably predict forthcoming trends in stock prices from active objects to autonomous agents this paper studies how to extend the concept of active objects into a structure of agents it first discusses the requirements for autonomous agents that are not covered by simple active objects we propose then the extension of the single behavior of an active object into a set of behaviors with a meta behavior scheduling their activities to make a concrete proposal based on these ideas we describe how we extended a framework of active objects named actalk into a generic multi agent platform named dima we discuss how this extension has been implemented we finally report on one application of dima to simulate economic models keywords active object agent implementation meta behavior modularity re usability simulation 1 introduction object oriented concurrent programming oocp is the most appropriate and promising technology to implement agents the concept of active object may be considered as the basic structure for building agents furthermore the combinat using context as a crystal ball rewards and pitfalls this paper discusses some of the potential rewards and pitfalls that can await designers wishing to incorporate context awareness schilit 94 brown 97 into interactive systems many of the issues are described in anecdotal form based on our experiences developing and evaluating the context aware guide system cheverst 99 cheverst 00 localisation using automatically selected landmarks from panoramic images the use of visual landmarks for robot localisation is a promising field it is apparent that the success of localisation by visual landmarks depends on the landmarks chosen good landmarks are those which remain reliable over time and through changes in position and orientation this paper describes a system which learns places by automatically selecting landmarks from panoramic images and uses them for localisation tasks an adaption of the biologically inspired turn back and look behaviour is used to evaluate potential landmarks normalised correlation is used to overcome the a ects of changes in illumination in the environment results from real robot experiments are reported showing successful localisation from up to one meter away from the learnt position 1 introduction visual localisation is one of the key problems in making successful autonomous robots vision as a sensor is the richest source of information about a mobile agent s environment and as such con context aware telephony over wap in this paper we introduce a novel approach to share context to enhance the social quality of remote mobile communication we provide an analysis of how people start a conversation in situations where they meet physically especially looking on the influence of the situation than this is compared to the way remote communication is initiated using mobile phones the lack of knowledge about the situation on the other end leads to initiation of calls that are not appropriate in the situation the solution we propose is to exchange context information before initiating the call we implemented this concept using the wireless application protocol wap the wml based application context call offers a phone interface that provides information about the receiver when setting up a call the caller can than decide based on that information to place the call to leave a message or to cancel the call privacy issues that arise from this technology are discussed too keywords contex towards a highly scalable and effective metasearch engine a metasearch engine is a system that supports unified access to multiple local search engines database selection is one of the main challenges in building a large scale metasearch engine the problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query in order to enable accurate selection metadata that reect the contents of each search engine need to be collected and used in this paper we propose a highly scalable and accurate database selection method this method has several novel features first the metadata for representing the contents of all search engines are organized into a single integrated representative such a representative yields both computation efficiency and storage efficiency second our selection method is based on a theory for ranking search engines optimally experimental results indicate that this new method is very effective an operational prototype system has been built based on the proposed approach recent publications of the multimedia information access research group elligence pages 7 35 buenos aires ar 1999 straccia 1997a umberto straccia a four valued fuzzy propositional logic in proceedings of ijcai 97 15th international joint conference on artificial intelligence pages 128 133 nagoya jp 1997 straccia 1997b umberto straccia a sequent calculus for reasoning in four valued description logics in proceedings of tableaux 97 international conference on analytic tableaux and related methods pages 343 357 pont a mousson fr 1997 published in the lecture notes in computer science series number 1227 springer verlag heidelberg de straccia 1998 umberto straccia a fuzzy description logic in proceedings of aaai 98 15th conference of the american association for artificial intelligence pages 594 599 madison us 1998 straccia 1999 umberto straccia four valued fuzzy description logics for representing multimedia objects content in fabio crestani and ga building efficient and effective metasearch engines frequently a user s information needs are stored in the databases of multiple search engines it is inconvenient and inefficient for an ordinary user to invoke multiple search engines and identify useful documents from the returned results to support unified access to multiple search engines a metasearch engine can be constructed when a metasearch engine receives a query from a user it invokes the underlying search engines to retrieve useful information for the user metasearch engines have other benefits as a search tool such as increasing the search coverage of the web and improving the scalability of the search in this article we survey techniques that have been proposed to tackle several underlying challenges for building a good metasearch engine among the main challenges the database selection problem is to identify search engines that are likely to return useful documents to a given query the document selection problem is to determine what documents to retrieve from each identified search engine the result merging problem is to combine the documents returned from multiple search engines we will also point out some problems that need to be further researched sql based association rule mining using commercial rdbms ibm db2 udb eee data mining is becoming increasingly important since the size of databases grows even larger and the need to explore hidden rules from the databases becomes widely recognized currently database systems are dominated by relational database and the ability to perform data mining using standard sql queries will definitely ease implementation of data mining however the performance of sql based data mining is known to fall behind specialized implementation and expensive mining tools being on sale in this paper we present an evaluation of sql based data mining on commercial rdbms ibm db2 udb eee we examine some techniques to reduce i o cost by using view and subquery those queries can be more than 6 times faster than setm sql query reported previously in addition we have made performance evaluation on parallel database environment and compared the performance result with commercial data mining tool ibm intelligent miner we prove that sql based data mining can achie towards an accommodation of delay in temporal active databases business rules can be formulated according to the eventcondition action structure of triggers in active databases however delays in the execution of such rules can cause unexpected and undesired side effects while business rules are commonly constructed from an external user s perspective users often neglect to cater for the cases in which unanticipated sequences of i o and rule activation events occur this paper examines this issue from the perspective of temporal databases and discusses a framework for accommodating delay in rule activation in order to do this the paper also outlines a flexible technique to ensure correct transaction sequencing in transaction time databases 1 introduction active database systems allow users to specify business rules commonly in terms of sets of event condition action e c a triplets that specify that certain actions should be invoked when certain events occur and certain conditions hold 17 such rules are useful in providing an active on line 3d gesture recognition utilising dissimilarity measures in the field of human computer interaction hci gesture recognition is becoming increasingly important as a mode of communication in addition to the more common visual aural and oral modes and is of particular interest to designers of augmentative and alternative communication aac systems for people with disabilities a complete microcomputer system is described gesrec3d which facilitates the data acquisition segmentation learning and recognition of 3 dimensional arm gestures the gesture data is acquired from a polhemus electro magnetic tracker system where sensors are placed on the finger wrist and elbow of one arm coded gestures are linked to user defined text to be typed or spoken by a text to speech engine which is integrated into the system a segmentation method and an algorithm for classification are both presented which includes acceptance rejection thresholds based on intra class and inter class dissimilarity measures results of recognition hits confusion embodied evolution embodying an evolutionary algorithm in a population of robots we introduce embodied evolution ee as a methodology for the automatic design of robotic controllers ee is an evolutionary robotics er technique that avoids the pitfalls of the simulate and transfer method allows the speed up of evaluation time by utilizing parallelism and is particularly suited to future work on multi agent behaviors in ee an evolutionary algorithm is distributed amongst and embodied within a population of physical robots that reproduce with one another while situated in the task environment we have built a population of eight robots and successfully implemented our first experiments the controllers evolved by ee compare favorably to hand designed solutions for a simple task we detail our methodology report our initial results and discuss the application of ee to more advanced and distributed robotics tasks 1 introduction our work is inspired by the following vision a large number of robots freely interact with each other in a shared environment atte fast supervised dimensionality reduction algorithm with applications to document categorization retrieval retrieval techniques based on dimensionality reduction such as latent semantic indexing lsi have been shown to improve the quality of the information being retrieved by capturing the latent meaning of the words present in the documents unfortunately the high computational and memory requirements of lsi and its inability to compute an effective dimensionality reduction in a supervised setting limits its applicability in this paper we present a fast supervised dimensionality reduction algorithm that is derived from the recently developed cluster based unsupervised dimensionality reduction algorithms we experimentally evaluate the quality of the lower dimensional spaces both in the context of document categorization and improvements in retrieval performance on a variety of different document collections our experiments show that the lower dimensional spaces computed by our algorithm consistently improve the performance of traditional algorithms such as c4 5 k nearest neighbor a practical approach for recovery of evicted variables src s charter is to advance the state of the art in computer systems by doing basic and applied research in support of our company s business objectives our interests and projects span scalable systems including hardware networking distributed systems and programming language technology the internet including the web e commerce and information retrieval and human computer interaction including user interface technology computer based appliances and mobile computing src was established in 1984 by digital equipment corporation we test the value of our ideas by building hardware and software prototypes and assessing their utility in realistic settings interesting systems are too complex to be evaluated solely in the abstract practical use enables us to investigate their properties in depth this experience is useful in the short term in refining our designs and invaluable in the long term in advancing our knowledge most of the major advances in information systems have come through this approach including personal computing distributed systems and the internet we also perform complementary work of a more mathematical character some of super logic programs recently considerable interest and research e ort has been given to the problem of finding a suitable extension of the logic programming paradigm beyond the class of normal logic programs in order to demonstrate that a class of programs can be justifiably called an extension of logic programs one should be able to argue that the proposed syntax of such programs resembles the syntax of logic programs but it applies to a significantly broader class of programs the proposed semantics of such programs constitutes an intuitively natural extension of the semantics of normal logic programs there exists a reasonably simple procedural mechanism allowing at least in principle to compute the semantics the proposed class of programs and their semantics is a special case of a more general non monotonic formalism which clearly links it to other well established non monotonic formalisms in this paper we propose a specific class of extended logic programs which will be modestly called super logic programs or just super programs we will argue that the class of super programs satisfies all of the above conditions and in addition is su ciently flexible to allow various application dependent extensions and modifications we also provide a brief description of a prolog implementation of a query answering interpreter for the class of super programs which is available via ftp and www keywords non monotonic reasoning logics of knowledge and beliefs semantics of logic programs and deductive databases an extended abstract of this paper appeared in the proceedings of the fifth international conference on principles of knowledge representation and reasoning kr 96 boston massachusetts 1996 pp 529 541 partially supported by the national science fou applying formal concepts to learning systems validation in the problem area of evaluating complex software systems there are two distinguished areas of research development and application identified by the two buzzwords validation and verification respectively from the perspective adopted by the authors verification is usually more formally based and thus can be supported by formal reasoning tools like theorem provers for instance the scope of verification approaches is limited by the difficulty of finding a sufficiently complete formalization to built upon in paramount realistic problem domains validation seems to be more appropriate although it is less stringent in character and therefore validation results are often less definite the aim of this paper is to exemplify a validation approach based on a clear and thoroughly formal theory in this way validation and verification should be brought closer to each other to allow for precise and sufficiently clear results the authors have selected the applicatio visual exploration of temporal object databases two complementary families of users tasks may be identified during database visualization data browsing and data analysis on the one hand data browsing involves extensively exploring a subset of the database using navigational interaction techniques classical object database browsers provide means for navigating within a collection of objects and amongst objects by way of their relationships in temporal object databases these techniques are not sufficient to adequately support time related tasks such as studying a snapshot of a collection of objects at a given instant or detecting changes within temporal attributes and relationships visual data analysis on the other hand is dedicated to the extraction of valuable knowledge by exploiting the human visual perception capabilities in temporal databases examples of data analysis tasks include observing the layout of a history detecting regularities and trends and comparing the evolution of the values taken by two or more histories in this paper we identify several users tasks related to temporal database exploration and we propose three novel visualization techniques addressing them the first of them is dedicated to temporal object browsing while the two others are oriented towards the analysis of quantitative histories all three techniques are shown to satisfy several ergonomic properties keywords temporal database object database data browsing and analysis visualization technique 1 query expansion using an interactive concept hierarchy query expansion is the process of supplementing an original query with additional terms in order to refine a search and increase retrieval effectiveness if the query expansion is interactive then the user and the system work together to expand the query the system usually suggests possible expansion terms and the user selects those they wish to add to the query studies have shown that interactive query expansion has the potential to improve retrieval effectiveness but that it rarely succeeds in achieving its potential it has been shown that users desire some control over the expansion process in order to achieve this the functionality of the system must be represented on the user interface in a comprehensible way the main aim of this study is to focus on a small aspect of the interface and investigate whether the method used to present potential query expansion terms has any effect on retrieval effectiveness the tool tested in this study automatically generates a hierarchical logical structure detection for heterogeneous document classes we present a fully implemented system based on generic document knowledge for detecting the logical structure of documents for which only general layout information is assumed in particular we focus on detecting the reading order our system integrates components based on computer vision articial intelligence and natural language processing techniques the prominent feature of our framework is its ability to handle documents from heterogeneous collections the system has been evaluated on a standard collection of documents to measure the quality of the reading order detection experimental results for each component and the system as a whole are presented and discussed in detail the performance of the system is promising especially when considering the diversity of the document collection keywords document analysis logical structure detection reading order detection natural language processing spatial reasoning 1 introduction the goal of document analysis is to automa declarative semantics of belief queries in mls deductive databases a logic based language called multilog for multi level secure relational databases has recently been proposed it has been shown that multilog is capable of capturing the notion of user belief of ltering unwanted and useless information in its proof theory additionally it can guard against a previously unknown security breach the so called surprise stories in this paper we outline a possible approach to a declarative characterization of belief queries in multilog in a very informal manner we show that for simple programs with belief queries the semantics is rather straight forward semantics for the general horn programs may be developed based on the understanding of the model theoretic characterization of belief queries developed in this paper keywords multi level security belief queries declarative semantics completeness introduction in a recent research jukic and vrbsky 8 demonstrate that users in the relational mls model potentially have a cluttered view continuous categories for a mobile robot autonomous agents make frequent use of knowledge in the form of categories categories of objects human gestures web pages and so on this paper describes a way for agents to learn such categories for themselves through interaction with the environment in particular the learning algorithm transforms raw sensor readings into clusters of time series that have predictive value to the agent we address several issues related to the use of an uninterpreted sensory apparatus and show specific examples where a pioneer 1 mobile robot interacts with objects in a cluttered laboratory setting introduction there is nothing more basic than categorization to our thought perception action and speech lakoff 1987 for autonomous agents categories often appear as abstractions of raw sensor readings that provide a means for recognizing circumstances and predicting effects of actions for example such categories play an important role for a mobile robot that navigates around obstacles formalising the knowledge content of case memory systems discussions of case based reasoning often reflect an implicit assumption that a case memory system will become better informed i e will increase in knowledge as more cases are added to the case base this paper considers formalisations of this knowledge content which are a necessary preliminary to more rigourous analysis of the performance of case based reasoning systems in particular we are interested in modelling the learning aspects of case based reasoning in order to study how the performance of a case based reasoning system changes as it accumlates problem solving experience the current paper presents a case base semantics which generalises recent formalisations of case based classification within this framework the paper explores various issues in assuring that these sematics are well defined and illustrates how the knowledge content of the case memory system can be seen to reside in both the chosen similarity measure and in the cases of the case base 1 introduction formal concepts of learning systems validation in use in the problem area of evaluating complex software systems there are two distinguished areas of research development and application identified by the two buzzwords validation and verification respectively from the perspective adopted by the authors cf o keefe o leary 1993 e g verification is usually more formally based and thus can be supported by formal reasoning tools like theorem provers for instance the scope of verification approaches is limited by the difficulty of finding a sufficiently complete formalization to built upon in paramount realistic problem domains validation seems to be more appropriate although it is less stringent in character and therefore validation results are often less definite the aim of this paper is to exemplify a validation approach based on a clear and thoroughly formal theory in this way validation and verification should be brought closer to each other for the benefit of a concerted action towards depend multi agent systems coalition formation 5 4 payoff division overview 119 5 contract nets coalition formation 119 1 chapter 5 contract nets coalition formation multi agent systems 6 lectures sept 2000 bahia blanca 5 1 general contract nets how to distribute tasks global market mechanisms implementations use a single centralized mediator announce bid award cycle distributed negotiation we need the following 1 define a task allocation problem in precise terms 2 define a formal model for making bidding and awarding decisions 5 1 general contract nets 120 chapter 5 contract nets coalition formation multi agent systems 6 lectures sept 2000 bahia blanca definition 5 1 task allocation problem a task allocation problem is given by 1 a set of tasks t 2 a set of agents a a a 3 a cost function cost i i i 2 t r stating the costs that agent i i i incurs by handling some tasks and 4 the initial allocation of tasks t init 1 1 1 t init views in a large scale xml repository we are interested in maintaining and querying views in a huge and highly heterogeneous xml repository web scale in this context views are very large and there is no apparent limitation to their size this raises interesting problems that we address in the paper i how to distribute views over several machines without having a negative impact on the query translation process ii how to quickly select the relevant part of a view given a query iii how to minimize the cost of communicating potentially large queries to the machines where they will be evaluated 1 introduction we believe that xml will soon take an important and increasing share of the data published on the web this represents a major opportunity to at last provide an intelligent access to this amazing source of information with that goal in mind the xyleme 18 project is building a warehouse which will store and provide sophisticate database like services over all the xml documents of the web notably performance analysis of mobile agents for filtering data streams on wireless networks wireless networks are an ideal environment for mobile agents since their mobility allows them to move across an unreliable link to reside on a wired host next to or closer to the resources that they need to use furthermore clientspecific data transformations can be moved across the wireless link and run on a wired gateway server reducing bandwidth demands in this paper we examine the tradeoffs faced when deciding whether to use mobile agents in a datafiltering application where numerous wireless clients filter information from a large data stream arriving across the wired network we develop an analytical model and use parameters from filtering experiments conducted during a u s navy fleet battle experiment fbe to explore the model s implications 1 introduction mobile agents are programs that can migrate from host to host in a network of computers at times and to places of their own choosing unlike applets both the code and the execution state heap and stack move with dynamic agents for dynamic service provisioning we claim that a dynamic agent infrastructure can provide a shift from static distributed computing to dynamic distributed computing and we have developed such an infrastructure to realize such a shift we shall show its impact on software engineering through a comparison with other distributed object oriented systems such as corba and dcom and demonstrate its value in highly dynamic system integration and service provisioning the infrastructure is java based light weight and extensible it differs from other agent platforms and client server infrastructures in its support of dynamic behavior modification of agents a dynamic agent is not designed to have a fixed set of predefined functions but instead to carry application specific actions which can be loaded and modified on the fly this allows a dynamic agent to adjust its capability for accommodating environment and requirement changes and play different roles across multiple applications the above features are supported b an agent based approach for manufacturing integration the ciimplex experience the production management system used by most manufacturers today is comprised of disconnected planning and execution processes and lacks the support for interoperability and collaboration needed for enterprise wide integration this situation often prevents the manufacturer from fully exploring market opportunities in a timely fashion to address this problem we are exploring an agent based approach to intelligent enterprise integration in this approach a set of agents with specialized expertise can be quickly assembled to help with the gathering of relevant information and knowledge to cooperate with each other and with other parts of the production management system and humans to arrive at timely decisions in dealing with various enterprise scenarios the proposed multi agent system including its architecture and implementation are presented and demonstrated through an example integration scenario involving real planning and execution software systems 1 introduction the p ubiquitous web information agents this paper gives a brief overview about the ai methods and techniques we have developed for building ubiquitous web information systems these methods from areas of machine learning logic programming knowledge representation and multi agent systems are discussed in the context of our prototypical information system mia mia is a web information system for mobile users who are equipped with a pda palm pilot a cellular phone and a gps device or cellular wap phone it captures the main issues of ubiquitous computing location awareness anytime information access and pda technology 1 introduction nowadays the biggest but also the most chaotic and unstructured source of information is the world wide web making this immense amount of information available for ubiquitous computing in daily life is a great challenge besides hardware issues for wireless ubiquitous computing that still are to be solved wireless communication blue tooth technologies wearable computing u decidable fragments of first order modal logics the paper considers the set ml1 of first order polymodal formulas the modal operators in which can be applied to subformulas of at most one free variable using the mosaic technique we prove a general satisfiability criterion for formulas in ml1 which reduces the modal satisfiability to the classical one the criterion is then used to single out a number of new in a sense optimal decidable fragments of various predicate modal logics 1 introduction the classical decision problem to single out expressive and decidable fragments of first order logic has a long history and hardly needs any justification after all classical first order logic was and still remains in the very center of logical studies both in mathematics and applications here are only three examples out of dozens of such fragments the choice is not accidental we shall use these results later on ffl the fragment containing only monadic predicate symbols 5 ffl the fragment with only two individual vari a fast multi dimensional algorithm for drawing large graphs we present a novel hierarchical force directed method for drawing large graphs the algorithm produces a graph embedding in an euclidean space e of any dimension a two or three dimensional drawing of the graph is then obtained by projecting a higher dimensional embedding into a two or three dimensional subspace of e projecting high dimensional drawings onto two or three dimensions often results in drawings that are smoother and more symmetric among the other notable features of our approach are the utilization of a maximal independent set filtration of the set of vertices of a graph a fast energy function minimization strategy e cient memory management and an intelligent initial placement of vertices our implementation of the algorithm can draw graphs with tens of thousands of vertices using a negligible amount of memory in less than one minute on a mid range pc 1 introduction graphs are common in many applications from data structures to networks from software engineering a novel server selection technique for improving the response time of a replicated service server replication is an approach often used to improve the ability of a service to handle a large number of clients one of the important factors in the efficient utilization of replicated servers is the ability to direct client requests to the best server according to some optimality criteria in this paper we target an environment in which servers are distributed across the internet and clients identify servers using our application layer anycasting service our goal is to allocate servers to clients in a way that minimizes a client s response time to that end we develop an approach for estimating the performance that a client would experience when accessing particular servers such information is maintained in a resolver that clients can query to obtain the identity of the server with the best response time our performance collection technique combines server push with client probes to estimate the expected response time a set of experiments is used to demonstrate the propert autonomous robot that uses symbol recognition and artificial emotion to attend the aaai conference this paper describes our approach in designing an autonomous robot for the aaai mobile robot challenge making the robot attend the national conference on ai the goal was to do a simplified version of the whole task by integrating methodologies developed in various research projects conducted in our laboratory original contributions are the use of a symbol recognition technique to make the robot read signs artificial emotion for expressing the state of the robot in the accomplishment of its goals a touch screen for human robot interaction and a charging station for allowing the robot to recharge when necessary all of these aspects are influenced by the different steps to be followed by the robot attendee to complete the task from start to end introduction laborius is a young research laboratory interested in designing autonomous systems that can assist human in real life tasks to do so robots need some sort of social intelligence giving them the ability to learning comprehensible conceptual user models for user adaptive meta web search in course of the oyster project our goal is to induce conceptual user models that allow for a transparent query refinement and information filtering in the domain of www meta search user models which describe a user s interest with respect to an underlying ontology allow for a manual user model editing process and also pose a well defined problem for a conceptual inductive learning task oyster is a research prototype that is currently being developed at the university of osnabruck introduction user modeling and machine learning user models represent assumptions about a user user modeling systems infer user models from user interaction store user models and induce new assumptions by reasoning about the models these models are used within the system in order to adapt to the user furthermore these models shall be accessible to the user they should be both understandable and manually modifyable incorporating machine learning into this framework often leads to intertwine indexing techniques for continuously evolving phenomena the management of spatial temporal and spatiotemporal data is becoming increasingly important in a wide range of applications this ongoing ph d project focuses on applications where spatial or temporal aspects of objects are continuously changing and there is a need for indexing techniques that track the changing data even in between explicit updates in spatiotemporal applications there is a need to record and efficiently query the history the current state and the predicted future behavior of continuously moving objects such as vehicles mobile telephones and people likewise in temporal applications and spatiotemporal applications with discrete change time intervals may be naturally related to the current time which continuously progresses the paper outlines the research agenda of the ph d project and describes briefly two access methods developed so far in this project 1 introduction recent years have shown both an increase in the amounts of rotational polygon containment and minimum enclosure an algorithm and a robust floating point implementation is given for rotational polygon containment given polygons p 1 p 2 p 3 p k and a container polygon c find rotations and translations for the k polygons that place them into the container without overlapping a version of the algorithm and implementation also solves rotational minimum enclosure givenaclass c of container polygons find a container c in c of minimum area for which containment has a solution the minimum enclosure is approximate it bounds the minimum area between 1 epsilon a and a experiments indicate that finding the minimum enclosure is practical for k 2 3 but not larger unless optimality is sacrificed or angles ranges are limited although these solutions can still be useful important applications for these algorithm to industrial problems are discussed the paper also gives practical algorithms and numerical techniques for robustly calculating polygon set intersection minkowski sum and range in view independent recognition of hand postures since human hand is highly articulated and deformable hand posture recognition is a challenging example in the research of view independent object recognition due to the difficulties of the modelbased approach the appearance based learning approach is promising to handle large variation in visual inputs however the generalization of many proposed supervised learning methods to this problem often suffers from the insufficiency of labeled training data this paper describes an approach to alleviate this difficulty by adding a large unlabeled training set combining supervised and unsupervised learning paradigms a novel and powerful learning approach the discriminant em d em algorithm is proposed in this paper to handle the case of small labeled training set experiments show that d em outperforms many other learning methods based on this approach we implement a gesture interface to recognize a set o a collaborative internet documents access scheme using acird in this paper we present a collaborative intelligent internet multi web sites documents search system using acird acird is a system that automatically learns the classification knowledge from web pages and applies the knowledge to automatic classification of web pages to some classes in a class hierarchy data mining technique is used to learn the association of terms to discover the hidden semantic connections between terms with the capabilities of acird it is straightforward to extend acird to collaborate multi web site document access based on the learned classification knowledge a collaborative two phase search engine is proposed which dispatches queries to distributed web sites to match documents and presents hierarchically navigable results to the internet users rather than conventional ranked flat results 1 introduction the rapid growth of the internet has changed the way of working and living that the internet becomes a major source of information and means of commun probabilistic deduction with conditional constraints over basic events we study the problem of probabilistic deduction with conditional constraints over basic events we show that globally complete probabilistic deduction with conditional constraints over basic events is np hard we then concentrate on the special case of probabilistic deduction in conditional constraint trees we elaborate very efficient techniques for globally complete probabilistic deduction in detail for conditional constraint trees with point probabilities we present a local approach to globally complete probabilistic deduction which runs in linear time in the size of the conditional constraint trees for conditional constraint trees with interval probabilities we show that globally complete probabilistic deduction can be done in a global approach by solving nonlinear programs we show how these nonlinear programs can be transformed into equivalent linear programs which are solvable in polynomial time in the size of the conditional constraint trees 1 introduction dealing wit conceptual linking ontology based open hypermedia this paper describes the attempts of the cohse project to define and deploy a conceptual open hypermedia service consisting of an ontological reasoning service which is used to represent a sophisticated conceptual model of document terms and their relationships a web based open hypermedia link service that can offer a range of different linkproviding facilities in a scalable and non intrusive fashion and integrated to form a conceptual hypermedia system to enable documents to be linked via metadata describing their contents and hence to improve the consistency and breadth of linking of www documents at retrieval time as readers browse the documents and authoring time as authors create the documents introduction concepts and metadata metadata is data that describes other data to enhance its usefulness the library catalogue or database schema are canonical examples for our purposes metadata falls into three broad categories catalogue information e g the artist dynamic cpu scheduling with imprecise knowledge of computation time the majority of the studies conducted in scheduling real time transactions mostly concentrate on concurrency control protocols while overlooking the cpu as being the primary resource consequently there are various techniques for scheduling the cpu in conventional time critical systems meanwhile there does not seem to be any technique that is adequately designed for scheduling such a resource in real time database rtdb systems in this paper we construct an efficient cpu scheduling scheme that minimizes the preemption rate in order to reduce the frequency by which synchronization protocols must be invoked along with their inherited performance degradation in addition we also introduce a new timing model upon which the newly introduced scheduler is incorpo rated in order to utilize the system s imprecise knowledge of computation time estimates keywords cpu scheduling lowering preemption timeliness functions and imprecise computation estimates 1 introductio return from the ant synthetic ecosystems for manufacturing control the synthetic ecosystems approach attempts to adopt basic principles of natural ecosystems in the design of multiagent systems natural agent systems like insect colonies are fascinating in that they are robust flexible and adaptive made up of millions of very simple entities these systems express a highly complex and coordinated global behavior there are several branches in different sciences for instance in biology physics economics or in computer science that focus on distributed systems of locally interacting entities their research yields a number of commonly observed characteristics to supply engineered systems with similar characteristics this thesis proposes a set of principles that should be observed when designing synthetic ecosystems each principle is systematically stated and motivated and its consequences for the manufacturing control domain are discussed stigmergy has shown its usefulness in the coordination of large crowds of agents in a synthetic ecosystem an adaptive and distributed framework for advanced ir it has been often noticed that modern ir gregory 1991 alan 1991 should exhibit capabilities that are sensitive to the document content integrate interactivity multimodality and multilinguality over a large scale and support the very dynamic nature of the current needs for information access so to be adaptable to chanes of the sources language and content style this paper discuss the architectural design aspects of trevi text retrieval and enrichment for vital information esprit project ep23311 a distributed object oriented java corba driven system for nlp driven news classification enrichment and delivery the advanced features of trevi include the extensive use of a well defined model mazzucchelli 1999 based on a typed mechanism for static dynamic control of the distributed process and on a principled representation of linguistic types into computational oo data structures and the adaptivity of the employed linguistic processors namely the robust and lexica discovering internet resources to enrich a structured personal information space the internet is a tremendous resource where one can find documents to enrich a personal information space the question is how can one find relevant documents and how can these be organized into an information space in this paper we describe a prototype which aims to provide the user with assistance in these two tasks our approach assumes the existence of an initial concept structure set up by the user this structure may contain only rudimentary descriptions for each concept the system s task is to find relevant documents from the internet and to insert them in the appropriate places in the concept structure 1 information management for internet users the amount of information available through the internet is overwhelming as a result most of this information goes unnoticed or gets lost again soon after having been noticed the problem is not new it is just being exacerbated by two factors a sudden growth in the number of information consumers accompanied by acceleration towards the standardization of multi agent systems architectures an overview this article briefly describes these groups efforts toward the standardization of multi agent systems architectures and sketches early works to define a multi agent systems architecture at the university of calgary however the main objective of this article is to give the reader a basic overview of the background and terminology in this exciting area of research a taxonomy of web agents in this paper we propose a taxonomy of web agents which encompasses agents that provide a text based interface to for example information retrieval services as well as avatarembodied guides that help visitors to navigate in virtual environments our taxonomy must be regarded as an instrument to delineate targets for research and the realization of prototype applications that demonstrate the usefulness of agent based intelligence on the web in addition we deploy our agent taxonomy to establish the implications particular target applications have with respect to software architecture and computational resources 1 introduction there is a lot of interest and work in the research and development of agent technology with applications on the web many types of web agents have been proposed in recent years which range from domain dependent agents like e commerce agents information gathering agents to function dependent agents like negotiation agents cooperating agents in addition towards efficient multi feature queries in heterogeneous environments applications like multimedia databases or enterprisewide information management systems have to meet the challenge of efficiently retrieving best matching objects from vast collections of data we present a new algorithm stream combine for processing multi feature queries on heterogeneous data sources stream combine is selfadapting to different data distributions and to the specific kind of the combining function furthermore we present a new retrieval strategy that will essentially speed up the output of relevant objects intelligent data analysis in medicine extensive amounts of knowledge and data stored in medical databases require the development of specialized tools for storing and accessing of data data analysis and effective use of stored knowledge and data this paper focuses on methods and tools for intelligent data analysis aimed at narrowing the increasing gap between data gathering and data comprehension the paper sketches the history of research that led to the development of current intelligent data analysis techniques discusses the need for intelligent data analysis in medicine and proposes a classification of intelligent data analysis methods the scope of the paper covers temporal data abstraction methods and data mining methods a selection of methods is presented and illustrated in medical problem domains presently data abstraction and data mining are attracting considerable research interest however the two technologies in spite of the fact that they share their central objective namely the intelligen conditional random fields probabilistic models for segmenting and labeling sequence data we present conditional random elds a framework for building probabilistic models to segment and label sequence data conditional random fields offer several advantages over hidden markov models and stochastic grammars for such tasks including the ability to relax strong independence assumptions made in those models conditional random fields also avoid a fundamental limitation of maximum entropy markov models memms and other discriminative markov models based on directed graphical models which can be biased towards states with few successor states we present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to hmms and memms on synthetic data 1 introduction the need to segment and label sequences arises in many different problems in several scientific fields hidden markov models hmms and stochastic grammars are well understood and widely used probabilistic models for such problems i a new clustering algorithm for segmentation of magnetic resonance images mixed initiative interfaces for learning tasks smartedit talks back applications of machine learning can be viewed as teacherstudent interactions in which the teacher provides training examples and the student learns a generalization of the training examples one such application of great interest to the iui community is adaptive user interfaces in the traditional learning interface the scope of teacher student interactions consists solely of the teacher user providing some number of training examples to the student learner and testing the learned model on new examples active learning approaches go one step beyond the traditional interaction model and allow the student to propose new training examples that are then solved by the teacher in this paper we propose that interfaces for machine learning should even more closely resemble human teacher student relationships a teacher s time and attention are precious resources an intelligent student must proactively contribute to the learning process by reasoning about the quality of its knowledge collaborating with the teacher and suggesting new examples for her to solve the paper describes a variety of rich interaction modes that enhance the learning process and presents a decision theoretic framework called diamand for choosing the best interaction we apply the framework to the smartedit programming by demonstration system and describe experimental validation and preliminary user feedback sensing techniques for mobile interaction we describe sensing techniques motivated by unique aspects of human computer interaction with handheld devices in mobile settings special features of mobile interaction include changing orientation and position changing venues the use of computing as auxiliary to ongoing real world activities like talking to a colleague and the general intimacy of use for such devices we introduce and integrate a set of sensors into a handheld device and demonstrate several new functionalities engendered by the sensors such as recording memos when the device is held like a cell phone switching between portrait and landscape display modes by holding the device in the desired orientation automatically powering up the device when the user picks it up the device to start using it and scrolling the display using tilt we present an informal experiment initial usability testing results and user reactions to these techniques keywords input devices interaction techniques sensing contextaware extracting collocations from text corpora a collocation is a habitual word combination collocational knowledge is essential for many tasks in natural language processing we present a method for extracting collocations from text corpora by comparison with the susanne corpus we show that both high precision and broad coverage can be achieved with our method finally we describe an application of the automatically extracted collocations for computing word similarities assessing software libraries by browsing similar classes functions and relationships comparing and contrasting a set of software libraries is useful for reuse related activities such as selecting a library from among several candidates or porting an application from one library to another the current state of the art in assessing libraries relies on qualitative methods to reduce costs and or assess a large collection of libraries automation is necessary although there are tools that help a developer examine an individual library in terms of architecture style etc we know of no tools that help the developer directly compare several libraries with existing tools the user must manually integrate the knowledge learned about each library automation to help developers directly compare and contrast libraries requires matching of similar components such as classes and functions across libraries this is different than the traditional component retrieval problem in which components are returned that best match a user s query rather we need to find those component let s talk socially intelligent agents for language conversation training abstract this paper promotes socially intelligent animated agents for the pedagogical task of english conversation training for native speakers of japanese since student agent conversations are realized as role playing interactions strong requirements are imposed on the agents affective and social abilities as a novel feature social role awareness is introduced to animated conversational agents that are by now strong affective reasoners but otherwise often lack the social competence observed with humans in particular humans may easily adjust their behavior depending on their respective role in a social setting whereas their synthetic pendants tend to be driven mostly by emotions and personality our main contribution is the incorporation of a social filter program to mental models of animated agents this program may qualify an agent s expression of its emotional state by the social context thereby enhancing the agent s believability as a conversational partner our implemented system is web based and demonstrates socially aware animated agents in a virtual coffee shop environment an experiment with our conversation system shows that users consider socially aware agents as more natural than agents that violate conventional practices lessons learned from the scientist s expert assistant project during the past two years the scientist s expert assistant sea team has been prototyping proposal development tools for the hubble space telescope in an effort to demonstrate the role of software in reducing support costs for the next generation space telescope ngst this effort has been a success the hubble space telescope has adopted two sea prototype tools the exposure time calculator and visual target tuner for operational use the space telescope science institute is building a new set of observing tools based on sea technology these tools will hopefully be a foundation that is easily adaptable to other observatories including ngst the sea project has aggressively pursued the latest software technologies including java distributed computing xml web distribution and expert systems some technology experiments proved to be dead ends while other technologies were unexpectedly beneficial we have also worked with other projects to foster collaboration between the vario a tutorial on support vector machines for pattern recognition abstract the tutorial starts with an overview of the concepts of vc dimension and structural risk minimization we then describe linear support vector machines svms for separable and non separable data working through a non trivial example in detail we describe a mechanical analogy and discuss when svm solutions are unique and when they are global we describe how support vector training can be practically implemented and discuss in detail the kernel mapping technique which is used to construct svm solutions which are nonlinear in the data we show how support vector machines can have very large even infinite vc dimension by computing the vc dimension for homogeneous polynomial and gaussian radial basis function kernels while very high vc dimension would normally bode ill for generalization performance and while at present there exists no theory which shows that good generalization performance is guaranteed for svms there are several arguments which support the observed high accuracy of svms which we review results of some experiments which were inspired by these arguments are also presented we give numerous examples and proofs of most of the key theorems there is new material and i hope that the reader will find that even old material is cast in a fresh light providing an embedded software environment for wireless pdas the use of wireless pdas is foreseen to outrun the one of pcs in the near future however for this to actually happen adequate software environments must be devised in order to allow the execution of various types of applications this paper introduces the base features of such an environment which is a customizable jvm based middleware in particular the middleware platform embeds services for appropriate resource management and for supporting novel pda oriented applications 1 introduction the use of wireless personal digital assistant pda devices is foreseen to outrun the one of pcs in the near future however for this to actually happen there is still the need to devise adequate software and hardware platforms the use of pdas should be as convenient as the one of pcs and in particular must not overly restrict the applications that are supported considering the ongoing effort towards providing convenient hardware platforms in industry this paper focuses on design issu mrml an extensible communication protocol for interoperability and benchmarking of multimedia information retrieval systems while in the area of relational databases interoperability is ensured by common communication protocols e g odbc jdbc using sql content based image retrieval systems cbirs and other multimedia retrieval systems are lacking both a common query language and a common communication protocol besides its obvious short term convenience interoperability of systems is crucial for the exchange and analysis of user data in this paper we present and describe an extensible xml based query markup language called mrml multimedia retrieval markup language mrml is primarily designed so as to ensure interoperability between dierent content based multimedia retrieval systems further mrml allows researchers to preserve their freedom in extending their system as needed mrml encapsulates multimedia queries in a way that enables multimedia mm query languages mm content descriptions mm query engines and mm user interfaces to grow independently from each other reaching a maximum of in will we have a wet summer soft computing models for long term rainfall forecasting long term rainfall prediction is very important to countries thriving on agro based economy in general climate and rainfall are highly non linear phenomena in nature giving rise to what is known as butterfly effect the parameters that are required to predict the rainfall are enormously complex and subtle so that uncertainty in a prediction using all these parameters is enormous even for a short period soft computing is an innovative approach to construct computationally intelligent systems that are supposed to possess humanlike expertise within a specific domain adapt themselves and learn to do better in changing environments and explain how they make decisions unlike conventional artificial intelligence techniques the guiding principle of soft computing is to exploit tolerance for imprecision uncertainty robustness partial truth to achieve tractability and better rapport with reality zadeh 1998 in this paper we analysed 87 years of rainfall data in kerala state the southern part of indian peninsula situated at latitude longitude pairs 8029 n 76057 e we attempted to train 5 soft computing based prediction models with 40 years of rainfall data for performance evaluation network predicted outputs were compared with the actual rainfall data simulation results reveal that soft computing techniques are promising and efficient using multi context systems to engineer executable agents in the area of agent based computing there are many proposals for specific system architectures and a number of proposals for general approaches to building agents as yet however there are few attempts to relate these together and even fewer attempts to provide methodologies which relate designs to architectures and then to executable agents this paper provides a first attempt to address this shortcoming we propose a general method of defining architectures for logic based agents which can be directly executed our approach is based upon the use of multi context systems and we illustrate its use with an example architecture capable of argumentation based negotiation 1 introduction agent based computing is fast emerging as a new paradigm for engineering complex distributed systems 13 27 an important aspect of this trend is the use of agent architectures as a means of delivering agent based functionality cf work on agent programming languages 14 23 25 in t keys for xml this paper the proposal extends the key speci cation of xml data by allowing one to specify keys in terms of xpath 24 expressions there are a number of technical problems in connection with xpath xpath is a relatively complex language in which one can not only move down the document tree but also sideways or upwards not to mention that predicates and functions can be embedded as well the problem with xpath is that questions about equivalence or inclusion of xpath expressions are as far as the authors are aware unresolved and these issues are importantifwewant to reason about keys as wedo in relational databases yet until we know how to determine the equivalence of xpath expressions there is no general method of saying whether two such speci cations are equivalent another technical issue is value equality xml schema restricts equality to text but the authors have encountered cases in whichkeys are not so restricted see section 7 1 for a more detailed discussion however the main reason for writing this paper is that none of the existing key proposals address the issue of hierarchical keys which appear to be ubiquitous in hierarchically structured databases especially in scienti c data formats a top level key may be used to identify components of a document and within each component a secondary key is used to identify sub components and so on moreover the authors believe that the use of keys for citing parts of a document is suciently important that it is appropriate to consider key speci cation independently of other proposals for constraining the structure of xml documents lock free scheduling of logical processes in parallel simulation with fixed lookahead information in a simulation model the overhead of asynchronous conservative parallel simulation lies in the mechanism used for propagating time updates in order for logical processes to safely advance their local simulation clocks studies have shown that a good scheduling algorithm should preferentially schedule processes containing events on the critical path this paper introduces a lock free algorithm for scheduling logical processes in conservative parallel discrete event simulation on shared memory multiprocessor machines the algorithm uses fetch add operations that help avoid inefficiencies associated with using locks the lock free algorithm is robust experiments show that compared with the scheduling algorithm using locks the lock free algorithm exhibits better performance when the number of logical processes assigned to each processor is small or when the workload becomes significant in models with large number of logical processes our algorithm sh an integrated ontology for the www knowledge intensive processing of www information should be founded on clear and uniform conceptualisation an integrated ontology covering different aspects of the www documents sites network addressing html code has been laid down upon which a knowledge base of the www domain is being built this knowledge base should support intelligent metasearch of the web in particular postprocessing of hit lists returned by external search engines 1 introduction during the last few years the world wide web has become one of the most widespread technologies of information presentation it is thus not surprising that many knowledge engineering ke projects focus on it some use html as a cheap ready made user interface other thrive to mine valuable information hidden inside existing www pages a necessary prerequisite of mutual comprehensibility and knowledge reuse among different ke communities and projects dealing with the web is a clear and unified conceptualisation wh human performance on clustering web pages a preliminary study with the increase in information on the world wide web it has become difficult to quickly find desired information without using multiple queries or using a topic specific search engine one way to help in the search is by grouping html pages together that appear in some way to be related in order to better understand this task we performed an initial study of human clustering of web pages in the hope that it would provide some insight into the difficulty of automating this task our results show that subjects did not cluster identically in fact on average any two subjects had little similarity in their web page clusters we also found that subjects generally created rather small clusters and those with access only to urls created fewer clusters than those with access to the full text of each web page generally the overlap of documents between clusters for any given subject increased when given the full text as did the percentage of documents clustered when analyzing individual subjects we found that each had different behavior across queries both in terms of overlap size of clusters and number of clusters these results provide a sobering note on any quest for a single clearly correct clustering method for web pages ontobroker how to make the www intelligent the world wide web can be viewed as the largest knowledge base that has ever existed however its support in query answering and automated inference is very limited we propose formalized ontologies as means to enrich web documents for representing semantic information to overcome this bottleneck ontologies enable informed search as well as the derivation of new knowledge that is not represented in the www the paper describes a software tool called ontobroker that provides the necessary support in realizing this idea basically it provides formalisms and tools for formulating queries for defining ontologies and for annotating html documents with ontological information 1 introduction the world wide web www contains huge amounts of knowledge about most subjects one can think of html documents enriched by multi media applications provide knowledge in different representations i e text graphics animated pictures video sound virtual reality etc hypertext li learning and tracking human motion using functional analysis we present a method for the modeling and tracking of human motion using a sequence of 2d video images our analysis is divided in two parts statistical learning and bayesian tracking first we estimate a statistical model of typical activities from a large set of 3d human motion data for this purpose the human body is represented as a set of articulated cylinders and the evolution of a particular joint angle is described by a time series specifically we consider periodic motion such as walking in this work and we develop a new set of tools that allows for the automatic segmentation of the training data into a sequence of identical motion cycles then we compute the mean and the principal components of these cycles using a new algorithm to account for missing information and to enforce smooth transitions between different cycles the learned temporal model provides a prior probability distribution over human motions which is used for tracking we adopt a bayesian perspective and approximate the posterior distribution of the body parameters using a particle filter the resulting algorithm is able to track human subjects in monocular video sequences and to recover their 3d motion in complex unknown environments 1 haptic perception of virtual roughness the texture of a virtual surface can both increase the sense of realism of an object as well as convey information about object identity type location function and so on it is crucial therefore that interface designers know the range of textural information available through the haptic modality in virtual environments the current study involves participants making roughness judgments on pairs of haptic textures experienced through a force feedback device the effect of texture frequency on roughness perception is analysed the potential range and resolution of textural information available through force feedback interaction are discussed keywords haptics force feedback texture perception introduction despite the increasing prevalence of haptics in today s computing environments the effective representation of haptic information is still a relatively new design problem for human computer interaction research force feedback interfaces in particular pose a variety of desi automatic text detection and tracking in digital video text which either appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video s structure and for classification in this paper we present algorithms for detecting and tracking text components that appear within digital video frames our system implements a scale space feature extractor that feeds an artificial neural processor to extract textual regions and track their movement over time the extracted regions can then be used as input to an appropriate optical character recognition system which produces indexible keywords keywords text detection text tracking video indexing digital libraries neural network wavelet the support of this effort by the department of defense under contract mda 9049 6c 1250 is gratefully acknowledged 1 introduction the increasing availability of online digital imagery and video has rekindled interest in the problems of how to index multimedia informa view based interpretation of real time optical flow for gesture recognition we have developed a real time view based gesture recognition system optical flow is estimated and segmented into motion blobs gestures are recognized using a rule based technique based on characteristics of the motion blobs such as relative motion and size parameters of the gesture e g frequency are then estimated using context specific techniques the system has been applied to create an interactive environment for children 1 introduction for many applications the use of hand and body gestures is an attractive alternative to the cumbersome interface devices for human computer interaction this is especially true for interacting in virtual reality environments where the user is no longer confined to the desktop and should be able to move around freely while special devices can be worn to achieve these goals these can be expensive and unwieldy there has been a recent surge in computer vision research to provide a solution that doesn t use such devices this paper describe machine learning for modeling dutch pronunciation variation this paper describes the use of rule induction techniques for the automatic extraction of phonemic knowledge and rules from pairs of pronunciation lexica this extracted knowledge allows the adaptation of speech processing systems to regional variants of a language as a case study we apply the approach to northern dutch and flemish the variant of dutch spoken in flanders a part of belgium based on celex and fonilex pronunciation lexica for northern dutch and flemish respectively in our study we compare two rule induction techniques transformationbased error driven learning tbedl brill 1995 and c5 0 quinlan 1993 and evaluate the extracted knowledge quantitatively accuracy and qualitatively linguistic relevance of the rules we conclude that whereas classication based rule induction with c5 0 is more accurate the transformation rules learned with tbedl can be more easily interpreted 1 introduction a central component of speech processing systems is a pronun towards text knowledge engineering we introduce a methodology for automating the maintenance of domain specific taxonomies based on natural language text understanding a given ontology is incrementally updated as new concepts are acquired from real world texts the acquisition process is centered around the linguistic and conceptual quality of various forms of evidence underlying the generation and refinement of concept hypotheses on the basis of the quality of evidence concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base appeared in aaai 98 proceedings of the 15th national conference on artificial intelligence july 26 30 1998 madison wisconsin forthcoming towards text knowledge engineering udo hahn klemens schnattinger l f computational linguistics group text knowledge engineering lab freiburg university werthmannplatz 1 d 79085 freiburg germany http www coling uni freiburg de abstract we introduce a me a survey of proxy cache evaluation techniques proxy caches are increasingly used around the world to reduce bandwidth requirements and alleviate delays associated with the world wide web in order to compare proxy cache performances objective measurements must be made in this paper we define a space of proxy evaluation methodologies based on source of workload used and form of algorithm implementation we then survey recent publications and show their locations within this space 1 introduction proxy caches are increasingly used around the world to reduce bandwidth and alleviate delays associated with the world wide web this paper describes the space of proxy cache evaluation methodologies and places current research within that space the primary contributions of this paper are threefold 1 definition and description of the space of evaluation techniques 2 appraisal of the di erent methods within that space and 3 a survey of cache evaluation techniques from the research literature in the next section we provide backgro the logic programming paradigm flexible queries to semi structured datasources the wg log approach a line of research is presented aimed at specifying both logical and navigational aspects of semi structured data sources such as web sites through the unifying notion of schema gracefully supporting schemata that are huge or subject to change the wg log language allows for a uniform representation of queries and views the latter expressing customized access structures to site information a survey of related work and some directions for future research involving fuzzy query techniques are also outlined 1 introduction and motivations modern network oriented information systems often have to deal with data that are semi structured i e lack the strict regular and complete structure required by traditional database management systems see abi97 and suc97 for a survey on semi structured data and related research information is semi structured also when the structure of data varies w r t time rather than w r t space even if data is fairly well structured such struc least squares temporal difference learning td is a popular family of algorithms for approximate policy evaluation in large mdps td works by incrementally updating the value function after each observed transition it has two major drawbacks it makes inefficient use of data and it requires the user to manually tune a stepsize schedule for good performance for the case of linear value function approximations and 0 the least squares td lstd algorithm of bradtke and barto bradtke and barto 1996 eliminates all stepsize parameters and improves data efficiency this paper extends bradtke and barto s work in three significant ways first it presents a simpler derivation of the lstd algorithm second it generalizes from 0 to arbitrary values of at the extreme of 1 the resulting algorithm is shown to be a practical formulation of supervised linear regression third it presents a novel intuitive interpretation of lstd as a model based reinforcement learning technique formal specification and design of mobile systems termination detection a classical problem in distributed computing is revisited in the new setting provided by the emerging mobile computing technology a simple solution tailored for use in ad hoc networks is employed as a vehicle for demonstrating the applicability of formal requirements and design strategies to the new field of mobile computing the approach is based on well understood techniques in specification refinement but the methodology is tailored to mobile applications and helps designers address novel concerns such as the mobility of hosts transient interactions and specific coordination constructs the proof logic and programming notation of mobile unity provide the intellectual tools required to carry out this task autonomous learning of sequential tasks experiments and analyses this paper presents a novel learning model clarion which is a hybrid model based on the twolevel approach proposed in sun 1995 the model integrates neural reinforcement and symbolic learning methods to perform on line bottom up learning i e learning that goes from neural to symbolic representations the model utilizes both procedural and declarative knowledge in neural and symbolic representations respectively tapping into the synergy of the two types of processes it was applied to deal with sequential decision tasks experiments and analyses in various ways are reported that shed light on the advantages of the model obstacles agent target figure 1 navigating through a minefield 1 introduction this paper presents a model that unifies neural symbolic and reinforcement learning it addresses the following three issues 1 it deals with autonomous learning it allows a situated agent to learn autonomously and continuously from on going experience in the world w awareness and privacy in mobile wearable computers ipads interpersonal awareness devices an inter personal awareness device or ipad is a handheld or wearable device designed to support awareness and collaboration between people who are in physical vicinity of each other this paper describes three ipad systems comparing their characteristics and approaches to the problem of opportunistic meeting the characteristics are identified according to the steinfield et al sjp99 awareness classification some of these mechanisms are discussed and some improvements and ideas are proposed in order to improve the systems key words cscw wearable computers awareness ipad mobility 1 introduction many studies about collaborative systems are concerned about the use of the desktop workstations and network technologies to provide support for distributed collaborative work and awareness these systems however are available on devices that are static and tied to the desk lh98 recently the development of mobile devices as wearable computers cellular phones and pdas perso dab interactive haptic painting with 3d virtual brushes we present a novel painting system with an intuitive haptic interface which serves as an expressive vehicle for interactively creating painterly works we introduce a deformable 3d brush model which gives the user natural control of complex brush strokes the force feedback enhances the sense of realism and provides tactile cues that enable the user to better manipulate the paint brush we have also developed a bidirectional two layer paint model that combined with a palette interface enables easy loading of complex blends onto our 3d virtual brushes to generate interesting paint effects on the canvas the resulting system dab provides the user with an artistic setting which is conceptually equivalent to a real world painting environment several users have tested dab and were able to start creating original art work within minutes an experimental performance evaluation of incremental materialized view maintenance in object databases abstract the development of techniques for supporting incremental maintenance of materialized views has been an active research area for over twenty years however although there has been much research on methods and algorithms there are surprisingly few systematic studies on the performance of different approaches as a result understanding of the circumstances in which materialized views are beneficial or not can be seen to lag behind research on incremental maintenance techniques this paper presents the results of an experimental performance analysis carried out in a system that incrementally maintains oql views in an odmg compliant object database the results indicate how the effectiveness of incremental maintenance is affected by issues such as database size and the complexity and selectivity of views 1 layered learning in genetic programming for a cooperative robot soccer problem we present an alternative to standard genetic programming gp that applies layered learning techniques to decompose a problem gp is applied to subproblems sequentially where the population in the last generation of a subproblem is used as the initial generation of the next subproblem this method is used to evolve agents to play keep away soccer a subproblem of robotic soccer that requires cooperation among multiple agents in a dynamic environment the layered learning paradigm allows gp to evolve better solutions faster than standard gp results show that the layered learning gp outperforms standard gp by evolving a lower fitness faster and an overall better fitness results indicate a wide area of future research with layered learning in gp cream creating relational metadata with a component based ontology driven annotation framework richly interlinked machine understandable data constitutes the basis for the semantic web annotating web documents is one of the major techniques for creating metadata on the web however annotation tools so far are restricted in their capabilities of providing richly interlinked and truely machine understandable data they basically allow the user to annotate with plain text according to a template structure such as dublin core we here present cream creating relational annotationbased metadata a framework for an annotation environment that allows to construct relational metadata i e metadata that comprises class instances and relationship instances these instances are not based on a fix structure but on a domain ontology we discuss some of the requirements one has to meet when developing such a framework e g the integration of a metadata crawler inference services document management and information extraction and describe its implementation viz ont o mat a component based ontology driven annotation tool direct value approximation for factored mdps we present a simple approach for computing reasonable policies for factored markov decision processes mdps when the optimal value function can be approximated by a compact linear form improving cross language retrieval with triangulated translation most approaches to cross language information retrieval assume that resources providing a direct translation between the query and document languages exist this paper presents research examining the situation where such an assumption is false here an intermediate or pivot language provides a means of transitive translation of the query language to that of the document via the pivot at the cost however of introducing much error the paper reports the novel approach of translating in parallel across multiple intermediate languages and fusing the results such a technique removes the error raising the effectiveness of the tested retrieval system up to and possibly above the level expected had a direct translation route existed across a number of retrieval situations and combinations of languages the approach proves to be highly effective inducing content based user models with inductive logic programming techniques in this paper we describe an approach for conceptual user visual specification of queries for finding patterns in time series data widespread interest in discovering features and trends in time series has generated a need for tools that support interactive exploration this paper introduces timeboxes a powerful graphical directmanipulation metaphor for the specification of queries over time series datasets our timefinder implementation of timeboxes supports interactive formulation and modification of queries thus speeding the process of exploring time series data sets and guiding data mining timefinder includes windows for timebox queries individual time series and details on demand other features include drag and drop support for query by example and graphical envelopes for displaying the extent of the entire data set and result set from a given query extensions involving increased expressive power and general temporal data sets are discussed db prism integrated data warehouses and knowledge networks for bank controlling db prism is an integrated data warehouse system the relations between technologies for human learning and agents in this position paper we review the historical emergence of agents optimal aggregation algorithms for middleware assume that each object in a database has m grades or scores one for each of m attributes forexamzuan object can ave a color grade t at tells ow red it is and a s ape grade t at tells ow round it is for eac attribute t ere is a sorted list w ic lists eac object and its grade under t at attribute sorted by grade ig est grade first eac object is assigned an overall grade t at is obtained bycom t ng t e attribute grades using a fixedm notone aggregation function orcombining rule suc as mh or average to determ h t e top k objects t at is k objects wit t e ig est overall grades t e naive algoritm mor access every object in t e database to find its grade under eac attribute fagin as given an algoritm fagin s algorit mit or fa t at is mh mh efficient for som mm t e aggregation functions fa isoptim al wit ig probability in t e worst case we analyze an elegant andrem lysim ple algoritm t e t res old algoritm or ta t at isoptim al in am stronger sense t an fa we s ow t at ta is essentiallyoptim al not just for som mmt k aggregation functions but for all of tem and not just in a ig probability worst case sense but over every database unlike fa w ic requires large buffers w ose sizemz grow unboundedly as t e database size grows ta requires only asmkzz constant size buffer ta allows early stopping w ic yields in a precise sense anapproxim ate version of t e top k answers we distinguis two types of access sorted access w ere t em u ewaresystem obtains t e grade of an object insom sorted list by proceeding t roug t e list sequentiallyfrom t e top and random access w ere t e mzueth resystem requests t e grade of object in a list and obtains it in one step we consider t e scenarios w ere ra towards adaptive fault tolerance for distributed multi agent systems this paper studies how to bring flexibility to fault tolerant systems firstly multi agent systems are identified as a very valuable basis for reaching this goal and reliability is also shown to be a rare and attractive feature for such systems we then propose a framework for building applications that provide adaptive fault tolerance and put forward the promising results obtained when testing the implementation of this framework we conclude with drawing some perspectives of evolution of our work goal directed adaptive behavior in second order neural networks leaning and evolving in the maxson architecture the paper presents a neural network architecture maxson based on second order connections that can learn a multiple goal approach avoid task using reinforcement from the environment it also enables an agent to learn vicariously from the successes and failures of other agents the paper shows that maxson can learn certain spatial navigation tasks much faster than traditional q learning as well as learn goal directed behavior increasing the agent s chances of long term survival the paper shows that an extension of maxson v maxson enables agents to learn vicariously and this improves the overall survivability of the agent population incremental reinforcement learning for designing multi agent systems designing individual agents so that when put together they reach a given global goal is not an easy task one solution to automatically build such large multi agent systems is to use decentralized learning each agent learns by itself its own behavior to that purpose reinforcement learning methods are very attractive as they do not require a solution of the problem to be known before hand nevertheless many hard points need to be solved for such a learning process to be viable among others the credit assignement problem combinatorial explosion and local perception of the world seem the most crucial and prevent optimal behavior in this paper we propose a framework based on a gradual learning of harder and harder tasks until the desired global behavior is reached the applicability of our paradigm is tested on computer experiments where many agents have to coordinate to reach a global goal our results show that incremental learning leads to better performances than more classical techniques we then discuss several improvements which could lead to even better performances on splitting and cloning agents embedded with cloning mechanisms an agent can balance its own loads by discharging computing tasks to its clones when it is over loaded in addition it s more reasonable to transfer the smarter smaller clones of an agent rather than the bulky agent itself in mobile computing in this paper a simple bdi agent model is formally established using this model the semantics of constructing new agents by inheritance and self identifying behavior of existing agents are precisely de ned four kinds of cloning mechanisms are identi ed the properties of each cloning mechanism and the relationships in between are studied and some implementation issues are also discussed data driven generation of decision trees for motif based assignment of protein sequences to functional families this paper describes an approach to data driven discovery of sequence motif based models in the form of decision trees for assigning error tolerant agents the use of agents in today s internet world is expanding rapidly yet agent developers markov techniques for object localization with force controlled robots this paper deals with object localization with forcecontrolled robots in the bayesian framework 1 it describes a method based on markov localization techniques with a monte carlo implementation applied for solving 3d 6 degrees of freedom global localization problems with force controlled robots the approach was successfully applied to problems such as the recursive localization of a box by a robot manipulator instant messaging and awareness of presence in webwho this is a study of how awareness of presence affects content of instant messages via an awareness tool webwho the awareness tool is an easily accessible web based system that visualises a large university computer lab the instant messaging system is one of the functions of the tool which allows students to virtually locate one another and to communicate via the instant messaging system as webwho is there to be accessed through any web browser it requires no programming skills or special software it may also be used from outside the computer lab by students located elsewhere steps towards c c a language for interactions we present in this paper our reflections about the requirements of http www playresearch com this report the following link takes you directly to a page with links to the core publications and other evaluation material in pdf format a tutorial dialogue system with knowledge based understanding and classification of student explanations we are engaged in a research project to create a tutorial dialogue system that helps students to explain the reasons behind their problem solving actions in order to help them learn with greater understanding currently we are pilottesting a prototype system that is able to analyze student explanations stated in their own words recognize the types of omissions that we typically see in these explanations and provide feedback the system takes a knowledge based approach to natural language understanding and uses a statistical text classifier as a backup the main features are robust parsing logic based representation of semantic content representation of pedagogical content knowledge in the form of a hierarchy of partial and complete explanations and reactive dialogue management a preliminary evaluation study indicates that the knowledge based natural language component correctly classifies 80 of explanations and produces a reasonable classification for all but 6 of explanations 1 knowledge based wrapper generation by using xml information extraction is the process of recognizing the particular fragments of a document that constitute its core semantic content however most previous information extraction systems were not effective for real world information sources due to difficulties in acquiring and representing useful domain knowledge and in dealing with structural heterogeneity inherent in different sources in order to resolve these problems this paper proposes a scheme of knowledge based wrapper generation for semi structured and labeled documents the implementation of an agent oriented information extraction system xtros is described in contrast with previous wrapper learning agents xtros represents both the domain knowledge and the wrappers by xml documents to increase modularity flexibility and interoperability among multiple parties xtros also facilitates simpler implementation of the wrapper generator by exploiting xml parsers and interpreters xtros shows good performance on several web sites in the domain of real estates and it is expected to be easily adaptable to different domains by plugging in appropriate xml based domain knowledge 1 ozone a zoomable interface for navigating ontology information we present ozone zoomable ontology navigator for searching and browsing ontological information ozone visualizes query conditions and provides interactive guided browsing for daml darpa agent markup language ontologies to visually represent objects in daml we define a visual model for its classes properties and relationships between them properties can be expanded into classes for query refinement the visual query can be formulated incrementally as users explore class and property structures interactively zoomable interface techniques are employed for effective navigation and usability keywords ontology daml browsing zoomable user interface zui jazz www the evaluation of microplanning and surface realization in the generation of multimodal acts of communication in this paper we describe an application domain which requires the computational simulation of human human communication in which one of the interlocutors has an expressive communication disorder the importance and evaluation of a process called here microplanning and surface realization for such communicative agents is discussed and a related exploratory study is described 1 building intelligent systems for mining information extraction rules from web pages by using domain knowledge previous researches on automatic information extraction experienced difficulties in acquiring and representing useful domain knowledge and in coping with the structural heterogeneity among different information sources as a result many real world information sources with complex document structures could not be correctly analyzed in order to resolve these problems this paper presents a method of building intelligent systems for mining information extraction rules from semi structured web pages by using domain knowledge this system automatically generates a wrapper for each information source and performs information extraction and information integration by applying this wrapper to the corresponding source both the domain knowledge and the wrapper are represented by xml documents to increase flexibility and interoperability by testing our prototype system on several real estate information sites we can claim that it creates the correct wrappers for most web sources and consequently facilitates effective information extraction for heterogeneous information sources 1 social mental shaping modelling the impact of sociality on the mental states of autonomous agents this paper presents a framework that captures how the social nature of agents that are situated in a multi agent environment impacts upon their individual mental states roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping this allows us to extend the standard belief desireintention model to account for how common social phenomena e g cooperation collaborative problem solving and negotiation can be integrated into a unified theoretical perspective that reflects a fully explicated model of the autonomous agent s mental state keywords multi agent systems agent interactions bdi models social influence 3 1 on securely scheduling a meeting when people want to schedule a meeting their agendas must be compared to find a time suitable for all participants at the same time people want to keep their agendas private this paper presents several approaches which intend to solve this contradiction a custom made protocol for secure meeting scheduling and a protocol based on secure distributed computing are discussed the security properties and complexity of these protocols are compared a trade off between trust and bandwidth requirements is shown to be possible by implementing the protocols using mobile agents keywords mobile agents secure distributed computation meeting scheduling 1 managing change on the web increasingly digital libraries are being defined that collect pointers to world wide web based resources rather than hold the resources themselves maintaining these collections is challenging due to distributed document ownership and high fluidity typically a collection s maintainer has to assess the relevance of changes with little system aid in this paper we describe the walden s paths path manager which assists a maintainer in discovering when relevant changes occur to linked resources the approach and system design was informed by a study of how humans perceive changes of web pages the study indicated that structural changes are key in determining the overall change and that presentation changes are considered irrelevant categories and subject descriptors i 3 7 digital libraries user issues h 5 4 hypertext hypermedia other maintenance general terms algorithms management design reliability experimentation human factors verification keywords walden s paths path maintenance 1 towards a layered approach for agent infrastructure the right tools for the right job it is clear by now that the take up of agent technologies and the wide use of such technologies in open environments depends on the provision of appropriate infrastructure to support the rapid development of applications in this paper we argue that the elements required for the development of infrastructure span three different fields which nevertheless have a great degree of overlap middleware technologies mobile agent and intelligent agent research all have significant contributions to make towards a holistic approach to infrastructure development but it is necessary to make clear distinctions between the requirements at each level and explain how they can be integrated so as to provide a clearer focus and allow the use of existing technologies our view of the requirements for infrastructure to support agent based systems has been formed through experience with developing an agent implementation environment based on a formal agent framework we argue that in order to provide support to developers this infrastructure must address both conceptual concerns relating the different types of entities and relationships between agent and non agent entities in the environment as well as more technical concerns this paper describes the general requirements for infrastructure the specific contributions from different areas and our own efforts in progressing towards them 1 the roadrunner project towards automatic extraction of web data introduction roadrunner is a research project that aims at developing solutions for automatically extracting data from large html data sources the target of our research are data intensive web sites i e html based sites that publish large amounts of data in a fairly complex structure in our view we aim at ideally seeing the data extraction process of a data intensive web site as a black box taking as input the url of an entry point to the site e g the home page and returning as output data extracted from html pages in the site in a structured database like format this paper describes the top level software architecture of the roadrunner system which has been specifically designed to automatize the data extraction process several components of the system have already been implemented and preliminary experiments show the feasibility of our ideas data intensive web sites usually share a number o extending a multi agent system for genomic annotation the explosive growth in genomic and soon expression and proteomic data exemplified by the human genome project is a fertile domain for the application of multi agent information gathering technologies furthermore hundreds of smaller profile yet still economically important organisms are being studied that require the efficient and inexpensive automated analysis tools that multiagent approaches can provide in this paper we give a progress report on the use of the decaf multi agent toolkit to build reusable information gathering systems for bioinformatics we will briefly summarize why bioinformatics is a classic application for information gathering how decaf supports it and recent extensions underway to support new analysis paths for genomic information 1 market protocols for decentralized supply chain formation in order to effectively respond to changing market conditions business partners must be able to rapidly form supply chains this thesis approaches the problem of automating supply chain formation the process of determining the participants in a supply chain who will exchange what with whom and the terms of the exchanges within an economic framework in this thesis supply chain formation is formalized as task dependency networks this model captures subtask decomposition in the presence of resource contention two important and challenging aspects of supply chain formation in order to form supply chains in a decentralized fashion price systems provide an economic framework for guiding the decisions of self interested agents in competitive price equilibrium agents choose optimal allocations with respect to prices and outcomes are optimal overall approximate competitive equilibria yield approximately optimal allocations different market protocols are proposed for agents to negotiate the allocation of resources to form supply chains in the presence of resource contention these protocols produce better solutions than the greedy protocols common in the artificial intelligence and multiagent systems literature the first protocol proposed is based on distributed progressive price based auctions and is analyzed with non strategic agent bidding policies the protocol often converges to high value supply chains and when competitive equilibria exist typically to approximate competitive equilibria however complemen tarities in agent production technologies can cause the protocol to wastefully allocate inputs to agents that do not produce their out quantum treemaps and bubblemaps for a zoomable image browser this paper describes two algorithms for laying out groups of objects in a 2d space filling manner quantum treemaps are a variation on existing treemap algorithms that are designed for laying out images or other objects of indivisible quantum size they build on the ordered treemap algorithm but guarantees that every generated rectangle will have a width and height that are an integral multiple of an input object size bubblemaps also fill space with groups of quantum sized objects but generate nonrectangular blobs and utilize space more efficiently both algorithms have been applied to photomesa an application that supports browsing of large numbers of images photomesa uses a zoomable user interface with a simple interaction designed for novices and family use keywords zoomable user interfaces zuis treemaps image browsers animation graphics jazz exploiting context to make delivered information relevant to tasks and users building truly context aware environments presents a greater challenge than using data transmitted by ubiquitous computing devices it requires shared understanding between humans and their computational environments this essay articulates some specific problems that can be addressed by representing context it explores the unique possibilities of design environments that model and represent domains tasks design guidelines solutions and their rationale and the larger context of such environments embedded in the physical world context in design is not a fixed entity sensed by devices but it is emerging and it is unbounded context aware environments must address these challenges to be more supportive to all stakeholders who design and evolve complex design artifacts automated derivation of complex agent architectures from analysis specifications multiagent systems have been touted as a way to meet the need for distributed software systems that must operate in dynamic and complex environments however in order for multiagent systems to be effective they must be reliable and robust engineering multiagent systems is a non trivial task providing ample opportunity for even experts to make mistakes formal transformation systems can provide automated support for synthesizing multiagent systems which can greatly improve their correctness and reliability this paper describes a semi automated transformation system that generates an agents internal architecture from the analysis specification for the mase methodology 1 a multi agent architecture for intelligent tutoring one of the most interesting realm among those ones brought up to success by the development of the internet is distance learning a key issue in such a field is the development of systems for supporting tutoring activities this paper is concerned with the presentation of an innovative architecture for intelligent tutoring which make use of software agents the way in which the knowledge is represented and stored is discussed together with the ability of our system to manage individual learning paths for different users the rationale for using agents is presented and the implementation of the system is discussed 1 a web odyssey from codd to xml introduction the web presents the database area with vast opportunities and commensurate challenges databases and the web are organically connected at many levels web sites are increasingly powered by databases collections of linked web pages distributed across the internet are themselves tempting targets for a database the emergence of xml as the lingua franca of the web brings some much needed order and will greatly facilitate the use of database techniques to manage web information this paper will discuss some of the developments related to the web from the viewpoint of database theory as we shall see the web scenario requires revisiting some of the basic assumptions of the area to be sure database theory remains as valid as ever in the classical setting and the database industry will continue to representamulti billion dollar target of applicability for the foreseeable future but the web represents an opportunityofanentirely di erent scale we are th from design to intention signs of a revolution ce such environment and be influenced by it ii openness software systems will be subject to decentralized management and will dynamically change their structure new components can be dynamically created or destroyed and via mobility will be able to roam in and out the permeable boundaries of different software systems thus the problem of openness is currently much broader than being simply a problem of interoperability iii locality in control the components of software systems will represent autonomous loci of control in fact most components of software systems will be active and will have local control over their activities although will be in need of coordinating these activities with other active components iv locality in interactions despite living in a fully connected world software components interact with each other accordingly to local geographical or logical patterns in other words systems will have to be modeled around clusters of locally interactin using probabilistic knowledge and simulation to play poker until recently artificial intelligence researchers who use games as their experimental testbed have concentrated on games of perfect information many of these games have been amenable to brute force search techniques in contrast games of imperfect information such as bridge and poker contain hidden information making similar search techniques impractical this paper describes recent progress in developing a high performance pokerplaying program the advances come in two forms first we introduce a new betting strategy that returns a probabilistic betting decision a probability triple that gives the likelihood of a fold call or raise occurring in a given situation this component unifies all the expert knowledge used in the program does a better job of representing the type of decision making needed to play strong poker and improves the way information is propagated throughout the program second real time simulations are used to compute the expected values of betting decisions the program generates an instance of the missing data subject to any constraints that have been learned and then simulates the rest of the game to determine a numerical result by repeating this a sufficient number of times a statistically meaningful sample is used in the program s decision making process experimental results show that these enhancements each represent major advances in the strength of computer poker programs 1 form based proxy caching for database backed web sites web caching proxy servers are essential for improving web performance and scalability and recent research has focused on making proxy caching work for database backed web sites in this paper we explore a new proxy caching framework that exploits the query semantics of html forms we identify two common classes of form based queries from real world database backed web sites namely keyword based queries and function embedded queries using typical examples of these queries we study two representative caching schemes within our framework i traditional passive query caching and ii active query caching in which the proxy cache can service a request by evaluating a query over the contents of the cache results from our experimental implementation show that our form based proxy is a general and flexible approach that efficiently enables active caching schemes for database backed web sites furthermore handling query containment at the proxy yields significant performance advantages over passive query caching but extending the power of the active cache to do full semantic caching appears to be less generally effective direct policy search using paired statistical tests direct policy search is a practical way to solve reinforcement learning problems involving continuous state and action spaces the goal becomes finding policy parameters that maximize a noisy objective function the pegasus method converts this stochastic optimization problem into a deterministic one by using fixed start states and fixed random number sequences for comparing policies ng jordan 1999 we evaluate pegasus and other paired comparison methods using the mountain car problem and a difficult pursuer evader problem we conclude that i paired tests can improve performance of deterministic and stochastic optimization procedures ii our proposed alternatives to pegasus can generalize better by using a different test statistic or changing the scenarios during learning iii adapting the number of trials used for each policy comparison yields fast and robust learning 1 formal respect logic tuple centres have s own that logic ba d languages can be e ectively exploited not only for building individual agents and enabling interagent communication in multi agent ssg ms butals for ruli ng inter agent communications as to builds cial behaviours in this paper we formally define the notion of logic tuple centre as well as the operationals emantics of the logic bas d language respect for the behaviours pecification of logic tuple centres for this purpos e we exploit a generals emantic framework for as ynchronous dis tributeds ys tems allowing a coordination medium to be formally denoted in as eparate and independent way with res pect to the whole coordinateds ys tem this s hows that a logic bas ed coordination medium does not limit agents and coordination languages to be logic bas ed but may ins tead enable agents of di erents orts and technologies to be combined and coordinated in an e ective way by exploiting a logic bas ed approach 1 coordinationm edia form ulti speech and hand transcribed retrieval this paper describes the issues and preliminary work involved in the creation of an information retrieval system that will manage the retrieval from collections composed of both speech recognised and ordinary text documents in previous work it has been shown that because of recognition errors ordinary documents are generally retrieved in preference to recognised ones means of correcting or eliminating the observed bias is the subject of this paper initial ideas and some preliminary results are presented general terms measurement experimentation keywords information retrieval spoken document retrieval mixed collections 1 structure and performance of decision support algorithms on active disks growth and usage trends for large decision support databases indicate that there is a need for architectures that scale the processing power as the dataset grows these trends indicate that the processing demand for large decision support databases is growing faster than the improvement in performance of commodity processors to meet this need several researchers have recently proposed active disk idisk architectures which integrate substantial processing power and memory into disk units in this paper we examine the utility of active disks for decision support databases we try to answer the following questions first is it possible to restructure algorithms for common decision support tasks to utilize active disks second how does the performance of active disks compare with that of traditional servers for these tasks finally how would active disks be integrated into the software architecture of decision support databases 1 introduction growth and usage trends for large decis learning hierarchical task models by defining and refining examples task models are used in many areas of computer science including planning intelligent tutoring plan recognition interface design and decision theory however developing task models is a significant practical challenge we present a task model development environment centered around a machine learning engine that infers task models from examples a novel aspect of the environment is support for a domain expert to refine past examples as he or she develops a clearer understanding of how to model the domain collectively these examples constitute a test suite that the development environment manages in order to verify that manual changes to the evolving task model do not have unintended consequences 1 an agent based framework for financial transactions ever changing internet technologies are creating revolutions in the way people interact with each other in particular business interactions are rapidly transforming and evolving toward more dynamic and automated solutions among various on line commercial activities financial services represent a fundamental component for developing and supplying many other e businesses the main idea behind efinance is to provide support by deploying software instruments that enable to automate many of b2b and b2c transactions the current degree of automation and personalisation of on line financial services is still very limited web interfaces or ad hoc tools still require a lots of human interactions agent technology seems to be one of the most promising approaches for evolving toward more flexible and dynamic solutions autonomous intelligent social and selfinterested software entities would act on behalf of final endusers and or business operators without the need for direct human involvement this paper describes an agent based system supporting automated business transactions the aim is to evaluate the main potential and the major limits of supplying financial services by deploying agents in a software environment keywords agent based interactions e finance electronic payments ontology 1 signal detection using ica application to chat room topic spotting signal detection and pattern recognition for online grouping huge amounts of data and retrospective analysis is becoming increasingly important as knowledge based standards such as xml and advanced mpeg gain popularity independent component analysis ica can be used to both cluster and detect signals with weak a priori assumptions in multimedia contexts ica of real world data is typically performed without knowledge of the number of non trivial independent components hence it is of interest to test hypotheses concerning the number of components or simply to test whether a given set of components is significant relative to a white noise null hypothesis it was recently proposed to use the so called bayesian information criterion bic approximation for estimation of such probabilities of competing hypotheses here we apply this approach to the understanding of chat we show that ica can detect meaningful context structures in a chat room log file 1 agents that talk back sometimes filter programs for affective communication this paper introduces a model of interaction between users and animated agents as well as inter agent interaction that supports basic features of affective conversation as essential requirements for animated agents capability to engage in and exhibit affective communication we motivate reasoning about emotion and emotion expression personality and social role awareness the main contribution of our paper is the discussion of so called filter programs that may qualify an agent s expression of its emotional state by its personality and the social context all of the mental concepts that determine emotion expression such as emotional state personality standards and attitudes have associated intensities for fine tuning the agent s reactions in user adapted environments distance from boundary as a metric for texture image retrieval a new metric is proposed for texture image retrieval which is based on the signed distance of the images in the database to a boundary chosen by the query this novel metric has three advantages 1 the boundary distance measures are relatively insensitive to the sample distributions 2 same retrieval results can be obtained with respect to different but visually similar queries 3 retrieval performance can be improved the boundaries are obtained by using a statistical learning algorithm called support vector machine svm and hence the boundaries can be simply represented by some vectors and their combination coefficients experimental results on the brodatz texture database indicate that a significantly better retrieval performance can be achieved as compared to the traditional euclidean distance based approach this technique can be further developed to learn pattern similarities among different texture classes and used in relevance feedback xml with data values typechecking revisited we investigate the typechecking problem for xml queries statically verifying that every answer to a query conforms to a given output dtd for inputs satisfying a given input dtd this problem had been studied by a subset of the authors in a simplified framework that captured the structure of xml documents but ignored data values we revisit here the typechecking problem in the more realistic case when data values are present in documents and tested by queries in this extended framework typechecking quickly becomes undecidable however it remains decidable for large classes of queries and dtds of practical interest the main contribution of the present paper is to trace a fairly tight boundary of decidability for typechecking with data values the complexity of typechecking in the decidable cases is also considered 1 why unary and binary operations in logic general result motivated by interval valued logics traditionally in logic only unary and binary operations are used as basic ones e g not and or while the only ternary and higher order operations are the operations which come from a combination of unary and binary ones for the classical logic with the binary set of truth values f0 1g the possibility to express an arbitrary operation in terms of unary and binary ones is well known it follows e g from the well known possibility to express an arbitrary operation in dnf form a similar representation result for 0 1 based logic was proven in our previous paper in this paper we expand this result to finite logics more general than classical logic and to multi d analogues of the fuzzy logic both motivated by interval valued fuzzy logics 1 engineering mobile agent applications via context dependent coordination the design and development of internet applications can take advantage of a paradigm based on autonomous and mobile agents however mobility introduces peculiar coordination problems in agent based internet applications first it suggests the exploitation of an infrastructure based on a multiplicity of local interaction spaces second it may require coordination activities to be adapted both to the characteristics of the execution environment where they occur and to the needs of the application to which the coordinating agents belong in this context this paper introduces the concept of context dependent coordination based on programmable interaction spaces on the one hand interaction spaces associated to different execution environments may be independently programmed so as to lead to differentiated environment dependent behaviors on the other hand agents can program the interaction spaces of the visited execution environments to obtain an application dependent behavior of the interaction spaces themselves several examples show how an infrastructure enforcing context dependent coordination can be effectively exploited to simplify and make more modular the design of internet applications based on mobile agents in addition the mars coordination infrastructure is presented as an example of a system in which the concept of context dependent coordination has found a clean and efficient implementation smart its friends a technique for users to easily establish connections between smart artefacts ubiquitous computing is associated with a vision of everything being connected to everything however for successful applications to emerge it will not be the quantity but the quality and usefulness of connections that will matter our concern is how qualitative relations and more selective connections can be established between smart artefacts and how users can retain control over artefact interconnection we propose context proximity for selective artefact communication using the context of artefacts for matchmaking we further suggest to empower users with simple but effective means to impose the same context on a number of artefacts to prove our point we have implemented smart its friends small embedded devices that become connected when a user holds them together and shakes them 1 coverage problems in wireless ad hoc sensor networks wireless ad hoc sensor networks have recently emerged as a premier research topic they have great long term economic potential ability to transform our lives and pose many new system building challenges sensor networks also pose a number of new conceptual and optimization problems some such as location deployment and tracking are fundamental issues in that many applications rely on them for needed information in this paper we address one of the fundamental problems namely coverage coverage in general answers the questions about quality of service surveillance that can be provided by a particular sensor network we first define the coverage problem from several points of view including deterministic statistical worst and best case and present examples in each domain by combining computational geometry and graph theoretic techniques specifically the voronoi diagram and graph search algorithms we establish the main highlight of the paper optimal polynomial time worst and average case algorithm for coverage calculation we also present comprehensive experimental results and discuss future research directions related to coverage in sensor networks i learning nonlinear dynamical systems using an em algorithm the expectation maximization em algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables 2 it has been applied to system identification in linear stochastic state space models where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously 9 we present a generalization of the em algorithm for parameter estimation in nonlinear dynamical systems the expectation step makes use of extended kalman smoothing to estimate the state while the maximization step re estimates the parameters using these uncertain state estimates in general the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states however if gaussian radial basis function rbf approximators are used to model the nonlinearities the integrals become tractable and the maximization step can be solved via systems of linear equations ontology related services in agent based distributed information infrastructures ontologies are an emerging paradigm to support declarativity interoperability and intelligent services in many areas such as agent based computation distributed information systems and expert systems in the context of designing a scalable agent based middleware for the realization of distributed organizational memories om we examine the question what ontology related services must be provided as middleware components to this end we discuss three basic dimensions of information that have fundamental impact on the usefulness of ontologies for oms namely formality stability and sharing scope of information a short discussion of techniques which are suited to find a balance in each of these dimensions leads to a characterization of roles of ontology related actors in the om scenario we describe the several roles with respect to their goals knowledge competencies rights and obligations these actor classes and the related competencies are candidates to define agent types speech acts and standard services in the envisioned om middleware 1 coordination infrastructure for virtual enterprises virtual enterprises ve and workflow management systems wfms require deployable and flexible infrastructures promoting the integration of heterogenous resources and services as well as the development of new ve s business processes in terms of workflow wf rules coordinating the activities of ve s component enterprises in this paper we argue that a suitable general purpose coordination infrastructure may well fit the needs of ve management in a highly dynamic and unpredictable environment like the internet by providing engineers with the abstractions and run time support to address heterogeneity of different sorts and to represent wf rules as coordination laws we discuss the requirements for ve infrastructures and suggest why ve management and wfms may be seen as coordination problems then we introduce the tucson coordination model and technology and show both in principle and in a simple case study how such a coordination infrastructure can support the design and deve the role of information extraction for textual cbr abstract the benefits of cbr methods in domains where cases are text depend on the underlying text representation today most tcbr approaches are limited to the degree that they are based on efficient but weak ir methods these do not allow for reasoning about the similarities between cases which is mandatory for many cbr tasks beyond text retrieval including adaptation or argumentation in order to carry out more advanced cbr that compares complex cases in terms of abstract indexes nlp methods are required to derive a better case representation this paper discusses how state of the art nlp ie methods might be used for automatically extracting relevant factual information preserving information captured in text structure and ascertaining negation it also presents our ongoing research on automatically deriving abstract indexing concepts from legal case texts we report progress toward integrating ie techniques and ml for generalizing from case texts to our cbr case representation 1 abstractions and infrastructures for the design and development of mobile agent organizations internet applications can take advantage of a paradigm based on autonomous and mobile agents however suitable abstractions and infrastructures are required for the effective engineering of such applications in this paper we argue that a conceptual framework for context dependent coordination supported by an infrastructure based on programmable media can promote a modular and easy to manage approach to the design and development of mobile agent applications in terms of computational organizations the mars coordination infrastructure is presented as an implementation of a coordination infrastructure promoting context dependent coordination a case study in the area of workflow management is introduced to clarify the concepts presented keywords mobile agents agent organizations coordination infrastructures agent oriented software engineering 1 creating a semantic web interface with virtual reality novel initiatives amongst the internet community such as internet2 1 and qbone 2 are based on the use of high bandwidth and powerful computers however the experience amongst the majority of internet users is light years from these emerging technologies we describe the construction of a distributed high performance search engine utilizing advanced threading techniques on a diskless linux cluster the resulting virtual reality scene is passed to a standard client machine for viewing this search engine bridges the gap between the internet of today and the internet of the future keywords internet searching high performance vrml visualization 1 event learning and robust policy heuristics in this paper we introduce a novel form of reinforcement learning called event learning or e learning events are ordered pairs of consecutive states we define the corresponding event value function learning rules which are guaranteed to converge to the optimal event value function are derived combining our method with a known robust control method the sds algorithm we introduce robust policy heuristics rph it is shown that rph a fast adapting non markovian policy is particularly useful for coarse models of the environment and for partially observed systems fast adaptation may allow to separate the time scale of learning to control a markovian process and the time scale of adaptation of a non markovian policy in our e learning framework the de nition of modules is straightforward e learning is well suited for policy switching and planning whereas rph alleviates the curse of dimensionality problem computer simulations of a two link pendulum with coarse discretization and noisy controller are shown to demonstrate the underlying principle date first version may 14 2001 second version may 19 2001 key words and phrases reinforcement learning robust control event representation continuous dynamical systems non markovian policy this work was supported by the hungarian national science foundation grant otka 32487 thanks are due to tor m aamodt for providing his double pendulum software 1 and to gabor szirtes for careful reading of the manuscript this work is a shortened version of the thesis work of i polik and i szita supervised by a lorincz submitted on december 6 2000 that won first prize in the student competition in hungary in section mathematical methods for computers on april 11 2001 pedagogical content knowledge in a tutorial dialogue system to support self explanation we are engaged in a research project to create a tutorial dialogue system that helps students learn through self explanation our current prototype is able to analyze students general explanations of their problem solving steps stated in their own words recognize the types of omissions that we often see in these explanations and provide feedback our approach to architectural tradeoffs is to equip the system with a sophisticated nlu component but to keep dialogue management simple the system has a knowledge based nlu component which performed with 81 accuracy in a preliminary evaluation study the system s approach to dialogue management can be characterised as classify and react in each dialogue cycle the system classifies the student input with respect to a hierarchy of explanation categories that represent common ways of stating complete or incomplete explanations of geometry rules the system then provides feedback based on that classification we consider what extensions are necessary or desirable in order to make the dialogues more robust learning markov processes this article we restrict our attention to discrete time dynamical systems typically we do not know the exact dynamics of the system so instead we consider a probabilistic state transition function p x t 1 jx t such a probabilistic formulation will be particularly useful when we try to learn the model from data the state space might be discrete nite or continuous innite for example we might just try to predict the probability that a stock goes up or down in which case f g more ambitiously we might try to predict its expected value in which case ir in general the state is a vector of state variables which we can partition into three kinds input variables ones which we can control output variables ones which we can observe and hidden or latent variables internal variables which we cannot directly control or observe we shall denote these by u t y t and x t respectively see figure 1 in this article we shall consider how to learn models of this kind we start by considering the special case in to be published in the encyclopedia of cognitive science macmillan 2002 1 x1 x2 x3 y1 y2 y3 u1 u2 figure 1 a generic discrete time dynamical system represented as a dynamic bayesian network dbn see bayesian belief networks for a denition u t is the input x t is the hidden state and y t is the output shaded nodes are observed clear nodes are hidden square nodes are xed inputs controls round nodes are random variables notice how what we see y t may depend on the actions that we take u t this can be used to model active perception x1 x2 x3 x4 x1 x2 x2 x3 x3 x4 figure 2 converting a second order markov model top into a rst order markov model bottom which all variables are observed i e mrml towards an extensible standard for multimedia querying and benchmarking in recent years the need for databases which query multimedia data by content has become apparent many commercial and non commercial research groups are trying to fulfill these needs the development of research can be described as moving in two directions ffl search for new useful query and interaction paradigms ffl deeper research to improve the performance of systems that have adopted a given query paradigm the search for new better performance given a query paradigm has led to clusters of systems which are similar in their interaction with the user and which give a certain set of interaction capabilities to the user it is already visible that research will move towards systems which enable the user to formulate multi paradigm queries in order to further improve results as a consequence of the above there is the need for ffl a common mechanism for shipping multi paradigm queries and their results which assures that the right query processor processes th relevance feedback and personalization a language modeling perspective many approaches to personalization involve learning short term and long term user models the user models provide context for queries and other interactions with the information system in this paper we discuss how language models can be used to represent context and support context based techniques such as relevance feedback and query disambiguation 1 overview from some perspectives personalization has been studied in information retrieval for some time if the goal of personalization is to improve the effectiveness of information access by adapting to individual users needs then techniques such as relevance feedback and filtering would certainly be considered to support personalization there has also been considerable research done mostly in the 1980s on user modeling for information retrieval this research had essentially the same goal as current research on personalization which is to build a model of a user s interests and preferences over time filtering systems too improving min max aggregation over spatial objects we examine the problem of computing min max aggregate queries over a collection of spatial objects each spatial object is associated with a weight value for example the average temperature or rainfall over the area covered by the object given a query rectangle the min max problem computes the minimum maximum weight among all objects intersecting the query rectangle traditionally such queries have been performed as range search queries assuming that the objects are indexed by a spatial access method the min max is computed as objects are retrieved this requires effort proportional to the number of objects intersecting the query interval which may be large a better approach is to maintain aggregate information among the index nodes of the spatial access method then various index paths can be eliminated during the range search in this paper we propose four optimizations that further improve the performance of min max queries our experiments show that the proposed optimizations offer drastic performance improvement over previous approaches moreover as a by product of this work we present an optimized version of the msb tree an index that has been proposed for the min max computation over 1 dimensional interval objects using models of score distributions in information retrieval empirical modeling of a number of different text search engines shows that the score distributions on a per query basis may be fitted approximately using an exponential distribution for the set of nonrelevant documents and a normal distribution for the set of relevant documents this model fits not only probabilistic search engines like inquery but also vector space search engines like smart and also lsi search engines the model also appears to be true of search engines operating on a number of different languages this leads to the hypothesis that all good text search engines operating on any language have similar characteristics the question then arises as to whether the shape of the score distributions reflects some underlying model of language or the search process itself we discuss how they arise given certain assumptions about word distributions in documents we then show that given a query for which relevance information is not available a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution these distributions can be used to map the scores of a search engine to probabilities this model has many possible applications for example the outputs of different search engines can be combined by averaging the probabilities optimal if the search engines are independent or by using the probabilities to select the best engine for each query results show that the technique performs as well as the best current combination techniques a number of different ir tasks may benefit from score modeling including filtering multi lingual retrieval and relevance feedback we also discuss possible future improvements to the process of score modeling 1 determining when to use an agent oriented software engineering paradigm with the emergence of agent oriented software engineering techniques software engineers have a new way of conceptualizing complex distributed software requirements to help determine the most appropriate software engineering methodology a set of defining criteria is required in this paper we describe out approach to determining these criteria as well as a technique to assist software engineers with the selection of a software engineering methodology based on those criteria 1 information triage using prospective criteria in many applications large volumes of time sensitive textual information require triage rapid approximate prioritization for subsequent action in this paper we explore the use of prospective indications of the importance of a time sensitive document for the purpose of producing better document filtering or ranking by prospective we mean importance that could be assessed by actions that occur in the future for example a news story may be assessed retrospectively as being important based on events that occurred after the story appeared such as a stock price plummeting or the issuance of many follow up stories if a system could anticipate prospectively such occurrences it could provide a timely indication of importance clearly perfect prescience is impossible however sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently we describe a process for creating and evaluating approximate information triage procedures that are based on prospective indications unlike many information retrieval applications for which document labeling is a laborious manual process for many prospective criteria it is possible to build very large labeled training corpora automatically such corpora can be used to train text classification procedures that will predict the prospective importance of each document this paper illustrates the process with two case studies demonstrating the ability to predict whether the stock price of one or more companies mentioned in a news story will move significantly following the appearance of that story we conclude by discussing that the comprehensibility of the learned classifiers can be critical to success 1 reasoning over conceptual schemas and queries in temporal databases this paper introduces a new logical formalism intended for temporal conceptual modelling as a natural combination of the wellknown description logic dlr and pointbased linear temporal logic with since and until the expressive power of the resulting dlrus logic is illustrated by providing a systematic formalisation of the most important temporal entity relationship data models appeared in the literature we define a query language where queries are nonrecursive datalog programs and atoms are complex dlrus expressions and investigate the problem of checking query containment under the constraints defined by dlrus conceptual schemas as well as the problems of schema satisfiability and logical implication although it is shown that reasoning in full dlrus is undecidable we identify the decidable in a sense maximal fragment dlr us by allowing applications of temporal operators to formulas and entities only but not to relation expressions we obtain the following hierarchy of complexity results a reasoning in dlr us with atomic formulas is exptime complete b satisfiability and logical implication of arbitrary dlr us formulas is expspace complete and c the problem of checking query containment of non recursive datalog queries under dlr us constraints is decidable in 2exptime theme based retrieval of web news efficient information retrieval of highly dynamic information such as web news is a complex task as a result search and retrieval environments for continuously updated news from other sources than the largest media conglomerates are almost absent on the internet global search engines do not index or classify news information from smaller network communities to address this problem i developed newssearch a news information management environment designed to improve retrieval efficiency of online news for the smaller networked communities newssearch search achieves its goal through a combination of techniques multiple indexing queues defining multiple gathering schedules to deal with different publication periodicities information retrieval techniques to news in order to classify them into a pre defined set of themes support vector machines which proved to be a fast and reliable classification technique newssearch proved to be a scalable solution with acceptable storage needs even while managing a fairly large collection of daily publications a combination of fine tuning of training strategies noise filtering of web news documents and multiple classifications enable newssearch to achieve a classification accuracy of 95 acknowledgements this work was supported in part by the praxis project ariadne pblico digital praxis xxi medida 3 1b and project sagres praxis p tit 1676 95 table of contents chapter i reasoning over conceptual schemas and queries in temporal databases this paper introduces a new logical formalism intended for temporal conceptual modelling as a natural combination of the wellknown description logic dlr and pointbased linear temporal logic with since and until the expressive power of the resulting dlrus logic is illustrated by providing a systematic formalisation of the most important temporal entity relationship data models appeared in the literature we define a query language where queries are nonrecursive datalog programs and atoms are complex dlrus expressions and investigate the problem of checking query containment under the constraints defined by dlrus conceptual schemas as well as the problems of schema satisfiability and logical implication although it is shown that reasoning in full dlrus is undecidable we identify the decidable in a sense maximal fragment dlr us by allowing applications of temporal operators to formulas and entities only but not to relation expressions we obtain the following hierarchy of complexity results a reasoning in dlr us with atomic formulas is exptime complete b satisfiability and logical implication of arbitrary dlr us formulas is expspace complete and c the problem of checking query containment of non recursive datalog queries under dlr us constraints is decidable in 2exptime using text classifiers for numerical classification consider a supervised learning problem in which examples contain both numerical and text valued features to use traditional featurevector based learning methods one could treat the presence or absence of a word as a boolean feature and use these binary valued features together with the numerical features however the use of a text classification system on this is a bit more problematic in the most straight forward approach each number would be considered a distinct token and treated as a word this paper presents an alternative approach for the use of text classification methods for supervised learning problems with numerical valued features in which the numerical features are converted into bag of words features thereby making them directly usable by text classification methods we show that even on purely numerical valued data the results of textclassification on the derived text like representation outperforms the more naive numbers as tokens representation and more importantly is competitive with mature numerical classification methods such as c4 5 and ripper 1 a comparison of usage evaluation and inspection methods for assessing groupware usability many researchers believe that groupware can only be evaluated by studying real collaborators in their real contexts a process that tends to be expensive and timeconsuming others believe that it is more practical to evaluate groupware through usability inspection methods deciding between these two approaches is difficult because it is unclear how they compare in a real evaluation situation to address this problem we carried out a dual evaluation of a groupware system with one evaluation applying userbased techniques and the other using inspection methods we compared the results from the two evaluations and concluded that while the two methods have their own strengths weaknesses and trade offs they are complementary because the two methods found overlapping problems we expect that they can be used in tandem to good effect e g applying the discount method prior to a field study with the expectation that the system deployed in the more expensive field study has a better chance of doing well because some pertinent usability problems will have already been addressed keywords evaluation groupware usability inspection evaluation techniques usage evaluation techniques fine granularity signature caching in object database systems in many of the emerging application areas for database systems data is viewed as a collection of objects the access pattern is navigational and a large fraction of the accesses are perfect match accesses queries on one or more words in text strings in the objects in these databases a typical example of such an application area is xml web storage in order to reduce the object access cost signature files can be used however traditional signature file maintenance is costly and to be beneficial a low update rate and high query selectivity is needed to make the maintenance and use of signatures beneficial in this report we present the sigcache approach instead of storing the signatures in separate signature files the signatures are stored together with their objects in addition the most frequently accessed signatures are stored in a main memory signature cache sigcache because the signatures are much smaller than the objects the increase in update cost is not s personalized web document filtering using reinforcement learning abstract document filtering is increasingly deployed in web environments to reduce information overload of users we formulate online information filtering as a reinforcement learning problem i e td 0 the goal is to learn user profiles that best represent his information needs and thus maximize the expected value of user relevance feedback a method is then presented that acquires reinforcement signals automatically by estimating user s implicit feedback from direct observations of browsing behaviors this learning by observation approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks field tests have been performed which involved 10 users reading a total of 18 750 html documents during 45 days compared to the existing document filtering techniques the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering 1 a case for dynamic view management this paper we present dynamat a system that manages dynamic collections of materialized aggregate views in a data warehouse at query time dynamat utilizes a dedicated disk space for storing computed aggregates that are further engaged for answering new queries queries are executed independently or can be bundled within a multi query expression in the latter case we present an execution mechanism that exploits dependencies among the queries and the materialized set to further optimize their execution during updates dynamat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window we show how to derive an efficient update plan with respect to the available maintenance window the different update policies for the views and the dependencies that exist among them categories and subject descriptors h 2 7 database management database administration data warehouse and re a study of approaches to hypertext categorization hypertext poses new research challenges for text classification hyperlinks html tags category labels distributed over linked documents and meta data extracted from related web sites all provide rich information for classifying hypertext documents how to appropriately represent that information and automatically learn statistical patterns for solving hypertext classification problems is an open question this paper seeks a principled approach to providing the answers specifically we define five hypertext regularities which may or may not hold in a particular application domain and whose presence or absence may significantly influence the optimal design of a classifier using three hypertext datasets and three well known learning algorithms naive bayes nearest neighbor and first order inductive learner we examine these regularities in different domains and compare alternative ways to exploit them our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial but seldom obvious in real world problems we find that adding the words in the linked neighborhood to the page having those links both inlinks and outlinks were helpful for all our classifiers on one data set but more harmful than helpful for two out of the three classifiers on the remaining datasets we also observed that extracting meta data from related web sites was extremely useful for improving classification accuracy in some of those domains finally the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy web pages closing the loop an agenda and justification based framework for selecting the next discovery task to perform we propose and evaluate an agenda and justificationbased architecture for discovery systems that contains a mechanism for selecting the next task to perform this framework has many desirable properties 1 its use of heuristics to perform and propose tasks facilitates the use of general discovery strategies that are able to use a variety of background knowledge 2 through the use of justifications its mechanism for selecting the next task to perform is able to reason about the appropriateness of the tasks being considered and 3 its mechanism for selecting the next task to perform also considers the users interests allowing a discovery program to tailor its behavior toward them we evaluate the extent to which both reasons and estimates of interestingness contribute to performance in the domain of protein crystallization with both aspects contributing to task selection a high fraction of discoveries by the hamb prototype were judged interesting by an expert 21 interesting and novel 45 interesting but rediscoveries 1 group task analysis for groupware usability evaluations techniques for inspecting the usability of groupware applications have recently been proposed these techniques focus on the mechanics of collaboration rather than the work context in which a system is used and offer time and cost savings by not requiring actual users or fully functional prototypes although these techniques are valuable adding information about task and work context could improve the quality of inspection results we introduce a method for analysing group tasks that can be used to add context to discount groupware evaluation techniques our method allows for the specification of collaborative scenarios and tasks by considering the mechanics of collaboration levels of coupling during task performance and variability in task execution we describe how this type of task analysis could be used in a new inspection technique based on cognitive walkthrough alternative representations and abstractions for moving sensors databases moving sensors refers to an emerging class of data intensive applications that impacts disciplines such as communication health care scientific applications etc these applications consist of a fixed number of sensors that move and produce streams of data as a function of time they may require the system to match these streams against stored streams to retrieve relevant data patterns with communication for example a speaking impaired individual might utilize a haptic glove that translates hand signs into written spoken words the glove consists of sensors for dierent nger joints these sensors report their location and values as a function of time producing streams of data these streams are matched against a repository of spatio temporal streams to retrieve the corresponding english character or word the contributions of this study are two folds first it introduces a framework to store and retrieve moving sensors data the framework advocates physical data independence and software reuse second we investigate alternative representations for storage and retrieval of data in support of query processing we quantify the tradeoff associated with these alternatives using empirical data from robocup soccer matches accelerating reinforcement learning through the discovery of useful subgoals an ability to adjust to changing environments and unforeseen circumstances is likely to be an important component of a successful autonomous space robot this paper shows how to augment reinforcement learning algorithms with a method for automatically discovering certain types of subgoals online by creating useful new subgoals while learning the agent is able to accelerate learning on a current task and to transfer its expertise to related tasks through the reuse of its ability to attain subgoals subgoals are created based on commonalities across multiple paths to a solution we cast the task of finding these commonalities as a multiple instance learning problem and use the concept of diverse density to find solutions we introduced this approach in 10 and here we present additional results for a simulated mobile robot task 1 a survey of methods for scaling up inductive algorithms one of the defining challenges for the kdd research community is to enable inductive learning algorithms to mine very large databases this paper summarizes categorizes and compares existing work on scaling up inductive algorithms we concentrate on algorithms that build decision trees and rule sets in order to provide focus and specific details the issues and techniques generalize to other types of data mining we begin with a discussion of important issues related to scaling up we highlight similarities among scaling techniques by categorizing them into three main approaches for each approach we then describe compare and contrast the different constituent techniques drawing on specific examples from published papers finally we use the preceding analysis to suggest how to proceed when dealing with a large problem and where to focus future research keywords scaling up inductive learning decision trees rule learning 1 introduction the knowledge discovery and data hyperqueries dynamic distributed query processing on the internet in this paper we propose a new framework for dynamic distributed query processing based on so called hyperqueries which are essentially query evaluation sub plans sitting behind hyperlinks we illustrate the flexibility of this distributed query processing architecture in the context of b2b electronic market places architecting an electronic market place as a data warehouse by integrating all thedatafromall participating enterprises in one centralized repository incurs severe problems using hyperqueries application integration is achieved via dynamic distributed query evaluation plans the electronic market place serves as an intermediary between clients and providers executing their sub queries referenced via hyperlinks the hyperlinks are embedded within data objects of the intermediary s database retrieving such a virtual object will automatically initiate the execution of the referenced hyperquery in order to materialize the entire object thus sensitive data remains under the full control of the data providers 1 probabilistic default reasoning with conditional constraints we present an approach to reasoning from statistical and subjective knowledge which is based on a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases more precisely we introduce the notions of lexicographic and conditional entailment for conditional constraints which are probabilistic generalizations of pearl s entailment in system lehmann s lexicographic entailment and geffner s conditional entailment respectively we show that the new formalisms have nice properties in particular they show a similar behavior as referenceclass reasoning in a number of uncontroversial examples the new formalisms however also avoid many drawbacks of reference class reasoning more precisely they can handle complex scenarios and even purely probabilistic subjective knowledge as input moreover conclusions are drawn in a global way from all the available knowledge as a whole we then show that the new formalisms also have nice general nonmonotonic properties in detail the new notions of lexicographic and conditional entailment have similar properties as their classical counterparts in particular they all satisfy the rationality postulates proposed by kraus lehmann and magidor and they have some general irrelevance and direct inference properties moreover the new notions of and lexicographic entailment satisfy the property of rational monotonicity furthermore the new notions of lexicographic and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints finally we provide algorithms for reasoning under the new formalisms and we analyze its computational com text database selection for longer queries a metasearch engine is a system that supports unified access to multiple local search engines one of the main challenges in building a large scale metasearch engine is to solve the database search engine selection problem which is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query for the database of each search engine a representative which indicates approximately the contents of the database is created in advance to enable database selection the representatives of all databases can be integrated into a single representative to make the selection process more scalable while an integrated representative with high scalability has just been proposed and has been found to be effective for short queries its effectiveness for longer queries is significantly lower in the internet environment most queries initially submitted by users are short queries however it has been found that better search effectiveness can often be achieved when additional terms are added to the initial queries through query expansion or relevance feedback the resulting queries are usually longer than the initial queries in this paper we propose a new method to construct database representatives and to decide which databases to select for longer queries experimental results are given to compare the performance of the new method with that of a previous method discovering unexpected information from your competitors web sites ever since the beginning of the web finding useful information from the web has been an important problem existing approaches include keyword based search wrapper based information extraction web query and user preferences these approaches essentially find information that matches the user s explicit specifications this paper argues that this is insufficient there is another type of information that is also of great interest i e unexpected information which is unanticipated by the user finding unexpected information is useful in many applications for example it is useful for a company to find unexpected information about its competitors e g unexpected services and products that its competitors offer with this information the company can learn from its competitors and or design counter measures to improve its competitiveness since the number of pages of a typical commercial site is very large and there are also many relevant sites competitors it is very difficult for a human user to view each page to discover the unexpected information automated assistance is needed in this paper we propose a number of methods to help the user find various types of unexpected information from his her competitors web sites experiment results show that these techniques are very useful in practice and also efficient keywords information interestingness web comparison web mining 1 incremental document clustering for web page classification introduction we consider document clustering for web pages traditionally the document classification task is carried out manually in order to assign a document to an appropriate class people would analyze the contents of the document first therefore a large amount of human effort would be required there has been some research work conducted on automatic text classification one approach is to learn the text classifiers by using the machine learning techniques however these algorithms are based on a set of positive and negative training examples for learning the text classifiers the quality of the resulting classifiers highly depends on the fitness of the training examples there are many terms and classes in the world wide web or just the web and many new terms and concepts are created everyday it is quite impossible to have domain experts to identify training examples to learn a classifier for each text class in the above manner in order to make the document cl si designer an integration framework for e commerce electronic commerce lets people purchase goods and exchange information on business transactions on line therefore one of the main challenges for the designers of the e commerce infrastructures is the information sharing retrieving data located in different sources thus obtaining an integrated view to overcome any contradiction or redundancy virtual catalogs synthesize this approach as they are conceived as instruments to dynamically retrieve information from multiple catalogs and present product data in a unified manner without directly storing product data from catalogs in this paper we propose si designer a support tool for the integration of data from structured and semi structured data sources developed within the momis mediator environment for multiple information sources project 1 building infrastructures for digital libraries digital libraries today are often monolithic systems in the future they will dissolve into collections of electronic services the challenge will be to provide an infrastructure that supports the user in dealing with this multitude of services such an infrastructure should offer integrated access to the combined contents of multiple services it should provide active dissemination of new contents and it needs to support the users in locating and combining the services most suitable to their needs in the global info program infrastructures for digital libraries components of such an infrastructure are being developed the federated query service demetrios and the alerting service hermes are both integration services that combine underlying services i e heterogenous information sources the gibraltar portal provides a meta service that supports the user in locating and applying various digital library services 1 chart of darkness mapping a large intranet we introduce and de ne the concept of dark matter on the web dark matter for a person or web crawler consists of pages that they cannot reach and view but which another observer can dark matter is important to our understanding of the web in that the portion of the web any of us can see depends on our viewpoint diffrent observers see different overlapping sections of the web however no one can see all of the web even if they want to we categorise the various types of dark matter that exist and how they may be discovered formal definitions of what constitutes lightness and darkness on the web are formulated in terms of reachability our case study of dark matter within the australian national university s intranet is reported we estimate that 87 of the anu intranet s information is dark to our local search service and 37 is potentially loadable web data unreachable to almost every web user finally we discuss some of the implications of dark matter for estimating the size of the web and for general web searching methods for sampling pages uniformly from the world wide web we present two new algorithms for generating uniformly random samples of pages from the world wide web building upon recent work by henzinger et al henzinger et al 2000 and bar yossef et al bar yossef et al 2000 both algorithms are based on a weighted random walk methodology the first algorithm directed sample operates on arbitrary directed graphs and so is naturally applicable to the web we show that in the limit this algorithm generates samples that are uniformly random the second algorithm undirected sample operates on undirected graphs thus requiring a mechanism for obtaining inbound links to web pages e g access to a search engine with this additional knowledge of inbound links the algorithm can arrive at a uniform distribution faster than directedsample and we derive explicit bounds on the time to convergence in addition we evaluate the two algorithms on simulated web data showing that both yield reliably uniform samples of pages we also compare our results with those of previous algorithms and discuss the theoretical relationships among the various proposed methods towards a semantic framework for service description the rapid development of the internet and of distributed computing has led to a proliferation of online service providers such as digital libraries web information sources electronically requestable traditional services and even software to software services such as those provided by persistence and event managers this has created a need for catalogs of services based on description languages covering both traditional and electronic services this paper presents a classification and a domainindependent characterisation of services which provide a foundation for their description to potential consumers for each of the service characteristics that we consider we identify the range of its possible values in di erent settings and when applicable we point to alternative approaches for representing these values the idea is that by merging this work was funded by an australian research council spirt grant entitled selfdescribing transactions operating in a open heterogeneous and distributed environment involving qut and gbst holdings pty ltd 1 2 these individual approaches and by mapping them into a unified notation it is possible to design service description languages suitable for advertisement and matchmaking within specific application settings 1 the retsina mas infrastructure retsina is an implemented multi agent system infrastructure that has been developed for several years and applied in many domains ranging from financial portfolio management to logistic planning in this paper we distill from our experience in developing mass to clearly define a generic mas infrastructure as the domain independent and reusable substratum that supports the agents social interactions in addition we show that the mas infrastructure imposes requirements on an individual agent if the agent is to be a member of a mas and take advantage of various components of the mas infrastructure although agents are expected to enter a mas and seamlessly and e ortlessly interact with the agents in the mas infrastructure the current state of the art demands agents to be programmed with the knowledge of what infrastructure they will utilize and what are various fallback and recovery mechanisms that the infrastructure provides by providing an abstract mas infrastructure model and a concrete implemented instance of the model retsina we contribute towards the development of principles and practice to make the mas infrastructure invisible and ubiquitous to the interacting agents accessing information and services on the daml enabled web the darpa agent markup language daml program aims to allow one to mark up web pages to indicate the meaning of their content it is intended that the results delivered by a daml enabled browser will more closely match the intentions of the user than is possible with today s syntactically oriented search engines in this paper we present our vision of a daml enabled search architecture we present a set of queries of increasing complexity that should be answered efficiently in a semantic web we describe several scenarios illustrating how queries are processed identifying the main software components necessary to facilitate the search we examine the issue of inference in search and we address how to characterize procedures and services in daml enabling a daml query language to find web sites with specified capabilities key words semantic web daml inference web services process modeling 1 focused web crawling a generic framework for specifying the user interest and for adaptive crawling strategies compared to the standard web search engines focused crawlers yield good recall as well as good precision by restricting themselves to a limited domain in this paper we do not introduce another focused crawler but we introduce a generic framework for focused crawling consisting of two major components 1 specification of the user interest and measuring the resulting relevance of a given web page the proposed method of specifying the user interest by a formula combining atomic topics significantly improves the expressive power of the user 2 crawling strategy ordering the links at the crawl frontier is a challenging task since pages of a low relevance may be on a path to highly relevant pages thus tunneling may be necessary the explicit specification of the user interest allows us to define topic specific strategies for tunneling our system ariadne is a prototype implementation of the proposed framework an experimental evaluation of different crawling strategies demonstrates the performance gain obtained by focusing a crawl and by dynamically adapting the focus 1 enlightened agents in tucson in the network centric computing era applications often involve sets of autonomous unpredictable and possibly mobile entities interacting within open dynamic and possibly unreliable environments intelligent environments are a typical case the complexity of such scenarios requires novel engineering tools providing effective support from the analysis to the deployment stage in this paper we illustrate the impact of a general purpose coordination infrastructure for multiagent systems providing a model a run time and suitable deployment tools on the engineering of such applications as a case study we consider the intelligent management of lights inside a building despite its simplicity this problem endorses the typical challenges of this class of applications the case study is built upon the tucson coordination infrastructure which provides engineers with both the abstractions and the run time support for effectively managing the application complexity i infrastr mixed initiative interaction mixed computation we show that partial evaluation can be usefully viewed as a programming model for realizing mixed initiative functionality in interactive applications mixed initiative interaction between two participants is one where the parties can take turns at any time to change and steer the flow of interaction we concentrate on the facet of mixed initiative referred to as unsolicited reporting and demonstrate how out of turn interactions by users can be modeled by jumping ahead to nested dialogs via partial evaluation our approach permits the view of dialog management systems in terms of their native support for staging and simplifying interactions we characterize three different voice based interaction technologies using this viewpoint in particular we show that the built in form interpretation algorithm fia in the voicexml dialog management architecture is actually a well disguised combination of an interpreter and a partial evaluator this work is supported in part by us national science foundation grants dge 9553458 and iis 9876167 1 1 rule discovery with a parallel genetic algorithm an important issue in data mining is scalability with respect to the size of the dataset being mined in the paper we address this issue by presenting a parallel ga for rule discovery this algorithm exploits both data parallelism by distributing the data being mined across all available processors and control parallelism by distributing the population of individuals across all available processors 1 context awareness and mobile phones this paper investigates some aspects of how context awareness can support users of mobile phones in particular the calling party the use of mobile and stationary phones is discussed in relation to situational properties of a phone conversation especially with regards to who might benefit from context awareness in this context an initial hypothesis is that mobile phone users communicate context information to each other verbally to a much higher degree than do stationary phone users mobile phone users could benefit much from context awareness technology in particular when about to make a call if they can receive context information regarding the person they are trying to reach prior to establishing the call we argue that such technology should require low amounts of explicit user interaction and could lead to less disrupting calls in inappropriate moments as well as less frustration for the calling party when a call is not answered keywords computer mediated communicati a natural interface to a virtual environment through computer vision estimated pointing gestures this paper describes the development of a natural interface to a virtual environment the interface is through a natural pointing gesture and replaces pointing devices which are normally used to interact with virtual environments the pointing gesture is estimated in 3d using kinematic knowledge of the arm during pointing and monocular computer vision the latter is used to extract the 2d position of the user s hand and map it into 3d off line tests of the system show promising results with an average errors of 76mm when pointing at a screen 2m away the implementation of a real time system is currently in progress and is expected to run with 25hz 1 schema evolution in heterogeneous database architectures a schema transformation approach in previous work we have a developed general framework to support schema transformation and integration in heterogeneous database architectures the framework consists of a hypergraph based common data model and a set of primitive schema transformations defined for this model higher level common data models and primitive schema transformations for them can be defined in terms of this lower level model a key feature of the framework is that both primitive and composite schema transformations are automatically reversible we have shown in earlier work how this allows automatic query translation from a global schema to a set of source schemas in this paper we show how our framework also readily supports evolution of source schemas allowing the global schema and the query translation pathways to be easily repaired as opposed to having to be regenerated after changes to source schemas 1 an expert system for analyzing firewall rules when deploying firewalls in an organization it is essential to verify that the firewalls are configured properly the problem of finding out what a given firewall configuration does occurs for instance when a new network administrator takes over or a third party performs a technical security audit for the organization while the problem can be approached via testing non intrusive techniques are often preferred existing tools for analyzing firewall configurations usually rely on hard coded algorithms for analyzing access lists in this paper we present a tool based on constraint logic programming clp which allows the user to write higher level operations for e g detecting common configuration mistakes our tool understands cisco router access lists and it is implemented using eclipse a constraint logic programming language the problem of analyzing firewall configurations lends itself quite naturally to be solved by an expert system we found it surprisingly easy to use logic statements to express knowledge on networking firewalls and common configuration mistakes for instance using an existing generic inference engine allowed us to focus on defining the core concepts and relationships in the knowledge base 1 an architecture and object model for distributed object oriented real time databases the confluence of computers communications and databases is quickly creating a distributed database where many applications require real time access to both temporally accurate and multimedia data this is particularly true in military and intelligence applications but these required features are needed in many commercial applications as well we are developing a distributed database called beehive which could offer features along different types of requirements real time fault tolerance security and quality of service for audio and video support of these features and potential trade offs between them could provide a significant improvement in performance and functionality over current distributed database and object management systems in this paper we present a high level design for beehive architecture and sketch the design of the beehive object model bom which extends object oriented data models by incorporating time and other features into objects resulting in a high configuration management for multi agent systems as heterogeneous distributed systems multi agent systems present some challenging configuration management issues there are the problems of knowing how to allocate agents to computers launch them on remote hosts and once the agents have been launched how to monitor their runtime status so as to manage computing resources effectively in this paper we present the retsina configuration manager recoma we describe its architecture how it uses agent infrastructure such as service discovery to assist the multi agent system administrator in allocating launching and monitoring a heterogeneous distributed agent system in a distributed and networked computing environment 1 1 learning lateral interactions for feature binding and sensory segmentation we present a new approach to the supervised learning of lateral interactions for the competitive layer model clm dynamic feature binding architecture the method is based on consistency conditions which were recently shown to characterize the attractor states of this linear threshold recurrent network for a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights an efficient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions we show the successful application of the method to a medical image segmentation problem of fluorescence microscope cell images 1 a framework for ontology integration one of the basic problems in the development of techniques for the semantic web is the integration of ontologies indeed the web is constituted by a variety of information sources each expressed over a certain ontology and in order to extract information from such sources their semantic integration and reconciliation in terms of a global ontology is required in this paper we address the fundamental problem of how to specify the mapping between the global ontology and the local ontologies we argue that for capturing such mapping in an appropriate way the notion of query is a crucial one since it is very likely that a concept in one ontology corresponds to a view i e a query over the other ontologies as a result query processing in ontology integration systems is strongly related to view based query answering in data integration 1 slow technology designing for reflection ascomputex are increcrxukd wove intothe fabric ofexc op life inteopxc dete mayhave tochange from crecouc only fast and e ficiej tools tobe use during alimite time inspej o situations to crexpk1 tek1 oxc that surrounds us and the eee is a part of our activitie for longpegxup oftime we precud slowtechnwoj adeud aged for tep px aime at reu jux andmomekj ofmekdd rek rathe thane ficiep1 inpejop xu 1 the aim of this pape is to dekjcc ade j philosophy for slowtexcpc cx to discuss gecuss decus principle and to reujjp some basic issue inintekpxuj dete from a more philosophical point ofvie we discuss escussx of son zz e andin 110 ho art asinstance of slowtexcoc11x and as exkjcu on howthe deup principle canbe applie inpractice keywords slow te xuud o deu op human computex inte comput ubiquitous computing sonitureiquitous computin 2000springe vexupc tobe publishe as hallns l re opdx j slow tekoopxpp deeopx for reujd1 x journal ofpeckkcj and ubiquitous computing springe vexucj 162 1 audio driven facial animation for audio visual reality in this paper we demonstrate a morphing based automated audio driven facial animation system based on an incoming audio stream a face image is animated with full lip synchronization and expression an animation sequence using optical flow between visemes is constructed given an incoming audio stream and still pictures of a face speaking different visemes rules are formulated based on coarticulation and the duration of a viseme to control the continuity in terms of shape and extent of lip opening in addition to this new viseme expression combinations are synthesized to be able to generate animations with new facial expressions finally various applications of this system are discussed in the context of creating audio visual reality 1 semantics based information retrieval in this paper we investigate the use of conceptual descriptions based on description logics for contentbased information retrieval and present several innovative contributions we provide a query byexamples retrieval framework which avoids the drawback of a sophisticated query language we extend an existing dl to deal with spatial concepts we provide a content based similarity measure based on the least common subsumer which extracts conceptual similarities of examples 1 introduction as more and more information of various kinds becomes available for an increasing number of users one major challenge for computer science is to provide e cient access and retrieval mechanisms this is not only true for web based information which by its nature tends to be highly unorganized and heterogeneous but also for dedicated databases which are designed to provide a particular service the guiding example of this paper is a tv assistant with a database containing tv program information its bringing robustness to end user programming in some cases end user programming allows the design of stand alone applications but none of the existing approaches is concerned by safety aspects of programming heavy techniques exist to develop safe applications particularly in non interactive domains they involve software engineering techniques and sometimes formal methods all these techniques are very far from end users our idea is to let this part to experts and to connect end user programming onto this safe conventional development starting from an existing functional core we built an interactive end user programming environment called genbuild which allows designing interactive stand alone applications genbuild allows the verification of some properties that are a first step towards the development of safe end user programming 1 face to face with your assistant realization issues of animated user interface agents for home appliances with the introduction of software agents and assistants the concept of so called social user interfaces evolved incorporating natural language interaction context awareness and anthropomorphic representations of visuals scales and degrees of freedom for interactions today s challenge is to build a suitable visualization architecture for anthropomorphic conversational user interfaces and to design for the believable and appropriate inclusion of human attributes such as emotions in a face toface interaction integrated approaches to these tasks are presented here 1 estimating the orientation and recovery of text planes in a single image a method for the fronto parallel recovery of paragraphs of text under full perspective transformation is presented the horizontal vanishing point of the text plane is found using an extension of 2d projection profiles this allows the accurate segmentation of the lines of text analysis of the lines will then reveal the style of justification of the paragraph and provide an estimate of the vertical vanishing point of the plane the text is finally recovered to a fronto parallel view suitable for ocr or other higher level recognition probability based clustering for document and user properties information retrieval systems can be improved by exploiting context information such as user and document features this article presents a model based on overlapping probabilistic or fuzzy clusters for such features the model is applied within a fusion method which linearly combines several retrieval systems the fusion is based on weights for the different retrieval systems which are learned by exploiting relevance feedback information this calculation can be improved by maintaining a model for each document and user cluster that way the optimal retrieval system for each document or user type can be identified and applied the extension presented in this article allows overlapping probabilistic clusters of features to further refine the process 1 enabling technologies for interoperability we present a new approach which proposes to minimize the numerous problems existing in order to have fully interoperable gis we discuss the existence of these heterogeneity problems and the fact that they must be solved to achieve interoperability these problems are addressed on three levels the syntactic structural and semantic level in addition we identify the needs for an approach performing semantic translation for interoperability and introduce a uniform description of contexts furthermore we discuss a conceptual architecture buster bremen university semantic translation for enhanced retrieval which can provide intelligent information integration based on a reclassification of information entities in a new context lastly we demonstrate our theories by sketching a real life scenario mixed initiative in interactions between software agents we have been working during the past several years on techniques for modeling the way that software agents can take and release the initiative while interacting together we are interested in building multiagent systems composed of software agents that can interact with human users in sophisticated ways which are analogous to human conversations in this paper we describe two projects we have worked on a multiagent approach for simulating conversations between software agents and the virtual theater 1 introduction the need for software agents that assist users in achieving various tasks collaborate with them entertain them or even act on their behalf is getting greater software agents are computer systems that exploit their own knowledge bases have their own goals and their own capabilities perform actions and interact with other agents as well as with people autonomy is an essential characteristic of such agents which they express when they take the initiative agents t knowledge base support for design and synthesis of multiagent systems blobworld image segmentation using expectation maximization and its application to image querying abstract retrieving images from large and varied collections using image content as a key is a challenging and important problem we present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture this blobworld representation is created by clustering pixels in a joint color texture position feature space the segmentation algorithm is fully automatic and has been run on a collection of 10 000 natural images we describe a system that uses the blobworld representation to retrieve images from this collection an important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results similar systems do not offer the user this view into the workings of the system consequently query results from these systems can be inexplicable despite the availability of knobs for adjusting the similarity metrics by finding image regions that roughly correspond to objects we allow querying at the level of objects rather than global image properties we present results indicating that querying for images using blobworld produces higher precision than does querying using color and texture histograms of the entire image in cases where the image contains distinctive objects index terms segmentation and grouping image retrieval image querying clustering expectation maximization 1 l fuzzy valued inclusion measure l fuzzy similarity and l fuzzy distance the starting point of this paper is the introduction of a new measure of inclusion of fuzzy set a in fuzzy set b previously used inclusion measures take values in the interval 0 1 the inclusion measure proposed here takes values in a boolean lattice in other words inclusion is viewed as an lfuzzy valued relation between fuzzy sets this relation is reflexive antisymmetric and transitive i e it is a fuzzy order relation in addition it possesess a number of properties which various authors have postulated as axiomatically appropriate for an inclusion measure we also define an l fuzzy valued measure of similarity between fuzzy sets and and an l fuzzy valued distance function between fuzzy sets these possess properties analogous to the ones of real valued similarity and distance functions keywords fuzzy relations inclusion measure subsethood l fuzzy sets similarity distance transitivity 1 vision based user interface for interacting with a virtual environment abstract this paper proposes a new and natural human computer interface for interacting with virtual environments the 3d pointing direction of a user in a virtual environment is estimated using monocular computer vision the 2d position of the user s hand is extracted in the image plane and then mapped to a 3d direction using knowledge about the position of the user s head and kinematic constraints of a pointing gesture due to the human motor system off line tests of the system show promising results the implementation of a real time system is currently in progress and is expected to run with 25hz 1 the intelligent surfer probabilistic combination of link and content information in pagerank the pagerank algorithm used in the google search engine greatly improves the results of web search by taking into account the link structure of the web pagerank assigns to a page a score proportional to the number of times a random surfer would visit that page if it surfed indefinitely from page to page following all outlinks from a page with equal probability we propose to improve pagerank by using a more intelligent surfer one that is guided by a probabilistic model of the relevance of a page to a query efficient execution of our algorithm at query time is made possible by precomputing at crawl time and thus once for all queries the necessary terms experiments on two large subsets of the web indicate that our algorithm significantly outperforms pagerank in the human rated quality of the pages returned while remaining efficient enough to be used in today s large search engines routing through the mist privacy preserving communication in ubiquitous computing environments ubiquitous computing is poised to revolutionize the way we compute and interact with each other however unless privacy concerns are taken into account early in the design process we will end up creating a very effective distributed surveillance system which would be a dream come true for electronic stalkers and big brothers we present a protocol which preserves the privacy of users and keeps their communication anonymous in effect we create a mist that conceals users from the system and other users yet users will still be able to enjoy seamless interaction with services and other entities that wander within the ubiquitous computing environment keywords ubiquitous computing privacy mist routers anonymous communication authentication security 1 versus a temporal web repository web data warehouses are useful for applications that need to process large amounts of web data in a short time this paper presents versus a web repository model supporting object versioning and distributed operation for this kind of applications versioning allows applications to save the time dimension of data enabling the development of new web applications distribution allows parallel operation over massive amounts of data we also present a prototype implementation of versus along with results collected from the analisys of the execution of a distributed web crawler simulator implemented as a versus application 1 consensus based methods applied to the intelligent user interface development in today s world properly designed user interfaces are becoming crucial for every information systems population of users is very differentiated so it is almost impossible to design for every information system a single equally appropriate user interface for each user instead we postulate to construct adoptive interfaces that take into account experiences of all the population to build users profile by means of consensus methods keywords intelligent user interface user profile consensus method 1 hardware software implementation for localization and classification systems pattern localization and classification are cpu time intensive being normally implemented in software however with lower performance than custom implementations custom implementation in hardware asic allows real time processing having higher cost and time to market than software implementation we present an alternative that represents a good trade off between performance and cost this paper presents initially some systems dealing with object localization and classification analyzing the performance and implementation of each work after we propose a system for localization and classification of shapes using reconfigurable devices fpga and a signal processor dsp available in a flexible codesign platform the system will be described using c and vhdl languages for the software and hardware parts respectively and has been implemented in an aptix prototyping platform 1 extending multi agent cooperation by overhearing much cooperation among humans happens following a common pattern by chance or deliberately a person overhears a conversation between two or more parties and steps in to help for instance by suggesting answers to questions by volunteering to perform actions by making observations or adding information we describe an abstract architecture to support a similar pattern in societies of articial agents our architecture involves pairs of so called service agents or services engaged in some tasks and unlimited number of suggestive agents or suggesters the latter have an understanding of the work behaviors of the former through a publicly available model and are able to observe the messages they exchange depending on their own objectives the understanding they have available and the observed communication the suggesters try to cooperate with the services by initiating assisting actions and by sending suggestions to the services these in eect may induce a change in services behavior to test our architecture we developed an experimental multi agent web site the system has been implemented by using a bdi toolkit jack intelligent agents keywords autonomous agents multiagent systems ai architectures distributed ai 1 event driven frp abstract functional reactive programming frp is a high level declarative language for programming reactive systems previous work on frp has demonstrated its utility in a wide range of application domains including animation graphical user interfaces and robotics frp has an elegant continuous time denotational semantics however it guarantees no bounds on execution time or space thus making it unsuitable for many embedded real time applications to alleviate this problem we recently developed real time frp rt frp whose operational semantics permits us to formally guarantee bounds on both execution time and space in this paper we present a formally verifiable compilation strategy from a new language based on rt frp into imperative code the new language called event driven frp e frp is more tuned to the paradigm of having multiple external events while it is smaller than rt frp it features a key construct that allows us to compile the language into efficient code we have used this language and its compiler to generate code for a small robot controller that runs on a pic16c66 micro controller because the formal specification of compilation was crafted more for clarity and for technical convenience we describe an implementation that produces more efficient code 1 visual web information extraction with lixto we present new techniques for supervised wrapper generation and automated web information extraction and a system called lixto implementing these techniques our system can generate wrappers which translate relevant pieces of html pages into xml lixto of which a working prototype has been implemented assists the user to semi automatically create wrapper programs by providing a fully visual and interactive user interface in this convenient user interface very expressive extraction programs can be created internally this functionality is reflected by the new logicbased declarative language elog users never have to deal with elog and even familiarity with html is not required lixto can be used to create an xml companion for an html web page with changing content containing the continually updated xml translation of the relevant information 1 how to interpret neural networks in terms of fuzzy logic neural networks are a very efficient learning tool e g for transforming an experience of an expert human controller into the design of an automatic controller it is desirable to reformulate the neural network expression for the input output function in terms most understandable to an expert controller i e by using words from natural language there are several methodologies for transforming such natural language knowledge into a precise form since these methodologies have to take into consideration the uncertainty fuzziness of natural language they are usually called fuzzy logics 1 extraction of semantic xml dtds from texts using data mining techniques although composed of unstructured texts documents contained in textual archives such as public announcements patient records and annual reports to shareholders often share an inherent though undocumented structure in order to facilitate efficient structure based search in archives and to enable information integration of text collections with related data sources this inherent structure should be made explicit as detailed as possible inferring a semantic and structured xml document type definition dtd for an archive and subsequently transforming the corresponding texts into xml documents is a successful method to achieve this objective the main contribution of this paper is a new method to derive structured xml dtds in order to extend previously derived flat dtds we use the diasdem framework to derive a preliminary unstructured xml dtd whose components are supported by a large number of documents however all xml tags contained in this preliminary dtd cannot a priori be assumed to be mandatory additionally there is no fixed order of xml tags and automatically tagging an archive using a derived dtd always implicates tagging errors hence we introduce the notion of probabilistic xml dtds whose components are assigned probabilities of being semantically and structurally correct our method for establishing a probabilistic xml dtd is based on discovering associations between resp frequent sequences of xml tags keywords semantic annotation xml dtd derivation knowledge discovery data mining clustering infinite horizon policy gradient estimation gradient based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value function methods in this paper we introduce gpomdp a simulation based algorithm for generating a biased estimate of the gradient of the average reward in partially observable markov decision processes pomdps controlled by parameterized stochastic policies a similar algorithm was proposed by kimura yamamura and kobayashi 1995 the algorithm s chief advantages are that it requires storage of only twice the number of policy parameters uses one free parameter 2 0 1 which has a natural interpretation in terms of bias variance trade off and requires no knowledge of the underlying state we prove convergence of gpomdp and show how the correct choice of the parameter is related to the mixing time of the controlled pomdp we briefly describe extensions of gpomdp to controlled markov chains continuous state observation and control spaces multiple agents higher order derivatives and a version for training stochastic policies with internal states in a companion paper baxter bartlett weaver 2001 we show how the gradient estimates generated by gpomdp can be used in both a traditional stochastic gradient algorithm and a conjugate gradient procedure to find local optima of the average reward query processing for moving objects with space time grid storage model with growing popularity of mobile computing devices and wireless communications managing dynamically changing information about moving objects is becoming feasible in this paper we implement a system that manages such information and propose two new algorithms one is an efficient range query algorithm with a ltering step which efficiently determines if a polyline corresponding to the trajectory of a moving object intersects with a given range the other is a nearest neighbor query algorithm we study the performance of the system which shows that despite the filtering step for moderately large ranges the range query algorithm we propose outperforms the algorithm without filtering co x defining what agents do together discussions of agent interactions frequently characterize behavior as coherent collaborative cooperative competitive or coordinated we propose a series of formal distinctions among these terms and several others we argue that all of these are specializations of the more foundational category of correlation which can be measured by the joint information of a system we also propose congruence as a category orthogonal to the others reflecting the degree to which correlation and its specializations satisfy user requirements then we explore the degree to which lack of correlation can arise purposefully and show the need to use formal stochasticity in cases where such lack of correlation is truly necessary such as in stochastic search keywords coordination correlation competition contention cooperation congruence communication command constraint construction conversation stigmergy agent interaction 1 robust classification systems for imprecise environments in real world environments it is usually difficult to specify target operating conditions precisely this uncertainty makes building robust classification systems problematic we show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions this robust performance extends across a wide variety of comparison frameworks including the optimization of metrics such as accuracy expected cost lift precision recall and workforce utilization in some cases the performance of the hybrid can actually surpass that of the best known classifier the hybrid is also efficient to build to store and to update finally we provide empirical evidence that a robust hybrid classifier is needed for many real world problems introduction traditionally classification systems have been built by experimenting with many different classifiers comparing their performance and choosing the classifier that performs best real time input of 3d pose and gestures of a user s hand and its applications for hci in this paper we introduce a method for tracking a user s hand in 3d and recognizing the hand s gesture in real time without the use of any invasive devices attached to the hand our method uses multiple cameras for determining the position and orientation of a user s hand moving freely in a 3d space in addition the method identifies predetermined gestures in a fast and robust manner by using a neural network which has been properly trained beforehand this paper also describes results of user study of our proposed method and its application for several types of applications including 3d object handling for a desktop system and 3d walk through for a large immersive display system 1 database schema matching using machine learning with feature selection schema matching the problem of finding mappings between the attributes of two semantically related database schemas is an important aspect of many database applications such as schema integration data warehousing and electronic commerce unfortunately schema matching remains largely a manual labor intensive process furthermore the effort required is typically linear in the number of schemas to be matched the next pair of schemas to match is not any easier than the previous pair in this paper we describe a system called automatch that uses machine learning techniques to automate schema matching based primarily on bayesian learning the system acquires probabilistic knowledge from examples that have been provided by domain experts this knowledge is stored in a knowledge base called the attribute dictionary when presented with a pair of new schemas that need to be matched and their corresponding database instances automatch uses the attribute dictionary to find an optimal matching we also report initial results from the automatch project 1 supporting reuse by delivering task relevant and personalized information technical cognitive and social factors inhibit the widespread success of systematic software reuse our research is primarily concerned with the cognitive and social challenges faced by software developers how to motivate them to reuse and how to reduce the difficulty of locating components from a large reuse repository our research has explored a new interaction style between software developers and reuse repository systems enabled by information delivery mechanisms instead of passively waiting for software developers to explore the reuse repository with explicit queries information delivery autonomously locates and presents components by using the developers partially written programs as implicit queries we have designed implemented and evaluated a system called codebroker which illustrates different techniques to address the essential challenges in information delivery to make the delivered information relevant to the task at hand and personalized to the background knowledge of an individual developer empirical evaluations of codebroker show that information delivery is effective in promoting reuse categories and subject descriptors d 2 13 software engineering reusable software reusable libraries reuse models d 2 2 software engineering design tools and techniques computer aided software engineering software libraries user interfaces h 5 2 information interfaces and presentation user interfaces interaction styles usercentered design i 2 11 artificial intelligence distributed artificial intelligence intelligent agents general terms design human factors keywords software reuse information delivery software agents discourse models user models high functionality applications 1 computation of the semantics of autoepistemic belief theories recently one of the authors introduced a simple and yet powerful non monotonic knowledge representation framework called the autoepistemic logic of beliefs aeb theories in aeb are called autoepistemic belief theories every belief theory t has been shown to have the least static expansion t which is computed by iterating a natural monotonic belief closure operator psi t starting from t this way the least static expansion t of any belief theory provides its natural non monotonic semantics which is called the static semantics it is easy to see that if a belief theory t is finite then the construction of its least static expansion t stops after countably many iterations however a somewhat surprising result obtained in this paper shows that the least static expansion of any finite belief theory t is in fact obtained by means of a single iteration of the belief closure operator psi t although this requires t to be of a special form we also show that t can be always put in th declarative procedural goals in intelligent agent systems an important concept for intelligent agent systems is goals goals have two aspects declarative a description of the state sought and procedural a set of plans for achieving the goal a declarative view of goals is necessary in order to reason about important properties of goals while a procedural view of goals is necessary to ensure that goals can be achieved efficiently in dynamic environments in this paper we propose a framework for goals which integrates both views we discuss the requisite properties of goals and the link between the declarative and procedural aspects then derive a formal semantics which has these properties we present a high level plan notation with goals and give its formal semantics we then show how the use of declarative information permits reasoning such as the detection and resolution of conflicts to be performed on goals 1 a hybrid approach to the profile creation and intrusion detection anomaly detection involves characterizing the behaviors of individuals or systems and recognizing behavior that is outside the norm this paper describes some preliminary results concerning the robustness and generalization capabilities of machine learning methods in creating user profiles based on the selection and subsequent classification of command line arguments we base our method on the belief that legitimate users can be classified into categories based on the percentage of commands they use in a specified period the hybrid approach we employ begins with the application of expert rules to reduce the dimensionality of the data followed by an initial clustering of the data and subsequent refinement of the cluster locations using a competitive network called learning vector quantization since learning vector quantization is a nearest neighbor classifier and new record presented to the network that lies outside a specified distance is classified as a masquerader thus this system does not require anomalous records to be included in the training set 1 model free least squares policy iteration we propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration our method is model free and completely off policy we are motivated by the least squares temporal difference learning algorithm lstd which is known for its efficient use of sample experiences compared to pure temporal difference algorithms lstd is ideal for prediction problems however it heretofore has not had a straightforward application to control problems moreover approximations learned by lstd are strongly influenced by the visitation distribution over states our new algorithm least squares policy iteration lspi addresses these issues the result is an off policy method which can use or reuse data collected from any source we test lspi on several problems including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trials secure mobile agent using strong non designated proxy signature it is expected that mobile agent will be widely used for electronic commerce as an important key technology if a mobile agent can sign a message in a remote server on behalf of a customer without exposing his her private key it can be used not only to search for special products or services but also to make a contract with a remote server to construct mobile agents kbc00 used an rsa based undetachable signature scheme but it does not provide server s non repudiation because the undetachable signature does not contain server s signature mobile agent is a very good application example of proxy signature and the undetachable signature can be considered as an example of proxy signature in this paper we show that secure mobile agent can be constructed using strong non designated proxy signature lkk01 which represents both the original signer s customer and the proxy signer s remote server signatures we provide rsa based and schnorr based constructions of secure mobile agent and moreover we show that the schnorr based scheme can be used very eciently in multi proxy mobile agent situation keywords secure mobile agent strong non designated proxy signature multi proxy signature 1 greedy function approximation a gradient boosting machine function approximation is viewed from the perspective of numerical optimization in function space rather than parameter space a connection is made between stagewise additive expansions and steepest descent minimization a general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion specific algorithms are presented for least squares least absolute deviation and huber m loss functions for regression and multi class logistic likelihood for classification special enhancements are derived for the particular case where the individual additive components are decision trees and tools for interpreting such treeboost models are presented gradient boosting of decision trees produces competitive highly robust interpretable procedures for regression and classification especially appropriate for mining less than clean data connections between this approach and the boosting methods of freund and shapire 1996 and friedman has simultaneous proxy evaluation the simultaneous proxy evaluation spe architecture is designed to evaluate multiple web proxies in parallel using object requests which are duplicated and passed to each proxy the spe architecture reduces problems of unrealistic test environments dated and or inappropriate workloads and is additionally applicable to contentbased prefetching proxies it is intended to measure byte and object hit rates client perceived latencies and cache consistency we characterize a space of proxy evaluation methodologies and place this architecture within it 1 introduction this paper presents a new architecture for the evaluation of proxy caches initially it grew out of research in techniques for prefetching in web caches in particular we found that existing mechanisms for the evaluation of proxy caches were not well suited to prefetching systems objective evaluation is paramount to all research whether applied or academic since this is certainly relevant when exploring various approac time series classification by boosting interval based literals a supervised classification method for temporal series even multivariate is presented it is based on boosting very simple classifiers clauses with one literal in the body the background predicates are based on temporal intervals two types of predicates are used i relative predicates such as increases and stays and ii region predicates such as always and sometime which operate over regions in the dominion of the variable experiments on di erent data sets several of them obtained from the uci repositories show that the proposed method is highly competitive with previous approaches keywords time series classification interval based literals boosting machine learning 1 mindless visualisations the wonder and unfortunately to the detriment of visualisation for the representation and comprehension of complex data sets is that to be most successful requires that they are tailored to suit the task and underlying data such a restriction enables visualisations to be well designed for the tasks to which they are known to be applied to and also to accommodate the style and range of data to be expected as normal the problem with this repeated redesign of visualisations is that the interface is often neglected and can even be solely dependent on the implementing technology used for the visualisation it is important to add such issues as the interface to visualisation considerations and to provide reusable concepts that will integrate with a range of metaphors and displays this position paper examines the issues surrounding such visualisation interfaces and presents a discussion of those issues 1 towards a living lab research facility and a ubiquitous computing research programme introduction my interest in the topic of this workshop stems from my current involvement in setting up a new research facility at the eindhoven university of technology this facility is called the living lab and is quite similar to related projects around the globe in that it aims to study how people experience a ubiquitous computing environment when they will inhabit it and use it for prolonged periods of time the slogan of this development is vacation on campus the project is currently at the initiation phase we have proposed a white paper 1 describing the concept and the research programme and we are currently working to involve stakeholders from different departments of the tu e e g architecture electrical engineering and technology management and of the local industry e g philips in the remaining of this position paper i outline our research concept and our programme concept vacation on campus the living lab will be a cros developing a market timing system using grammatical evolution this study examines the potential of an evolutionary automatic programming methodology grammatical evolution to uncover a series of useful fuzzy technical trading rules for the iseq the official equity index of the irish stock exchange index values for the period 29 03 93 to 4 12 1997 are used to train and test the model the preliminary findings indicate that the methodology has much potential 1 support vector machine active learning with applications to text classification support vector machines have met with significant success in numerous real world learning tasks however like most machine learning algorithms they are generally applied using a randomly selected training set classified in advance in many settings we also have the option of using pool based active learning instead of using a randomly selected training set the learner has access to a pool of unlabeled instances and can request the labels for some number of them we introduce a new algorithm for performing active learning with support vector machines i e an algorithm for choosing which instances to request next we provide a theoretical motivation for the algorithm using the notion of a version space we present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings keywords active learning selective sampling support vector machines classification relevance feedback abbreviations svm support vector machine tsvm transductive support vector machine 1 monotonic and residuated logic programs in this paper we define the rather general framework of monotonic logic programs where the main results of definite logic programming are validly extrapolated whenever defining new logic programming extensions we can thus turn our attention to the stipulation and study of its intuitive algebraic properties within the very general setting then the existence of a minimum model and of a monotonic immediate consequences operator is guaranteed and they are related as in classical logic programming afterwards we study the more restricted class of residuated logic programs which is able to capture several quite distinct logic programming semantics namely generalized annotated logic programs fuzzy logic programming hybrid probabilistic logic programs and possibilistic logic programming we provide the embedding of possibilistic logic programming build it a computer vision based interaction technique of a planning tool for construction and design it is time to go beyond the established approaches in human computer interaction with the augmented reality ar design strategy humans are able to behave as much as possible in a natural way behavior of humans in the real world with other humans and or real world objects following the fundamental constraints of natural way of interacting we derive a set of recommendations for the next generation of user interfaces the natural user interface nui the concept of nui is presented in form of a runnable demonstrator a computer vision based interaction technique for a planning tool for construction and design tasks keywords augmented reality natural user interface computer vision based interaction 1 marian flexible interoperability for federated digital libraries federated digital libraries are composed of distributed autonomous heterogeneous information services but provide users with a transparent integrated view of collected information respecting different information sources autonomy in this paper we discuss a federated system for the networked digital library of theses and dissertations ndltd an international consortium of universities libraries and other supporting institutions focused on electronic theses and dissertations etds the ndltd has so far allowed its members considerable autonomy though agreements are developing on metadata standards and on support of the open archives initiative that eventually will promote greater homogeneity at present federation requires dealing flexibly with differences among systems ontologies and data formats our solution involves adapting marian an objectoriented digital library retrieval system developed with support by nlm and nsf to serve as mediation middleware for the federated ndltd collection components of the solution include 1 the use of several harvesting techniques 2 an architecture based on object oriented ontologies of search modules and metadata 3 diversity within the harvested data joined to a single collection view for the user and 4 an integrated framework for addressing such questions as data quality information compression and flexible search the system can handle very large dynamic collections an adaptable relationship between the collection view and harvested data facilitates adding new sites to the federation and adapting to changes in existing sites marian s modular architecture and powerful and flexible data model work together to build an effective integrated solution within a simple uniform framework we present both the general design of the system and operational details of a preliminary federated collection involving several thousand etds in four different formats and two languages from usa and europe search history for user support in information seeking interfaces the research overview described focuses on the design of search history displays to support information seeking is it examines users is activities current and potential use of histories and building on this theoretical framework assesses prototype interfaces that integrate these histories into search systems preliminary results described indicate search history use in coordinated work mental model building and end user is strategies searchers create and use external records of their actions and the corresponding results by writing typing notes using copy and paste functions and making printouts recording user actions and results in computerized systems automates this process and enables the creation of search history displays that support users in their is existing systems provide search history capabilities however these often do not offer enough flexibility for users legal information has been selected as the domain for the research keywords history information a meta search method reinforced by cluster descriptors a meta search engine acts as an agent for the participant search engines it receives queries from users and redirects them to one or more of the participant search engines for processing a meta search engine incorporating many participant search engines is better than a single global search engine in terms of the number of pages indexed and the freshness of the indexes the meta search engine stores descriptive data i e descriptors about the index maintained by each participant search engine so that it can estimate the relevance of each search engine when a query is received the ability for the meta search engine to select the most relevant search engines determines the quality of the final result to facilitate the selection process the document space covered by each search engine must be described not only concisely but also precisely existing methods tend to focus on the conciseness of the descriptors by keeping a descriptor for a search engine s entire index this paper proposes to cluster a search engine s document space into clusters and keep a descriptor for each cluster we show that cluster descriptors can provide a finer and more accurate representation of the document space and hence enable the meta search engine to improve the selection of relevant search engines two cluster based search engine selection scenarios i e independent and high correlation are discussed in this paper experiments verify that the cluster based search engine selection can effectively identify the most relevant search engines and improve the quality of the search results consequently 1 a multi agent system for advising and monitoring students navigating instructional web sites a growing community of teachers at all levels of the educational system provides course material in the form of hypertext multimedia documents in most cases this is done by creating a course web site this paper explores the issues related to the design of software systems that aid teachers in monitoring how students use their sites and proactively advise students navigating the sites in connection to these functions two important topics in current applications of technology to education are discussed firstly the definition of a set of criteria allowing the evaluation of the appropriateness of multi media and hypertext technologies vis vis to classic course support material and in particular textbooks secondly the issue of the utility and acceptability of proactive user interfaces such as interface agents or personal assistant agents a multi agent system capable of advising and monitoring students navigating instructional web sites is introduced and it is used as a basis for discussion of the above two topics the system generates and uses a set of indicators evaluating how much use is made of hypertext and multimedia tools as well as indicators of usefulness and cognitive support of the proactive user interface keywords tutoring systems multi agent systems world wide web autonomous interface agents digital course material xml 1 battery aware static scheduling for distributed real time embedded systems this paper addresses battery aware static scheduling in batterypowered distributed real time embedded systems as suggested by previous work reducing the discharge current level and shaping its distribution are essential for extending the battery lifespan we propose two battery aware static scheduling schemes the first one optimizes the discharge power profile in order to maximize the utilization of the battery capacity the second one targets distributed systems composed of voltage scalable processing elements pes it performs variable voltage scheduling via efficient slack time re allocation which helps reduce the average discharge power consumption as well as flatten the discharge power profile both schemes guarantee the hard real time constraints and precedence relationships in the real time distributed embedded system specification based on previous work we develop a battery lifespan evaluation metric which is aware of the shape of the discharge power profile our experimental results show that the battery lifespan can be increased by up to 29 by optimizing the discharge power file alone our variable voltage scheme increases the battery lifespan by up to 76 over the non voltage scalable scheme and by up to 56 over the variable voltage scheme without slack time reallocation 1 clipping and analyzing news using machine learning techniques generating press clippings for companies manually requires a considerable amount of resources we describe a system that monitors online newspapers and discussion boards automatically the system extracts classifies and analyzes messages and generates press clippings automatically taking the specific needs of client companies into account key components of the system are a spider an information extraction engine a text classifier based on the support vector machine that categorizes messages by subject and a second classifier that analyzes which emotional state the author of a newsgroup posting was likely to be in by analyzing large amount of messages the system can summarize the main issues that are being reported on for given business sectors and can summarize the emotional attitude of customers and shareholders towards companies using labeled and unlabeled data to learn drifting concepts for many learning tasks where data is collected over an extended period of time one has to cope two problems the distribution underlying the data is likely to change and only little labeled training data is available at each point in time a typical example is information filtering i e the adaptive classification of documents with respect to a particular user interest both the interest of the user and the document content change over time a filtering system should be able to adapt to such concept changes since users often give little feedback a filtering system should also be able to achieve a good performance even if only few labeled training examples are provided this paper proposes a method to recognize and handle concept changes with support vector machines and to use unlabeled data to reduce the need for labeled data the method maintains windows on the training data whose size is automatically adjusted so that the estimated generalization error is minimized the approach is both theoretically well founded as well as effective and efficient in practice since it does not require complicated parameterization it is simpler to use and more robust than comparable heuristics experiments with simulated concept drift scenarios based on real world text data compare the new method with other window management approaches and show that it can effectively select an appropriate window size in a robust way in order to achieve an acceptable performance with fewer labeled training examples the proposed method exploits unlabeled examples in a transductive way 1 mobile agent based compound documents this paper presents a mobile agent based framework for building mobile compound document which can each be dynamically composed of mobile agents and can migrate itself over a network as a whole with all its embedded agents the key of this framework is that it builds a hierarchical mobile agent system that enables multiple mobile agents to be combined into a single mobile agent the framework also provides several value added mechanisms for visually manipulating components embedded in a compound document and for sharing a window on the screen among the components this paper describes this framework and some experiences in the implementation of a prototype system currently using java the both implementation language and component development language and then illustrates several interesting applications to demonstrate the framework s utility and flexibility 1 combining labeled and unlabeled data with co training we consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available in particular we consider a setting in which the description of each example can be partitioned into two distinct views motivated by the task of learning to classify web pages for example the description of a web page can be partitioned into the words occurring on that page and the words occurring in hyperlinks that point to that page we assume that either view of the example would be su cient for learning if we had enough labeled data but our goal is to use both views together to allow inexpensive unlabeled data to augment amuch smaller set of labeled examples speci cally the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view and then each algorithm s predictions on new unlabeled examples are used to enlarge the training set of the other our goal in this paper is to provide a pac style analysis for this setting and more broadly a pac style framework for the general problem of learning from both labeled and unlabeled data we also provide empirical results on real web page data indicating that this use of unlabeled examples can lead to signi cant improvement of hypotheses in practice as part of our analysis we provide new re a hybrid projection based and radial basis function architecture initial values and global optimization we introduce a hybrid architecture of projection based units and radial basis functions as a general function estimation scheme in particular we introduce an optimization scheme which has several steps and assures a convergence to a useful solution during training a determination whether a gaussian unit should be removed is applied this results in a final architecture with much smaller number of units the proposed global constrained optimization does not lead to overfitting which happens when the rbf width becomes too small this is achieved by a regularization mechanism classification and regression results are demonstrated on various benchmark data sets and compared with several variants of rbf networks the most striking performance improvement is achieved on the vowel data set 5 keywords projection units rbf units hybrid network architecture smlp clustering regularization 1 introduction the duality between projection based approximation and radial kernel techniques for specialized search engines it is emerging that it is very difficult for the major search engines to provide a comprehensive and up to date search service of the web even the largest search engines index only a small proportion of static web pages and do not search the web s backend databases that are estimated to be 500 times larger than the static web the scale of such searching introduces both technical and economic problems what is more in many cases users are not able to retrieve the information they desire because of the simple and generic search interface provided by the major search engines a necessary response to these search problems is the creation of specialized search engines these search engines search just for information in a particular topic or category on the web such search engines will have smaller and more manageable indexes and have a powerful domainspecific search interface this paper discusses the issues in this area and gives an overview of the techniques for building specialized search engines keywords specialized search engine information retrieval focused crawling taxonomy web search 1 graph theoretic clustering for image grouping and retrieval image retrieval algorithms are generally based on the assumption that visually similar images are located close to each other in the feature space since the feature vectors usually exist in a very high dimensional space a parametric characterization of their distribution is impossible so non parametric approaches like the k nearest neighbor search are used for retrieval this paper introduces a graph theoretic approach for image retrieval by formulating the database search as a graph clustering problem by using a constraint that retrieved images should be consistent with each other close in the feature space as well as being individually similar close to the query image the experiments that compare retrieval precision with and without clustering showed an average precision of 0 76 after clustering which is an improvement by 5 56 over the average precision before clustering 1 motivation computing feature vectors is an essential step in image database retrieval algorithm mining usability information from log files amulti pronged approach rooms is configurable by its occupants in how they organize various tools housing their data documents and graphics the tw system provides for synchronous and asynchronous user interactions but importantly these interactions are in the context of relevant data the work in this experiment was characterized by several full group meetings for planning and coordination interspersed with periods of individual activity asychronous work and smaller coordination meetings of two or three team members around the hand off of output from a task used as input for another task collected data the native version of tw produces a server based log file that contains information about the identity of users entering the distributed application the identity of the rooms through which users navigate file uploads and message passing between users this set of interactions was deemed too rudimentary for capturing the type of data needed for usability analysis since the source co multi user and security support for multi agent systems this paper discusses the requirements an agent system needs to be secure in particular the paper introduces a classification of modern distributed systems and examines the delegation concept from a security point of view after discussing the peculiar security and delegation issues present in distributed object systems mobile agent systems and in multi agent systems a case study is presented describing the multi user and security support that is being built into the jade platform rank aggregation revisited the rank aggregation problem is to combine many different rank orderings on the same set of candidates or alternatives in order to obtain a better ordering rank aggregation has been studied extensively in the context of social choice theory where several voting paradoxes have been discovered the problem data mining approaches for intrusion detection in this paper we discuss our research in developing general and systematic methods for intrusion detection the key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior and use the set of relevant system features to compute inductively learned classifiers that can recognize anomalies and known intrusions using experiments on the sendmail system call data and the network tcpdump data we demonstrate that we can construct concise and accurate classifiers to detect anomalies we provide an overview on two general data mining algorithms that we have implemented the association rules algorithm and the frequent episodes algorithm these algorithms can be used to compute the intra and inter audit record patterns which are essential in describing program or user behavior the discovered patterns can guide the audit data gathering process and facilitate feature selection to meet the challenges of both efficient learning mining and real time detection we propose an agent based architecture for intrusion detection systems where the learning agents continuously compute and provide the updated detection models to the detection agents london calling gis vr and the victorian period the bolles collection of tufts university represents a comprehensive and integrated collection of sources on the history and topography of victorian london texts images maps and three dimensional reconstructions are all interconnected forming a body of material that transcends the limits of print publication and exploits the flexibility of the electronic medium the perseus digital library has incorporated geographic information system and virtual reality technologies in a set of tools intended to help readers synthesize and visualize the numerous temporal and spatial interconnections between bolles collection materials the tools which are applicable to any large assemblage of related documents also help readers grasp the complex temporal spatial interactions that shape historical materials in general automatic generation of fuzzy logic rule bases examples i learning fuzzy rule based systems with genetic algorithms can lead to very useful descriptions of several problems many different alternative descriptions can be generated in many cases a simple rule base similar to rule bases designed by humans is preferable since it has a higher possibility of being valid in unforeseen cases thus the main idea of this paper is to study the genetic fuzzy rule base learning algorithm frbl 1 by examples from the machine learning repository 2 and to compare it with some other approaches starts stanford proposal for internet meta searching document sources are available everywhere both within the internal networks of organizations and on the internet even individual organizations use search engines from different vendors to index their internal document collections these search engines are typically incompatible in that they support different query models and interfaces they do not return enough information with the query results for adequate merging of the results and finally in that they do not export metadata about the collections that they index e g to assist in resource discovery this paper describes starts an emerging protocol for internet retrieval and search that facilitates the task of querying multiple document sources starts has been developed in a unique way it is not a standard but a group effort coordinated by stanford s digital library project and involving over 11 companies and organizations the objective of this paper is not only to give an overview of the starts protocol proposal but agents in annotated worlds virtual worlds offer great potential as environments for education entertainment and collaborative work agents that function effectively in heterogeneous virtual spaces must have the ability to acquire new behaviors and useful semantic information from those contexts the human computer interaction literature discusses how to construct spaces and objects that provide knowledge in the world that aids human beings to perform these tasks in this paper we describe how to build comparable annotated environments containing explanations of the purpose and uses of spaces and activities that allow agents quickly to become intelligent actors in those spaces examples are provided from our application domain believable agents acting as inhabitants and guides in a children s exploratory world keywords believability human like qualities of synthetic agents synthetic agents 1 introduction today s virtual environments present opportunities for simulation and interaction involving ma using process algebras to formally specify mobile agent data integrity properties a case study this paper shows how cryptographic protocols for mobile agent data integrity properties can be formally specified by using spi calculus an extension of calculus with cryptographic properties in particular by means of a case study it is shown how a specification technique initially conceived only for classical cryptographic protocols can be used in the context of mobile agents as well our case study includes the spi calculus specification of a sample mobile agent data integrity protocol and of its security property verifying sequential consistency on shared memory multiprocessors by model checking the memory model of a shared memory multiprocessor is a contract between the designer and programmer of the multiprocessor the sequential consistency memory model specifies a total order among the memory read and write events performed at each processor a trace of a memory system satisfies sequential consistency if there exists a total order of all memory events in the trace that is both consistent with the total order at each processor and has the property that every read event to a location returns the value of the last write to that location descriptions of shared memory systems are typically parameterized by the number of processors the number of memory locations and the number of data values it has been shown that even for finite parameter values verifying sequential consistency on general shared memory systems is undecidable we observe that in practice shared memory systems satisfy the properties of causality and data independence causality is the property that values of read events flow from values of write events data independence is the property that all traces can be generated by renaming data values from traces where the written values are distinct from each other if a causal and data independent system also has the property that the logical order of write events to each location is identical to their temporal order then sequential consistency can be verified algorithmically specifically we present a model checking algorithm to verify sequential consistency on such systems for a finite number of processors and memory locations and an arbitrary number of data values 1 quest querying specialized collections on the web ensuring access to specialized web collections in a fast evolving web environment requires flexible techniques for orientation and querying the adoption of meta search techniques for web collections is hindered by the enormous heterogeneity of the resources in this paper we introduce quest a system for querying specialized collections on the web coordinated reinforcement learning we present several new algorithms for multiagent reinforcement learning a common feature of these algorithms is a parameterized structured representation of a policy or value function this structure is leveraged in an approach we call coordinated reinforcement learning by which agents coordinate both their action selection activities and their parameter updates within the limits of our parametric representations the agents will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space our methods differ from many previous reinforcement learning approaches to multiagent coordination in that structured communication and coordination between agents appears at the core of both the learning algorithm and the execution architecture our experimental results comparing our approach to other rl methods illustrate both the quality of the policies obtained and the additional benefits of coordination intrusion detection a bibliography this document contains more than 600 references dated from 1980 to 2001 we undoubtedly have forgotten some important citations either through oversight or ignorance moreover errors may remain in the citations thus we ask for your indulgence and more importantly for your help send us a note if you nd any errors and let us know of any omissions at humboldt in robocup 99 the paper describes the architecture and the scientific goals of the virtual soccer team at humboldt 99 which is the successor of vice champion at humboldt 98 from robocup 98 in paris and world champion at humboldt 97 from robocup 97 in nagoya scientific goals are the development of agent oriented techniques and learning methods steps toward accommodating variable position tracking accuracy in a mobile augmented reality system the position tracking accuracy of a location aware mobile system can change dynamically as a function of the user s location and other variables specific to the tracker technology used this is especially problematic for mobile augmented reality systems which ideally require extremely precise position tracking for the user s head but which may not always be able to achieve the necessary level of accuracy while it is possible to ignore variable positional accuracy in an augmented reality user interface this can make for a confusing system for example when accuracy is low virtual objects that are nominally registered with real ones may be too far off to be of use algorithms for temporal query operators in xml databases the contents of an xml database or xml web data warehouse is seldom static new documents are created documents are deleted and more important documents are updated in many cases we want to be able to search in historical versions retrieve documents valid at a certain time query changes to documents etc this can be supported by extending the system with temporal database features in this paper we describe the new query operators needed in order to support an xml query language which supports temporal operations we also describe the algorithms which can make efficient implementation of these query operators possible keywords xml temporal databases query processing 1 rewriting logic roadmap and bibliography machine 218 7 ccs and lotos 230 208 314 45 89 311 309 201 8 the calculus 316 292 9 concurrent objects and actors 218 220 300 302 304 10 the unity language 218 11 concurrent graph rewriting 223 12 dataflow 223 13 neural networks 223 14 real time systems including timed automata timed transition systems hybrid automata and timed petri nets 268 262 and 15 the tile logic 146 147 135 model of synchronized concurrent computation 232 39 34 148 gongeroos 99 team this article presents gongeroos 99 approach to the robocup simulator league challenge rough set theory a data mining tool for semiconductor manufacturing the growing volume of information poses interesting challenges and calls for tools that discover properties of data data mining has emerged as a discipline that contributes tools for data analysis discovery of new knowledge and autonomous decisionmaking in this paper the basic concepts of rough set theory and other aspects of data mining are introduced the rough set theory offers a viable approach for extraction of decision rules from data sets the extracted rules can be used for making predictions in the semiconductor industry and other applications this contrasts other approaches such as regression analysis and neural networks where a single model is built one of the goals of data mining is to extract meaningful knowledge the power generality accuracy and longevity of decision rules can be increased by the application of concepts from systems engineering and evolutionary computation introduced in this paper a new rule structuring algorithm is proposed the concepts presented in the paper are illustrated with examples cross entropy guided ant like agents finding dependable primary backup path patterns in networks telecommunication network owners and operators have for half a century been well aware of the potential loss of revenue if a major trunk is damaged thus dependability at high cost has been implemented a simple effective and common dependability scheme is 1 1 protection with 100 capacity redundancy in the network a growing number of applications in need of dependable connections with specific requirements to bandwidth and delay have started using the internet which only provides best effort transport as their base communication service in this paper we adopt the 1 1 protection scheme and incorporate it as part of a routing system applicable for internet infrastructures 100 capacity redundancy is no longer required a distributed stochastic path finding routing algorithm based on rubinstein s cross entropy method for combinatorial optimisation is presented early results from monte carlo simulations indeed indicate that the algorithm is capable of finding pairs of independent primary and backup paths satisfying specific bandwidth a constraints agent mediated electronic commerce scientific and technological roadmap this report is that a big part of internet users have already sampled buying over the web f i 40 in the uk and a significat part qualify themselves as regular shoppers f i 10 in the uk again important differences between countries may be detected with respect to the expenses produced for instance finland spent 20 times more that spain on a per capita basis the forecasts for european buying goods and services for the year 2002 suggest that the current 5 2 million shoppers will increase until 28 8 millions and the european revenues from the current eur3 032 million to eur57 210 million finally a significant increase in the number of european executives that believe in the future of electronic commerce has been observed 33 in 1999 up from 23 in 1998 situated computing a paradigm to enhance the mobile user s interaction when people interact with computers they have to pay attention for things that are not related to the situation of the problems because the interfaces are not contextualized to their working environment hence it is sometimes hard to integrate computers as embedded tools which facilitate users to accomplish their objectives easily in the working life situated computing is a new paradigm for mobile computer users based on their physical context and activities carried out in the workspace it defines the infrastructure how the situated interaction can be provided using applications in this chapter we first describe a model called situation metaphor to design interaction between the user and mobile computers as the basis for the situated computing thereafter a framework called situated information filing and filtering siff is presented as the foundation for situated application development in general a three stages schema is given considerting the top stage for situated applications four applications based on the siff are also presented to demonstrate the enhancement of mobile user s interaction that can be achieved cs 395t large scale data mining fall 2001 ojects the vectors x 1 x n onto the principal components note that the eigenvectors of are the left singular vectors of the matrix 1 p n 1 x 1 x 2 x n thus pca can be obtained from the svd of mean centered data the mean centering is the important dierence between pca and svd and can yield qualitatively dierent results for data sets where the mean is not equal to 0 as shown in gure 1 svd pca svd pca figure 1 the leading singular vector may not be in the same direction as the principal component 1 2 cs 395t large scale data mining 2 clustering considerations clustering is the grouping together of similar objects usually the clustering problem is posed as a livinglab a white paper the livinglab is a planned research infrastructure that is pivotal for user system interaction research in the next decade this article presents the concept and outlines a research programme that will be served by this facility these future plans are motivated by a vision of future developments concerning interaction with intelligent environments efficiently querying moving objects with pre defined paths in a distributed environment due to the recent growth of the world wide web numerous spatio temporal applications can obtain their required information from publicly available web sources we consider those sources maintaining moving objects with predefined paths and schedules and investigate different plans to perform queries on the integration of these data sources efficiently examples of such data sources are networks of railroad paths and schedules for trains running between cities connected through these networks a typical query on such data sources is to find all trains that pass through a given point on the network within a given time interval we show that traditional filter semi join plans would not result in efficient query response times on distributed spatio temporal sources hence we propose a novel spatio temporal filter called deviation filter that exploits both the spatial and temporal characteristics of the sources in order to improve the selectivity we also report on our experiments in comparing the performances of the alternative query plans and conclude that the plan with spatio temporal filter is the most viable and superior plan a contract decommitment protocol for automated negotiation in time variant environments negotiation is a fundamental mechanism in distributed multi agent systems since negotiation is a time spending process in many scenarios agents have to take into account the passage of time and to react to uncertain events the possibility to decommit from a contract is considered a powerful technique to manage this aspect this paper considers interactions among self interested and autonomous agent each with their own utility function and focuses on incomplete information we define a negotiation model based on asynchronous message passing in which the negotiation doesn t end when an agreement is reached but when the consequences of the contract have happened i e the action is done in this model the agent utility functions are time dependent we present an extension of the contract net protocol that implements the model learning of kick in artificial soccer soccer simulation is a suitable domain for research in artificial intelligence this paper describes a new ball kicking skill that uses case based learning in many situations a single kick command is not sufficient to reach the desired ball movement hence a skill is needed that finds a suitable sequence of kicks the new kicking skill was developed for the at humboldt artificial soccer team engineering agentspeak l a formal computational model perhaps the most successful agent architectures and certainly the best known are those based on the belief desire intention bdi framework despite the wealth of research that has accumulated on both formal and practical aspects of this framework however there remains a gap between the formal models and the implemented systems in this paper we build on earlier work by rao aimed at narrowing this gap by developing a strongly typed formal yet computational model of the bdi based agentspeak l language agentspeak l is a programming language based on the procedural reasoning system prs and the distributed multi agent reasoning system dmars which determines the behaviour of the agents it implements in developing the model we add to rao s work identify some omissions and progress beyond the description of a particular language by giving a formal specification of a general bdi architecture that can be used as the basis for providing further formal specifications of more sophisticated systems multi document summarization and visualization in the informedia digital video library the informedia digital video library project provided a technological foundation for full content indexing and retrieval of video and audio media the library now contains over 2000 hours of video and is growing daily a good query engine is not sufficient for information retrieval because often the candidate result sets grow in number as the library grows video digests summarize sets of stories from the library providing users with a visual mechanism for interactive browsing and query refinement these digests are generated dynamically under the direction of the user based on automatically derived metadata from the video library informedia digital video library foundation work the informedia digital video library focused on the development and integration of technologies for information extraction from video and audio content to enable its full content search and retrieval over two terabytes 2000 hours 5 000 segments of online data was collected with automatically generated metadata and indices for retrieving video segments from this library informedia successfully pioneered the automatic creation of multimedia abstractions demonstrated empirical proofs of their relative benefits and gathered usage data of different summarizations and abstractions fundamental research and prototyping was conducted in the following areas shown with a sampling of references to particular work modelling rational inquiry in non ideal agents the construction of rational agents is one of the goals that has been pursued in artificial intelligence ai in most of the architectures that have been proposed for this kind of agents its behaviour is guided by its set of beliefs in our work rational agents are those systems that are permanently engaged in the process of rational inquiry thus their beliefs keep evolving in time as a consequence of their internal inference procedures and their interaction with the environment both ai researchers and philosophers are interested in having a formal model of this process and this is the main topic in our work beliefs have been formally modelled in the last decades using doxastic logics the possible worlds model and its associated kripke semantics provide an intuitive semantics for these logics but they seem to commit us to model agents that are logically omniscient and perfect reasoners we avoid these problems by replacing possible worlds by conceivable situations which ar mining the web to create minority language corpora the web is a valuable source of language specific resources but the process of collecting organizing and utilizing these resources is difficult we describe corpusbuilder an approach for automatically generating web search queries for collecting documents in a minority language it differs from pseudo relevance feedback in that retrieved documents are labeled by an automatic language classifier as relevant or irrelevant and this feedback is used to generate new queries we experiment with various query generation methods and query lengths to find inclusion exclusion terms that are helpful for retrieving documents in the target language and find that using odds ratio scores calculated over the documents acquired so far was one of the most consistently accurate query generation methods we also describe experiments using a handful of words elicited from a user instead of initial documents and show that the methods perform similarly experiments applying the same approach to multiple languages are also presented showing that our approach generalizes to a variety of languages 1 using augmented reality to visualise architecture designs in an outdoor environment this paper presents the use of a wearable computer system to visualise outdoor architectural features using augmented reality the paper examines the question how does one visualise a design for a building modification to a building or extension to an existing building relative to its physical surroundings the solution presented to this problem is to use a mobile augmented reality platform to visualise the design in spatial context of its final physical surroundings the paper describes the mobile augmented reality platform tinmith2 used in the investigation the operation of the system is described through a detailed example of the system in operation the system was used to visualise a simple extension to a building on one of the university of south australia campuses sview architecture overview and system description this report presents an architecture overview and a system description of the sview system the system provides developers service providers and users of electronic services with an open and extendible service infrastructure that allows far reaching user control this is accomplished by collecting the services of an individual user in a virtual briefcase services come in the form of self contained service components i e including both logic and data and the briefcase is mobile to allow it to follow as the user moves between different hosts and terminals a specification of how to build such service components and the infrastructure for handling briefcases is presented a reference implementation of the specification as well as extensions in the form of service components is also described the purpose of the report is to serve as a technical reference for developers of sview services and software infrastructure that builds on sview technology keywords electronic services personal service environments user interfaces mobility personalization service collaboration component based software engineering may 2001 sics technical report t2001 06 issn 1100 3154 isrn sics t 2001 06 se 2 bylund 1 the abels framework for linking distributed simulations using software agents many simulations need access to dynamicallychanging data from other sources such as sensors or even other simulations for example a forest fire simulation may need data from sensors in the forest and a weather simulation at a remote site we have developed an agentbased software framework to facilitate the dynamic exchange of data between distributed simulations and other remote data resources the framework called abels agent based environment for linking simulations allows independently designed simulations to communicate seamlessly with no a priori knowledge of the details of other simulations and data resources this paper discusses our architecture and current implementation using sun microsystems jini technology and the d agents mobile agent system this paper extends earlier work by describing the implementation of the brokering system for matching data producers and consumers and providing additional details of other components some considerations about embodied agents as computers are being more and more part of our world we feel the urgent need of proper user interface to interact with the use of command lines typed on a keyboard are more and more obsolete specially as computers are receiving so much attention from a large audience the metaphor of face to face communication applied to human computer interaction is finding a lot of attraction humans are used since they are born to communicate with others seeing faces interpreting their expressions understanding speech are all part of our development and growth but face to face conversation is very complex as it involved a huge number of factors we speak with our voice but also with our hand eye face and body our gesture modifies emphasizes contradicts what we say by words the production of speech and nonverbal behaviors work in parallel and not in antithesis they seem to be two different forms voice and body gestures of the same process speech they add info towards group communication for mobile participants group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements moreover location awareness is clearly central to mobile applications such as traffic management and smart spaces in this paper we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group membership management service suitable for proximity groups we describe a novel approach to efficient coverage estimation giving applications feedback on the proportion of the area of interest covered by a proximity group and also discuss our approach to partition anticipation the personal interaction panel a two handed interface for augmented reality this paper describes the introduction of a new interaction paradigm to augmented reality applications the everyday tool handling experience of working with pen and notebooks is extended to create a three dimensional two handed interface that supports easy to understand manipulation tasks in augmented and virtual environments in the design step we take advantage from the freedom given by our very low demands on hardware and augment form and functionality to this device on the basis of examples from object manipulation augmented research environments and scientific visualization we show the generality of applicability although being in the first stages implementation we consider the wide spectrum of suitability for different purposes models for information integration turning local as view into global as view there are basically two approaches for designing a data integration system in the global as view approach one defines the concepts in the global schema as views over the sources whereas in the local as view approach one characterizes the sources as views over the global schema the goal of this paper is to verify whether we can transform a data integration system built with the local as view approach into a system following the global as view approach we study the problem in a setting where the global schema is expressed in the relational model with inclusion dependencies and the queries used in the integration systems both the queries on the global schema and the views in the mapping are expressed in the language of conjunctive queries the result we present is that such a transformation exists we can always transform a local as view system into a global as view system such that for each query the set of answers to the query wrt the former is the same as the set of answers wrt the latter knowledge management in heterogeneous data warehouse environments this paper addresses issues related to knowledge management in the context of heterogeneous data warehouse environments the traditional notion of data warehouse is evolving into a federated warehouse augmented by a knowledge repository together with a set of processes and services to support enterprise knowledge creation refinement indexing dissemination and evolution use of satellite image referencing algorithms to characterize asphaltic concrete mixtures a natural way to test the structural integrity of a pavement is to send signals with different frequencies through the pavement and compare the results with the signals passing through an ideal pavement for this comparison we must determine how for the corresponding mixture the elasticity e depends on the frequency f in the range from 0 1 to 10 5 hz it is very expensive to perform measurements in high frequency area above 20 hz to avoid these measurements we can use the fact that for most of these mixtures when we change a temperature the new dependence changes simply by scaling thus instead of performing expensive measurements for different frequencies we can measure the dependence of e on moderate frequencies f for different temperatures and then combine the resulting curves into a single master curve in this paper we show how fuzzy techniques can help to automate this combination flexible interoperability in a federated digital library of theses and dissertations federated digital libraries are composed of autonomous possibly heterogeneous information services distributed across the internet federation provides users with a seamless integrated view of the collected information we are creating a federated system for the networked digital library of theses and dissertations ndltd an international consortium of universities libraries and other supporting institutions focused on electronic theses and dissertations etds the ndltd allows its members minimal restrictions and maximal autonomy so federating requires dealing flexibly with differences among ontologies data formats and finding aids involving several thousand etds in four formats and two languages secure communication for secure agent based electronic commerce applications although electronic commerce is a relatively new concept it has already become a normal aspect of our daily life the software agent technology is also relatively new in the area of electronic commerce software agents could be used for example to search for the lowest prices and the best services to buy goods on behalf of a user etc these applications involve a number of security issues such as communications security system security and application security that have to be solved this paper describes how communications security is added to a lightweight agent framework secure agent based electronic commerce applications require communications security services adding these services is a rst basis and an important enabler for the framework in order to be used for secure electronic commerce applications human behavior models for game theoretic agents case of crowd tipping this paper describes an effort to integrate human behavior models from a range of ability stress emotion decision theoretic and motivation literatures into a game theoretic framework our goal is to create a common mathematical framework cmf and a simulation environment that allows one to research and explore alternative behavior models to add realism to software agents e g human reaction times constrained rationality emotive states and cultural influences our cmf is based on a dynamical gametheoretic approach to evolution and equilibria in markov chains representing states of the world that the agents can act upon in these worlds the agents utilities payoffs are derived by a deep model of cognitive appraisal of intention achievement including assessment of emotional activation decay relative to concern ontologies and subject to integrated stress and related constraints we present the progress to date on the mathematical framework and on an environment for editing the various elements of the cognitive appraiser utility generators concern ontologies and markov chains we summarize a prototype of an example training game for counter terrorism and crowd management future research needs are elaborated including validity issues and the gaps in the behavioral literatures that agent developers must struggle with mobile agent organizations mobile agents are a useful paradigm other than a useful technology for the development of complex internet applications however the effective development of mobile agent applications requires suitable models and infrastructures this paper proposes an organizational approach to the high level design of mobile agent applications the idea is to models the internet as a multiplicity of local and active organizational contexts intended as the places where coordination activities of application agents occur and are ruled the paper discusses the advantages and the generality of such an approach also with the help of a case study in the area of tourist assistance a software fault tree approach to requirements analysis of an intrusion detection system requirements analysis for an intrusion detection system ids involves deriving requirements for the ids from analysis of the intrusion domain when the ids is as here a collection of mobile agents that detect classify and correlate system and network activities the derived requirements include what activities the agent software should monitor what intrusion characteristics the agents should correlate where the ids agents should be placed to feasibly detect the intrusions and what countermeasures the software should initiate this paper describes the use of software fault trees for requirements identification and analysis in an ids intrusions are divided into seven stages following ruiu and a fault subtree is developed to model each of the seven stages reconnaissance penetration etc two examples are provided this approach was found to support requirements evolution as new intrusions were identified incremental development of the ids and prioritization of countermeasures reasoning about intentions in uncertain domains the design of autonomous agents that are situated in real world domains involves dealing with uncertainty in terms of dynamism observability and non determinism these three types of uncertainty when combined with the real time requirements of many application domains imply that an agent must be capable of eectively coordinating its reasoning as such situated belief desire intention bdi agents need an ecient intention reconsideration policy which denes when computational resources are spent on reasoning i e deliberating over intentions and when resources are better spent on either object level reasoning or action this paper presents an implementation of such a policy by modelling intention reconsideration as a partially observable markov decision process pomdp the motivation for a pomdp implementation of intention reconsideration is that the two processes have similar properties and functions as we demonstrate in this paper our approach achieves better results than existing intention reconsideration frameworks as is demonstrated empirically in this paper application of art2 networks and self organizing maps to collaborative filtering since the world wide web has become widespread more and more applications exist that are suitable for the application of social information filtering techniques in collaborative filtering preferences of a user are estimated through mining data available about the whole user population implicitly exploiting analogies between users that show similar characteristics super logic programs recently considerable interest and research e ort has been given to the problem of finding a suitable extension of the logic programming paradigm beyond the class of normal logic programs in order to demonstrate that a class of programs can be justifiably called an extension of logic programs one should be able to argue that the proposed syntax of such programs resembles the syntax of logic programs but it applies to a significantly broader class of programs the proposed semantics of such programs constitutes an intuitively natural extension of the semantics of normal logic programs there exists a reasonably simple procedural mechanism allowing at least in principle to compute the semantics the proposed class of programs and their semantics is a special case of a more general non monotonic formalism which clearly links it to other well established non monotonic formalisms in this paper we propose a specific class of extended logic programs which will be modestly called super logic programs or just super programs we will argue that the class of super programs satisfies all of the above conditions and in addition is su ciently flexible to allow various application dependent extensions and modifications we also provide a brief description of a prolog implementation of a query answering interpreter for the class of super programs which is available via ftp and www keywords non monotonic reasoning logics of knowledge and beliefs semantics of logic programs and deductive databases an extended abstract of this paper appeared in the proceedings of the fifth international conference on principles of knowledge representation and reasoning kr 96 boston massachusetts 1996 pp 529 541 partially supported by the national science fou supporting internet scale multi agent systems ts a model of agentscape from the agent perspective that is the location comprising the middleware and the resources are represented by a location manager agent and resource objects calls from an agent to the middleware are modeled by requests to the location manager agent to for example create an agent or move an agent information about resources residing at the location can be retrieved by binding to the resource objects which are local distributed objects these objects can be accessed only within the location they reside not from outside the location for development of agent applications an application programming interface api and a runtime system rts are provided see fig 1 the default api and rts can be extended to provide a higher level application programming interface with for example a model that offers more structure and semantics to the agent application developer within agentscape management of large scale agent systems is an important issue includi what can knowledge representation do for semi structured data the problem of modeling semi structured data is important in many application areas such as multimedia data management biological databases digital libraries and data integration graph schemas buneman et al 1997 have been proposed recently as a simple and elegant formalism for representing semistructured data in this model schemas are represented as graphs whose edges are labeled with unary formulae of a theory and the notions of conformance of a database to a schema and of subsumption between two schemas are defined in terms of a simulation relation several authors have stressed the need of extending graph schemas with various types of constraints such as edge existence and constraints on the number of outgoing edges in this paper we analyze the appropriateness of various knowledge representation formalisms for representing and reasoning about graph schemas extended with constraints we argue that neither first order logic nor logic programming nor frame based languages interpretation of shape related iconic gestures in virtual environments the interpretation of iconic gestures in spatial domains is a promising idea to improve the communicative capabilities of human computer interfaces so far approaches towards gesture recognition focused mainly on deictic and emblematic gestures iconics viewed as iconic signs in the sense of peirce are different from deictics and emblems for their relation to the referent is based on similarity in the work reported here the breakdown of the complex notion of similarity provides the key idea towards a computational model of gesture semantics for iconic gestures based on an empirical study we describe first steps towards a recognition model for shape related iconic gestures and its implementation in a prototype gesture recognition system observations are focused on spatial concepts and their relation to features of iconic gestural expressions the recognition model is based on a graphmatching method which compares the decomposed geometrical structures of gesture and object agents acting and moving in healthcare scenario a paradigm for telemedical collaboration the present paper describes a novel approach to the analysis and development of telemedicine systems based on the multi agent paradigm an agent is an autonomous social reactive and proactive entity sometimes also mobile since telemedicine is grounded on communication and sharing of resources agents are suitable for its analysis and implementation and we adopted them for developing a prototype telemedical agent aliasing on the world wide web prevalence and performance implications aliasing occurs in web transactions when requests containing different urls elicit replies containing identical data payloads aliasing can cause cache misses and there is reason to suspect that offthe shelf web authoring tools might increase aliasing on the web existing research literature however says little about the prevalence of aliasing in user initiated transactions or its impact on endto end performance in large multi level cache hierarchies storing and querying multiversion xml documents using durable node numbers managing multiple versions of xml documents represents an important problem for many traditional applications such as software configuration control as well as new ones such as link permanence of web documents research on managing multiversion xml documents seeks to provide efficient and robust techniques for storing retrieving and querying such documents in this paper we present a novel approach to version management that achieves these objectives by a scheme based on durable node numbers and timestamps for the elements of xml documents we first present efficient storage and retrieval techniques for multiversion documents then we explore the indexing and clustering strategies needed to assure efficient support for complex queries on content and on document evolution signer independent continuous sign language recognition based on srn hmm a divide and conquer approach is presented for signer independent continuous chinese sign language csl recognition in this paper the problem of continuous csl recognition is divided into the subproblems of isolated csl recognition we combine the simple recurrent network srn with the hidden markov models hmm in this approach the improved srn is introduced for segmentation of continuous csl outputs of srn are regarded as the states of hmm and the lattice viterbi algorithm is employed to search the best word sequence in the hmm framework experimental results show srn hmm approach has better performance than the standard hmm one rivalry and interference with a head mounted display perceptual factors that effect monocular transparent a k a see thru head mounted displays include binocular rivalry visual interference and depth of focus compositional design and maintenance of broker agents a generic broker agent architecture is introduced designed in a principled manner using the compositional development method for multi agent systems desire a flexible easily adaptable agent architecture results in which in addition facilities have been integrated that provide automated support of the agents own maintenance therefore the agent is not only easily adaptable but it shows adaptive behaviour to meet new requirements supported by communication with a maintenance agent 1 introduction to support users on the world wide web various types of agents can be and actually have been developed for example to support brokering processes in electronic commerce personal assistant agents can be developed that support a user offering products or services at the web or agents that support search for information on products within a user s scope of interest or agents that combine both functionalities moreover mediating agents can be developed that communicate both with using category based collaborative filtering in the active webmuseum collaborative filtering is an important technology for creating useradapting web sites in general the efforts of improving filtering algorithms and using the predictions for the presentation of filtered objects are decoupled therefore common measures or metrics for evaluating collaborative filtering recommender systems focus mainly on the prediction algorithm it is hard to relate the classic measurements to actual user satisfaction because of the way the user interacts with the recommendations determined by their representation influences the benefits for the user we propose an abstract access paradigm which can be applied to the design of filtering systems and at the same time formalizes the access to filtering results via multi corridors based on content based categories this leads to new measures which better relate to the user satisfaction we use these measures to evaluate the use of various kinds of multi corridors for our prototype user adapting web site the active webmuseum smart playing cards a ubiquitous computing game abstract recent technological advances allow for turning parts of our everyday environment into so called smart environments in this paper we present the smart playing cards application a ubiquitous computing game that augments a classical card game with information technological functionality in contrast to developing new games around the abilities of available technology furthermore we present the requirements such an application makes on a supporting software infrastructure for ubiquitous computing gleaning answers from the web introduction this position paper summarizes my recent and ongoing research on web information extraction and retrieval i describe wrapper induction and verification techniques for extracting data from structured sources boosted wrapper induction an extension of these techniques to handle natural text elixir our e cient and expressive language for xml information retrieval techniques and applications for text genre classification and stochastic models for xml schema alignment the unifying theme of these various research projects is to develop enabling technologies that facilitate the rapid development of large web services for data access and integration 2 wrapper induction and verification a wide variety of valuable textual information resides on the web but very little is in a machineunderstandable form such as xml instead the content three new algorithms for projective bundle adjustment with minimal parameters bundle adjustment is a technique used to compute the maximum likelihood estimate of structure and motion from image feature correspondences it practice large non linear systems have to be solved most of the time using an iterative optimization process starting from a sub optimal solution obtained by using linear methods the behaviour in terms of convergence and the computational cost of this process depend on the parameterization used to represent the problem i e of structure and motion multi agent systems as intelligent virtual environments intelligent agent systems have been the subject of intensive research over the past few years they comprise one of the most promising computing approaches ever able to address issues that require abstract modelling and higher level reasoning virtual environments on the other hand offer the ideal means to produce simulations of the real world for purposes of entertainment education and others the merging of these two fields seems to have a lot to offer to both research and applications if progress is made on a co ordinated manner and towards standardization this paper is a presentation of vital an intelligent multi agent system able to support general purpose intelligent virtual environment applications algorithms for temporal query operators in xml databases the contents of an xml database or xml web data warehouse is seldom static new documents are created documents are deleted and more important documents are updated in many cases we want to be able to search in historical versions retrieve documents valid at a certain time query changes to documents etc this can be supported by extending the system with temporal database features in this paper we describe the new query operators needed in order to support an xml query language which supports temporal operations we also describe the algorithms which can make efficient implementation of these query operators possible keywords xml temporal databases query processing 1 adaptive and intelligent technologies for web based education the paper provides a review of adaptive and intelligent technologies in a context of web based distance education we analyze what kind of technologies are available right now how easy they can be implemented on the web and what is the place of these technologies in large scale web based education evaluating look to talk a gaze aware interface in a collaborative environment we present look to talk a gaze aware interface for directing a spoken utterance to a software agent in a multiuser collaborative environment through a prototype and a wizard of oz woz experiment we show that quot look totalk is indeed a natural alternative to speech and other paradigms experiments in meta level learning with ilp when considering new datasets for analysis with machine learning algorithms we encounter the problem of choosing the algorithm which is best suited for the task at hand the aim of meta level learning is to relate the performance of different machine learning algorithms to the characteristics of the dataset the relation is induced on the basis of empirical data about the performance of machine learning algorithms on the different datasets applying a new multidimentional framework to the evaluation of multiagent system methodologies because of the great interest in using multiagent systems mas in a wide variety of applications in recent years agentoriented methodologies and related modeling techniques have become a priority for the development of large scale agentbased systems the work we present here belongs to the disciplines of software engineering and distributed artificial intelligence more specifically we are interested in software engineering aspects involved in the development of multiagent systems mas several methodologies have been proposed for the development of mas for the most part these methodologies remain incomplete they are either an extension of object oriented methodologies or an extension of knowledge based methodologies in addition too little effort has gone into the standardization of mas methodologies platforms and environments it seems obvious therefore that software engineering aspects of the development of mas still remain an open field the success of the agent paradigm requires systematic methodologies for the specification analysis and design of non toy mas applications we here present a framework called muccmas which stands for multidimensional framework of criteria for the comparison of mas methodologies that enabled us to make a comparative analysis of existing main mas methodologies the overall results of this comparative analysis are presented here results from this work have also lead us to propose a unification scheme much in the same spirit as that of uml for mas methodologies our goal is to make mas design and development more systematic and to contribute to the standardisation of mas methodologies and platforms scanmail a voicemail interface that makes speech browsable readable and searchable increasing amounts of public corporate and private speech data are now available on line these are limited in their usefulness however by the lack of tools to permit their browsing and search the goal of our research is to provide tools to overcome the inherent difficulties of speech access by supporting visual scanning search and information extraction we describe a novel principle for the design of uis to speech data what you see is almost what you hear wysiawyh in wysiawyh automatic speech recognition asr generates a transcript of the speech data the transcript is then used as a visual analogue to that underlying data a graphical user interface allows users to visually scan read annotate and search these transcripts users can also use the transcript to access and play specific regions of the underlying message we first summarize previous studies of voicemail usage that motivated the wysiawyh principle and describe a voicemail ui scanmail that embodies wysiawyh we report on a laboratory experiment and an 18 user two month field trial evaluation scanmail outperformed a state of the art voicemail system on core voicemail tasks this was attributable to scanmail s support for visual scanning search and information extraction while the asr transcripts contain errors they nevertheless improve the efficiency of voicemail processing transcripts either provide enough information for users to extract key points or to navigate to important regions of the underlying speech which they can then play directly measuring knowledge with workflow management systems expert knowledge is captured in the process design in organisations knowledge becomes embedded in routines processes practices as well as norms and can be evaluated by decisions or actions to which it leads for example measurable efficiencies speed or quality gains knowledge develops over time through experience that includes what we absorb from courses books and mentors as well as informal learning in this paper we analyse workflow history and demonstrate that workflow management systems enable knowledge measurement 1 active learning for automatic speech recognition state of the art speech recognition systems are trained using transcribed utterances preparation of which is labor intensive and time consuming in this paper we describe a new method for reducing the transcription effort for training in automatic speech recognition asr active learning aims at reducing the number of training examples to be labeled by automatically processing the unlabeled examples and then selecting the most informative ones with respect to a given cost function for a human to label we automatically estimate a confidence score for each word of the utterance exploiting the lattice output of a speech recognizer which was trained on a small set of transcribed data we compute utterance confidence scores based on these word confidence scores then selectively sample the utterances to be transcribed using the utterance confidence scores in our experiments we show that we reduce the amount of labeled data needed for a given word accuracy by 27 enabling ad hoc collaboration through schedule learning and prediction the transferal of the desktop interface to the world at large is not the goal of ubiquitous computing rather ubiquitous computing strives to increase the responsiveness of the world at large to the individual a large part of this responsiveness is improved communication with other individuals in this paper we describe a system that can enable ad hoc collaboration between several people by creating a model of the daily schedules of individuals and by performing predictions based on this model using gps data we learn to distinguish locations and track the times that these locations are visited in addition we use markov models to predict which locations might be visited next based on the user s previous behavior a visual modality for the augmentation of paper in this paper we describe how we have enhanced our multimodal paper based system rasa with visual perceptual input we briefly explain how rasa improves upon current decisionsupport tools by augmenting rather than replacing the paperbased tools that people in command and control centers have come to rely upon we note shortcomings in our initial approach discuss how we have added computer vision as another input modality in our multimodal fusion system and characterize the advantages that it has to offer we conclude by discussing our current limitations and the work we intend to pursue to overcome them in the future a multidimentional framework for the evaluation of multiagent system methodologies because of the great interest in using multiagent systems mas in a wide variety of applications in recent years agent oriented methodologies and related modeling techniques have become a priority for the development of large scale agent based systems the work we present here belongs to the disciplines of software engineering and distributed artificial intelligence more specifically we are interested in software engineering aspects involved in the development of multiagent systems mas several methodologies have been proposed for the development of mas for the most part these methodologies remain incomplete they are either an extension of object oriented methodologies or an extension of knowledge based methodologies in addition too little effort has gone into the standardization of mas methodologies platforms and environments it seems obvious therefore that software engineering aspects of the development of mas still remains an open field the success of the agent paradigm requires systematic methodologies for the specification analysis and design of non toy mas applications we present in this paper a new framework called muccmas which stands for multidimensional framework of criteria for the comparison of mas methodologies that enabled us to make a comparative analysis of existing main mas methodologies adaptive load sharing for network processors a novel scheme for processing packets in a router is presented which provides for load sharing among multiple network processors distributed within the router it is complemented by a feedback control mechanism designed to prevent processor overload incoming traffic is scheduled to multiple processors based on a deterministic mapping the mapping formula is derived from the robust hash routing also known as the highest random weight hrw scheme introduced in k w ross ieee network 11 6 1997 and d g thaler et al ieee trans networking 6 1 1998 no state information on individual flow mapping needs to be stored but for each packet a mapping function is computed over an identifier vector a predefined set of fields in the packet an adaptive extension to the hrw scheme is provided in order to cope with biased traffic patterns we prove that our adaptation possesses the minimal disruption property with respect to the mapping and exploit that property in order to minimize the probability of flow reordering simulation results indicate that the scheme achieves significant improvements in processor utilization a higher number of router interfaces can thus be supported with the same amount of processing power i the security architecture of the m m mobile agent framework in the mobile agent programming model small threads of execution migrate from machine to machine performing their operations locally for being able to deploy such a model into real world applications security is a vital concern in the m m project we have developed a system that departures from the traditional platform based execution model for mobile agents in m m there are no agent platforms instead there is a component framework that allows the applications to become able of sending and receiving agents by themselves in a straightforward manner in this paper we examine the security mechanisms available in m m and how integration with existing applications is done one difficult aspect of this work is that all the features must work with the security mechanisms that already exist on the applications this is so because the components are integrated from within into the applications which already have security mechanisms in place currently m m provides features like fine grain security permissions encryption of agents and data certificate distribution using ldap and cryptographic primitives for agents for validating the approach and solutions found we have integrated the framework into several off the shelf web servers having the security mechanisms running with no problems discovering seeds of new interest spread from premature pages cited by multiple communities the world wide web is a great source of new topics significant for trend birth and creation in this paper we propose a method for discovering topics which stimulate communities of people into earnest communications on the topics meaning and grow into a trend of popular interest here the obtained are web pages which absorb attentions of people from multiple interest communities it is shown by a experiments to a small group of people that topics in such pages can trigger the growth of peoples interests beyond the bounds of existing communities medical document information retrieval through active user interfaces this paper reports our preliminary design and implementation towards the development of kavanah a system to help users retrieve information and discover knowledge for a medical domain application the goal of this system is to adaptively react to the dynamic changes in the user s interests and preferences in searching for information within the context of the on going information retrieval task the context in which the user seeks information is modeled by an active user interface through analyzing the user s interactions with the system to dynamically construct an ontology of concepts representing the user s information seeking context we implement the system using unified medical language system knowledge base as a test bed concepts and architecture of a security centric mobile agent server mobile software agents are software components that are able to move in a network they are often considered as an attractive technology in electronic commerce applications although security concerns prevail in this paper we describe the architecture and concepts of the semoa server a runtime environment for java based mobile agents its architecture has a focus on security and easy extendability and offers a framework for transparent content inspection of agents by means of filters we implemented filters that handle agent signing and authentication as well as selective encryption of agent contents filters are applied transparently such that agents need not be aware of the security services provided by the server a semiotic communication model for interface design this research wants to contribute to the creation of a semiotic framework for interface design using the jakobson s communication model to analyse the hci approach to interface development we explain how some central factors of communication are not enough considered by designers modeling temporal consistency in data warehouses real world changes are generally discovered delayed by computer systems the typical update patterns for traditional data warehouses on an overnight or even weekly basis enlarge this propagation delay until the information is available to knowledge workers the main contribution of the paper is the identification of two different temporal characterizations of the information appearing in a data warehouse one is the classical description of the time instant when a given fact occurred the other represents the instant when the information has been entered into the system we present an approach for modeling conceptual time consistency problems and introduce a data model that deals with timely delays and supports knowledge workers to determine what the situation was in the past knowing only the information available at a given instant of time 1 on the robustness of some cryptographic protocols for mobile agent protection mobile agent security is still a young discipline and most naturally the focus up to the time of writing was on inventing new cryptographic protocols for securing various aspects of mobile agents however past experience shows that protocols can be flawed and flaws in protocols can remain unnoticed for a long period of time the game of breaking and fixing protocols is a necessary evolutionary process that leads to a better understanding of the underlying problems and ultimately to more robust and secure systems although to the best of our knowledge little work has been published on breaking protocols for mobile agents it is inconceivable that the multitude of protocols proposed so far are all flawless as it turns out the opposite is true we identify flaws in protocols proposed by corradi et al karjoth et al and karnik et al including protocols based on secure co processors selecting a fuzzy logic operation from the dnf cnf interval how practical are the resulting operations in classical two valued logic cnf and dnf forms of each propositional formula are equivalent to each other in fuzzy logic cnf and dnf forms are not equivalent they form an interval that contains the fuzzy values of all classically equivalent propositional formulas if we want to select a single value from this interval then it is natural to select a linear combination of the interval s endpoints in particular we can do that for cnf and dnf forms of and and or thus designing natural fuzzy analogues of classical and and or operations stratum approaches to temporal dbms implementation database interfaces legacy systems previous approaches to implementing temporal dbmss have assumed that a temporal dbms must be built from scratch employing an integrated architecture and using new temporal implementation techniques such as temporal indexes and join algorithms however this is a very large and time consuming task this paper explores approaches to implementing a temporal dbms as a stratum on top of an existing non temporal dbms rendering implementation more feasible by reusing much of the functionality of the underlying conventional dbms more specifically the paper introduces three stratum meta architectures each with several specific architectures based on a new set of evaluation criteria advantages and disadvantages of the specific architectures are identified the paper also classifies all existing temporal dbms implementations according to the specific architectures they employ it is concluded that a stratum architecture is the best short medium and perhaps even longterm approach to implementing a temporal dbms 1 differential join prices for parallel queues social optimality dynamic pricing algorithms and application to internet pricing we consider a system of identical parallel queues served by a single server and distinguished only by the price charged at entry a poisson stream of customers joins the queue by a greedy policy that minimizes a disutility that combines price and congestion a special case of linear disutility is analyzed for which it is shown that the individually optimal greedy queue join policy is nearly socially optimal for this queueing system a markov decision theoretic framework is formulated for dynamic pricing in the general case this queueing system has application in the pricing of internet services nexus distributed data management concepts for location aware applications nowadays mobile computers like subnotebooks or personal digital assistants as well as cellular phones can not only communicate wirelessly but they can also determine their position via appropriate sensors like dgps socalled location aware applications take advantage of this fact and structure information according to the position of their users in order to be able to assign data to a certain location these information systems have to refer to spatial computer models the nexus project which is supported by the deutsche forschungsgemeinschaft dfg german research foundation aims at the development of a generic infrastructure that serves as a basis for location aware applications the central task of this platform deals with the data management persona a contextualized and personalized web search recent advances in graph based search techniques derived from kleinberg s work 1 have been impressive this paper further improves the graph based search algo rithm in two dimensions firstly variants of kleinberg s techniques do not take into account the semantics of the query string nor of the nodes being searched as a result polysemy of query words cannot be resolved this paper presents an interactive query scheme utilizing the simple web ontology provided by the open directory project to resolve meanings of a user query secondly we extend a recently proposed personalized version of the kleinberg algorithm 3 simulation results are presented to illustrate the sensitivity of our technique we outline the implementation of our algorithm in the persona personalized web search system parameterized logic programs where computing meets learning abstract in this paper we describe recent attempts to incorporate learning into logic programs as a step toward adaptive software that can learn from an environment although there are a variety of types of learning we focus on parameter learning of logic programs one for statistical learning by the em algorithm and the other for reinforcement learning by learning automatons both attempts are not full edged yet but in the former case thanks to the general framework and an e cient em learning algorithm combined with a tabulated search we have obtained very promising results that open up the prospect of modeling complex symbolic statistical phenomena 1 reasoning with concrete domains description logics are knowledge representation and reasoning formalisms which represent conceptual knowledge on an abstract logical level concrete domains are a theoretically well founded approach to the integration of description logic reasoning with reasoning about concrete objects such as numbers time intervals or spatial regions in this paper the complexity of combined reasoning with description logics and concrete domains is investigated we extend alc d which is the basic description logic for reasoning with concrete domains by the operators feature agreement and feature disagreement for the extended logic called alcf d an algorithm for deciding the abox consistency problem is devised the strategy employed by this algorithm is vital for the efficient implementation of reasoners for description logics incorporating concrete domains based on the algorithm it is proved that the standard reasoning problems for both logics alc d and alcf d are pspace co a two stage scheme for dynamic hand gesture recognition in this paper a scheme is presented for recognizing hand gestures using the output of a hand tracker which tracks a rectangular window bounding the hand region a hierarchical scheme for dynamic hand gesture recognition is proposed based on state representation of the dominant feature trajectories using an a priori knowledge of the way in which each gesture is performed boosting and rocchio applied to text filtering we discuss two learning algorithms for text filtering modified rocchio and a boosting algorithm called adaboost we show how both algorithms can be adapted to maximize any general utility matrix that associates cost or gain for each pair of machine prediction and correct label we first show that adaboost significantly outperforms another highly effective text filtering algorithm we then compare adaboost and rocchio over three large text filtering tasks overall both algorithms are comparable and are quite effective adaboost produces better classifiers than rocchio when the training collection contains a very large number of relevant documents however on these tasks rocchio runs much faster than adaboost 1 introduction with the explosion in the amount of information available electronically information filtering systems that automatically send articles of potential interest to a user are becoming increasingly important if users indicate their interests to a filtering system planetp infrastructure support for p2p information sharing storage technology trends are providing massive storage in extremely small packages while declining computing costs are resulting in a rising number of devices per person the confluence of these trends are presenting a new critical challenge to storage and file system designers how to enable users to effectively manage use and share huge amounts of data stored across a multitude of devices in this paper we present a novel middleware storage system planetp which is designed from first principles as a peerto peer p2p semantically indexed storage layer planetp makes two novel design choices to meet the above challenge first planetp concentrates on content based querying for information retrieval and assumes that the unit of storage is a shipper of xml allowing it to index arbitrary data for search and retrieval regardless of the applications used to create and manipulate the data second planetp adopts a p2p approach avoiding centralization of storage and indexing this makes planetp particularly suitable for information sharing among ad hoc groups of users each of which may have to manage data distributed across multiple devices planetp is targeted for groups of up to 1000 users results from studying communities of lo0 200 peers running on a cluster of pcs indicates that planetp should scale well to the 1000 member threshold finally we describe breezefs a semantic file system that we have implemented to validate planetp s utility methods and metrics for cold start recommendations ve have developed a method for recommending items that combines content and collaborative data under a single probabifistic framework we benchmark our algorithm against a nayve bayes classifier on the cold start problem where we wish to recommend items that no one in the commu nity has yet rated ve systematically explore three testing methodologies using a publicly available data set and explain how these methods apply to specific real world appli cations ve advocate heuristic recommeuders when bench marking to give competent baseline performance ve introduce a nev perfbrmance metric the croc curve and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems though the emphasis of onr testing is on cold start recommending our methods fbr recommending and evaluation are general incremental learning of control knowledge for nonlinear problem solving in this paper we advocate a learning method where a deductive and an inductive strategies are combined to efficiently learn control knowledge the approach consists of initially bounding the explanation to a predetermined set of problem solving features since there is no proof that the set is sufficient to capture the correct and complete explanation for the decisions the control rules acquired are then refined if and when applied incorrectly to new examples the method is especially significant as it applies directly to nonlinear problem solving where the search space is complete we present hamlet a system where we implemented this learning method within the context of the prodigy architecture hamlet learns control rules for individual decisions corresponding to new learning opportunities offered by the nonlinear problem solver that go beyond the linear one these opportunities involve among other issues completeness quality of plans and opportunistic decision making finally we show empirical results illustrating hamlet s learning performance hyssop natural language generation meets knowledge discovery in databases in this paper we present hyssop a system that generates natural language hypertext summaries of insights resulting from a knowledge discovery process we discuss the synergy between the two technologies underlying hyssop natural language generation nlg and knowledge discovery in databases kdd we first highlight the advantages of natural language hypertext as a summarization medium for kdd results showing the gains that it provides over charts and tables in terms of conciseness expressive versatility and ease of interpretation for decision makers second we highlight how kdd technologies and in particular olap and data mining can implement key tasks of automated natural language data summary generation in a more domain independent and scalable way than the human written heuristic rule approach of previous systems empirical risk approximation an induction principle for unsupervised learning unsupervised learning algorithms are designed to extract structure from data without reference to explicit teacher information the quality of the learned structure is determined by a cost function which guides the learning process this paper proposes empirical risk approximation as a new induction principle for unsupervised learning the complexity of the unsupervised learning models are automatically controlled by the two conditions for learning i the empirical risk of learning should uniformly converge towards the expected risk ii the hypothesis class should retain a minimal variety for consistent inference the maximal entropy principle with deterministic annealing as an efficient search strategy arises from the empirical risk approximation principle as the optimal inference strategy for large learning problems parameter selection of learnable data structures is demonstrated for the case of k means clustering 1 what is unsupervised learning learning algorithms are desi spiderserver the metasearch engine of webnaut search engines on the web are valuable tools for searching information according to a user s interests whether an individual or a software agent in the present article we describe the design and the operation mode of spiderserver a metasearch engine used for the submission of a query followed by the retrieving of results from five popular search engines spiderserver is the metasearch engine of the webnaut system but it can be easily used by any other metasearch platform there are two files for every search engine describing the phases of query formation and filtering respectively these files contain directions on the way a query must be modified for a specific search engine and on the methodology spiderserver must follow in order to parse the results from the specific search engine the ultimate goal is to construct platform independent meta search engines which can be easily programmed to adapt to any search engine available on the web user modelling as an aid for human web this paper explores how user modelling can work as an aid for human assistants in a user support system for web sites information about the user can facilitate for the assistant the tailoring of the consultation to the individual needs of the user such information can be represented and structured in a user model made available for the assistant a user modelling approach has been implemented and deployed in a real web environment as part of a user support system following the deployment we have analysed consultation dialogue logs and answers to a questionnaire for participating assistants the initial results show that assistants consider user modelling to be helpful and that consultation dialogues can be an important source for user model data collection node similarity in networked information spaces netvorked information spaces contain information entities corresponding to nodes vhich are orlrle l ed y associm i ms or r esporldirlg 1 o links irl he nel wor k exarrq les of nel wor ked information spaces are the world wide web vhere information entities are veb pages and associations are hyperlinks the scientific literature vhere information entities are articles and associations are references to other articles similariw betveen information entities in a net vorked information space can be defined not only based on the content of the information entities but also based on the connectivity established by the associations present this paper explores the definition of similariw based on connectivity only and proposes several algorithms r i his mr pose our rrlei r i s i ake mjvard age o i he local rleigh or hoo ts o i he rmcjes irl i he rlel is no required as long as a query engine is available for fo oving inks and extracting he necessary local neighbourhoods for similarity estimation tvo variations of similarity estimation beveen vo nodes are described one based on he separate local neighbourhoods of he nodes and another based on he join local neighbourhood expanded from boh nodes a he same ime the algorithms are imp emened and evaluated on he citation graph of computer science the immediate application of his vork is in finding papers similar o a given paper he web extended experimental explorations of the necessity of user guidance in case based learning this is an extended report focussing on experimental results to explore the necessity of user guidance in case based knowledge acquisition it is covering a collection of theoretical investigations as well the methodology of our approach is quite simple we choose a well understood area which is tailored to case based knowledge acquisition furthermore we choose a prototypical case based learning algorithm which is obviously suitable for the problem domain under consideration then we perform a number of knowledge acquisition experiments they clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases as a consequence we develop scenarios of user guidance based on these theoretical concepts we prove a few theoretical results characterizing the power of our approach next we perform a new series of more constrained results which support our theoretical investigations the main experiments deal with the difficulties of learning from randomly arrange discovering fuzzy classification rules with genetic programming and co evolution in essence data mining consists of extracting knowledge from data this paper proposes a co evolutionary system for discovering fuzzy classification rules the system uses two evolutionary algorithms a genetic programming gp algorithm evolving a population of fuzzy rule sets and a simple evolutionary algorithm evolving a population of membership function definitions the two populations co evolve so that the final result of the coevolutionary process is a fuzzy rule set and a set of membership function definitions which are well adapted to each other in addition our system also has some innovative ideas with respect to the encoding of gp individuals representing rule sets the basic idea is that our individual encoding scheme incorporates several syntactical restrictions that facilitate the handling of rule sets in disjunctive normal form we have also adapted gp operators to better work with the proposed individual encoding scheme a case study on case based and symbolic learning extended abstract stefan wess and christoph globig university of kaiserslautern p o box 3049 d 67653 kaiserslautern germany fwess globigg informatik uni kl de abstract contrary to symbolic learning approaches which represent a learned concept explicitly case based approaches describe concepts implicitly by a pair cb sim i e by a measure of similarity sim and a set cb of cases this poses the question if there are any differences concerning the learning power of the two approaches in this article we will study the relationship between the case base the measure of similarity and the target concept of the learning process to do so we transform a simple symbolic learning algorithm the version space algorithm into an equivalent case based variant the achieved results strengthen the hypothesis of the equivalence of the learning power of symbolic and case based methods and show the interdependency between the measure used by a case based algorithm and the target concept introduction i phidgets easy development of physical interfaces through physical widgets physical widgets or phidgets are to physical user interfaces what widgets are to graphical user interfaces similar to widgets phidgets abstract and package input and output devices they hide implementation and construction details they expose functionality through a well defined api and they have an optional on screen interactive interface for displaying and controlling device state unlike widgets phidgets also require a connection manager to track how devices appear on line a way to link a software phidget with its physical counterpart and a simulation mode to allow the programmer to develop debug and test a physical interface even when no physical device is present our evaluation shows that everyday programmers using phidgets can rapidly develop physical interfaces personalized information management for web intelligence web intelligence can be defined as the process of scanning and tracking information on the world wide web so as to gain competitive advantages this paper describes a system known as flexible organizer for competitive intelligence foci that transforms raw urls returned by internet search engines into personalized information portfolios foci builds information portfolios by gathering and organizing on line information according to a user s needs and preferences through a novel method called user configurable clustering a user can personalize his her portfolios in terms of the content as well as the information structure the personalized portfolios can then be used to track new information and organize them into appropriate folders accordingly we show a sample session of foci which illustrates how a user may create and personalize an information portfolio according to his her preferences and how the system discovers novel information groupings while organizing familiar information according to the userdefined themes autodoc a search and navigation tool for web based program documentation we present a search and navigation tool for use with automatically generated program documentation which builds trails in the information space three user interfaces are suggested which show the web pages in context and hence better explain the structure of the code searching people on the web according to their interests due to lack of structural data and information explosion on the world wide web www searching for useful information is becoming increasingly a difficult task traditional search engines on the web render some form of assistance but perform sub optimally when dealing with context sensitive queries to overcome this niche search engines serving specific web communities evolved these engines index only pages of high quality and relevance to a specific domain and make use of context information for searching this poster presents bullsi search publicly available at http dm2 comp nus edu sg a fielded domain specific search engine that helps web users locate computer science faculty members homepages and email addresses by specifying a research interest teaching interest or name the system is able to automatically extract and index research interests teaching interests owners email addresses and names from a set of discovered homepages experimental evaluation shows that the proposed algorithms are very accurate and are independent of structures in different homepages towards a network file system for roaming users pervasive computing aims at offering access to user s data anytime anywhere in a transparent manner however realizing such a vision necessitates several improvements in the way servers and user s terminals interact most notably client terminals should not tightly rely on an information server which can be temporarily unavailable in a mobile situation they should rather exploit all information servers available in a given context through loose coupling with both centralized servers and groups of terminals in a serverless manner in this position paper we present the design rationale of a network file system that implements transparent adaptive file access according to the users specific situations e g device in use network connectivity etc a secure infrastructure for service discovery access in pervasive computing security is paramount to the success of pervasive computing environments the system presented in this paper provides a communications and security infrastructure that goes far in advancing the goal of anywhere anytime computing our work securely enables clients to access and utilize services in heterogeneous networks we provide a service registration and discovery mechanism implemented through a hierarchy of service management the system is built upon a simplified public key infrastructure that provides for authentication non repudiation anti playback and access control smartcards are used as secure containers for digital certificates the system is implemented in java and we use extensible markup language as the sole medium for communications and data exchange currently we are solely dependent on a base set of access rights for our distributed trust model however we are expanding the model to include the delegation of rights based upon a predefined policy in our proposed expansion instead of exclusively relying on predefined access rights we have developed a flexible representation of trust information in prolog that can model permissions obligations entitlements and prohibitions in this paper we present the implementation of our system and describe the modifications to the design that are required to further enhance distributed trust our implementation is applicable to any distributed service infrastructure whether the infrastructure is wired mobile or ad hoc optimal camera parameter selection for state estimation with applications in object recognition in this paper we introduce a formalism for optimal camera parameter selection for iterative state estimation we consider a framework based on shannon s information theory and select the camera parameters that maximize the mutual information i e the information that the captured image conveys about the true state of the system the technique explicitly takes into account the a priori probability governing the computation of the mutual information thus a sequential decision process can be formed by treating the a posteriori probability at the current time step in the decision process as the a priori probability for the next time step the convergence of the decision process can be proven uml for behavior oriented multi agent simulations developing multi agent simulations seems to be rather straight forward as active entities in the original correspond to active agents in the model thus plausible behaviors can be produced rather easily however for real world applications they must satisfy some requirements concerning verification validation and reproducibility using a standard framework for designing a multi agent model one can gain further advantages like fast learnability wide understandability and possible transfer an autonomous spacecraft agent prototype this paper describes the new millennium remote agent nmra architecture for autonomous spacecraft control systems the architecture supports challenging requirements of the autonomous spacecraft domain not usually addressed in mobile robot architectures including highly reliable autonomous operations over extended time periods in the presence of tight resource constraints hard deadlines limited observability and concurrent activity a hybrid architecture nmra integrates traditional real time monitoring and control with heterogeneous components for constraint based planning and scheduling robust multi threaded execution and model based diagnosis and reconfiguration novel features of this integrated architecture include support for robust closed loop generation and execution of concurrent temporal plans and a hybrid procedural deductive executive we implemented a prototype autonomous spacecraft agent within the architecture and successfully demonstrated the prototype in the c computational web intelligence cwi synergy of computational intelligence and web technology with explosive growth of e business on the internet and wireless networks users face more and more challenging networks based application problems in terms of intelligent eapplications to increase the qoi quality of intelligence of ebusiness we propose a new research area called computational web intelligence cwi based on both computational intelligence ci and web technology wt generally the intelligent e brainware using cwi techniques plays an important role in smart e business in this paper fundamental concepts basic methods major applications and future trends of cwi are described to briefly show a general framework of cwi from different aspects ittalks a case study in the semantic web and daml effective use of the vast quantity of information now available on the web will require the use of semantic web markup languages such as the darpa agent markup language daml such languages will enable the automated gathering and processing of much information that is currently available but insufficiently utilized effectively such languages will facilitate the integration of multi agent systems with the existing information infrastructure as part of our exploration of semantic web technology and daml in particular we have constructed ittalks a web based system for automatic and intelligent notification of information technology talks in this paper we describe the ittalks system and discuss the numerous ways in which the use of semantic web concepts and daml extend its ability to provide an intelligent online service to both the human community and the agents assisting them 1 optimizing search engines using clickthrough data this paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data intuitively a good information retrieval system should present relevant documents high in the ranking with less relevant documents following below while previous approaches to learning retrieval functions from examples exist they typically require training data generated from relevance judgments by experts this makes them difficult and expensive to apply the goal of this paper is to develop a method that utilizes clickthrough data for training namely the query log of the search engine in connection with the log of links the users clicked on in the presented ranking such clickthrough data is available in abundance and can be recorded at very low cost taking a support vector ma chine svm approach this paper presents a method for learning retrieval functions from a theoretical perspective this method is shown to be well founded in a risk minimiza tion framework furthermore it is shown to be feasible even for large sets of queries and features the theoretical results are verified in a controlled experiment it shows that the method can effectively adapt the retrieval function of a meta search engine to a particular group of users outperforming google in terms of retrieval quality after only a couple of hundred training examples learning behavioral parameterization using spatio temporal case based reasoning this paper presents an approach to learning an optimal behavioral parameterization in the framework of a case based reasoning methodology for autonomous navigation tasks it is based on our previous work on a behavior based robotic system that also employed spario temporal case based reasoning 3 in the selection of behavioral parameters but was not capable of learning new parameterizations the present method extends the case based reasoning module by making it capable of learning new and optimizing the existing cases where each case is a set of behavioral parameters the learning process can either be a separate training process or be part of the mission execution in either case the robot learns an optimal parameterization of its behavior for different environments it encounters the goal of this research is not only to automatically optimize the performance of the robot but also to avoid the manual configuration of behavioral parameters and the initial configuration of a case library both of which require the user to possess good knowledge of robot behavior and the performance of numerous experiments the presented method was integrated within a hybrid robot architecture and evaluated in extensive computer simulations showing a significant increase in the performance over a nonadaptive system and a performance comparable to a non learning cbr system that uses a hand coded case library generating code for agent uml sequence diagrams for several years a new category of description techniques exists agent uml 10 which is based on uml agent uml is an extension of uml to tackle dierences between agents and objects since this description technique is rather new it does not supply tools or algorithms for protocol synthesis protocol synthesis corresponds to generate code for a formal description of a protocol the derived program behaves like the formal description this work presents rst elements to help designers generating code for agent uml sequence diagrams the protocol synthesis is applied to the example of english auction protocol agent oriented software engineering ion the process of defining a simplified model of the system that emphasises some of the details or properties while suppressing others organisation 1 the process of identifying and managing interrelationships between various problem solving components next the characteristics of complex systems need to be enumerated 8 complexity frequently takes the form of a hierarchy that is a system that is composed of inter related sub systems each of which is in turn hierarchic in structure until the lowest level of elementary sub system is reached the precise nature of these organisational relationships varies between sub systems however some generic forms such as client server peer team etc can be identified these relationships are not static they often vary over time the choice of which components in the system are primitive is relatively arbitrary and is defined by the observer s aims and objectives hierarchic systems evolve more quickly than non hiera text based content search and retrieval in ad hoc p2p communities we consider the problem of content search and retrieval in peer to peer p2p communities p2p computing is a potentially powerful model for information sharing between ad hoc groups of users because of its low cost of entry and natural model for resource scaling with community size as p2p communities grow in size however locating information distributed across the large number of peers becomes problematic we present a distributed text based content search and retrieval algorithm to address this problem our algorithm is based on a state of the art text based document ranking algorithm the vector space model instantiated with the tfxldf ranking rule a naive application of tfxldf woum require each peer in a community to collect an inverted index of the entire community this is costly both in terms of bandwidth and storage instea we show how tfxldf can be approximated given compact summaries of peers local inverted indexes we make three contributions a we show how the tfxldf rule can be adapted to use the index summaries b we provide a heuristic for adaptively determining the set of peers that shoum be contacted for a query and c we show that our algorithm tracks tfxldf s performance very closely regardless of how documents are distributed throughout the community furthermore our algorithm preserves the main flavor of tfxldf by retrieving close to the same set of documents for any given query cross entropy guided ant like agents finding cyclic paths in scarcely meshed networks telecommunication network owners and operators have for half a century been well aware of the potential loss of revenue if a major trunk is damaged thus dependability at high cost has been implemented a simple effective and common dependability scheme is 1 1 protection with 100 capacity redundancy in the network a growing number of applications in need of dependable connections with specific requirements to bandwidth and delay have started using the internet which only provides best effort transport as their base communication service in this paper we adopt the 1 1 protection scheme and incorporate it as part of a routing system applicable for internet infrastructures 100 capacity redundancy is no longer required a distributed stochastic path finding routing algorithm based on rubinstein s cross entropy method for combinatorial optimisation is presented early results from monte carlo simulations indeed indicate that the algorithm is capable of finding pairs of independent primary and backup paths satisfying specific bandwidth a constraints data quality in e business applications in e business scenarios an evaluation of the quality of exchanged data is essential for developing service based applications and correctly performing cooperative activities data of low quality can spread all over the cooperative system but at the same time improvement can be based on comparing data correcting them and disseminating high quality data in this paper an xml based broker service for managing data quality in cooperative systems is presented which selects the best available data from different services such a broker also supports data quality improvements based on feedbacks to source services efficient web search on mobile devices with multi modal input and intelligent text summarization ease of browsing and searching for information on mobile devices has been an area of increasing interest in the world wide web research community 1 2 3 6 7 while some work has been done to enhance the usability of handwriting recognition to input queries through techniques such as automatic word suggestion 2 the use of speech as an input mechanism has not been extensively studied this paper presents a system which combines spoken query in addition to automatic title summarization to ease searching for information on a mobile device preliminary usability study with 10 subjects indicates that spoken queries is preferred over other input methods inverted files and dynamic signature files for optimisation of web directories web directories are taxonomies for the classification of web documents this kind of ir systems present a specific type of search where the document collection is restricted to one area of the category graph this paper introduces a specific data architecture for web directories which improves the performance of restricted searches that architecture is based on a hybrid data structure composed of an inverted file with multiple embedded signature files two variants based on the proposed model are presented hybrid architecture with total information and hybrid architecture with partial information the validity of this architecture has been analysed by means of developing both variants to be compared with a basic model the performance of the restricted queries was clearly improved specially the hybrid model with partial information which yielded a positive response under any load of the search system managing data quality in cooperative information systems extended abstract massimo mecella 1 monica scannapieco 1 2 antonino virgillito 1 roberto baldoni i tiziana catarci 1 and carlo batini 3 i universirk di roma la sapienza dipartimento di informatica e sistemistica mecella monscan virgi baldoni catarci dis uniromal it 2 consiglio nazionale delle ricerche istituto di analisi dei sistemi ed informatica iasi cnr 3 universirk di milano bicocca dipartimento di informatica sistemistica e comunicazione batinidisco unimib it abstract current approaches to the development of cooperative information systems are based on services to be offered by cooperating organizations and on the opportunity of building coordinators and brokers on top of such services the quality of data exchanged and provided by different services hampers such approaches as data of low quality can spread all over the cooperative system at the same time improvement can be based on comparing data correcting them and disseminating high quality data in this paper a service based framework for managing data quality in cooperative information systems is presented an xml based model for data and quality data is proposed and the design of a broker for data which selects the best available data from different services is presented such a broker also supports the improvement of data based on feedbacks to source services an autonomous page ranking method for metasearch engines this paper the topics are derived from the user s query the reputation of each result page on the query topic is computed and the value used to rank the result pages across all participating search engines without biasing the ranking towards any of the sources s cream semi automatic creation of metadata abstract richly interlinked machine understandable data constitute the basis for the semantic web we provide a framework s cream that allows for creation of metadata and is trainable for a specific domain annotating web documents is one of the major techniques for creating metadata on the web the implementation of s cream ontomat supports now the semi automatic annotation of web pages this semi automatic annotation is based on the information extraction component amilcare ontomat extract with the help of amilcare knowledge structure from web pages through the use of knowledge extraction rules these rules are the result of a learningcycle based on already annotated pages 1 agent oriented specification for patient scheduling systems in hospitals introduction patient scheduling in hospitals is a complex task which requires new computational methods e g market mechanisms and enhanced support by software agents these demands are addressed by the medpage project medical path agents which covers the development of a multiagent system for which an agent oriented specification will be presented firstly based on field studies in five german hospitals the hospital domain is analysed c f 9 in this domain analysis a generic hospital structure is derived and the relevant co ordination objects for patient scheduling are identified secondly hospital specific scheduling problems are discussed on the foundation of this domain analysis the architecture of the medpage multi agent system is developed taking actual agent oriented methodologies into account the agents consisting of an individual schedule and utility function are modeled and the co ordination mechanism determining the agent interactions is described f a framework for knowledge management on the semantic web the semantic web can be a very promising platform for developing knowledge management systems however the problem is how to represent knowledge in the machine understandable form so that relevant knowledge can be found by machine agents in this paper we present a knowledge management approach based on rdf compatible format for representing rules and on a novel technique for the annotation of knowledge sources by using conditional statements the approach is based on our existing semantic web tools the main benefit is high improvement in the precision by searching for knowledge as well as the possibility to retrieve a composition of knowledge sources which are relevant for the problem solving selection of behavioral parameters integration of discontinuous switching via case based reasoning with continuous adaptation via learning momentum this paper studies the effects of the integration of two learning algorithms case based reasoning cbr and learning momentum lm for the selection of behavioral parameters in real time for robotic navigational tasks use of cbr methodology in the selection of behavioral parameters has already shown significant improvement in robot performance 3 6 7 14 as measured by mission completion time and success rate it has also made unnecessary the manual configuration of behavioral parameters from a user however the choice of the library of cbr cases does affect the robot s performance and choosing the right library sometimes is a difficult task especially when working with a real robot in contrast learning momentum does not depend on any prior information such as cases and searches for the quot right quot parameters in real time this results in high mission success rates and requires no manual configuration of parameters but it shows no improvement in mission completion time 2 this work combines the two approaches so that cbr discontinuously switches behavioral parameters based on given cases whereas lm uses these parameters as a starting point for the real time search for the quot right quot parameters the integrated system was extensively evaluated on both simulated and physical robots the tests showed that on simulated robots the integrated system performed as well as the cbr only system and outperformed the lm only system whereas on real robots it significantly outperformed both cbr only and lm only systems versus a model for a web repository web data warehouses can prove useful to applications that process large amounts of web data versus is a model for a repository for web data management applications supporting object versioning and distributed operation versus applications control the distribution and the integration of data this paper presents the design of versus and our prototype implementation keywords web data repository versioning distributed database qualitative velocity and ball interception abstract in many approaches for qualitative spatial reasoning navigation of an agent in a more or less static environment is considered e g in the double cross calculus 12 however in general real environment are dynamic which means that both the agent itself and also other objects and agents in the environment may move thus in order to perform spatial reasoning not only qualitative distance and orientation information is needed as e g in 1 but also information about relative velocity of objects see e g 2 therefore we will introduce concepts for qualitative and relative velocity quick to left neutral quick to right we investigate the usefulness of this approach in a case study namely ball interception of simulated soccer agents in the robocup 10 we compare a numerical approach where the interception point is computed exactly a strategy based on reinforcement learning a method with qualitative velocities developed in this paper and the na ve method where the agent simply goes directly to the actual ball position key words cognitive robotics multiagent systems spatial reasoning 1 towards robust teams with many agents agents in deployed multi agent systems monitor other agents to coordinate and collaborate robustly however as the number of agents monitored is scaled up two key challenges arise i the number of monitoring hypotheses to be considered can grow exponentially in the number of agents and ii agents become physically and logically unconnected unobservable to their peers this paper examines these challenges in teams of cooperating agents focusing on a monitoring task that is of particular importance to robust teamwork detecting disagreements among team members we present yoyo a highly scalable disagreement detection algorithm which guarantees sound detection in time linear in the number of agents despite the exponential number of hypotheses in addition we present new upper bounds about the number of agents that must be monitored in a team to guarantee disagreement detection both yoyo and the new bounds are explored analytically and empirically in thousands of monitoring problems scaled to thousands of agents integrating peer to peer networking and computing in the agentscape framework the combination of peer to peer networking and agentbased computing seems to be a perfect match agents are cooperative and communication oriented while peerto peer networks typically support distributed systems in which all nodes have equal roles and responsibilities agentscape is a framework designed to support large scale multi agent systems pole extends this framework with peerto peer computing this combination facilitates the development and deployment of new agent based peer to peer applications and services prometheus a methodology for developing intelligent agents abstract as agents gain acceptance as a technology there is a growing need for practical methods for developing agent applications this paper presents the prometheus methodology which has been developed over several years in collaboration with agent oriented software the methodology has been taught at industry workshops and university courses it has proven effective in assisting developers to design document and build agent systems prometheus differs from existing methodologies in that it is a detailed and complete start to end methodology for developing intelligent agents which has evolved out of industrial and pedagogical experience this paper describes the process and the products of the methodology illustrated by a running example 1 a framework for designing modeling and analyzing agent based software systems the agent paradigm is gaining popularity because it brings intelligence reasoning and autonomy to software systems agents are being used in an increasingly wide variety of applications from simple email filter programs to complex mission control and safety systems however there appears to be very little work in defining practical software architecture modeling and analysis tools that can be used by software engineers this should be contrasted with object oriented paradigm that is supported by models such as uml and case tools that aid during the analysis design and implementation phases of object oriented software systems in our research we are developing a framework and extensions to uml to address this need our approach is rooted in the bdi formalism but stresses the practical software design methods instead of reasoning about agents in this paper we describe our preliminary ideas index terms agent oriented programming objectoriented programming bdi uml 1 experiences developing a thin client multi device travel planning application many applications now require access from diverse humancomputer interaction devices such as desktop computers web browsers pdas mobile phones pagers and so on we describe our experiences developing a multi device travel planning application built from reusable components many of these developed from several different previous projects we focus on key user interface design and component adaptation and integration issues as encountered in this problem domain we report on the results of a useability evaluation of our prototype and our current research directions addressing hci and interface development problems we encountered an architecture for building multi device thin client web user interfaces we describe a new approach to providing adaptable thin client interfaces for web based information systems developers specify web based interfaces using a high level mark up language based on the logical structure of the user interface at run time this single interface description is used to automatically provide an interface for multiple web devices e g desk top html and mobile wml based systems as well as highlight hide or disable interface elements depending on the current user and user task our approach allows developers to much more easily construct and maintain adaptable webbased user interfaces than other current approaches layered learning this paper presents layered learning a hierarchical machine learning paradigm layered learning applies to tasks for which learning a direct mapping from inputs to outputs is intractable with existing learning algorithms given a hierarchical task decomposition into subtasks layered learning seamlessly integrates separate learning at each subtask layer the learning of each subtask directly facilitates the learning of the next higher subtask layer by determining at least one of three of its components i the set of training examples ii the input representation and or iii the output representation we introduce layered learning in its domainindependent general form we then present a full implementation in a complex domain namely simulated robotic soccer 1 introduction machine learning ml algorithms select a hypothesis from a hypothesis space based on a set of training examples such that the chosen hypothesis is predicted to characterize unseen examples referee an open framework for practical testing of recommender systems using researchindex automated recommendation e g personalized product recommendation on an ecommerce web site is an increasingly valuable service associated with many databases typically online retail catalogs and web logs currently a major obstacle for evaluating recommendation algorithms is the lack of any standard public real world testbed appropriate for the task in an attempt to fill this gap we have created referee a framework for building recommender systems using researchindex a huge online digital library of computer science research papers so that anyone in the research community can develop deploy and evaluate recommender systems relatively easily and quickly research index is in many ways ideal for evaluating recommender systems especially so called hybrid recommenders that combine information filtering and collaborative filtering techniques the documents in the database are associated with a wealth of content information author title abstract full text and collaborative information user behaviors as well as linkage information via the citation structure our framework supports more realistic evaluation metrics that assess user buy in directly rather than resorting to offline metrics like prediction accuracy that may have little to do with end user utility the sheer scale of researchindex over 500 000 documents with thousands of user accesses per hour will force algorithm designers to make real world trade offs that consider performance not just accuracy we present our own tradeoff decisions in building an example hybrid recommender called pd live the algorithm uses content based similarity information to select a set of documents from which to recommend and collaborative information to rank the documents pd live performs reasonably well compared to other recommenders in researchindex an approach to relate the web communities through bipartite graphs the web harbors a large number of community structures early detection of community structures has many purposes such as reliable searching and selective advertising in this paper we investigate the problem of extracting and relating the web community structures from a large collection of web pages by performing hyper link analysis the proposed algorithm extracts the potential community signatures by extracting the corresponding dense bipartite graph dbg structures from the given data set of web pages further the proposed algorithm can also be used to relate the extracted community signatures we report the experimental results conducted on 10 gb trec text retrieval conference data collection that contains 1 7 million pages and 21 5 million links the results demonstrate that the proposed approach extracts meaningful community signatures and relates them creatures entertainment software agents with artificial life we present a technical description of creatures a commercial home entertainment software package creatures provides a simulatedenvironment in which exist a number of synthetic agents that a user can interact with in real time the agents known as creatures are intended as sophisticated virtual pets the internal architecture of the creatures is strongly inspired by animal biology each creature has a neural network responsible for sensory motorcoordinationand behavior selection and an artificial biochemistry that models a simple energy metabolism along with a hormonal system that interacts with the neural network to model diffuse modulation of neuronal activity and staged ontogenetic development a biologically inspired learning mechanism allows the neural network to adapt during the lifetime of a creature learning includes the ability to acquire a simple verb object language automatic translation of paradigm models into pltl based software systems have evolved from monolythic programs to systems constructed from parallel cooperative components as can be currently found in objectoriented applications although powerfull these cooperative systems are also more di cult to verify a comparison of decision making criteria and optimization methods for active robotic sensing this work presents a comparison of decision making criteria and optimization methods for active sensing in robotics active sensing incorporates the following aspects i where to position sensors and ii how to make decisions for next actions in order to maximize information gain and minimize costs we concentrate on the second aspect where should the robot move at the next time step pros and cons of the most often used statistical decision making strategies are discussed reconciling co operative planning and automated co ordination in multiagent systems in the context of cooperative work the coordination of activities is provided essentially by two opposite classes of approaches based on the notion of situated action and planning actually given the complexity and dynamism of current cooperative scenarios the need of model supporting both the approaches has emerged a similar situation can be recognised in multiagent system coordination where two classes of approaches are clearly distinguishable with properties similar to the situated action and planning cases the one defined in literature as subjective coordination accounts for realising coordination exclusively by means of the skills and the situated actions of the individual agents exploiting their intelligence and communication capability among the other properties these approaches promote autonomy flexibility and intelligence in coordination processes the other defined in literature as objective coordination accounts for realising coordination exploiting coordination media which mediate agent interactions and govern them according to laws which reflect social goals and norms among the other properties these approaches promote automation efficiency and prescriptiveness in coordination processes in this work the importance to support both approaches in the same coordination context is remarked and a conceptual framework derived from activity theory is proposed where both approaches effectively co exist and work at different collaborative levels exploiting both the flexibility and intelligence of the subjective approaches and the prescription and automation of the objective ones a hybrid mobile robot architecture with integrated planning and control research in the planning and control of mobile robots has received much attention in the past two decades two basic approaches have emerged from these research efforts deliberative vs reactive these two approaches can be distinguished by their different usage of sensed data and global knowledge speed of response reasoning capability and complexity of computation their strengths are complementary and their weaknesses can be mitigated by combining the two approaches in a hybrid architecture this paper describes a method for goal directed collision free navigation in unpredictable environments that employs a behavior based hybrid architecture with asynchronously operating behavioral modules it differs from existing hybrid architectures in two important ways 1 the planning module produces a sequence of checkpoints instead of a conventional complete path and 2 in addition to obstacle avoidance the reactive module also performs target reaching under the control of a self organizing neural network the neural network is trained to perform fine smooth motor control that moves the robot through the checkpoints these two aspects facilitate a tight integration between high level planning and low level control which permits real time performance and easy path modification even when the robot is en route to the goal position self organization in multiagent systems from agent interaction to agent organization in this paper we suggest a new sociological concept to the study of self organization in multiagent systems first we discuss concepts of self organization typically used in dai from a sociological point of view all these concepts are missing the special quality of organizations as self organizing social entities therefore we present a concept of organization based on the habitus field theory of pierre bourdieu efficient learning of reactive robot behaviors with a neural q learning approach the purpose of this paper is to propose a neural q learning approach designed for online learning of simple and reactive robot behaviors in this approach the q function is generalized by a multi layer neural network allowing the use of continuous states and actions the algorithm uses a database of the most recent learning samples to accelerate and guarantee the convergence each neural q learning function represents an independent reactive and adaptive behavior which maps sensorial states to robot control actions a group of these behaviors constitutes a reactive control scheme designed to fulfill simple missions the paper centers on the description of the neural q learning based behaviors showing their performance with an autonomous underwater vehicle auv in a target following mission simulated experiments demonstrate the convergence and stability of the learning system pointing out its suitability for online robot learning advantages and limitations are discussed ephemeral and persistent personalization in adaptive information access to scholarly publications on the web we show how personalization techniques can be exploited to implement more adaptive and effective information access systems in electronic publishing we distinguish persistent or long term and ephemeral or short term personalization and we describe how both of them can be profitably applied in information filtering and retrieval systems used via a specialized web portal by physicists in their daily job by means of several experimental results we demonstrate that persistent personalization is needed and useful for information filtering systems and ephemeral personalization leads to more effective and usable information retrieval systems learning reactive robot behaviors with neural q leaning the purpose of this paper is to propose a neural q learning approach designed for online learning of simple and reactive robot behaviors in this approach the q function is generalized by a multi layer neural network allowing the use of continuous states and actions the algorithm uses a database of the most recent learning samples to accelerate and guarantee the convergence each neural q learning function represents an independent reactive and adaptive behavior which maps sensorial states to robot control actions a group of these behaviors constitutes a reactive control scheme designed to fulfill simple missions the paper centers on the description of the neural q learning based behaviors showing their performance with an autonomous underwater vehicle auv in a target following mission simulated experiments demonstrate the convergence and stability of the learning system pointing out its suitability for online robot learning advantages and limitations are discussed ensemble learning for intrusion detection in computer networks the security of computer networks plays a strategic role in modern computer systems in order to enforce high protection levels against threats a number of software tools are currently developed intrusion detection systems aim at detecting intruder who eluded the first line protection in this paper a pattern recognition approach to network intrusion detection based on ensemble learning paradigms is proposed the potentialities of such an approach for data fusion and some open issues are outlined rf ipf a weighting scheme for multimedia information retrieval region based approach has become a popular research trend in the field of multimedia database retrieval in this paper we present the region frequency and inverse picture frequency rf ipf weighting a measure developed to unify region based multimedia retrieval systems with textbased information retrieval systems the weighting measure gives the highest weight to regions that occur often in a small number of images in the database these regions are considered discriminators with this weighting measure we can blend image retrieval techniques with tf idfbased text retrieval techniques for large scale web applications the rf ipf weighting has been implemented as a part of our experimental simplicity image retrieval system and tested on a database of about 200 000 general purpose images experiments have shown that this technique is effective in discriminating images of different semantics additionally the overall similarity approach enables a simple querying interface for multimedia information retrieval systems 1 multi sensor context aware clothing inspired by perception in biological systems distribution of a massive amount of simple sensing devices is gaining more support in detection applications a focus on fusion of sensor signals instead of strong analysis algorithms and a scheme to distribute sensors results in new issues especially in wearable computing where sensor data continuously changes and clothing provides an ideal supporting structure for simple sensors this approach may prove to be favourable experiments with a body distributed sensor system investigate the influence of two factors that affect classification of what has been sensed an increase in sensors enhances recognition while adding new classes or contexts depreciates the results finally a wearable computing related scenario is discussed that exploits the presence of many sensors evaluation of recommender algorithms for an internet information broker based association rules are a widely used technique to generate recommendations in commercial and research recommender systems since more and more web sites especially of retailers offer automatic recommender services using web usage mining evaluation of recommender algorithms becomes increasingly important in this paper we first present a framework for the evaluation of different aspects of recommender systems based on the process of discovering knowledge in databases of fayyad et al and then we focus on the comparison of the performance of two recommender algorithms based on frequent itemsets the first recommender algorithm uses association rules and the other recommender algorithm is based on the repeat buying theory known from marketing research for the evaluation we concentrated on how well the patterns extracted from usage data match the concept of useful recommendations of users we use 6 month of usage data from an educational internet information broker and compare useful recommendations identified by users from the target group of the broker with the results of the recommender algorithms the results of the evaluation presented in this paper suggest that frequent itemsets from purchase histories match the concept of useful recommendations expressed by users with satisfactory accuracy higher than 70 and precision between 60 and 90 also the evaluation suggests that both algorithms studied in the paper perform similar on real world data if they are tuned properly text based content search and retrieval in ad hoc p2p communities we consider the problem of content search and retrieval in peer to peer p2p communities p2p computing is a potentially powerful model for information sharing between ad hoc groups of users because of its low cost of entry and natural model for resource scaling with community size as p2p communities grow in size however locating information distributed across the large number of peers becomes problematic we present a distributed text based content search and retrieval algorithm to address this problem our algorithm is based on a state of the art text based document ranking algorithm the vector space model instantiated with the tfxldf ranking rule a naive application of tfxldf woum require each peer in a community to collect an inverted index of the entire community this is costly both in terms of bandwidth and storage instea we show how tfxldf can be approximated given compact summaries of peers local inverted indexes we make three contributions a we show how the tfxldf rule can be adapted to use the index summaries b we provide a heuristic for adaptively determining the set of peers that shoum be contacted for a query and c we show that our algorithm tracks tfxldf s performance very closely regardless of how documents are distributed throughout the community furthermore our algorithm preserves the main flavor of tfxldf by retrieving close to the same set of documents for any given query active proxy g optimizing the query execution process in the grid the grid environment facilitates collaborative work and allows many users to query and process data over geographically dispersed data repositories over the past several years there has been a growing interest in developing applications that interactively analyze datasets potentially in a collaborative setting we describe an active proxy g service that is able to cache query results use those results for answering new incoming queries generate subqueries for the parts of a query that cannot be produced from the cache and submit the subqueries for final processing at application servers that store the raw datasets we present an experimental evaluation to illustrate the effects of various design tradeojj5 we also show the benefits that two real applications gain from using the middleware tinmith metro new outdoor techniques for creating city models with an augmented reality wearable computer this paper presents new techniques for capturing and viewing on site 3d graphical models for large outdoor objects using an augmented reality wearable computer we have developed a software system known as tinmithmetro tinmith metro allows users to control a 3d constructive solid geometry modeller for building graphical objects of large physical artefacts for example buildings in the physical world the 3d modeller is driven by a new user interface known as tinmith hand which allows the user to control the modeller using a set of pinch gloves and hand tracking these techniques allow user to supply their ar renderers with models that would previously have to be captured with manual time consuming and or expensive methods boosting interval based literals variable length and early classification in previous works a system for supervised time series classification has been presented it is based on boosting very simple classifiers only one literal the used predicates are based on temporal intervals there are two types of predicates i relative predicates such as increases and stays and ii region predicates such as always and sometime which operate ver regions in the domain of the variable using extreme programming for knowledge transfer this paper presents the application of extreme programming to the software agents research and development group at trlabs regina the group had difficulties maintaining its identity due to a very rapid turnover and lack of strategic polarization the application of extreme programming resulted in a complete reorientation of the development culture which now forms a continuous substrate in every individual a multi agent reflective architecture for web search assistance nowadays the web is overloaded with documents and even finding out whether some of the items selected by search engines are worth reading is a tedious and time consuming job this paper proposes a general framework and specific assistant agents that are used to enrich browsers with the ability to characterise the user interests and to influence results of search engines according to such interests to enrich web browsers a reflective architecture was used that captures control from browser objects at run time and intertwines further activities when some conditions are verified information retrieval on the world wide web and active logic a survey and problem definition as more information becomes available on the world wide web there are currently over 4 billion pages covering most areas of human endeavor it becomes more difficult to provide effective search tools for information access today people access web information through two main kinds of search interfaces browsers clicking and following hyperlinks and query engines queries in the form of a set of keywords showing the topic of interest the first process is tentative and time consuming and the second may not satisfy the user because of many inaccurate and irrelevant results better support is needed for expressing one s information need and returning high quality search results by web search tools there appears to be a need for systems that do reasoning under uncertainty and are flexible enough to recover from the contradictions inconsistencies and irregularities that such reasoning involves a parameterized algebra for event notification services event notification services are used in various applications such as digital libraries stock tickers traffic control or facility management however to our knowledge a common semantics of events in event notification services has not been defined so far in this paper we propose a parameterized event algebra which describes the semantics of composite events for event notification systems the parameters serve as a basis for flexible handling of duplicates in both primitive and composite events 1 dynamic agent discovery one of the issues that has gained importance in the real world applications of agent systems is the bootstrapping of agents inferring web communities through relaxed cocitation and dense bipartite graphs community forming is one of the important activity in the web the web harbors a large number of communities a community is a group of content creators that manifests itself as a set of interlinked pages given a large collection of pages our aim is to find potential communities in the web in the literature ravi kumar et al 18 proposed a trawling method to find potential communities by abstracting a core of the community as a group of pages that form a complete bipartite graph cbg web page as a node and link as an edge between two nodes the trawling approach extracts a small group of pages that form a cbg which is a signature of a potential community a scalable integrated region based image retrieval system statistical clustering is critical in designing scalable image retrieval systems in this paper we presentascalable algorithm for indexing and retrieving images based on region segmentation the method uses statistical clustering on region features and irm integrated region matching a measure developed to evaluate overall similaritybetween images that incorporates properties of all the regions in the images by a region matching scheme compared with retrieval based on individual regions our overall similarity approach a reduces the inuence of inaccurate segmentation b helps to clarify the semantics of a particular region and c enables a simple querying interface for region based image retrieval systems the algorithm has been implemented as a part of our experimental simplicity image retrieval system and tested on large scale image databases of both general purpose images and pathology slides experiments have demonstrated that this technique maintains the accuracy and robustness of the original system while reducing the matching time significantly making context explicit in communicating objects introduction one can speak about context only with reference to something no definition of context out of context the context of an object the context of interaction the context of a problem solving etc however only the context of interaction between agents seems of interest because it is in this context that other contexts are referenced or evolve for example if an object as a telephone could provide you the context in which the called person is free in meeting phone on voice recorder you could balance your wish to establish your communication versus the availability of the called person several domains have already elaborated their own working definition of context in human machine interaction a context is a set of information that could be used to define and interpret a situation in which interact agents in the context aware applications community the context is composed of a set of information for characterizing the situation in which interact humans applic the diagnosis frontend of the dlv system this paper presents the diagnosis frontend of dlv which is a knowledge representation system under development at the technische universit t wien the kernel language of the system is an extension of disjunctive logic programming dlp by integrity constraints it offers frontends to several advanced knowledge representation formalisms the formal model of diagnosis employed in the frontend includes both abductive diagnosis over dlp theories and consistency based diagnosis for each of the two diagnosis modalities generic diagnoses single error diagnoses and subset minimal diagnoses are considered we illustrate the use of the frontend by showing the dlv encodings of several diagnosis problems thereafter we discuss implementation issues diagnostic reasoning is implemented on the dlv engine through suitable translations of diagnostic problems into disjunctive logic programs such that their stable models correspond to diagnoses for the six kinds of diagnostic reasoning problems emerging from above such reductions are provided identifying the subject of documents in digital libraries automatically using contemporary information databases contain millions of electronic documents the immense number of documents makes it difficult to conduct efficient searches on the internet several studies have found that associating documents with a subject or list of topics can make them easier to locate online 5 6 7 effective cataloging of information is performed manually requiring extensive resources consequently at present most information is not cataloged this paper will present the findings of a study based on a software tool textanalysis that automatically identifies the subject of a document we tested documents in two subject categories geography and family studies the present study follows an earlier one that examined the subject categories of industrial management and general management enhancing the sense of other learners in student centred web based education student centred learning can be used in web courses to increase student activity motivation and commitment educo is a system for student centred learning both for the learners and the teachers students can use educo within a standard web browser to navigate towards useful information and web resources gathered into the system the key issue is that every participant can see everyone else in the system and their navigational steps so that the feeling of student companions taking part in the same tasks is increased the implications of this type of social navigation are discussed along with the description of the system itself 1 the esleeve a novel wearable computer configuration for the discovery of situated information this paper describes work in progress on wearable computing configurations which provide audio and visual output based on the position and orientation of the user we introduce the esleeve a wearable wrist computer with position and heading sensors combined with a user interface employing speech recognition and a small display comparing statistical and content based techniques for answer validation on the web answer validation is an emerging topic in question answering where open domain systems are often required to rank huge amounts of candidate answers we present a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be estimated by exploiting the redundancy of web information two techniques are considered in this paper a statistical approach which uses the web to obtain a large amount of pages and a content based approach which analyses text snippets retrieved by the search engine both the approaches do not require to download the documents experiments carried out on the trec 2001 judged answer collection show that a combination of the two approaches achieves a high level of performance i e about 88 success rate mansion a structured middleware environment for agents developing processes intended to roam in largescale heterogeneous distributed systems is difficult their environment is unstructured and interoperability issues often emerge mansion is a new paradigm to provide a large scale structured environment for mobile agents autonomous migrating processes security transparency and interoperability are important design guidelines for the development of mansion designing data warehouses with oo conceptual models ions of our work we believe that our innovative approach provides a theoretical foundation for the use of oo databases and object relational databases in data warehouses mdb and olap applications we use uml to design data warehouses because it considers an information system s structural and dynamic properties at the conceptual level more naturally than do classic approaches such as the entityrelationship model further uml provides powerful mechanisms such as the object constraint language and the object query language for embedding data warehouse constraints and initial user requirements in the conceptual model this approach to modeling a data warehouse system yields simple yet powerful extended uml class diagrams that represent main data warehouse properties at the conceptual level multidimensional modeling properties multidimensional modeling structures information into facts and dimensions we define a fact as an item of interest for an enterpri least squares conformal maps for automatic texture atlas generation a texture atlas is an efficient color representation for 3d paint systems the model to be textured is decomposed into charts homeomorphic to discs each chart is parameterized and the unfolded charts are packed in texture space existing texture atlas methods for triangulated surfaces suffer from several limitations requiring them to generate a large number of small charts with simple borders the discontinuities between the charts cause artifacts and make it difficult to paint large areas with regular patterns query evaluation for mediators over web catalogs the web catalogs like yahoo and open directory are very useful for browsing and querying the web although they index only a fraction of the pages that are indexed by search engines these catalogs are hand crafted by domain experts and are therefore of high quality we present a model for building mediators over web catalogs so as to provide users with customized views of such catalogs we focus on query evaluation specifically on the complexity of query answering by the mediator two views of classifier systems this work suggests two ways of looking at michigan classifier systems as genetic algorithm based systems and as reinforcement learning based systems and argues that the former is more suitable for traditional strength based systems while the latter is more suitable for accuracy based xcs the dissociation of the genetic algorithm from policy determination in xcs is noted and the two types of michigan classifier system are contrasted with pittsburgh systems self organization in ad hoc sensor networks an empirical study research in classifying and recognizing complex concepts has been directing its focus increasingly on distributed sensing using a large amount of sensors the colossal amount of sensor data often obstructs traditional algorithms in centralized approaches where all sensor data is directed to one central location to be processed spreading the processing of sensor data over the network seems to be a promising option but distributed algorithms are harder to inspect and evaluate using self sufficient sensor boards with short range wireless communication capabilities we are exploring approaches to achieve an emerging distributed perception of the sensed environment in realtime through clustering experiments in both simulation and real world platforms indicate that this is a valid methodology being especially promising for computation on many units with limited resources web page classification using spatial information extracting and processing information from web pages is an important task in many areas like constructing search engines information retrieval and data mining from the web common approach in the extraction process is to represent a page as a bag of words and then to perform additional processing on such a flat representation in this paper we propose a new hierarchical representation that includes browser screen coordinates for every html object in a page such spatial information allows the definition of heuristics for recognition of common page areas such as header left and right menu footer and center of a page we show a preliminary experiment where our heuristics are able to correctly recognize objects in 73 of cases finally we show that a naive bayes classifier taking into account the proposed representation clearly outperforms the same classifier using only information about the content of documents monadic datalog and the expressive power of languages for web information extraction research on information extraction from web pages wrapping has seen much activity in recent times particularly systems implementations but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping in this paper we first study monadic datalog as a wrapping language over ranked or unranked tree structures using previous work by neven and schwentick we show that this simple language is equivalent to full monadic second order logic mso in its ability to specify wrappers we believe that mso has the right expressiveness required for web information extraction and thus propose mso as a yardstick for evaluating and comparing wrappers using the above result we study the kernel fragment elog of the elog wrapping language used in the lixto system a visual wrapper generator the striking fact here is that elog exactly captures mso yet is easier to use indeed programs in this language can be entirely visually specified we also formally compare elog to other wrapping languages proposed in the literature storhouse relational manager rm active storage hierarchy database system and applications this paper describes how database systems can use and exploit a cost effective active storage hierarchy by active storage hierarchy we mean a database system that uses all storage media i e optical tape and disk to store and retrieve data and not just disk we describe and emphasize the active part whereby all storage types are used to store raw data that is converted to strategic business information we describe an evolution to the data warehouse concept called atomic data store whereby atomic data is stored in the database system atomic data is defined as storing all the historic data values and executing queries against the historic queries we also describe a data warehouse information collection flow and central data store hub and spoke architecture used to feed data into data marts we also describe a commercial product storhouse relational manager rm rm is a commercial relational database system that executes sql queries directly against data stored on the storage hierarchy i e tape optical disk we conclude with a brief overview of a real world at t call detail warehouse cdw case study a scalable and ontology based p2p infrastructure for semantic web services semantic web services are a promising combination of semantic web and web service technology aiming at providing means of automatically executing discovering and composing semantically marked up web services we envision peer to peer networks which allow for carrying out searches in real time on permanently reconfiguring networks to be an ideal injastructure for deploying a network of semantic web service providers however p2p networks evolving in an unorganized manner suffer jom serious scalability problems limiting the number of nodes in the network creating network overload and pushing search times to unacceptable limits we address these problems by imposing a deterministic shape on p2p networks we propose a graph topology which allows for very efficient broadcast and search and we provide an efficient topology construction and maintenance algorithm which crucial to symmetric peer to peer networks does neither require a central server nor super nodes in the network we show how our scheme can be made even more efficient by using a globally known ontology to determine the organization of peers in the graph topology allowing for efficient concept based search self adaptive operator scheduling using the religion based ea the optimal choice of the variation operators mutation and crossover and their parameters can be decisive for the performance of evolutionary algorithms eas usually the type of the operators such as gaussian mutation remains the same during the entire run and the probabilistic frequency of their application is determined by a constant parameter such as a fixed mutation rate however recent studies have shown that the optimal usage of a variation operator changes during the ea run in this study we combined the idea of self adaptive mutation operator scheduling with the religion based ea rbea which is an agent model with spatially structured and variable sized subpopulations religions in our new model osrbea we used a selection of different operators such that each operator type was applied within one specific subpopulation only our results indicate that the optimal choice of operators is problem dependent varies during the run and can be handled by our self adaptive osrbea approach operator scheduling could clearly improve the performance of the already very powerful rbea and was superior compared to a classic and other advanced ea approaches equip a software platform for distributed interactive systems equip is a new software platform designed and engineered to support the development and deployment of distributed interactive systems such as mixed reality user interfaces that combine distributed input and output devices to create a coordinated experience equip emphasises cross language development currently c and java modularisation extensibility interactive performance and heterogeneity of devices from handheld devices to large servers and visualisation machines and networks including both wired and wireless technologies a key element of equip is its shared data service which combines ideas from tuplespaces general event systems and collaborative virtual environments this data service provides a uniquely balanced treatment of state and event based communication it also supports distributed computation through remote class loading as well as passive data distribution equip has already been used in several projects within the equator interdisciplinary research collaboration irc in the uk and is freely available in source form currently known to work on windows irix and macos x platforms learning reactive robot behaviors with a neural q learning approach the purpose of this paper is to propose a neural q learning approach designed for online learning of simple and reactive robot behaviors in this approach the q function is generalized by a multi layer neural network allowing the use of continuous states and actions the algorithm uses a database of the most recent learning samples to accelerate and guarantee the convergence each neural q learning function represents an independent reactive and adaptive behavior which maps sensorial states to robot control actions a group of these behaviors constitutes a reactive control scheme designed to fulfill simple missions the paper centers on the description of the neural q learning based behaviors showing their performance with an autonomous underwater vehicle auv in a target following mission simulated experiments demonstrate the convergence and stability of the learning system pointing out its suitability for online robot learning advantages and limitations are discussed creatures artificial life autonomous software agents for home entertainment this paper gives a technical description of creatures a commercial home entertainment software package creatures provides a simulated arquake an outdoor indoor augmented reality first person application this pap er presents an outdoor indoor augmented re ality first person applic ationar 2uake we have developal arquake is an extension of the desktop game quake and as such we are investigating how to convert a desktop first person application into an outdoor indoor mobile augmented reality application we present an archire cture for a low cost moderately accurate six degrees of freedom tracking system based on gp digital compass and fiducial vision based tracking usability issues such as monster selection colour and input devies are discussed a second application for ar architectural design visualisation is presented user behavior analysis of location aware search engine rapid growth of internet access from mobile users puts much importance on location specific information on the web an unique web service called mobile info search mis from ntt laboratories gathers the information and provide location aware search facilities we performed association rule mining and sequence pattern mining against the access log which was accumulated at the mis site in order to get some insight into the behavior of mobile users regarding the spatial information on the web detail web log mining process and the rules we derived are reported in this paper using declarative constraints to specify the data model of multi user application complex applications such as multi user applications may fruitfully be viewed as being composed of a number of independent agents which access and modify a shared data structure we will view this shared data so as to include the domain data being viewed and edited as well as the entire user interface state are you ready for yottabytes storhouse federated and object relational solution this paper describes how federated and object relational database systems can exploit cost effective active storage hierarchies by active storage hierarchy we mean a database system that uses all storage media i e optical tape and disk to store and retrieve data and not just disk a detailed discussion of the atomic data store data warehouse concept can be found in cb 99 these also describe a commercial relational database product storhouse relational manager rm that executes sql queries directly against data stored in a complete storage hierarchy this paper focuses on applications that can use and may even require the use of emerging federated and object relational database technologies our analysis is based on two products now in development we will refer to these as storhouse fed a federated database system that includes storhouse rm and storhouse orm an object relational database system we conclude by describing candidate applications with an emphasis on the federal sector that can exploit the combination of costeffective active storage hierarchy with federated and or object relational database technology c4 1 building a community hierarchy for the web based on bipartite graphs in this paper we propose an approach to extract and relate the communities by considering a community signature as a group of content creators that manifests itself as a set of interlinked pages we abstract a community signature as a group of pages that form a dense bipartite graph dbg and proposed an algorithm to extract the dbgs from the given data set also using the proposed approach the extracted communities can be grouped to form a high level communities we apply the proposed algorithm on 10 gb trec text retrieval conference data set and extract a three level community hierarchy the extracted community hierarchy facilitates an easy analysis of low level communities and provides a way to understand the sociology of the web facilitating message exchange though middle agents to utilize services provided by other agents a requesting agent needs to locate and communicate with these service providers specifically in order to interoperate with the providers the requesting agent should know 1 the service provider s interface 2 the ontology that defines concepts used by the provider and 3 the agent communication language acl the agent uses so that it can parse and understand the communication currently deployed multi agent systems mas encode the interface description and the ontology within a service provider s capability description or advertisement that is registered with a middle agent however this assumes a common acl between communicating agents we demonstrate how agents can communicate with each other using a template based shallow parsing approach to constructing and decomposing messages thus relaxing assumptions on the acls and message formats used practical guidelines for the readability of it architecture diagrams this paper presents the work done to establish guidelines for the creation of readable it architecture diagrams and gives some examples of guidelines and some examples of improved diagrams these guidelines are meant to assist practicing it architects in preparing the diagrams to communicate their architectures to the various stakeholders diagramming has always been important in information technology it but the recent interest in itarchitecture the widespread use of software and developments in electronic communication make it necessary to again look at the rt of making diagrams for this particular class and its users the guidelines indicate how various visual attributes like hierarchy layout color form graphics etc can contribute to the readability of it architecture diagrams the emphasis is on the outward appearance of diagrams some additional support is given for the thinking reasoning processes while designing or using a set of diagrams and an attempt is made to arrive at a rationale of these guidelines an evaluation process has been performed with three groups of practicing it architects the outcome of this evaluation is presented this work is part of a more comprehensive research project on visualisation of it architecture comparing top k lists motivated by several applications we introduce various distance measures between top k lists some of these distance measures are metrics while others are not for each of these latter distance measures we show that they are almost a metric in the following two seemingly unrelated aspects i they satisfy a relaxed version of the polygonal hence triangle inequality and ii there is a metric with positive constant multiples that bound our measure above and below this is not a coincidence we show that these two notions of almost being a metric are formally identical based on the second notion we define two distance measures to be equivalent if they are bounded above and below by constant multiples of each other we thereby identify a large and robust equivalence class of distance measures besides the applications to the task of identifying good notions of dis similarity between two top k lists our results imply polynomial time constant factor approximation algorithms for the rank aggregation problem dkns01 with respect to a large class of distance measures to appear in siam j on discrete mathematics extended abstract to appear in 2003 acm siam symposium on discrete algorithms soda 03 a machine learning approach to building domain specific search engines domain specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general web wide search engines unfortunately they are also difficult and timeconsuming to maintain this paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain specific search engines we describe new research in reinforcement learning text classification and information extraction that enables efficient spidering populates topic hierarchies and identifies informative text segments using these techniques we have built a demonstration system a search engine for computer science research papers available at www cora justresearch com 1 introduction as the amount of information on the world wide web grows it becomes increasingly difficult to find just what wewant while general purpose search engines suchas altavista and hotbot offer high coverage they often provi exploiting schema knowledge for the integration of heterogeneous sources information sharing from multiple heterogeneous sources is a challenging issue which ranges from database to ontology areas in this paper we propose an intelligent approach to information integration which takes into account semantic conflicts and contradictions caused by the lack of a common shared ontology our goal is to provide an integrated access to information sources allowing a user to pose a single query and to receive a single unified answer we propose a semantic approach to integration where the conceptual schema of each source is provided adopting a common standard data model and language and description logics plus clustering techniques are exploited description logics is used to obtain a semi automatic generation of a common thesaurus to solve semantic heterogeneities and to derive a common ontology clustering techniques are used to build the global schema i e the unified view of the data to be used for query processing keywords intelligent informat path constraints on deterministic graphs we study path constraints for deterministic graph model 9 a variation of semistructured data model in which data is represented as a rooted edge labeled directed graph with deterministic edge relations the path constraint languages considered include the class of word constraints introduced in 4 the language p c investigated in 8 and an extension of p c defined in terms of regular expressions complexity results on the implication and finite implication problems for these constraint languages are established 1 introduction semistructured data is characterized as having no type constraints irregular structure and rapidly evolving or missing schema 1 6 examples of such data can be found on the worldwide web in biological databases and after data integration in particular documents of xml extensible markup language 5 can also be viewed as semistructured data 10 the unifying idea in modeling semistructured data is the representation of data as an edge labeled r on using degrees of belief in bdi agents the past few years have seen a rise in the popularity of the use of mentalistic attitudes such as beliefs desires and intentions to describe intelligent agents many of the models which formalise such attitudes do not admit degrees of belief desire and intention we see this as an understandable simplification but as a simplification which means that the resulting systems cannot take account of much of the useful information which helps to guide human reasoning about the world this paper starts to develop a more sophisticated system based upon an existing formal model of these mental attributes 1 introduction in the past few years there has been a lot of attention given to building formal models of autonomous software agents pieces of software which operate to some extent independently of human intervention and which therefore may be considered to have their own goals and the ability to determine how to achieve those goals many of these formal models are based on the use of what sort of control system is able to have a personality this paper outlines a design based methodology for the study of mind as a part of the broad discipline of artificial intelligence within that framework some architectural requirements for human like minds are discussed and some preliminary suggestions made regarding mechanisms underlying motivation emotions and personality a brief description is given of the nursemaid or minder scenario being used at the university of birmingham as a framework for research on these problems it may be possible later to combine some of these ideas with work on synthetic agents inhabiting virtual reality environments 1 introduction personality belongs to a whole agent most work in ai addresses only cognitive aspects of the design of intelligent agents e g vision and other forms of perception planning problem solving the learning of concepts and generalisations natural language processing motor control etc only a tiny subset of ai research has been concerned with motivation and emotion document categorization and query generation on the world wide web using webace we present webace an agent for exploring and categorizing documents on the world wide web based on a user profile the heart of the agent is an unsupervised categorization of a set of documents combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set the document categories are not given a priori we present the overall architecture and describe two novel algorithms which provide significant improvement over hierarchical agglomeration clustering and autoclass algorithms and form the basis for the query generation and search component of the agent we report on the results of our experiments comparing these new algorithms with more traditional clustering algorithms and we show that our algorithms are fast and scalable y authors are listed alphabetically 1 introduction the world wide web is a vast resource of information and services t multistrategy learning for information extraction information extraction ie is the problem of filling out pre defined structured summaries from text documents we are interested in performing ie in non traditional domains where much of the text is often ungrammatical such as electronic bulletin board posts and web pages we suggest that the best approach is one that takes into account many different kinds of information and argue for the suitability of a multistrategy approach we describe learners for ie drawn from three separate machine learning paradigms rote memorization term space text classification and relational rule induction by building regression models mapping from learner confidence to probability of correctness and combining probabilities appropriately it is possible to improve extraction accuracy over that achieved by any individual learner we describe three different multistrategy approaches experiments on two ie domains a collection of electronic seminar announcements from a university computer science de improved tracking of multiple humans with trajectory prediction and occlusion modeling a combined 2d 3d approach is presented that allows for robust tracking of moving bodies in a given environment as observed via a single uncalibrated video camera lowlevel features are often insufficient for detection segmentation and tracking of non rigid moving objects therefore an improved mechanism is proposed that combines lowlevel image processing and mid level recursive trajectory estimation information obtained during the tracking process the resulting system can segment and maintain the tracking of moving objects before during and after occlusion at each frame the system also extracts a stabilized coordinate frame of the moving objects this stabilized frame can be used as input to motion recognition modules the approach enables robust tracking without constraining the system to know the shape of the objects being tracked beforehand although some assumptions are made about the characteristics of the shape of the objects and how they evolve with time experim class representation and image retrieval with non metric distances one of the key problems in appearance based vision is understanding how to use a set of labeled images to classify new images classification systems that can model human performance or that use robust image matching methods often make use of similarity judgments that are non metric but when the triangle inequality is not obeyed most existing pattern recognition techniques are not applicable we note that exemplar based or nearest neighbor methods can be applied naturally when using a wide class of non metric similarity functions the key issue however is to find methods for choosing good representatives of a class that accurately characterize it we show that existing condensing techniques for finding class representatives are ill suited to deal with non metric dataspaces we then focus on developing techniques for solving this problem emphasizing two points first we show that the distance between two images is not a good measure of how well one image can represent another in non metric spaces instead we use the vector correlation between the distances from each image to other previously seen images second we show that in non metric spaces boundary points are less significant for capturing the structure of a class experiments in information retrieval from spoken documents this paper describes the experiments performed as part of the trec 97 spoken document retrieval track the task was to pick the correct document from 35 hours of recognized speech documents based on a text query describing exactly one document among the experiments we described here are vocabulary size experiments to assess the effect of words missing from the speech recognition vocabulary experiments with speech recognition using a stemmed language model using confidence annotations that estimate of the correctness of each recognized word using multiple hypotheses from the recognizer and finally we also measured the effects of corpus size on the sdr task despite fairly high word error rates information retrieval performance was only slightly degraded for speech recognizer transcribed documents 1 introduction for the first time the 1997 text retrieval conference trec97 included an evaluation track for information retrieval on spoken documents in this paper we describe introducing a two level grammar concept for design extended abstract this paper a few investigation may first illustrate the differences of our new concepts from known hierarchies even if l g is regular l dg may be not context free there is no need for a proof as it is folklore that pattern languages are usually not context free we present a few further trivialities to warm up in case j v j 1 l dg is regular if and only if l g is regular if t 1 and t 2 are disjoint it holds 1 membership for l dg is uniformly decidable 2 emptyness for l dg is uniformly decidable 3 finiteness for l dg is uniformly decidable there is no need for an explicit proof the results above are immediately inherited from classical results in formal language theory cf hu79 further properties seem to require particular assumptions for the intended application domain languages which satisfy t 1 n t 2 6 are of a particular importance it may also be of a special interest to consider languages with a certain rate of letters from t 1 n t 2 in terminal words 3 2 graph grammars text classification by bootstrapping with keywords em and shrinkage when applying text classification to complex tasks it is tedious and expensive to hand label the large amounts of training data necessary for good performance this paper presents an alternative approach to text classification that requires no labeled documents instead it uses a small set of keywords per class a class hierarchy and a large quantity of easilyobtained unlabeled documents the keywords are used to assign approximate labels to the unlabeled documents by termmatching these preliminary labels become the starting point for a bootstrapping process that learns a naive bayes classifier using expectation maximization and hierarchical shrinkage when classifying a complex data set of computer science research papers into a 70 leaf topic hierarchy the keywords alone provide 45 accuracy the classifier learned by bootstrapping reaches 66 accuracy a level close to human agreement on this task 1 introduction when provided with enough labeled training examples a variety of distributed query scheduling service an architecture and its implementation we present the systematic design and development of a distributed query scheduling service dqs in the context of diom a distributed and interoperable query mediation system 26 dqs consists of an extensible architecture for distributed query processing a three phase optimization algorithm for generating efficient query execution schedules and a prototype implementation functionally two important execution models of distributed queries namely moving query to data or moving data to query are supported and combined into a unified framework allowing the data sources with limited search and filtering capabilities to be incorporated through wrappers into the distributed query scheduling process algorithmically conventional optimization factors such as join order are considered separately from and refined by distributed system factors such as data distribution execution location heterogeneous host capabilities allowing for stepwise refinement through three optimization phases compilation parallelization site selection and execution a subset of dqs algorithms has been implemented in java to demonstrate the practicality of the architecture and the usefulness of the distributed query scheduling algorithm in optimizing execution schedules for inter site queries emediator a next generation electronic commerce server this paper presents emediator an electronic commerce server prototype that demonstrates ways in which algorithmic support and game theoretic incentive engineering can jointly improve the efficiency of ecommerce eauctionhouse the configurable auction server includes a variety of generalized combinatorial auctions and exchanges pricing schemes bidding languages mobile agents and user support for choosing an auction type we introduce two new logical bidding languages for combinatorial markets the xor bidding language and the or of xors bidding language unlike the traditional or bidding language these are fully expressive they therefore enable the use of the clarke groves pricing mechanism for motivating the bidders to bid truthfully eauctionhouse also supports supply demand curve bidding ecommitter the leveled commitment contract optimizer determines the optimal contract price and decommitting penalties for a variety of leveled commitment contracting mechanisms taking into account that rational agents will decommit strategically in nash equilibrium it also determines the optimal decommitting strategies for any given leveled commitment contract eexchangehouse the safe exchange planner enables unenforced anonymous exchanges by dividing the exchange into chunks and sequencing those chunks to be delivered safely in alternation between the buyer and the seller curio a novel solution for efficient storage and indexing in data warehouses efficient query processing is a critical requirement for data warehousing systems as decision support applications often require interactive response times to answer complex ad hoc queries e g aggregations multi way joins over vast repositories of data e g hundreds of gigabytes to terabytes in size the most common approach used to improve on line analytical processing olap query performance is to utilize indexes or access structures to quickly access the base data a major drawback to this approach is that it often incurs significant overhead as the access structures must be stored in addition to the base data in this paper we present curio a data repository and olap query server which provides drastically improved performance for ad hoc queries while simultaneously reducing the storage costs associated with warehousing curio a data storage and access technology for data warehouses recently developed by muninn technologies llc is based on a novel paradigm that all integrated document caching and prefetching in storage hierarchies based on markov chain predictions f3 733e 05 large multimedia document archives may hold a major fraction of their data in tertiary storage libraries for cost reasons this paper develops an integrated approach to the vertical data migration between the tertiary secondary and primary storage in that it reconciles speculative prefetching to mask the high latency of the tertiary storage with the replacement policy of the document caches at the secondary and primary storage level and also considers the interaction of these policies with the tertiary and secondary storage request scheduling the integrated migration policy is based on a continuoustime markov chain model for predicting the expected number of accesses to a document within a specified time horizon prefetching is initiated only if that expectation is higher than those of the documents that need to be dropped from secondary storage to free up the necessary space in addition the possible resource contention at the tertiary and secondary storage is tak mindreader querying databases through multiple examples users often can not easily express their queries for example in a multimedia image by content setting the user might want photographs with sunsets in current systems like qbic the user has to give a sample query and to specify the relative importance of color shape and texture even worse the user might want correlations between attributes like for example in a traditional medical record database a medical researcher might want to find mildly overweight patients where the implied query would be weight height asymp 4 lb inch our goal is to provide a user friendly but theoretically solid method to handle such queries we allow the user to give several examples and optionally their goodness scores and we propose a novel method to guess which attributes are important which correlations are important and with what weight our contributions are twofold a we formalize the problem as a minimization problem and show how to solve for the optimal solution completely av from theory to practice the utep robot in the aaai 96 and aaai 97 robot contests in this paper we describe the control aspects of diablo the utep mobile robot participant in two aaai robot competitions in the first competition event one of the aaai 96 robot contest diablo consistently scored 285 1 out of a total of 295 points in the second competition our robot won the first place in the event tidy up of the home vacuum contest the main goal in this paper will be to show how the agent theories based on action theories developed at utep and by saffiotti et al was used in the building of diablo 1 introduction we participated 2 in the aaai 96 robot navigation contest knh97 and the aaai 97 home vacuum contest in the first competition our team scored 285 points in all runs of the contest out of a total of 295 points and was placed third in the finals in the second competition we won the first place in the event tidy up in this paper we relate theory of agents particularly the one developed at utep for higher level control and the one by shaping a cbr view with xml case based reasoning has found increasing application on the internet as an assistant in internet commerce stores and as a reasoning agent for online technical support the strength of cbr in this area stems from its reuse of the knowledge base associated with a particular application thus providing an ideal way to make personalised configuration or technical information available to the internet user since case data may be one aspect of a company s entire corporate knowledge system it is important to integrate case data easily within a company s it infrastructure using industry specific vocabulary we suggest xml as the likely candidate to provide such integration some applications have already begun to use xml as a case representation language we review these and present the idea of a standard case view in xml that can work with the vocabularies or namespaces being developed by specific industries earlier research has produced version 1 0 of a case based mark up investigating interactions between agent conversations and agent control components exploring agent conversation in the context of fine grained agent coordination research has raised several intellectual questions the major issues pertain to interactions between different agent conversations the representations chosen for different classes of conversations the explicit modeling of interactions between the conversations and how to address these interactions this paper is not so ambitious as to attempt to address these questions only frame them in the context of quantified scheduling centric multi agent coordination research 1 introduction based on a long history of work in agents and agent control components for building distributed ai and multi agent systems we are attempting to frame and address a set of intellectual questions pertaining to agent conversation interaction lies at the heart of the matter the issue is interaction between different agent conversations that possibly occur at different levels of abstraction but also interaction between the m building domain specific search engines with machine learning techniques domain specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with the general web wide search engines for example www campsearch com allows complex queries by agegroup size location and cost over summer camps unfortunately these domain specific search engines are difficult and time consuming to maintain this paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain specific search engines we describe new research in reinforcement learning text classification and information extraction that automates efficient spidering populating topic hierarchies and identifying informative text segments using these techniques we have built a demonstration system a search engine for computer science research papers it already contains over 33 000 papers and is publicly available at www cora jprc com 1 introduction as the amount of information on the world recognition of human action using moment based features the performance of different classification approaches is evaluated using a view based approach for motion representation the view based approach uses computer vision and image processing techniques to register and process the video sequence 6 23 two motion representations called motion energy images and motion history images 6 are then constructed these representations collapse the temporal component in a way that no explicit temporal analysis or sequence matching is needed statistical descriptions are then computed using momentbased features and dimensionality reduction techniques for these tests we used 7 hu moments which are invariant to scale and translation principal components analysis is used to reduce the dimensionality of this representation the system is trained using different subjects performing a set of examples of every action to be recognized given these samples k nearest neighbor gaussian and gaussian mixture classifiers are used to recognize new acti a media independent content language for integrated text and graphics generation this paper describes a media independent knowledge representation scheme or content language for describing the content of communicative goals and actions the language is used within an intelligent system for automatically generating integrated text and information graphics presentations about complex quantitative information the language is designed to satisfy four requirements to represent information about complex quantitative relations and aggregate properties compositionality to represent certain pragmatic distinctions needed for satisfying communicative goals and to be usable as input by the media specific generators in our system 1 introduction this paper describes a media independent knowledge representation scheme or content language for describing the content of communicative goals and actions the language is used within an intelligent system for automatically generating integrated text and information graphics 1 presentations about complex quantitative infor estimating dependency structure as a hidden variable this paper introduces a probability model the mixture of trees that can account for sparse dynamically changing dependence relationships we present a family of efficient algorithms that use em and the minimum spanning tree algorithm to find the ml and map mixture of trees for a variety of priors including the dirichlet and the mdl priors 1 introduction a fundamental feature of a good model is the ability to uncover and exploit independencies in the data it is presented with for many commonly used models such as neural nets and belief networks the dependency structure encoded in the model is fixed in the sense that it is not allowed to vary depending on actual values of the variables or with the current case however dependency structures that are conditional on values of variables abound in the world around us consider for example bitmaps of handwritten digits they obviously contain many dependencies between pixels however the pattern of these dependencies will vary acr using decision tree confidence factors for multiagent control although decision trees are widely used for classification tasks they are typically not used for agent control this paper presents a novel technique for agent control in a complex multiagent domain based on the confidence factors provided by the c4 5 decision tree algorithm using robotic soccer as an example of such a domain this paper incorporates a previously trained decision tree into a full multiagent behavior that is capable of controlling agents throughout an entire game along with using decision trees for control this behavior also makes use of the ability to reason about action execution time to eliminate options that would not have adequate time to be executed successfully this multiagent behavior represents a bridge between low level and high level learning in the layered learning paradigm the newly created behavior is tested empirically in game situations 1 introduction multiagent systems is the subfield of ai that aims to provide both principles for construction experiences with selecting search engines using meta search search engines are among the most useful and high profile resources on the internet the problem of finding information on the internet has been replaced with the problem of knowing where search engines are what they are designed to retrieve and how to use them this paper describes and evaluates savvysearch a meta search engine designed to intelligently select and interface with multiple remote search engines the primary meta search issue examined is the importance of carefully selecting and ranking remote search engines for user queries we studied the efficacy of savvysearch s incrementally acquired meta index approach to selecting search engines by analyzing the effect of time and experience on performance we also compared the meta index approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme 1 introduction search engines are powerful tools for assisting the otherwise unmanageable task of navigating the rapidly ex distributional clustering of words for text classification this paper applies distributional clustering pereira et al 1993 to document classification the approach clusters words into groups based on the distribution of class labels associated with each word thus unlike some other unsupervised dimensionality reduction techniques such as latent semantic indexing we are able to compress the feature space much more aggressively while still maintaining high document classification accuracy experimental results obtained on three real world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2 accuracy significantly better than latent semantic indexing deerwester et al 1990 class based clustering brown et al 1992 feature selection by mutual information yang and pederson 1997 or markovblanket based feature selection koller and sahami 1996 we also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clusteri secure object spaces a coordination model for agents coordination languages based on shared data spaces are well suited to programming parallel applications composed of cooperating software components secure object spaces sos extend this model to support composition of independently developed mutually suspicious software components sos provides flexible and efficient security facilities based on a cryptographic re interpretation of object types we discuss the implementation of secure object spaces in the context of a java based mobile agent system 1 introduction coordination languages based on shared data spaces have been around for over fifteen years oftentimes researchers have advocated their use for structuring distributed and concurrent systems this because the mode of communication provided by these languages sometimes called generative communication is anonymous processes interact by reading and writing entries in a shared space without having to know their interlocutor s and is uncoupled processes are not an efficient boosting algorithm for combining preferences we study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions this problem of combining preferences arises in several applications such as that of combining the results of different search engines or the collaborativefiltering problem of ranking movies for a user based on the movie rankings provided by other users in this work we begin by presenting a formal framework for this general problem we then describe and analyze an efficient algorithm called rankboost for combining preferences based on the boosting approach to machine learning we give theoretical results describing the algorithm s behavior both on the training data and on new test data not seen during training we also describe an efficient implementation of the algorithm for a particular restricted but common case we next discuss two experiments we carried out to assess the performance of rankboost in the first experiment we used the algorithm to combine different web search strategies each of which is a query expansion for a given domain the second experiment is a collaborative filtering task for making movie recommendations providing haptic hints to automatic motion planners in this paper we investigate methods for enabling a human operator and an automatic motion planner to cooperatively solve a motion planning query our work is motivated by our experience that automatic motion planners sometimes fail due to the difficulty of discovering critical configurations of the robot that are often naturally apparent to a human observer our goal is to develop techniques by which the automatic planner can utilize easily generated user input and determine natural ways to inform the user of the progress made by the motion planner we show that simple randomized techniques inspired by probabilistic roadmap methods are quite useful for transforming approximate user generated paths into collision free paths and describe an iterative transformation method which enables one to transform a solution for an easier version of the problem into a solution for the original problem we also show that simple visualization techniques can provide meaningful representatio an fft based algorithm for multichannel blind deconvolution a new update equation for the general multichannel blind deconvolution mcbd of a convolved mixture of source signals is derived it is based on the update equation for blind source separation bss which has been shown to be an alternative interpretation 1 of the natural gradient applied to the minimization of some mutual information criterion 2 computational complexity is held at a minimum by carrying out the separation equalization task in the frequency domain the algorithm is compared to similar known blind algorithms and its validity is demonstrated by simulations of real world acoustic filters in order to assess the performance of the algorithm performance measures for multichannel blind deconvolution of signals are given in the paper 1 introduction blind separation blind deconvolution and the combination of both multichannel blind deconvolution are tasks that have to be carried out in an increasing number of applications particularly in acoustics and communicati reinventing the familiar exploring an augmented reality design space for air traffic control this paper describes our exploration of a design space for an augmented reality prototype we began by observing air traffic controllers and their interactions with paper flight strips we then worked with a multi disciplinary team of researchers and controllers over a period of a year to brainstorm and prototype ideas for enhancing paper flight strips we argue that augmented reality is more promising and simpler to implement than the current strategies that seek to replace flight strips with keyboard monitor interfaces we also argue that an exploration of the design space with active participation from the controllers is essential not only for designing particular artifacts but also for understanding the strengths and limitations of augmented reality in general keywords augmented reality design space interactive paper participatory design video prototyping introduction air traffic control is a complex collaborative activity with well established and successful work p a temporal description logic for reasoning about actions and plans a class of interval based temporal languages for uniformly representing and reasoning about actions and plans is presented actions are represented by describing what is true while the action itself is occurring and plans are constructed by temporally relating actions and world states the temporal languages are members of the family of description logics which are characterized by high expressivity combined with good computational properties the subsumption problem for a class of temporal description logics is investigated and sound and complete decision procedures are given the basic languagetl f is considered rst it is the composition of a temporal logictl able to express interval temporal networks together with the non temporal logicf a feature description logic it is proven that subsumption in this language is an np complete problem then it is shown how to reason with the more expressive languagestlu fu andtl alcf the former adds disjunction both at the temporal and non temporal sides of the language the latter extends the non temporal side with set valued features i e roles and a propositionally complete language 1 an experimental comparison of localization methods localization is the process of updating the pose of a robot in an environment based on sensor readings in this experimental study we compare two recent methods for localization of indoor mobile robots markov localization which uses a probability distribution across a grid of robot poses and scan matching which uses kalman filtering techniques based on matching sensor scans both these techniques are dense matching methods that is they match dense sets of environment features to an a priori map to arrive at results for a range of situations we utilize several different types of environments and add noise to both the dead reckoning and the sensors analysis shows that roughly the scan matching techniques are more efficient and accurate but markov localization is better able to cope with large amounts of noise these results suggest hybrid methods that are efficient accurate and robust to noise 1 introduction to carry out tasks such as delivering objects an indoor ro employing em and pool based active learning for text classification this paper shows how a text classifier s need for labeled training data can be reduced by a combination of active learning and expectation maximization em on a pool of unlabeled data query by committee is used to actively select documents for labeling then em with a naive bayes model further improves classification accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model we also present a metric for better measuring disagreement among committee members it accounts for the strength of their disagreement and for the distribution of the documents experimental results show that our method of combining em and active learning requires only half as many labeled training examples to achieve the same accuracy as either em or active learning alone keywords text classification active learning unsupervised learning information retrieval 1 introduction in many settings for learning text classifiers obtaining lab logic based subsumption architecture in this paper we describe a logic based ai architecture based on brooks subsumption architecture we axiomatize each of the layers of control in his system separately and use independent theorem provers to derive each layer s output actions given its inputs we implement the subsumption of lower layers by higher layers using circumscription we give formal semantics to our approach 1 introduction in brooks proposed a reactive architecture embodying an approach to robot control different on various counts from traditional approaches he decomposed the problem into layers corresponding to levels of behavior rather than according to a sequential functional form within this setting he introduced the idea of subsumption that is that more complex layers could not only depend on lower more reactive layers but could also influence their behavior the resulting architecture was one that could service simultaneously multiple potentially conflicting goals in a reactive fashi vector based natural language call routing this paper describes a domain independent automatically trained natural language call router for directing incoming calls in a call center our call router directs customer calls based on their response to an open ended how may i direct your call prompt routing behavior is trained from a corpus of transcribed and hand routed calls and then carried out using vectorbased information retrieval techniques terms consist of n gram sequences of morphologically reduced content words while documents representing routing destinations consists of weighted term frequencies derived from calls to that destination in the training corpus based on the statistical discriminating power of the n gram terms extracted from the caller s request the caller is 1 routed to the appropriate destination 2 transferred to a human operator or 3 asked a disambiguation question in the last case the system dynamically generates queries tailored to the caller s request and the destinations with which it is consistent based on our extension of the vector model evaluation of the call router performance over a financial services call center using both accurate transcriptions of calls and fairly noisy speech recognizer output demonstrated robustness in the face of speech recognition errors furthermore our system showed a substantial improvement in performance over existing systems by correctly routing 93 8 of the calls after punting 10 2 of all calls to a human operator on transcription with approximately 4 degradation in performance when using speech recognizer output with a 23 word error rate regularizing adaboost boosting methods maximize a hard classification margin and are known as powerful techniques that do not exhibit overfitting for low noise cases also for noisy data boosting will try to enforce a hard margin and thereby give too much weight to outliers which then leads to the dilemma of non smooth fits and overfitting therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept 1 adaboost reg and regularized versions of 2 linear and 3 quadratic programming adaboost experiments show the usefulness of the proposed algorithms in comparison to another soft margin classifier the support vector machine 1 introduction boosting and other ensemble methods have been used with success in several applications e g ocr 12 7 for low noise cases several lines of explanation have been proposed as candidates for explaining the well functioning of boosting methods a breiman proposed that relational learning techniques for natural language information extraction the recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information one type of processing appropriate for many tasks is information extraction a type of text skimming that retrieves specific types of information from text although information extraction systems have existed for two decades these systems have generally been built by hand and contain domain specific information making them difficult to port to other domains a few researchers have begun to apply machine learning to information extraction tasks but most of this work has involved applying learning to pieces of a much larger system this paper presents a novel rule representation specific to natural language and a learning system rapier which learns information extraction rules rapier takes pairs of documents and filled templates indicating the information to be ext simultaneous learning of negatively correlated neural networks a new approach to designing neural network ensembles has been proposed recently 1 experimental studies on some regression tasks have shown that the new approach performs significantly better than previous ones 1 this paper presents a new algorithm for designing neural network ensembles for classification problems with noise this new algorithm is different from that used for regression tasks although the idea is similar the idea behind this new algorithm is to encourage different individual networks in an ensemble to learn different parts or aspects of the training data so that the whole ensemble can learn the whole training data better negatively correlated networks are trained with a novel correlation penalty term in the error function to encourage such specialisation in our algorithm individual networks are trained simultaneously rather than sequentially this provides an opportunity for different networks to interact with other and to specialise experiments on two real w parsing as information compression by multiple alignment unification and search sp52 this article presents and discusses examples illustrating aspects of the proposition described in the accompanying article wolff 1998 that parsing may be understood as information compression by multiple alignment unification and search icmaus the later examples show that the multiple alignment framework as described in the accompanying article has expressive power which is comparable with other context sensitive systems used to represent the syntax of natural languages in all the examples the sp52 model described in the accompanying article is capable of finding an alignment which is intuitively correct and assigning to it a compression score which is higher than for any other alignment the congruance which has been found between this range of alignments produced by a system which is dedicated to information compression and what is judged to be correct in terms of linguistic intuition lends support to the hypothesis that linguistic intuition is itself a product o essential principles for workflow modelling effectiveness while the specification languages of workflow management systems focus on process execution semantics the successful development of workflows relies on a fuller conceptualisation of business processing including process semantics traditionally the success of conceptual modelling techniques has depended largely on the adequacy of certain requirements conceptualisation following the conceptualisation principle expressive power following the one hundred principle comprehensibility and formal foundation an equally important requirement particularly with the increased conceptualisation of business aspects is business suitability in this paper the focus is on the suitability of workflow modelling for a commonly encountered class of operational business processing e g those of insurance claims bank loans and land conveyancing based on a previously conducted assessment of a number of integrated techniques the results of which are summarised in this paper fiv reliable communication for highly mobile agents the provision of a reliable communication infrastructure for mobile agents is still an open research issue the challenge to reliability we address in this work does not come from the possibility of faults but rather from the mere presence of mobility which complicates the problem of ensuring the delivery of information even in a fault free network for instance the asynchronous nature of message passing and agent migration may cause situations where messages forever chase a mobile agent that moves frequently from one host to another current solutions rely on conventional technologies that either do not provide a solution for the aforementioned problem because they were not designed with mobility in mind or enforce continuous connectivity with the message source which in many cases defeats the very purpose of using mobile agents in this paper we propose an algorithm that guarantees delivery to highly mobile agents using a technique similar to a distributed snapshot a numbe formalisms for multi agent systems this report is the result of a panel discussion at the first uk workshop on foundations of multi agent systems fomas 96 all members of the panel are authors listed alphabetically as knowledge representation language for direct manipulation within an agent system is exemplified in the work of konolige on formalisms for modelling belief and logic as a programming language is evidenced in the work of fisher on concurrent metatem all of these strands of work can claim some measure of success however a common failing of formal work both in ai and multi agent systems is that its role is not clear formal agent theories are agent specifications not only in the sense of providing descriptions and constraints on agent behaviour but also in the sense that one understands the term specification from mainstream software engineering namely that they provide a base from which to design implement and verify agent systems agents are a natural next step for software engineering they represent a fundamentally new way of considering complex distributed systems containing societies of cooperating autonomous components if we aim to build such systems then principled techniques will be required for their design and implementation we aim to assist the development of such systems by providing formalisms and notations that can be used to specify the desirable behaviour of agents and multi agent systems a requirement is that we should be able to move in a principled way from specifications of such systems to implementations the properties identified by using a formalism serve to measure and evaluate implementations of agent systems some properties currently seem to be unimplementable because they deal with an idealised aspect of agency such as knowledge still t the dynamics of database views the dynamics of relational database can be specified by means of reiter s formalism based on the situation calculus the specification of transaction based database updates is given in terms of successor state axioms ssas for the base tables of the database these axioms completely describe the contents of the tables at an arbitrary state of the database that is generated by the execution of a legal primitive transaction and thus solve the frame problem for databases in this paper we show how to derive action effect based ssas for views from the ssas for the base tables we prove consistency properties for those axioms in addition we establish the relationship between the derived ssa and the view definition as a static integrity constraint of the database we give applications of the derived ssas to the problems of view maintenance and checking proving and enforcement of integrity constraints 1 introduction the situation calculus sc mh1 is a family of many sorted lang using case based learning to improve genetic algorithm based design optimization in this paper we describe a method for improving genetic algorithm based optimization using case based learning the idea is to utilize the sequence of points explored during a search to guide further exploration the proposed method is particularly suitable for continuous spaces with expensive evaluation functions such as arise in engineering design empirical results in two engineering design domains and across different representations demonstrate that the proposed method can significantly improve the efficiency and reliability of the ga optimizer moreover the results suggest that the modification makes the genetic algorithm less sensitive to poor choices of tuning parameters such as mutation rate 1 introduction genetic algorithms gas goldberg 1989 are search algorithms that simulate the process of natural selection gas attempt to find a good solution to some problem e g finding the maximum of a function by randomly generating a collection population of potential independent component analysis a flexible non linearity and decorrelating manifold approach independent components analysis finds a linear transformation to variables which are maximally statistically independent we examine ica and algorithms for finding the best transformation from the point of view of maximising the likelihood of the data in particular we discuss the way in which scaling of the unmixing matrix permits a static nonlinearity to adapt to various marginal densities we demonstrate a new algorithm that uses generalised exponential functions to model the marginal densities and is able to separate densities with light tails we characterise the manifold of decorrelating matrices and show that it lies along the ridges of high likelihood unmixing matrices in the space of all unmixing matrices we show how to find the optimum ica matrix on the manifold of decorrelating matrices and as an example use the algorithm to find independent component basis vectors for an ensemble of portraits 1 introduction finding a natural cooordinate system is an essential first s on case based representability and learnability of languages within the present paper we investigate case based representability as well as case based learnability of indexed families of uniformly recursive languages since we are mainly interested in case based learning with respect to an arbitrary fixed similarity measure case based learnability of an indexed family requires its representability first we show that every indexed family is case based representable by positive and negative cases if only positive cases are allowed the class of representable families is comparatively small furthermore we present results that provide some bounds concerning the necessary size of case bases we study in detail how the choice of a case selection strategy influences the learning capabilities of a case based learner we define different case selection strategies and compare their learning power to one another furthermore we elaborate the relations to gold style language learning from positive and both positive and negative examples 1 introdu supporting flexibility a case based reasoning approach this paper presents a case based reasoning system ta3 we address the flexibility of the case based reasoning process namely flexible retrieval of relevant experiences by using a novel similarity assessment theory to exemplify the advantages of such an approach we have experimentally evaluated the system and compared its performance to the performance of non flexible version of ta3 and to other machine learning algorithms on several domains introduction flexible computation there are many situation when resources are scarce and when the system has to make decision as to how to proceed with further computation in order to make the right tradeoff between quality of the answer and resources needed in several domains such as medicine robotics financing etc if the answer to a query is not produced within a certain time limit it may become useless such family of problems has been a motivation for designing anytime algorithms dean boddy 1988 frisch haddawy 1994 algorit markovian models for sequential data hidden markov models hmms are statistical models of sequential data that have been used successfully in many machine learning applications especially for speech recognition furthermore in the last few years many new and promising probabilistic models related to hmms have been proposed we first summarize the basics of hmms and then review several recent related learning algorithms and extensions of hmms including in particular hybrids of hmms with artificial neural networks input output hmms which are conditional hmms using neural networks to compute probabilities weighted transducers variable length markov models and markov switching state space models finally we discuss some of the challenges of future research in this very active area 1 introduction hidden markov models hmms are statistical models of sequential data that have been used successfully in many applications in artificial intelligence pattern recognition speech recognition and modeling of biological identifying distinctive subsequences in multivariate time series by clustering most time series comparison algorithms attempt to discover what the members of a set of time series have in common we investigate a different problem determining what distinguishes time series in that set from other time series obtained from the same source in both cases the goal is to identify shared patterns though in the latter case those patterns must be distinctive as well an efficient incremental algorithm for identifying distinctive subsequences in multivariate real valued time series is described and evaluated with data from two very different sources the response of a set of bandpass filters to human speech and the sensors of a mobile robot 1 introduction given two or more sequences of discrete tokens a dynamic programming algorithm exists for finding the longest common subsequence they share cormen leiserson rivest 1990 this basic algorithm has been adapted in various ways to find patterns shared by real valued time series as well kruskall sankoff 1983 domain specific knowledge acquisition for conceptual sentence analysis the availability of on line corpora is rapidly changing the field of natural language processing nlp from one dominated by theoretical models of often very specific linguistic phenomena to one guided by computational models that simultaneously account for a wide variety of phenomena that occur in real world text thus far among the best performing and most robust systems for reading and summarizing large amounts of real world text are knowledge based natural language systems these systems rely heavily on domain specific handcrafted knowledge to handle the myriad syntactic semantic and pragmatic ambiguities that pervade virtually all aspects of sentence analysis not surprisingly however generating this knowledge for new domains is ti towards lifetime maintenance of case base indexes for continual case based reasoning abstract one of the key areas of case based reasoning is how to main tain the domain knowledge in the face of a changing environment during case retrieval a key process of cbr feature value pairs attached to the cases are used to rank the cases for the user different feature value pairs may have different importance measures in this process often represented by feature weights attached to the cases how to maintain the weights so that they are up to date and current is one of the key factors deter mining the success of cbr our focus in this paper is on the lifetime maintenance of the feature weights in a case base our task is to de sign a cbr maintenance system that not only learns a user s preference in the selection of cases but also tracks the user s evolving preferences in the cases our approach is to maintain feature weighting in a dy namic context through an integration with a learning system inspired by a back propagation neural network in this paper we explain the new system architecture and reasoning algorithms contrasting our approach with the previous ones the effectiveness of the system is demonstrated through experiments in a real world application domain 1 using reinforcement learning to spider the web efficiently consider the task of exploring the web in order to find pages of a particular kind or on a particular topic this task arises in the construction of search engines and web knowledge bases this paper argues that the creation of efficient web spiders is best framed and solved by reinforcement learning a branch of machine learning that concerns itself with optimal sequential decision making one strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future we present an algorithm for learning a value function that maps hyperlinks to future discounted reward by using naive bayes text classifiers experiments on two real world spidering tasks show a three fold improvement in spidering efficiency over traditional breadth first search and up to a two fold improvement over reinforcement learning with immediate reward only keywords reinforcement learning text classification world wide web spidering crawlin choosing good distance metrics and local planners for probabilistic roadmap methods this paper presents a comparative evaluation of different distance metrics and local planners within the context of probabilistic roadmap methods for motion planning both c space and workspace distance metrics and local planners are considered the study concentrates on cluttered three dimensional workspaces typical e g of mechanical designs our results include recommendations for selecting appropriate combinations of distance metrics and local planners for use in motion planning methods particularly probabilistic roadmap methods our study of distance metrics showed that the importance of the translational distance increased relative to the rotational distance as the environment become more crowded we find that each local planner makes some connections than none of the others do indicating that better connected roadmaps will be constructed using multiple local planners we propose a new local planning method we call rotate at s that outperforms the common straight line in c space method in crowded environments learning to locate an object in 3d space from a sequence of camera images this paper addresses the problem of determining an object s 3d location from a sequence of camera images recorded by a mobile robot the approach presented here allows people to train robots to recognize specific objects by presenting it examples of the object to be recognized a decision tree method is used to learn significant features of the target object from individual camera images individual estimates are integrated over time using bayes rule into a probabilistic 3d model of the robot s environment experimental results illustrate that the method enables a mobile robot to robustly estimate the 3d location of objects from multiple camera images 1 introduction in recent years there has been significant progress in the field of mobile robotics applications such as robots that guide blind or mentally handicapped people robots that clean large office buildings and department stores robots that assist people in recreational activities etc are slowly getting in reach man a parametric alternative to grids for occupancy based world modeling in the paper we consider an occupancy based approach for range data fusion as it is used in mobile robotics we tackle the major problem of this approach which is the redundancy of stored and processed data caused by using the grid representation of the occupancy function by proposing a parametric piece wise linear representation when applied to the vision based world exploration the new representation is shown to have advantages over the former one which include its suitability for radial range data its efficiency in representing and fusing range data and its convenience for navigation map extraction the proposed technique is implemented on a mobile robot boticelli the results obtained from running the robot are presented 1 introduction in mobile robot world exploration the occupancybased approach is one of the most commonly used 7 13 3 2 8 4 11 in this approach the exploration policy is determined by the occupancy model of the world which is built from the r redundancy and inconsistency detection in large and semi structured case bases with the dramatic proliferation of case based reasoning systems in commercial applications many case bases are now becoming legacy systems they represent a significant portion of an organization s assets but they are large and difficult to maintain one of the contributing factors is that these case bases are often large and yet unstructured or semi structured they are represented in natural language text adding to the complexity is the fact that the case bases are often authored and updated by different people from a variety of knowledge sources making it highly likely for a case base to contain redundant and inconsistent knowledge in this paper we present methods and a system for maintaining large and semi structured case bases we focus on two difficult problems in case base maintenance redundancy and inconsistency detection these two problems are particularly pervasive when one deals with an semi structured case base we will discuss both algorithms and a system for solvi point based temporal extensions of sql and their efficient implementation this chapter introduces a new approach to temporal extensions of sql the main difference from most of the current proposals is the use single time points rather than intervals or various other complexvalues for references to time while still achieving efficient query evaluation the proposed language sql tp extends the syntax of sql 92 to handle temporal data in a natural way it adds a single data type to represent a linearly ordered universe of time instants the semantics of the new language naturally extends the standard sql semantics and eliminates or fixes many of the problems connected with defining a precise semantics to temporal query languages based on explicit interval valued temporal attributes the efficient query evaluation procedure is based on a compilation technique that translates sql tp queries to sql 92 therefore existing off shelf database systems can be used as back ends for implementations based on this approach to manage temporal data 1 why another temp fuzzy finite state automata can be deterministically encoded into recurrent neural networks there has been an increased interest in combining fuzzy systems with neural networks because fuzzy neural systems merge the advantages of both paradigms on the one hand parameters in fuzzy systems have clear physical meanings and rule based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way on the other hand there exist powerful algorithms for training various neural network models however most of the proposed combined architectures are only able to process static input output relationships i e they are not able to process temporal input sequences of arbitrary length fuzzy finite state automata ffas can model dynamical processes whose current state depends on the current input and previous states unlike in the case of deterministic finite state automata dfas ffas are not in one particular state rather each state is occupied to some degree defined by a membership function based on previous work on encoding dfas in discrete tim a formal approach to detecting security flaws in object oriented databases this paper is to show an efficient decision algorithm for detecting a security flaw under a given authorization this problem is solvable in polynomial time in practical cases by reducing it to the congruence closure problem this paper also mentions the problem of finding a maximal subset of a given authorization under which no security flaw exists graph structured views and their incremental maintenance we study the problem of maintaining materialized views of graph structured data the base data consists of records containing identifiers of other records the data could represent traditional objects with methods attributes and a class hierarchy but it could also represent a lower level data structure we define simple views and materialized views for such graph structured data analyzing options for representing record identity and references in the view we develop incremental maintenance algorithms for these views 1 introduction relational views are useful for controlling data access specifying contents of caches or remote copies and other data management tasks in this paper we study how to extend this view concept and the associated maintenance algorithms to what we call a graph structured database gsdb informally a gsdb is a collection of objects that may contain pointers graph edges to other objects a gsdb can represent web pages lotus notes documents o visual information retrieval from large distributed on line repositories ion vir systems differ in the level of abstraction in which content is indexed for example images may be indexed at various levels such as at the feature level e g color texture and shape object level e g moving foreground object syntax level e g video shot and semantic level e g image subject and so forth most automatic vir systems aim at lowlevel features while the high level indexes are usually generated manually interaction among different levels is an exciting but unsolved issue generality vir systems differ in their specificity of the domain of visual information for example customized feature sets can be developed to incorporate specific domain knowledge such as those in medical and remote sensing applications other more general vir systems aim at indexing unconstrained visual information such as that on the internet content collection vir systems differ in the methods in which new visual information is ad using the condensation algorithm for robust vision based mobile robot localization to navigate reliably in indoor environments a mobile robot must know where it is this includes both the ability of globally localizing the robot from scratch as well as tracking the robot s position once its location is known vision has long been advertised as providing a solution to these problems but we still lack efficient solutions in unmodified environments many existing approaches require modification of the environment to function properly and those that work within unmodified environments seldomly address the problem of global localization in this paper we present a novel vision based localization method based on the condensation algorithm 17 18 a bayesian filtering method that uses a samplingbased density representation we show how the conden sation algorithm can be used in a novel way to track the position of the camera platform rather than tracking an object in the scene in addition it can also be used to globally localize the camera platform given a visua 1bc a first order bayesian classifier in this paper we present 1bc a first order bayesian classifier our approach is to view individuals as structured terms and to distinguish between structural predicates referring to subterms e g atoms from molecules and properties applying to one or several of these subterms e g a bond between two atoms we describe an individual in terms of elementary features consisting of zero or more structural predicates and one property these features are considered conditionally independent following the usual naive bayes assumption 1bc has been implemented in the context of the first order descriptive learner tertius and we describe several experiments demonstrating the viability of our approach 1 introduction in this paper we present 1bc a first order bayesian classifier while the propositional bayesian classifier makes the naive bayes assumption of statistical independence of elementary features one attribute taking on a particular value given the class value it is not i remote agent to boldly go where no ai system has gone before renewed motives for space exploration have inspired nasa to work toward the goal of establishing a virtual presence in space through heterogeneous effets of robotic explorers information technology and artificial intelligence in particular will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents in this paper we describe the remote agent a specific autonomous agent architecture based on the principles of model based programming on board deduction and search and goal directed closed loop commanding that takes a significant step toward enabling this future this architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines resource constraints and concurrent activity among tightly coupled subsystems the remote agent integrates constraint based temporal planning and scheduling robust multi threaded execution and model based mode identification and reconfiguration the demonstration of the integrated system as an on board controller for deep space one nasa s rst new millennium mission is scheduled for a period of a week in late 1998 the development of the remote agent also provided the opportunity to reassess some of ai s conventional wisdom about the challenges of implementing embedded systems tractable reasoning and knowledge representation we discuss these issues and our often contrary experiences throughout the paper efficient data mining for path traversal patterns abstract in this paper we explore a new data mining capability that involves mining path traversal patterns in a distributed information providing environment where documents or objects are linked together to facilitate interactive access our solution procedure consists of two steps first we derive an algorithm to convert the original sequence of log data into a set of maximal forward references by doing so we can filter out the effect of some backward references which are mainly made for ease of traveling and concentrate on mining meaningful user access sequences second we derive algorithms to determine the frequent traversal patterns i e large reference sequences from the maximal forward references obtained two algorithms are devised for determining large reference sequences one is based on some hashing and pruning techniques and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required performance of these two methods is comparatively analyzed it is shown that the option of selective scan is very advantageous and can lead to prominent performance improvement sensitivity analysis on various parameters is conducted index terms data mining traversal patterns distributed information system world wide web performance analysis twenty one at trec 7 ad hoc and cross language track this paper describes the official runs of the twenty one group for trec 7 the twenty one group participated in the ad hoc and the cross language track and made the following accomplishments we developed a new weighting algorithm which outperforms the popular cornell version of bm25 on the ad hoc collection for the clir task we developed a fuzzy matching algorithm to recover from missing translations and spelling variants of proper names also for clir we investigated translation strategies that make extensive use of information from our dictionaries by identifying preferred translations main translations and synonym translations by defining weights of possible translations and by experimenting with probabilistic boolean matching strategies 1 introduction twenty one is a 2 mecu project with 12 partners funded by the eu telematics programme sector information engineering the project subtitle is development of a multimedia information transaction and dissemination tool twenty an architecture for mobile bdi agents bdi belief desire intention is a mature and commonly adopted architecture for intelligent agents bdi agents are autonomous entities able to work in teams and react to changing environmental conditions however the current computational model adopted by bdi has problems which amongst other limitations prevent the development of mobile agents in this paper we discuss an architecture tomas transaction oriented multi agent system that addresses these issues by combining bdi and the distributed nested transaction paradigms an algorithm is presented which enable agents in tomas to become mobile 1 introduction intelligent agents are a very active area of ai research wj95 sho93 of the various agent architectures which have been proposed bdi belief desire intention rg92 is probably the most mature and has been adopted by a few industrial applications bdi agents are autonomous entities able to work in teams and react to changing environmental conditions mobile m realising the full potential of workflow modelling a practical perspective in a broad sense a workflow provides a partial or complete automation of a process at a level above traditional implementation platforms much of the emphasis of workflow specifications as deployed by workflow management systems focuses on process execution semantics this includes a process s pre and post conditions and the sequence repetition choice parallelism and synchronisation which characterises inter process triggering the successful development of workflows requires not only this but in general a fuller conceptualisation of a domain s processing including process semantics given the diversity of business processing configurations which in absence of a complete theoretical foundation follow experience and intuition the characterisations indeed the cognition of workflows still appears coarse in this regard a wellspring of modelling techniques paradigms and informal formal method extensions which address the analysis of organisational processing structures the relationlog system user manual release 1 0 this document introduces the relationlog system through the use of examples all examples used here are available as part of the relationlog release in the directory named demo 2 installing relationlog 3 2 installing relationlog improving performance of case based classification using context based relevance classification involves associating instances with particular classes by maximizing intra class similarities and minimizing inter class similarities thus the way similarity among instances is measured is crucial for the success of the system in case based reasoning it is assumed that similar problems have similar solutions the case based approach to classification is founded on retrieving cases from the case base that are similar to a given problem and associating the problem with the class containing the most similar cases similarity based retrieval tools can advantageously be used in building flexible retrieval and classification systems case based classification uses previously classified instances to label unknown instances with proper classes classification accuracy is affected by the retrieval process the more relevant the instances used for classification the greater the accuracy the paper presents a novel approach to case based classification the algorithm is bas a description logic with transitive and inverse roles and role hierarchies transitive roles play an important r le in the adequate representation of aggregated objects they allow these objects to be described by referring to their parts without specifying a level of decomposition in horrocks gough 1997 the description logic dl alch r is presented which extends alc with transitive roles and a role hierarchy it is argued in sattler 1998 that alch r is well suited to the representation of aggregated objects in applications that require various part whole relations to be distinguished some of which are transitive for example a medical knowledge base could contain the following entries defining two different parts of the brain namely the gyrus and the cerebellum in contrast to a gyrus a cerebellum is an integral organ and furthermore a functional component of the brain hence the role is component which is a non transitive sub role of is part is used to describe the relation between the brain and the cerebellum is component is part gyrus consists brain mass is part brain cerebellum organ is component brain however alch r does not allow the simultaneous description of parts by means of the whole to which they belong and of wholes by means of their constituent parts one or other is possible but not both to overcome this limitation we present the dl alchi r which extends alch r with inverse converse roles allowing for example the use of has part as well as is part 1 using alchir we can define a tumorous brain as tumorous brain brain tumorous has part tumorous part of this work was carried out while being a guest at irst incremental recompilation of knowledge approximating a general formula from above and below by horn formulas its horn envelope and horn core respectively was proposed in 22 as a form of knowledge compilation supporting rapid approximate reasoning on the negative side this scheme is static in that it supports no updates and has certain complexity drawbacks pointed out in 17 on the other hand the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity theoretic impediments even in the horn case as was pointed out in 6 and is further demonstrated in the present paper more fundamentally these schemes are not inductive in that they may lose in a single update any positive properties of the represented sets of formulas small size horn structure etc in this paper 1 we propose a new scheme incremental recompilation which combines horn approximation and model based updates this scheme is inductive and very efficient free of using guidelines to constrain interactive case based htn planning this paper describes hicap a general purpose and interactive case based planning architecture hicap is a decision support tool for planning a hierarchical course of action it integrates a hierarchical task editor hte with a conversational case based planner nacodae htn hte maintains a task hierarchy representing guidelines that constrain the final plan hte also encodes the hierarchical organization responsible for these tasks this supports bookkeeping which is crucial for real world large scale planning tasks hte can be used to activate nacodae htn to interactively refine user selected guideline tasks into a concrete plan our application of hicap to the task of noncombatant evacuation operations inspired its architecture in this application our empirical evaluation with modsaf simulations confirms that the plans output by hicap outperform those generated using alternative approaches on three dimensions using wg log to represent semistructured data the example of oem in this paper we discuss the possibility to represent synthetically semistructured information via a loose notion of schema we say that data are semistructured when although some structure is present it is not as strict regular or complete as the one required by the traditional database management systems our proposal is based on wg log a graph based language for the representation of www site information we show how information encoded in a typical semistructured information model as oem can be represented and queried by means of the wg log language and how the tsimmis and wg log web query system can be integrated to allow site content exploration and exploitation by means of wg log 1 introduction we say that data are semistructured when although some structure is present it is not as strict regular or complete as the one required by the traditional database management systems see abi97 for a survey on semistructured data information is semistructured also when learning to catch applying nearest neighbor algorithms to dynamic control tasks steven l salzberg 1 and david w aha 2 1 introduction dynamic control problems are the subject of much research in machine learning e g selfridge sutton barto 1985 sammut 1990 sutton 1990 some of these studies investigated the applicability of various k nearest neighbor methods dasarathy 1990 to solve these tasks by modifying control strategies based on previously gained experience e g connell utgoff 1987 atkeson 1989 moore 1990 1991 however these previous studies did not highlight the fact that small changes in the design of these algorithms drastically alter their learning behavior this paper describes a preliminary study that investigates this issue in the context of a difficult dynamic control task learning to catch a ball moving in a three dimensional space an important problem in robotics research geng et al 1991 our thesis in this paper is that agents can improve substantially at physical tasks by storing experiences without explicitly projective rotations applied to a pan tilt stereo head a non metric pan tilt stereo head consists of a weakly calibrated stereo rig mounted on a pan tilt mechanism it is called non metric since neither the kinematics of the mechanism nor camera calibration are required the lie group of projective rotations homographies of projective space corresponding to pure rotations is an original formalism to model the geometry of such a pan tilt system a rodrigues alike formula as well as a minimal parameterization of projective rotations are introduced based on this the practical part devises a numerical optimization technique for accurately estimating projective rotations from point correspondences only this procedure recovers sufficient geometry to operate the system the experiments validate and evaluate the proposed approach on real image data they show the weak calibration image prediction and homing of a non metric pan tilt head 1 introduction one of the most useful sensors in computer vision is a pan and tilt stereo head improving data driven wordclass tagging by system combination in this paper we examine how the differences in modelling between different data driven systems performing the same nlp task can be exploited to yield a higher accuracy than the best individual system we do this by means of an experiment involving the task of morpho syntactic wordclass tagging four well known tagger generators hidden markov model memory based transformation rules and maximum entropy are trained on the same corpus data after comparison their outputs are combined using several voting strategies and second stage classifiers all combination taggers outperform their best component with the best combination showing a 19 1 lower error rate than the best individual tagger introduction in all natural language processing nlp systems we find one or more language models which are used to predict classify and or interpret language related observations traditionally these models were categorized as either rule based symbolic or corpusbased probabilistic recent dynamic service matchmaking among agents in open information environments introduction the amount of services and deployed software agents in the most famous offspring of the internet the world wide web is exponentially increasing in addition the internet is an open environment where information sources communication links and agents themselves may appear and disappear unpredictably thus an effective automated search and selection of relevant services or agents is essential for human users and agents as well we distinguish three general agent categories in the cyberspace service providers service requester and middle agents service providers provide some type of service such as finding information or performing some particular domain specific problem solving requester agents need provider agents to perform some service for them agents that help locate others are called middle agents 2 matchmaking is the process of finding an appropriate provider for a requester thr authoritative sources in a hyperlinked environment the link structure of a hypermedia environment can be a rich source of information about the content of the environment provided we have effective means for understanding it versions of this principle have been studied in the hypertext research community and in a context predating hypermedia through journal citation analysis in the field of bibliometrics but for the problem of searching in hyperlinked environments such as the world wide web it is clear from the prevalent techniques that the information inherent in the links has yet to be fully exploited in this work we develop a new method for automatically extracting certain types of information about a hypermedia environment from its link structure and we report on experiments that demonstrate its effectiveness for a variety of search problems on the www the central problem we consider is that of determining the relative authority of pages in such environments this issue is central to a number of basic hypertext search t inverted files versus signature files for text indexing two well known indexing methods are inverted files and signature files we have undertaken a detailed comparison of these two approaches in the context of text indexing paying particular attention to query evaluation speed and space requirements we have examined their relative performance using both experimentation and a refined approach to modeling of signature files and demonstrate that inverted files are distinctly superior to signature files not only can inverted files be used to evaluate typical queries in less time than can signature files but inverted files require less space and provide greater functionality our results also show that a synthetic text database can provide a realistic indication of the behavior of an actual text database the tools used to generate the synthetic database have been made publicly available confluence of computer vision and interactive graphics for augmented reality augmented reality ar is a technology in which a user s view of the real world is enhanced or augmented with additional information generated from a computer model using ar technology users can interact with a combination of real and virtual objects in a natural way this paradigm constitutes the core of a very promising new technology for many applications however before it can be applied successfully ar has to ful ll very strong requirements including precise calibration registration and tracking of sensors and objects in the scene as well as a detailed overall understanding of the scene at ecrcwe see computer vision and image processing technology play an increasing role in acquiring appropriate sensor and scene models to balance robustness with automation weintegrate automatic image analysis with both interactive user assistance and input from magnetic trackers and cad models also in order to meet the requirements of the emerging global information society using explicit requirements and metrics for interface agent user model correction the complexity of current computer systems and software warrants research into methods to decrease the cognitive load on users determining how to get the right information into the right form with the right tool at the right time has become a monumental task one necessitating intelligent interfaces agents with the ability to predict the users needs or intent an accurate user model is considered necessary for effective prediction of user intent methods for maintaining accurate user models is the main thrust of this paper we describe an approach for dynamically correcting an interface agent s user model based on utility theory we explicitly take into account an agent s requirements and metrics for measuring the agent s effectiveness of meeting those requirements using these requirements and metrics we develop a requirements utility function that determines when a user model should be corrected and how we present a correction model based on a multi agent bidding scalability in formal concept analysis this paper presents the results of experiments carried out with a set of 4 000 medical discharge summaries in which were recognised 1 962 attributes from the unified medical language system umls in this domain the objects are medical documents 4 000 and the attributes are umls terms extracted from the documents 1 962 when formal concept analysis is used to iteratively analyse and visualize this data complexity and scalability become critically important although the amount of data used in this experiment is small compared with the size of primary memory in modern computers the results are still important since the probability distributions which determine the efficiencies are likely to remain stable as the size of the data is increased our work presents two outcomes firstly we present a methodology for exploring knowledge in text documents using formal concept analysis by employing conceptual scales created as the result of direct manipulation of a line diagram the conceptual scales lead to small derived purified contexts that are represented using nested line diagrams secondly we present an algorithm for the fast determination of purified contexts from a compressed representation of the large formal context our work draws on existing encoding and compression techniques to show how rudimentary data analysis can lead to substantial efficiency improvements to knowledge visualisation c fl 1993 blackwell publishers 238 main street cambridge ma 02142 usa and 108 cowley road oxford ox4 1jf uk scalability in formal concept analysis 3 error correcting output coding for text classification this paper applies error correcting output coding ecoc to the task of document categorization ecoc of recent vintage in the ai literature is a method for decomposing a multiway classification problem into many binary classification tasks and then combining the results of the subtasks into a hypothesized solution to the original problem there has been much recent interest in the machine learning community about algorithms which integrate advice from many subordinate predictors into a single classifier and error correcting output coding is one such technique we provide experimental results on several real world datasets extracted from the internet which demonstrate that ecoc can offer significant improvements in accuracy over conventional classification algorithms 1 introduction error correcting output coding is a recipe for solving multi way classification problems it works in two stages first independently construct many subordinate classifiers each responsible for r logic based subsumption architecture in this paper we describe a logic based ai architecture based on brooks subsumption architecture we axiomatize each of the layers of control in his system separately and use independent theorem provers to derive each layer s output actions given its inputs we implement the subsumption of lower layers by higher layers using circumscription we give formal semantics to our approach 1 introduction in brooks proposed a reactive architecture embodying an approach to robot control different on various counts from traditional approaches he decomposed the problem into layers corresponding to levels of behavior rather than according to a sequential functional form within this setting he introduced the idea of subsumption that is that more complex layers could not only depend on lower more reactive layers but could also influence their behavior the resulting architecture was one that could service simultaneously multiple potentially conflicting goals in a reactive fashi natural sounding speech synthesis using variable length units the goal of this work was to develop a speech synthesis system which concatenates variable length units to create naturalsounding speech our initial work in this area showed that by careful design of system responses to ensure consistent intonation contours natural sounding speech synthesis was achievable with word and phrase level concatenation in order to extend the flexibility of this framework we focused on the problem of generating novel words from a corpus of sub word units the design of the sub word units was motivated by perceptual studies that investigated where speech could be spliced with minimal audible distortion and what contextual constraints were necessary to maintain in order to produce natural sounding speech the sub word corpus is searched during synthesis using a viterbi search which selects a sequence of units based on how well they individually match the input specification and on how well they sound as an ensemble this concatenative speech synthesis syste an evaluation of statistical approaches to text categorization abstract this paper focuses on a comparative evaluation of a wide range of text categorization methods including previously published results on the reuters corpus and new results of additional experiments a controlled study using three classifiers knn llsf and word was conducted to examine the impact of configuration variations in five versions of reuters on the observed performance of classifiers analysis and empirical evidence suggest that the evaluation results on some versions of reuters were significantly affected by the inclusion of a large portion of unlabelled documents mading those results difficult to interpret and leading to considerable confusions in the literature using the results evaluated on the other versions of reuters which exclude the unlabelled documents the performance of twelve methods are compared directly or indirectly for indirect compararions knn llsf and word were used as baselines since they were evaluated on all versions of reuters that exclude the unlabelled documents as a global observation knn llsf and a neural network method had the best performance except for a naive bayes approach the other learning algorithms also performed relatively well classifying unseen cases with many missing values handling missing attribute values is an important issue for classifier learning since missing attribute values in either training data or test unseen data affect the prediction accuracy of learned classifiers in many real kdd applications attributes with missing values are very common this paper studies the robustness of four recently developed committee learning techniques including boosting bagging sasc and sascmb relative to c4 5 for tolerating missing values in test data boosting is found to have a similar level of robustness to c4 5 for tolerating missing values in test data in terms of average error in a representative collection of natural domains under investigation bagging performs slightly better than boosting while sasc and sascmb perform better than them in this regard with sascmb performing best furthermore we propose a novel voting weight scheme for the committee learning techniques although it is very simple it can improve the robustness of all these partitioning based clustering for web document categorization clustering techniques have been used by manyintelligent software agents in order to retrieve lter and categorize documents available on the world wide web clustering is also useful in extracting salient features of related web documents to automatically formulate queries and search for other similar documents on the web traditional clustering algorithms either use a priori knowledge of document structures to de ne a distance or similarity among these documents or use probabilistic techniques such as bayesian classi cation many of these traditional algorithms however falter when the dimensionality of the feature space becomes high relative to the size of the document space in this paper we introduce two new clustering algorithms that can e ectively cluster documents even in the presence of a very high dimensional feature space these clustering techniques which are based on generalizations of graph partitioning do not require pre speci ed ad hoc distance functions and are capable of automatically discovering document similarities or associations we conduct several experiments on real web data using various feature selection heuristics and compare our clustering schemes to standard distance based techniques such ashierarchical agglomeration clustering and bayesian classi cation methods such as autoclass execution monitoring of high level robot programs imagine a robot that is executing a program on line and insofar as it is reasonable to do so it wishes to continue with this on line program execution no matter what exogenous events occur in the world execution monitoring is the robot s process of observing the world for discrepancies between the actual world and its internal representation of it and recovering from such discrepancies we provide a situation calculus based account of such on line program executions with monitoring this account relies on a specification for a single step interpreter for the logic programming language golog the theory is supported by an implementation that is illustrated by a standard blocks world in which a robot is executing a golog program to build a suitable tower the monitor makes use of a simple kind of planner for recovering from malicious exogenous actions performed by another agent after performing the sequence of actions generated by the recovery procedure th robustness of case initialized genetic algorithms we investigate the robustness of case initialized genetic algorithm cigar systems with respect to problem indexing when confronted with a series of similar problems cigar stores potential solutions in a case base or an associative memory and retrieves and uses these solutions to help improve a genetic algorithm s performance over time defining similarity among the problems or indexing is key to performance improvement we study four indexing schemes on a class of simple problems and provide empirical evidence of cigar s robustness to imperfect indexing feature subset selection in text learning this paper describes several known and some new methods for feature subset selection on large text data experimental comparison given on real world data collected from web users shows that characteristics of the problem domain and machine learning algorithm should be considered when feature scoring measure is selected our problem domain consists of hyperlinks given in a form of small documents represented with word vectors in our learning experiments naive bayesian classifier was used on text data the best performance was achieved by the feature selection methods based on the feature scoring measure called odds ratio that is known from information retrieval an intelligent multi agent architecture for information retrieval from the internet the world wide web www offers an uncountable number of documents which deal with information from a neverending list of topics thus the question of whether to find information turned into a question of how to find relevant information search engines with crawler based indexes vary in recall and offer a very bad precision meta search engines try to overcome these lacks based upon a specialised monolithic architecture for information extraction information filtering and integration of heterogenous information resources few search engines employ intelligent techniques in order to increase precision on the other hand user modeling techniques become more and more popular many personalized agent based system for web browsing are currently developed it is a straightforward idea to incorporate the idea of user modeling with machine learning methods into web search services we propose an abstract prototype which is being developed at the university of osnabruck and which incorporat autonomous cyber agents rules for collaboration a cyber agent is any program machine or person engaged in computer enabled work thus cyber agents can vary considerably in complexity and intelligence can they despite their variety be organized to collaborate effectively both empirical evidence and theory suggest that they can moreover there seem to be simple rules for designing problem solving organizations in which collaboration among cyber agents is automatic and scale effective adding agents tends to improve solution quality adding computers tends to improve solution speed this paper develops some of these rules 1 introduction computer networks make it possible to interconnect and therefore organize large numbers of distributed cyber agents varying in type from simple programs to skilled humans our goal is to develop a class of organizations in which such agents can collaborate easily and effectively more specifically our goal is to develop methods for routinely solving arbitrary instances of the following learning to extract symbolic knowledge from the world wide web the world wide web is a vast source of information accessible to computers but understandable only to humans the goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the world wide web such a knowledge base would enable much more e ective retrieval of web information and promote new uses of the web to support knowledge based inference and problem solving our approach istodevelop a trainable information extraction system that takes two inputs the rst is an ontology that de nes the classes e g company person employee product and relations e g employed by produced by ofinterest when creating the knowledge base the second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations given these inputs the system learns to extract information from other pages and hyperlinks on the web this paper describes our general approach several machine learning algorithms for this task and promising initial results with a prototype system that has created a knowledge base describing university people courses and research projects efficient use of signatures in object oriented database systems signatures are bit strings which are generated by applying some hash function on some or all of the attributes of an object the signatures of the objects can be stored separately from the objects themselves and can later be used to filter out candidate objects during perfect match queries in an object oriented database system oodb using logical oids an object identifier index oidx is needed to map from logical oid to the physical location of the object in this report we show how the signatures can be stored in the oidx and used to reduce the average object access cost in a system we also extend this approach to transaction time temporal oodbs toodb where this approach is even more beneficial because maintaining signatures comes virtually for free we develop a cost model that we use to analyze the performance of the proposed approaches and this analysis shows that substantial gain can be achieved keywords signatures object oriented database systems temporal objec optimization of constrained frequent set queries with 2 variable constraints currently there is tremendous interest in providing ad hoc mining capabilities in database management systems as a first step towards this goal in 15 we proposed an architecture for supporting constraint based human centered exploratory mining of various kinds of rules including associations introduced the notion of constrained frequent set queries cfqs and developed effective pruning optimizations for cfqs with 1 variable 1 var constraints while 1 var constraints are useful for constraining the antecedent and consequent separately many natural examples of cfqs illustrate the need for constraining the antecedent and consequent jointly for which 2 variable 2 var constraints are indispensable developing pruning optimizations for cfqs with 2 var constraints is the subject of this paper but this is a difficult problem because i in 2var constraints both variables keep changing and unlike 1 var constraints there is no fixed target for pruning ii as we show conv learning models of other agents using influence diagrams we adopt decision theory as a descriptive paradigm to model rational agents we use influence diagrams as a modeling representation of agents which is used to interact with them and to predict their behavior in this paper we provide a framework that an agent can use to learn the models of other agents in a multi agent system mas based on their observed behavior since the correct model is usually not known with certainty our agents maintain a number of possible models and assign a probability to each of them being correct when none of the available models is likely to be correct we modify one of them to better account for the observed behaviors the modification refines the parameters of the influence diagram used to model the other agent s capabilities preferences or beliefs the modified model is then allowed to compete with the other models and the probability assigned to it being correct can be arrived at based on how well it predicts the behaviors of the other agent alrea query rewriting for semistructured data we address the problem of query rewriting for tsl a language for querying semistructured data we develop and present an algorithm that given a semistructured query q and a set of semistructured views v finds rewriting queries i e queries that access the views and produce the same result as q our algorithm is based on appropriately generalizing containment mappings the chase and unification techniques that were developed for structured relational data we also develop an algorithm for equivalence checking of tsl queries we show that the algorithm is sound and complete for tsl i e it always finds every tsl rewriting query of q and we discuss its complexity we extend the rewriting algorithm to use available structural constraints such as dtds to find more opportunities for query rewriting we currently incorporate the algorithm in the tsimmis system 1 introduction recently many semistructured data models query and view definition languages have been proposed 2 bimodal system for interactive indexing and retrieval of pathology images the prototype of a system to assist the physicians in differential diagnosis of lymphoproliferative disorders of blood cells from digitized specimens is presented the user selects the region of interest roi in the image which is then analyzed with a fast robust color segmenter queries in a database of validated cases can be formulated in terms of shape similarity invariant fourier descriptors texture multiresolution simultaneous autoregressive model color l u v space and area derived from the delineated roi the uncertainty of the segmentation process obtained through a numerical method determines the accuracy of shape description number of fourier harmonics tenfold cross validated classification over a database of 261 color 640 theta480 images was implemented to assess the system performance the ground truth was obtained through immunophenotyping by flow cytometry to provide a natural man machine interface most input commands are bimodal either using t temporal objects for spatio temporal data models and a comparison of their representations abstract currently there are strong efforts to integrate spatial and temporal database technology into spatio temporal database systems this paper views the topic from a rather fundamental perspective and makes several contributions first it reviews existing temporal and spatial data models and presents a completely new approach to temporal data modeling based on the very general notion of temporal object the definition of temporal objects is centered around the observation that anything that changes over time can be expressed as a function over time for the modeling of spatial objects the well known concept of spatial data types is employed as specific subclasses linear temporal and spatial objects are identified second the paper proposes the database embedding of temporal objects by means of the abstract data type adt approach to the integration of complex objects into databases furthermore we make statements about the expressiveness of different temporal and spatial database embeddings third we consider the combination of temporal and spatial objects into spatio temporal objects in relational databases we explain various alternatives for spatio temporal data models and databases and compare their expressiveness spatio temporal objects turn out to be specific instances of temporal objects 1 a control architecture for flexible internet auction servers the flexibility to support both high activity and low activity auctions is required by any system that allows bidding by both humans and software agents we present the control architecture of the michigan internet auctionbot and discuss some of the system engineering issues that arose in its design 1 introduction the michigan internet auctionbot is a highly configurable auction server built to support research on electronic commerce and multiagent negotiation 3 the first generation architecture was simple and robust and allowed us to concentrate on other aspects of the system however several inefficiencies made it problematic to run auctions with very fast interactions we have redesigned the core auctionbot architecture in order to improve overall performance while still meeting the original goal a system that is configurable maintainable and capable of conducting a large number of simultaneous auctions in auctionbot architecture nomenclature we say an auction is open how do program understanding tools affect how programmers understand programs in this paper we explore the question of whether program understanding tools enhance or change the way that programmers understand programs the strategies that programmers use to comprehend programs vary widely program understanding tools should enhance or ease the programmer s preferred strategies rather than impose a fixed strategy that may not always be suitable we present observations from a user study that compares three tools for browsing program source code and exploring software structures in this study 30 participants used these tools to solve several high level program understanding tasks these tasks required a broad range of comprehension strategies we describe how these tools supported or hindered the diverse comprehension strategies used keywords fisheye views program comprehension program understanding tools reverse engineering software maintenance software visualization user study 1 introduction program understanding tools should help programmers to a web agent for the maintenance of a database of academic contacts this paper to present all details of the confidence rules for academia however these are fully discussed in magnanelli 1997 it is important to point out that we consider the information in the web not only as free to use but also as true and updated the agent is unable to detect that information is wrong in the case that the correct information is not available 6 user interaction the thisl broadcast news retrieval system this paper described the thisl spoken document retrieval system for british and north american broadcast news the system is based on the abbot large vocabulary speech recognizer using a recurrent network acoustic model and a probabilistic text retrieval system we discuss the development of a realtime british english broadcast news system and its integration into a spoken document retrieval system detailed evaluation is performed using a similar north american broadcast news system to take advantage of the trec sdr evaluation methodology we report results on this evaluation with particular reference to the effect of query expansion and of automatic segmentation algorithms 1 introduction thisl is an esprit long term research project in the area of speech retrieval it is concerned with the construction of a system which performs good recognition of broadcast speech from television and radio news programmes from which it can produce multimedia indexing data the principal obj an experimental clp platform for integrity constraints and abduction integrity constraint and abduction are important in query answering systems for enhanced query processing and for expressing knowledge in databases a straightforward characterization of the two is given in a subset of the language chr originally intended for writing constraint solvers to be applied for clp languages this subset has a strikingly simple computational model that can be executed using existing prolog based technology together with earlier results this confirms chr as a multiparadigm platform for experimenting with combinations of top down and bottom up evaluation disjunctive databases and as shown here integrity constraint and abduction 1 introduction constraint logic programming clp 10 is established as an extension to logic programming that adds higher expressibility and in some cases more efficient query evaluation clp has also given rise to a field of constraint databases 14 in the present paper we suggest clp techniques applied for defini the decor toolbox for workflow embedded organizational memory access we shortly motivate the idea of business process oriented knowledge management bpokm and sketch the basic approaches to achieve this goal then we describe the decor delivery of context sensitive organisational knowledge project which develops tests and consolidates new methods and tools for bpokm decor builds upon the knowmore framework abecker et al 1998 abecker et al 2000 for organizational memories om but tries to overcome some limitations of this approach in the decor project three end user environments serve as test beds for validation and iterative improvement of innovative approaches to build 1 knowledge archives organised around formal representations of business processes to facilitate navigation and access 2 active information delivery services which in collaboration with a workflow tool to support weaklystructured knowledge intensive work offer the user in a context sensitive manner helpful information from the knowledge archive and 3 on maximum clique problems in very large graphs we present an approach for clique and quasi clique computations in very large multi digraphs we discuss graph decomposition schemes used to break up the problem into several pieces of manageable dimensions a semiexternal greedy randomized adaptive search procedure grasp for finding approximate solutions to the maximum clique problem and maximum quasiclique problem in very large sparse graphs is presented we experiment with this heuristic on real data sets collected in the telecommunications industry these graphs contain on the order of millions of vertices and edges 1 introduction the proliferation of massive data sets brings with it a series of special computational challenges many of these data sets can be modeled as very large multidigraphs m with a special set of edge attributes that represent special characteristics of the application at hand 1 understanding the structure of the underlying digraph d m is essential for storage organization and information retrieval collection and exploitation of expert knowledge in web assistant systems recent research and commercial developments have highlighted the importance of human involvement in user support for web information systems in our earlier work a web assistant system has been introduced which is a hybrid support system with human web assistants and computer based support an important issue with web assistant systems is how to make optimal use of these support resources we use a knowledge management approach with frequently asked questions for a question answering system that acts as a question filter for the human assistants knowledge is continuously collected from the assistants and exploited to augment the question answering capabilities our system has been deployed and evaluated by an analysis of conversation logs and questionnaires for users and assistants the results show that our approach is feasible and useful lessons learned are summarised in a set of recommendations representing and querying xml with incomplete information we study the representation and querying of xml with incomplete information we consider a simple model for xml data and their dtds a very simple query language and a representation system for incomplete information in the spirit of the representations systems developed by imielinski and lipski for relational databases in the scenario we consider the incomplete information about an xml document is continuously enriched by successive queries to the document we show that our representation system can represent partial information about the source document acquired by successive queries and that it can be used to intelligently answer new queries we also consider the impact on complexity of enriching our representation system or query language with additional features the results suggest that our approach achieves a practically appealing balance between expressiveness and tractability the research presented here was motivated by the xyleme project at inria whose objectiveittodevelop a data warehouse for web xml documents 1 a logical view of structured files f3 733e 05 structured data stored in files can benefit from standard database technology in particular we show here how such data can be queried and updated using declarative database languages we introduce the notion of f3 967e 05 structuring f3 733e 05 schema which consists of a grammar annotated with database programs based on a structuring schema a file can be viewed as a database structure queried and updated as such for f3 967e 05 f3 733e 05 queries we show that almost standard database optimization techniques can be used to answer queries without having to construct the entire database for f3 967e 05 f3 733e 05 updates we study in depth the propagation to the file of an update specified on the database view of this file the problem is not feasible in general and we present a number of negative results the positive results consist of techniques that allow to propagate updates efficiently under some reasonable f3 967e 05 locality f3 733e 05 conditions on structure identification of fuzzy classifiers for complex and high dimensional problems data driven identification of classifiers has to deal with structural issues like the selection of the relevant features and effective initial partition of the input domain therefore the identification of fuzzy classifiers is a challenging topic decision tree dt generation algorithms are effective in feature selection and extraction of crisp classification rules hence they can be used for the initialization of fuzzy systems because fuzzy classifiers have much flexible decision boundaries than dts fuzzy models can be more parsimonious than dts hence to get compact easily interpretable and transparent classification system a new structure identification algorithm is proposed where genetic algorithm ga based parameter optimization of the dt initialized fuzzy sets is combined with similarity based rule base simplification algorithms the performance of the approach is studied on a specially designed artificial data an application to the cancer classification problem is also shown modified gath geva fuzzy clustering for identification of takagi sugeno fuzzy models the construction of interpretable takagi sugeno ts fuzzy models by means of clustering is addressed first it is shown how the antecedent fuzzy sets and the corresponding consequent parameters of the ts model can be derived from clusters obtained by the gath geva algorithm to preserve the partitioning of the antecedent space linearly transformed input variables can be used in the model this may however complicate the interpretation of the rules to form an easily interpretable model that does not use the transformed input variables a new clustering algorithm is proposed based on the expectation maximization em identification of gaussian mixture models this new technique is applied to two well known benchmark problems the mpg miles per gallon prediction and a simulated second order nonlinear process the obtained results are compared with results from the literature data driven generation of compact accurate and linguistically sound fuzzy classifiers based on a decision tree initialization the data driven identification of fuzzy rule based classifiers for high dimensional problems is addressed a binary decision tree based initialization of fuzzy classifiers is proposed for the selection of the relevant features and e ective initial partitioning of the input domains of the fuzzy system fuzzy classifiers have more flexible decision boundaries than decision trees dts and can therefore be more parsimonious hence the decision tree initialized fuzzy classifier is reduced in an iterative scheme by means of similarity driven rule reduction to improve classification performance of the reduced fuzzy system a genetic algorithm with a multi objective criterion searching for both redundancy and accuracy is applied the proposed approach is studied for i an artificial problem ii the wisconsin breast cancer classification preprint submitted to elsevier science 19 may 2002 problem and iii a summary of results is given for a set of well known classification problems available from the internet iris ionospehere glass pima and wine data charting past present and future research in ubiquitous computing the proliferation of computing into the physical world promises more than the ubiquitous availability of computing infrastructure it suggests new paradigms of interaction inspired by constant access to information and computational capabilities for the past decade application driven research in ubiquitous computing ubicomp has pushed three interaction themes natural interfaces context aware applications and automated capture and access to chart a course for future research in ubiquitous computing we review the accomplishments of these efforts and point to remaining research challenges research in ubiquitous computing implicitly requires addressing some notion of scale whether in the number and type of devices the physical space of distributed computing or the number of people using a system we posit a new area of applications research everyday computing focussed on scaling interaction with respect to time just as pushing the availability of computing away from the traditional desktop fundamentally changes the relationship between humans and computers providing continuous interaction moves computing from a localized tool to a constant companion designing for continuous interaction requires addressing interruption and resumption of interaction representing passages of time and providing associative storage models inherent classroom 2000 an experiment with the instrumentation of a living educational environment one potentially useful feature of future computing environments is the ability to capture the live experiences of the occupants and to provide that record to users for later access and review over the last 3 years we have designed and extensively used a particular instrumented environment the classroom designed to facilitate the easy capture of the traditional lecture experience we will describe the history of the classroom 2000 project at georgia tech and provide results of extended evaluations of the impact of automated capture on the teaching and learning experience in addition to understanding the impact of automated capture in this educational domain there are many important lessons to take away from this long term largescale experiment with a living ubiquitous computing environment the environment needs to address issues of scale and extensibility and there needs to be a way to continuously evaluate the effectiveness of the environment and understand and react to the w hybrid heuristics for optimal design of artificial neural networks designing the architecture and correct parameters for the learning algorithm is a tedious task for modeling an optimal artificial neural network ann which is smaller faster and with a better generalization performance in this paper we explain how a hybrid algorithm integrating genetic algorithm ga simulated annealing sa and other heuristic procedures can be applied for the optimal design of an ann this paper is more concerned with the understanding of current theoretical developments of evolutionary artificial neural networks eanns using gas and how the proposed hybrid heuristic procedures can be combined to produce an optimal ann the proposed meta heuristic can be regarded as a general framework for adaptive systems that is systems that can change their connection weights architectures and learning rules according to different environments without human intervention optimal design of neural nets using hybrid algorithms selection of the topology of a network and correct parameters for the learning algorithm is a tedious task for designing an optimal artificial neural network ann which is smaller faster and with a better generalization performance genetic algorithm ga is an adaptive search technique based on the principles and mechanisms of natural selection and survival of the fittest from natural evolution simulated annealing sa is a global optimization algorithm that can process cost functions possessing quite arbitrary degrees of nonlinearities discontinuities and stochasticity but statistically assuring a optimal solution in this paper we explain how a hybrid algorithm integrating the desirable aspects of ga and sa can be applied for the optimal design of an ann this paper is more concerned with the understanding of current theoretical developments of evolutionary artificial neural networks eanns using gas and other heuristic procedures and how the proposed hybrid and other heuristic procedures can be combined to produce an optimal ann beyond fitts law models for trajectory based hci tasks trajectory based interactions such as navigating through nested menus drawing curves and moving in 3d worlds are becoming common tasks in modern computer interfaces users performances in these tasks cannot be successfully modeled with fitts law as it has been applied to pointing tasks therefore we explore the possible existence of robust regularities in trajectory based tasks we used steering through tunnels as our experimental paradigm to represent such tasks and found that a simple steering law indeed exists the paper presents the motivation analysis a series of four experiments and the applications of the steering law active disks several application and technology trends indicate that it might be both profitable and feasible to move computation closer to the data that it processes in this paper we evaluate active disk architectures which integrate significant processing power and memory into a disk drive and allow application specific code to be downloaded and executed on the data that is being read from written to disk the key idea is to offload bulk of the processing to the disk resident processors and to use the host processor primarily for coordination scheduling and combination of results from individual disks to program active disks we propose a stream based programming model which allows disklets to be executed efficiently and safely simulation results for a suite of seven algorithms from three application domains commercial data warehouses image processing and satellite data processing indicate that for these algorithms active disks outperform conventional disk architectures 1 introduction selectivity estimation in spatial databases selectivity estimation of queries is an important and wellstudied problem in relational database systems in this paper we examine selectivity estimation in the context of geographic information systems which manage spatial data such as points lines poly lines and polygons in particular we focus on point and range queries over two dimensional rectangular data we propose several techniques based on using spatial indices histograms binary space partitionings bsps and the novel notion of spatial skew our techniques carefully partition the input rectangles into subsets and approximate each partition accurately we present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques we evaluate them using synthetic as well as real life tiger datasets based on our experiments we identify a bsp based partitioning that we call min skew which consistently provides the most accurate selectivity estimates for spatial queries stable haptic interaction with virtual environments a haptic interface is a kinesthetic link between a human operator and a virtual environment this paper addresses fundamental stability and performance issues associated with haptic interaction it generalizes and extends the concept of a virtual coupling network an artificial link between the haptic display and a virtual world to include both the impedance and admittance models of haptic interaction a benchmark example exposes an important duality between these two cases linear circuit theory is used to develop necessary and sufficient conditions for the stability of a haptic simulation assuming the human operator and virtual environment are passive these equations lead to an explicit design procedure for virtual coupling networks which give maximum performance while guaranteeing stability by decoupling the haptic display control problem from the design of virtual environments the use of a virtual coupling network frees the developer of haptic enabled virtual reality models fr integrating a modern knowledge based system architecture with a legacy va database the athena and eon projects at stanford ion output guideline based treatment advice guideline kb server protg psm guideline interpreter eon pca psm guideline based quality assessment medcritic psm va medical record vista dhcp enduser gui cprs enduser gui cprs legacy database mediator athenaeum psm output assessment of physician actions tzolkin dbms psm temporal abstraction tzolkin dbms psm temporal abstraction output guideline based treatment advice figure 1 athena system incorporating the eon architecture for component based decision support each eon component carries out a specific problemsolving task for automated decision support of guideline based care advani et al 2 in the eon architecture to carry it out such tasks include the monitoring the execution of an applied guideline using the eon protocol compliance advisor pca psm and assessing the quality of the guideline based treatment using the medcritic psm see figure 1 the athenaeum mediator all th generalized isolation level definitions commercial databases support different isolation levels to allow programmers to trade off consistency for a potential gain in performance the isolation levels are defined in the current ansi standard but the definitions are ambiguous and revised definitions proposed to correct the problem are too constrained since they allow only pessimistic locking implementations this paper presents new specifications for the ansi levels our specifications are portable they apply not only to locking implementations but also to optimistic and multi version concurrency control schemes furthermore unlike earlier definitions our new specifications handle predicates in a correct and flexible manner at all levels 1 introduction this paper gives new precise definitions of the ansisql isolation levels 6 unlike previous proposals 13 6 8 the new definitions are both correct they rule out all bad histories and implementation independent our specifications allow a wide range of concurr answering queries using views with arithmetic comparisons we consider the problem of answering queries using views where queries and views are conjunctive queries with arithmetic comparisons cqacs over dense orders previous work only considered limited variants of this problem without giving a complete solution we have developed a novel algorithm to obtain maximally contained rewritings mcrs for queries having left or right semi interval comparison predicates for semi interval queries we show that the language of finite unions of cqac rewritings is not sufficient to find a maximally contained solution and identify cases where datalog is sufficient finally we show that it is decidable to obtain equivalent rewritings for cqac queries time responsive indexing schemes for moving points we develop new indexing schemes for storing a set of points in one or two dimensions each moving along a linear trajectory so that a range query at a given future time t q can be answered efficiently the novel feature of our indexing schemes is that the number of i os required to answer a query depends not only on the size of the data set and on the number of points in the answer but also on the difference between t q and the current time queries close to the current time are answered fast while queries that are far away in the future or in the past may take more time center for geometric computing department of computer science duke university durham nc 27708 usa supported in part by army research office muri grant daah04 96 1 0013 by a sloan fellowship by nsf grants eia 9870724 eia 997287 and ccr 9732787 and by grant from the u s israeli binational science foundation email pankaj cs duke edu y center for geometric computing department of computer sci intelligent crawling on the world wide web with arbitrary predicates the enormous growth of the world wide web in recent years has made it important to perform resource discovery efficiently consequently several new ideas have been proposed in recent years among them a key technique is focused crawling which is able to crawl particular topical portions of the world wide web quickly without having to explore all web pages in this paper we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the world wide web while performing the crawling specifically the intelligent crawler uses the inlinking web page content candidate url structure or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl this is a much more general framework than the focused crawling technique which is based on a pre defined understanding of the topical structure of the web the techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user defined predicates such as topical queries keyword queries or any combinations of the above unlike focused crawling it is not necessary to provide representative topical examples since the crawler can learn its way into the appropriate topic we refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure the learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates learning search engine specific query transformations for question answering we introduce a method for learning query transformations that improves the ability to retrieve answers to questions from an information retrieval system during the training stage the method involves automatically learning phrase features for classifying questions into different types automatically generating candidate query transformations from a training set of question answer pairs and automatically evaluating the candidate transforms on target information retrieval systems such as real world general purpose search engines at run time questions are transformed into a set of queries and re ranking is performed on the documents retrieved we present a prototype search engine tritus that applies the method to web search engines blind evaluation on a set of real queries from a web search engine log shows that the method significantly outperforms the underlying web search engines as well as a commercial search engine specializing in question answering keywords web search quer intelligent retrieval of digital images from large geospatial databases in this paper we present the development of a spatial data management system utilizing sketch based queries for the content based retrieval of digital images from topographic databases we discuss our overall strategy and associated algorithmic and implementational aspects and present the associated database design issues the query tools devised in this research are employing user provided sketches of the shape and spatial configuration of the object s which should appear in the images to be retrieved our strategy is scaleindependent it is inspired by least squares matching lsm and represents an extension of lsm to function with a variety of raster representations the results are ranked according to statistical scores and the user can subsequently narrow or broaden his her search according to the previously obtained results and the purpose of the search 1 introduction intelligent image retrieval from large databases is one of the novel applications which are receiving increa athena mining based interactive management of text databases abstract we describe athena a system for creating exploiting and maintaining a hierarchy of textual documents through interactive miningbased operations requirements of any such system include speed and minimal end user e ort athena satis es these requirements through linear time classi cation and clustering engines which are applied interactively to speed the development of accurate models naive bayes classi ers are recognized to be among the best for classifying text we show that our specialization of the naive bayes classi er is considerably more accurate 7 to 29 absolute increase in accuracy than a standard implementation our enhancements include using lidstone s law of succession instead of laplace s law under weighting long documents and over weighting author and subject we also present a new interactive clustering algorithm c evolve for topic discovery c evolve rst nds highly accurate cluster digests partial clusters gets user feedback to merge and correct these digests and then uses the classi cation algorithm to complete the partitioning of the data by allowing this interactivity in the clustering process c evolve achieves considerably higher clustering accuracy 10 to 20 absolute increase in our experiments than the popular k means and agglomerative clustering methods 1 querying xml documents in xyleme this paper on query processing the query language supporting xyleme graphical user interface is an extension of oql 6 10 and provides a mix of database and information retrieval characteristics it is consistent with the requirements published by the w3c xml query working group 20 and similar to many languages proposed by the database community for text or semistructured databases 1 5 7 11 still due to the fact that we are considering millions of documents query processing acquires in xyleme a truly distinct flavor although we rely mainly on database optimization techniques notably the use of an algebra as in 5 8 9 12 15 we need to adapt them to meet new and interesting challenges case based learning algorithms abstract storing and using specific instances improves the performance of several supervised learning algorithms these include algorithms that learn decision trees classification rules and distributed networks however no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks in this paper we describe a framework and methodology called instance based learning that generates classification predictions using only specific instances instance based learning algorithms do not maintain a set of abstractions derived from specific instances this approach extends the nearest neighbor algorithm which has large storage requirements we describe how storage requirements can be significantly reduced with at most minor sacrifices in learning rate and classification accuracy while the storage reducing algorithm performs well on several realworld databases its performance degrades rapidly with the level of attribute noise in training instances therefore we extended it with a significance test to distinguish noisy instances this extended algorithm s performance degrades gracefully with increasing noise levels and compares favorably with a noise tolerant decision tree algorithm case based learning beyond classification of feature vectors the dominant theme of case based research at recent ml conferences has been on classifying cases represented by feature vectors however other useful tasks can be targeted and other representations are often preferable we review the recent literature on case based learning focusing on alternative performance tasks and more expressive case representations we also highlight topics in need of additional research 1 introduction the majority of machine learning ml research has focussed on supervised learning tasks in which class labeled cases each represented as a vector of features are given to a learning algorithm that induces a concept description this description can then be used to predict the class labels of unlabeled cases one approach for solving supervised learning tasks called case based 3 involves storing cases often as hproblem solutioni pairs and retrieving them to solve similar problems this distinguishes their behavior from approaches that greedily repla learning to refine case libraries initial results conversational case based reasoning cbr systems which incrementally extract a query description through a user directed conversation are advertised for their ease of use however designing large case libraries that have good performance i e precision and querying efficiency is difficult cbr vendors provide guidelines for designing these libraries manually but the guidelines are difficult to apply we describe an automated inductive approach that revises conversational case libraries to increase their conformance with design guidelines revision increased performance on three conversational case libraries 1 introduction in the context of the ecml 97 workshop entitled case based learning beyond classification of feature vectors this paper s contribution focuses on using machine learning methods to assist in the design of case libraries these libraries are designed for solution retrieval rather than classification tasks and each case might contain a unique solution cas feature weighting for lazy learning algorithms learning algorithms differ in the degree to which they process their inputs prior to their use in performance tasks many algorithms eagerly compile input samples and use only the compilations to make decisions others are lazy they perform less precompilation and use the input samples to guide decision making the performance of many lazy learners significantly degrades when samples are defined by features containing little or misleading information distinguishing feature relevance is a critical issue for these algorithms and many solutions have been developed that assign weights to features this chapter introduces a categorization framework for feature weighting approaches used in lazy similarity learners and briefly surveys some examples in each category 1 1 introduction lazy learning algorithms are machine learning algorithms mitchell 1997 that are welcome members of procrastinators anonymous purely lazy learners typically display the following characteristics aha 19 the omnipresence of case based reasoning in science and application a surprisingly large number of research disciplines have contributed towards the development of knowledge on lazy problem solving which ischaracterized by its storage of ground cases and its demand driven response to queries case based reasoning cbr is an alternative increasingly popular approach for designing expert systems that implements this approach this paper lists pointers to some contributions in some related disciplines that o er insights for cbr research we then outline a small number of navy applications based on this approach that demonstrate its breadth of applicability finally we list a few successful and failed attempts to apply cbr and list some predictions on the future roles of cbr in applications 1 case based reasoning case based reasoning cbr is a multi disciplinary subject that focuses on the reuse of experiences i e cases it is di cult to nd consensus on more detailed de nitions of cbr because it means di erent things to di erent groups of people for example consider its interpretation by the following three groups cognitive scientists cbr is a plausible high level model for cognitive processing kolodner scalable consistency protocols for distributed services abstract a common way to address scalability requirements of distributed services is to employ server replication and client caching of objects that encapsulate the service state the performance of such a system could depend very much on the protocol implemented by the system to maintain consistency among object copies we explore scalable consistency protocols that never require synchronization and communication between all nodes that have copies of related objects we achieve this by developing a novel approach called local consistency lc lc based protocols can provide increased flexibility and efficiency by allowing nodes control over how and when they become aware of updates to cached objects we develop two protocols for implementing strong consistency using this approach and demonstrate that they scale better than a traditional invalidation based consistency protocol along the system load and geographic distribution dimensions of scale index terms scalable services distributed objects replication caching consistency protocols 1 techniques for automatic digital video composition recent developments in digital technology have enabled a class of video based applications that were not previously viable however digital video production systems face the challenge of accessing the inherently linear and time dependent media of audio and video and providing effective means of composing them into a cohesive piece for presentation moreover there are no appropriate metrics that allow for assessment of the quality of an automatically composed video piece techniques presently available are limited in scope and do not account for all the features of a composition this dissertation presents metrics that evaluate the quality of a video composition in addition it proposes techniques for automatic composition of video presentations as well as improvements in access to digital video data yet another challenge faced by video production systems is the customization of the presentation to suit user profiles for instance certain elements of video compositions such as v ontological overhearing the collaboration between two intelligent agents can be greatly enhanced if a third agent who has some understanding of the communication between the first two intervenes giving appropriate information or acting helpfully without having been explicitly involved the behavior of this third agent quite common in human interaction is called overhearing we present an agent architecture modeling this behavior in particular we focus on overhearing based on ontological reasoning that is the overhearer semantically selects pieces of communication according to his own knowledge ontologically organized and goals abc an intelligent robocup agent this paper will cover the basics of our virtual soccer team abc aiken benzacar cocosco it provides an overview of the agent s architecture and in depth explanation of its various modules the agent s approach to achieving intelligent behavior as contrasted with other approaches is discussed i introduction the international robocup competition is held yearly and pits teams of simulated soccer playing agents against each other the simulated environment in which the agents interact is designed to reproduce the challenges associated with an embodied agent the sensory information provided to each agent is incomplete noisy and sporadic with this imperfect data as a base the agents are challenged to behave as a cohesive team in attempting to win their matches this paper discusses the design and implementation of an artificially intelligent robocup agent abc its overall design is modeled after the architecture used by the winning team of the 1997 robocup competition as textural features for image database retrieval this paper presents two feature extraction methods and two decision methods to retrieve images having some section in them that is like the user input image the features used are variances of gray level co occurrences and line angle ratio statistics constituted by a 2 d histogram of angles between two intersecting lines and ratio of mean gray levels inside and outside the regions spanned by those angles the decision method involves associating with any pair of images either the class relevant or irrelevant a gaussian classifier and nearest neighbor classifier are used a protocol that translates a frame throughout every image to automatically define for any pair of images whether they are in the relevance class or the irrelevance class is discussed experiments on a database of 300 gray scale images with 9 600 groundtruth image pairs showed that the classifier assigned 80 of the image pairs we were sure were relevant to the relevance class correctly the actual retrieval accuracy is greater than this lower bound of 80 pattern search methods for molecular geometry problems this paper deals with the application of pattern search methods to the numerical solution of a class of molecular geometry problems with important applications in molecular physics and chemistry the goal is to nd a conguration of a cluster or a molecule with minimum total energy the minimization problems in this class of geometry molecular problems have no constraints and the objective function is smooth the diculties arise from the existence of several local minima and especially from the expensive function evaluation total energy and the possible non availability of rst order derivatives we introduce a pattern search approach that attempts to exploit the physical nature of the problem by using energy lowering geometrical transformations numerical results with a particular instance of this new class of pattern search methods and with the parallel direct search method of dennis and torczon are presented showing the promise of our approach key words molecular dynamic logic programming in this paper we investigate updates of knowledge bases represented by logic programs in order to represent negative information we use generalized logic programs which allow default negation not only in rule bodies but also in their heads we start by introducing the notion of an update p oplus u of a logic program p by another logic program u subsequently we provide a precise semantic characterization of p oplus u and study some basic properties of program updates in particular we show that our update programs generalize the notion of interpretation update we then extend this notion to compositional sequences of logic programs updates p 1 oplus p 2 oplus dots defining a dynamic program update and thereby introducing the paradigm of emph dynamic logic programming this paradigm significantly facilitates modularization of logic programming and thus modularization of non monotonic reasoning as a whole specifically suppose that we are given a set of logic program modules each describing a different state of our knowledge of the world different states may represent different time points or different sets of priorities or perhaps even different viewpoints consequently program modules may contain mutually contradictory as well as overlapping information the role of the dynamic program update is to employ the mutual relationships existing between different modules to precisely determine at any given module composition stage the declarative as well as the procedural semantics of the combined program resulting from the modules incremental maintenance of materialized oql views the importance of materialized views has grown significantly with the advent of data warehousing and olap technology this increases the relevance of solutions to the problem of incrementally maintaining materialized views so far most work on this problem has been confined to relational settings proposals that apply to object databases have either used non standard models or fallen short of providing a comprehensive framework this paper contributes a solution to the incremental view maintenance problem for a large class of views expressed in oql the query language of the odmg standard for object databases the solution applies to immediate update propagation and works for any update operation on views de ned over a substantial subset of odmg types the approach presented has been fully implemented and preliminary performance results are reported implementing schema theoretic models of animal behavior in robotic systems formal models of animal sensorimotor behavior can provide effective methods for generating robotic intelligence in this paper we describe how schema theoretic models of the praying mantis are implemented on a hexapod robot equipped with a real time color vision system the model upon which the implementation is based was developed by ethologists studying mantids this implementation incorporates a wide range of behaviors including obstacle avoidance prey acquisition predator avoidance mating and chantlitaxia behaviors 1 introduction ecological robotics refers to incorporating aspects of the relationship a robot maintains with its environment into its control system i e its ecology 4 one means for developing such a control system is by exploiting models of behavior developed by ethologists or neuroscientists although considerable research has been conducted in the modeling of neural controllers based on animal models e g 3 5 14 incorporation of environmental int control states and motivated agency one of the challenges faced by researchers in the behaviour modelling of life like characters is the need to develop a systematic framework in which to ask questions about the types of internal state life like characters might possess and how those different states interact we propose a solution based on a cognitively inspired multi layered agent architecture composed of reactive deliberative and meta management layers and a recursive design based research methodology wherein each new design gradually increases our explanatory power and allows us to account for more and more of the phenomena of interest by describing a variety of broad but shallow complete agents at the information level and showing how these designs realise mental states and processes we aim to provide a rich and deep explanatory framework from which to explore motivated autonomous agency early experiments have concentrated on a the requirements of goal processing b the emergence of analyzing web robots and their impact on caching understanding the nature and the characteristics of web robots is an essential step to analyze their impact on caching using a multi layer hierarchical workload model this paper presents a characterization of the workload generated by autonomous agents and robots this characterization focuses on the statistical properties of the arrival process and on the robot behavior graph model a set of criteria is proposed for identifying robots in real logs we then identify and characterize robots from real logs applying a multi layered approach using a stack distance based analytical model for the interaction between robots and web site caching we assess the impact of robots requests on web caches our analyses point out that robots cause a significant increase in the miss ratio of a server side cache robots have a referencing pattern that completely disrupts locality assumptions these results indicate not only the need for a better understanding of the behavior of robots but also the need of web caching policies that treat robots requests differently than human generated requests transforming paper documents into xml format with wisdom the transformation of scanned paper documents to a form suitable for an internet browser is a complex process that requires solutions to several problems the application of an ocr to some parts of the document image is only one of the problems in fact the generation of documents in html format is easier when the layout structure of a page has been extracted by means of a document analysis process the adoption of an xml format is even better since it can facilitate the retrieval of documents in the web nevertheless an effective transformation of paper documents into this format requires further processing steps namely document image classification and understanding wisdom is a document processing system that operates in five steps document analysis document classification document understanding text recognition with an ocr and text transformation into html xml format the innovative aspects described in the paper are the preprocessing algorithm the adaptive page segmen using case based reasoning for supporting continuous improvement processes the goal of the ipqm project a collaboration of the fraunhofer institute for manufacturing engineering and automation ipa in stuttgart and the fraunhofer institute for experimental software engineering iese in kaiserslautern is to develop a technical infrastructure to support continuous improvement processes we describe the approach we took in some detail and focus on the implementation of the ipqm system and its currently ongoing evaluation in the healthcare sector we also give an outlook on intended extensions of the system and its application in other domains web metasearch as belief aggregation web metasearch requires a mechanism for combining rank ordered lists of ratings returned by multiple search engines in response to a given user query we view this as being analogous to the need for combining degrees of belief in probabilistic and uncertain reasoning in artificial intelligence this paper describes a practical method for performing web metasearch based on a novel transformationbased theory of belief aggregation the consensus ratings produced by this method take into account the item ratings rankings output by individual search engines as well as the user s preferences copyright c fl 2000 american association for artificial intelligence www aaai org all rights reserved introduction web search engines wse use tools ranging from simple text based search to more sophisticated methods that attempt to understand the intended meanings of both queries and data items there has been much work in this area in recent years the link structure of the web has obprm an obstacle based prm for 3d workspaces this paper we consider an obstacle based prm supporting image search on the web while pages on the web contain more and more multimedia information such as images videos and audio today search engines are mostly based on textual information there is an emerging need of a new generation of search engines that try to exploit the full multimedia information present on the web the approach presented in this paper is based on a multimedia model intended to describe the various multimedia components their structure and their relationships with a pre defined taxonomy of concepts in order to support the information retrieval process a prototype of an image search engine based on this approach is presented as a first step in this direction and results are discussed this research has been funded by the ec esprit long term research program project no 9141 hermes foundations of high performance multimedia information management systems 1 introduction the wide use of the world wide web www across internet is making of vital importance the proble probabilistic roadmap methods are embarrassingly parallel in this paper we report on our experience parallelizing probabilistic roadmap motion planning methods prms we show that significant scalable speedups can be obtained with relatively little effort on the part of the developer our experience is not limited to prms however in particular we outline general techniques for parallelizing types of computations commonly performed in motion planning algorithms and identify potential difficulties that might be faced in other efforts to parallelize sequential motion planning methods 1 introduction automatic motion planning has application in many areas such as robotics virtual reality systems and computer aided design although many different motion planning methods have been proposed most are not used in practice since they are computationally infeasible except for some restricted cases e g when the robot has very few degrees of freedom dof 12 16 indeed there is strong evidence that any complete planner one that is guaran flexible and scalable cost based query planning in mediators a transformational approach the internet provides access to a wealth of information for any given topic or application domain there are a variety of available information sources however current systems such as search engines or topic directories in the world wide web offer only very limited capabilities for locating combining and organizing information mediators systems that provide integrated access and database like query capabilities to information distributed over heterogeneous sources are critical to realize the full potential of meaningful access to networked information query planning the task of generating a cost efficient plan that computes a user query from the relevant information sources is central to mediator systems however query planning is a computationally hard problem due to the large number of possible sources and possible orderings on the operations to process the data moreover the choice of sources data processing operations and their ordering strongly affects the plan c fast approximate evaluation of olap queries for integrated statistical data we have developed a mediator architecture that integrates statistical information about energy products from several government agencies such as the bureau of labor statistics the energy information administration and the california energy commission our architecture has a dual mode of operation first our system can retrieve live data from databases and web sources from these agencies this allows the users to obtain completely up to date data however for complex analytical queries that typically require large amounts of data and processing live access does not offer the level of interactivity that some users require second our system can warehouse the information from the data sources to allow for complex analytical queries to be executed much more efficiently however the data would be only as recent as the last update to the data warehouse in this paper we describe the architecture and focus on how to perform analytical queries against the data warehouse very efficiently we present results using a fast wavelet based technique for progressive evaluation of range sum queries this technique allows for returning an approximate result to the query very efficiently and for fast convergence to the exact result we envision users exploring many complex queries using the very fast approximate results as guidance and only obtaining the exact results for those queries that are deemed of interest we present experimental results showing the efficiency of both approximate and exact queries 1 simplifying data access the energy data collection edc project the massive amount of statistical and text data available from government agencies has created a set of daunting challenges to both research and analysis communities these problems include heterogeneity size distribution and control of terminology at the digital government research center we are investigating solutions to these key problems in this paper we focus on 1 ontological mappings for terminology standardization 2 data integration across data bases with high speed query processing and 3 interfaces for query input and presentation of results this collaboration between researchers from columbia university and the information sciences institute of the university of southern california employs technology developed at both locations in particular the sensus ontology the sims multi database access planner the lkb automated dictionary and terminology analysis system and others the pilot application targets gasoline data from the bureau of labor statistics the energy information administration of the department of energy the census bureau and other government agencies 1 a model of bdi agent in game theoretic framework a model of bdi agent in game theoretic framework is presented the desire is represented as agent s goal to achieve a maximum level of utility a reasoning process based on agent s rational behavior is proposed this process determines agent s intention it is also shown how to use the backward induction consistently with the assumption of the common knowledge of rationality 1 introduction we are going to discuss the following problem how does a rational agent use its knowledge in decision making since the problem is general we put it in a game theoretic framework in the theory of games agent s rationality is understood as a way of maximizing the utility of the agent relatively to its knowledge the knowledge may concern the game that is to be played as well as the agents participating in a play the main task of the paper is to model bdi agent that is supposed to live in the world of dynamic games agent s belief is identified with the knowledge about the game and abo cooperation mechanisms in a multi agent distributed environment in the paper we present our work on design and analysis of agent cooperation in distributed systems the work is not completed yet so that some parts of it especially the formal framework should be viewed as a preliminary version multi agent systems are represented by bdi automata i e asynchronous automata composed of non deterministic agents equipped with mental attitudes like belief desire and intentions these attitudes are acquired by the agents by executing so called mental actions behaviours of multi agent systems are represented by prime event structures the prime event structure when augmented with utility functions defined on the terminal nodes of the structure may be viewed as games in extensive form defined on local states rather than on global states as in the classical definition the definition of knowledge in our framework captures the change of state due to action executions a notion of local knowledge based protocols is defined a game theory method of backwards induction is applied in order to obtain a very natural definition of rationality in agent s behaviours all the notions are exemplified using the running example for this example we construct several cooperation mechanisms for the agents a team formation mechanism is one of them team formation by self interested mobile agents a process of team formation by autonomous agents in a distributed environment is presented since the environment is distributed there are serious problems with communication and consistent decision making inside a team to deal with these problems the standard technique of token passing in a computer network is applied the passing cycle of the token serves as the communication route it assures consistent decision making inside the team maintaining its organizational integrity on the other hand it constitutes a component of the plan of the cooperative work performed by a complete team two algorithms for team formation are given the first one is based on simple self interested agents that still can be viewed as reactive agents see 14 although augmented with knowledge goal and cooperation mechanisms the second one is based on sophisticated self interested agents moreover the algorithm based on fully cooperative agents which is an adaptation of the static agent virtual organizations within the framework of network computing a case study we study the concept of agent virtual organization and show how it relates to the paradigm of network based computing 28 we also discuss the paradigm of bdi agent trying to show that sophisticated architecture of bdi agent can not be efficiently applied for large worlds as the working example of virtual organization we consider a model of virtual enterprise key words agent virtual organization agent based manufacturing virtual enterprise formation 1 introduction autonomous adaptive and cooperative software mobile agents are well suited for domains that require constant adaptation to changing distributed environment or changing demands actually cyberspace and manufacturing enterprise are such domains so that there is increasing interest in applying agent technologies there cyberspace in the shape of the internet intranets and the world wide web has grown phenomenally in recent years cyberspace now contains enormous amounts of information and is also being increasingly on bounding schemas for ldap directories as our world gets more networked ever increasing amounts of information are being stored in ldap directories while ldap directories have considerable flexibility in the modeling and retrieval of information for network applications the notion of schema they provide for enabling consistent and coherent representation of directory information is rather weak in this paper we propose an expressive notion of bounding schemas for ldap directories and illustrate their practical utility bounding schemas are based on lower bound and upper bound specifications for the content and structure of an ldap directory given a bounding schema specification we present algorithms to efficiently determine i if an ldap directory is legal w r t the bounding schema and ii if directory insertions and deletions preserve legality finally we show that the notion of bounding schemas has wider applicability beyond the specific context of ldap directories 1 introduction x 500 styl agent based distance vector routing mobile agents are being proposed for an increasing variety of applications distance vector routing dvr is an example of one application that can benefit from an agent based approach dvr algorithms such as rip have been shown to cause considerable network resource overhead due to the large number of messages generated at each host router throughout the route update process many of these messages are wasteful since they do not contribute to the route discovery process however in an agent based solution the number of messages is bounded by the number of agents in the system in this paper we present an agent based solution to dvr in addition we will describe agent migration strategies that improve the performance of the route discovery process namely random walk and structured walk dynamic agent population in agent based distance vector routing the intelligent mobile agent paradigm can be applied to a wide variety of intrinsically parallel and distributed applications network routing is one such application that can be mapped to an agent based approach the performance of any agent based system will depend on its agent population although a lot of research has been conducted on agent based systems little consideration has been given to the importance of agent population in dynamic networks a large number of constituent agents can increase the resource overhead of the system thereby impeding the overall performance of the network towards resource efficient and scalable routing an agent based approach mobile agents are being proposed for an increasing variety of applications agent mobility can be exploited to implement a scalable system level solutions network routing is one such domain that can benefit from an agent based approach shortest path routing algorithms enjoy a widespread use in most communication networks however large amounts of routing data exchanged in these algorithms consume substantial bandwidth making conventional routing schemes less scalable agent oriented programming in linear logic this thesis investigates how a linear logic programming language such as lygon can be used in the implementation of agent oriented programs agent oriented programming is a recent computational framework of interest to both academic and industrial researchers agent methodology is being successfully utilised in designing complex distributed applications that require concurrency reasoning communication sharing and integration of knowledge and of course intelligence on the other hand linear logic a logic of resource consumption provides the possibility to construct efficient tools for modelling updates reasoning about the environment and implementing concurrency linear logic has been used as a basis for creating a number of programming languages one of these is the logic programming language lygon the aim of this thesis is to investigate the possibility of implementing agents with lygon a number of experiments have been carried out and results analysed which dynamic function placement for data intensive cluster computing optimally partitioning application andfilesystem functionality within a cluster of clients and servers is a difficult problem dueto dynamic variations in application behavior resource availability and workload mixes thispaper presents a bacus a run time systemthat monitors and dynamically changes function placement for applications that manipulate largedata sets several examples of data intensive workloads are used to show the importance ofproper function placement and its dependence on dynamic run time characteristics withperformance differences frequently reaching 2 10x we evaluate how well the abacusprototype adapts to run time system behavior including both long term variation e g filterselectivity and short term variation e g multi phase applications and inter applicationresource contention our experiments with abacus indicate that it is possible to adapt inall of these situations and that the adaptation converges most quickly in those cases where theperformance impact is most significant 1 highly concurrent shared storage shared storage arrays enable thousands of storage devices to be shared and directly accessed by end hosts over switched system area networks promising databases and filesystems highly scalable reliable storage in such systems hosts perform access tasks read and write and management tasks migration and reconstruction of data on failed devices each task translates into multiple phases of low level device i os so that concurrent host tasks can span multiple shared devices and access overlapping ranges potentially leading to inconsistencies for redundancy codes and for data read by end hosts highly scalable concurrency control and recovery protocols are required to coordinate on line storage management and access tasks while expressing storage level tasks as acid transactions ensures proper concurrency control and recovery such an approach imposes high performance overhead results in replication of work and does not exploit the available knowledge about storage le incommonsense rethinking web search results the world wide web is a rich annotation system which allows people to relate to documents and sites from different perspectives people describe comment relate or mock other web pages in the context of their document this richness is currently not reflected in snippets presented by web search engines where a search result is represented by the text found in the web document alone this paper proposes a new method for representing documents in web search engines results this method is based on recent trends in search engine technology and provides descriptions of the retrieved documents assembled from people s commentary and annotations on the web this paper suggests a new way for automatically retrieving and reusing people s annotations on the web incorporating these annotations into a search engine for creating a hybrid directory search engine allowing for both automatic retrieval and on the fly human authored summaries i introduction it is common knowledge that many use using common hypertext links to identify the best phrasal description of target web documents this paper describes previous work which studied and compared the distribution of words in web documents with the distribution of words in normal flat texts based on the findings from this study it is suggested that the traditional ir techniques cannot be used for web search purposes the same way they are used for normal text collections e g news articles then based on these same findings i will describe a new document description model which exploits valuable anchor text information provided on the web that is ignored by the traditional techniques the problem amitay 1997 has found through a corpus analysis of a 1000 web pages that the lexical distribution in documents which were written especially for the web home pages is significantly different than the lexical distribution observed in a corpus of normal english language the british national corpus 100 000 000 words for example in the web documents collection there were some html files which contained no v on the extension of uml with use case maps concepts descriptions of reactive systems focus heavily on behavioral aspects often in terms of scenarios to cope with the increasing complexity of services provided by these systems behavioral aspects need to be handled early in the design process with flexible and concise notations as well as expressive concepts uml offers different notations and concepts that can help describe such services however several necessary concepts appear to be absent from uml but present in the use case map ucm scenario notation in particular use case maps allow scenarios to be mapped to different architectures composed of various component types the notation supports structured and incremental development of complex scenarios at a high level of abstraction as well as their integration ucms specify variations of run time behavior and scenario structures through sub maps pluggable into placeholders called stubs this paper presents how ucm concepts could be used to extend the semantics a reliable multicast protocol for distributed mobile systems design and evaluation abstract reliable multicast is a powerful communication primitive for structuring distributed programs in which multiple processes must closely cooperate together in this paper we propose a protocol for supporting reliable multicast in a distributed system that includes mobile hosts and evaluate the performance of our proposal through simulation we consider a scenario in which mobile hosts communicate with a wired infrastructure by means of wireless technology our proposal provides several novel features the sender of each multicast may select among three increasingly strong delivery ordering guarantees fifo causal total movements do not trigger the transmission of any message in the wired network as no notion of hand off is used the set of senders and receivers group may be dynamic size of data structures at mobile hosts size of message headers number of messages in the wired network for each multicast are all independent on the number of group members the wireless network is assumed to provide only incomplete spatial coverage and message losses could occur even within cells movements are not negotiated and a mobile host that leaves a cell may enter any other cell perhaps after a potentially long disconnection the simulation results show that the proposed protocol has good performance and good scalability properties 1 what semiotics can and cannot do for hci semiotics is the mathematics of the humanities in the sense that it provides an abstract language covering a diversity of special sign usages language pictures movies theatre etc in this capacity semiotics is helpful for bringing insights from older media to the task of interface design and for defining the special characteristics of the computer medium however semiotics is not limited to interface design but may also contribute to the proper design of program texts and yield predictions about the interaction between computer systems and their context of use keywords computer based signs aesthetics context of use 0 the mathematics of the humanities in my experience semiotics can be useful for the hci field but the purely analytic character of traditional semiotics has to be supplemented by a constructive one in addition the semiotic community has to acquire a solid understanding of the technical possibilities and limitations of computer systems in order to b intelligent agents a new technology for future distributed sensor systems this master thesis deals with intelligent agents and the possibility to use the intelligent agent technology in future distributed sensor systems the term future distributed sensor system refers to a system based on several sensors that will be developed within a period of five to ten years since researchers have not agreed on a more precise definition of intelligent agents we first examined what constitutes an intelligent agent and made a definition suited for our application domain we used our definition as a base for investigating if and how intelligent agents can be used in future distributed sensor systems we argue that it is not interesting to come up with a general agent definition applicable to every agent instead one should make a foundation for a definition when this is done we can decide on more specific features depending on the task the agent will perform and in what domain the agent will work in finally we conclude that it is possible to use the agent technology i mostly unsupervised statistical segmentation of japanese kanji sequences given the lack of word delimiters in written japanese word segmentation is generally considered a crucial first step in processing japanese texts typical japanese segmentation algorithms rely either on a lexicon and syntactic analysis or on pre segmented data but these are labor intensive and the lexico syntactic techniques are vulnerable to the unknown word problem in contrast we introduce a novel more robust statistical method utilizing unsegmented training data despite its simplicity the algorithm yields performance on long kanji sequences comparable to and sometimes surpassing that of state of the art morphological analyzers over a variety of error metrics the algorithm also outperforms another mostly unsupervised statistical algorithm previously proposed for chinese additionally we present a two level annotation scheme for japanese to incorporate multiple segmentation granularities and introduce two novel evaluation metrics both based on the notion of a compatible bracket that can account for multiple granularities simultaneously integrating reactive and scripted behaviors in a life like presentation agent animated agents based either on real video cartoon style drawings or even model based 3d graphics offer great promise for computer based presentations as they make presentations more lively and appealing and allow for the emulation of conversation styles known from human human communication in this paper we describe a life like interface agent which presents multimedia material to the user following the directives of a script the overall behavior of the presentation agent is partly determined by such a script and partly by the agent s self behavior in our approach the agent s behavior is defined in a declarative specification language behavior specifications are used to automatically generate a control module for an agent display system the first part of the paper describes the generation process which involves ai planning and a two step compilation since the manual creation of presentation scripts is tedious and error prone we also address the automated generation of presentation scripts which may be forwarded to the interface agent the second part of the paper presents an approach for multimedia presentation design which combines hierarchical planning with temporal reasoning 1 1 keywords human like qualities of synthetic agents life like qualities presentation agents integrating models of personality and emotions into lifelike characters a growing number of research projects in academia and industry have recently started to develop lifelike agents as a new metaphor for highly personalised human machine communication a strong argument in favour of using such characters in the interface is the fact that they make humancomputer interaction more enjoyable and allow for communication styles common in human human dialogue in this paper we discuss three ongoing projects that use personality and emotions to address different aspects of the affective agent user interface a puppet uses affect to teach children how the different emotional states can change or modify a character s behaviour and how physical and verbal actions in social interactions can induce emotions in others b the inhabited market place uses affect to tailor the roles of actors in a virtual market place and c presence uses affect to enhance the believability of a virtual character and produce a more natural conversational manner int exploration of perceptual computing for smart its the future success of ubiquitous computing depends to a big part on how well applications can adapt to their environment and act accordingly this thesis has set itself the goal of exploring perceptual computing for smart its which is one such ubiquitous computing vision normal forms for defeasible logic defeasible logic is an important logic programming based nonmonotonic reasoning formalism which has an efficient implementation it makes use of facts strict rules defeasible rules defeaters and a superiority relation representation results are important because they can help the assimilation of a concept by confining attention to its critical aspects in this paper we derive some representation results for defeasible logic in particular we show that the superiority relation does not add to the expressive power of the logic and can be simulated by other ingredients in a modular way also facts can be simulated by strict rules finally we show that we cannot simplify the logic any further in a modular way strict rules defeasible rules and defeaters form a minimal set of independent ingredients in the logic 1 introduction normal forms play an important role in computer science examples of areas where normal forms have proved fruitful include logic 10 where normal forms o realtime personal positioning system for wearable computers context awareness is an important functionality for wearable computers in particular the computer should know where the person is in the environment this paper proposes an image sequence matching technique for the recognition of locations and previously visited places as in single word recognition in speech recognition a dynamic programming algorithm is proposed for the calculation of the similarity of different locations the system runs on a stand alone wearable computer such as a libretto pc using a training sequence a dictionary of locations is created automatically these locations are then be recognized by the system in realtime using a hatmounted camera 1 introduction obtaining user location is one of the important functions for wearable computers in two applications one is automatic self summary and the other is contextaware user interface in self summary the user is wearing a small camera and a small computer capturing and recording every event of his her daily a multi agent approach to vehicle monitoring in motorway this paper describes caselp a prototyping environment for multiagent systems mas and its adoption for the development of a distributed industrial application caselp employs architecture definition communication logic and procedural languages to model a mas from the top level architecture down to procedural behavior of each agent s instance the executable specification which is obtained can be employed as a rapid prototype which helps in taking quick decisions on the best possible implementation solutions such capabilities have been applied to a distributed application of elsag company in order to assess the best policies for data communication and database allocation before the concrete implementation the application consists in remote traffic control and surveillance over service areas on an italian motorway employing automatic detection and car plate reading at monitored gates caselp allowed to predict data communication performance statistics under differe ai at ibm research ibm has played an active role in ai research since the field s inception more than 50 years ago in a trend that reflects the increasing demand for applications that behave intelligently ibm today carries out most ai research in an interdisciplinary fashion by combining ai techniques with other computing techniques to solve difficult technical problems 1 multiagent systems specification by uml statecharts aiming at intelligent manufacturing multiagent systems are a promising new paradigm in computing which are contributing to various fields many theories and technologies have been developed in order to design and specify multiagent systems however no standard procedure is used at present industrial applications often have a complex structure and need plenty of working resources they require a standard specification method as well as the standard method to design and specify software systems we believe that one of the key words is simplicity for their wide acceptance in this paper we propose a method to specify multiagent systems namely with uml statecharts we use them for specifying almost all aspects of multiagent systems because we think that it is an advantage to keep everything in one type of diagram we apply an evaluation of linguistically motivated indexing schemes in this article we describe a number of indexing experiments based on indexing terms other than simple keywords these experiments were conducted as one step in validating a linguistically motivated indexing model the problem is important but not new what is new in this approach is the variety of schemes evaluated it is important since it should not only help to overcome the well known problems of bag of words representations but also the difficulties raised by non linguistic text simplification techniques such as stemming stop word deletion and term selection our approach in the selection of terms is based on part of speech tagging and shallow parsing the indexing schemes evaluated vary from simple keywords to nouns verbs adverbs adjectives adjacent word pairs and head modifier pairs our findings apply to information retrieval and most of related areas term selection for filtering based on distribution of terms over time in this article we investigate the use of time distributions in retrieval tasks specifically we introduce a novel term selection method namely term occurrence uniformity tou based on the hypothesis that terms which occur uniformly in time are more valuable than others our empirical evaluation so far has neither proved nor disproved this hypothesis however results are promising and suggest the need for a deeper theoretical and empirical investigation our current concern is filtering but this line of research may easily be extended to other retrieval tasks which involve temporally dependent data 1 introduction information filtering is the process of searching in large amounts of data for information which matches a user information need the filtering task is usually described as the inverse of the traditional retrieval task in retrieval a one time user request called query is matched to a static collection of information objects in filtering users issue a long term r a plan based agent architecture for interpreting natural language dialogue this paper describes a plan based agent architecture for modeling nl cooperative dialogue in particular the paper focuses on the interpretation dialogue and explanation of its coherence by means of the recognition of the speakers underlying intentions the approach we propose makes it possible to analyze and explain in a uniform way several apparently unrelated linguistic phenomena which have been often studied separately and treated via ad hoc methods in the models of dialogue presented in the literature our model of linguistic interaction is based on the idea that dialogue can be seen as any other interaction among agents therefore domain level and linguistic actions are treated in a similar way our agent architecture is based on a two level representation of the knowledge about acting at the metalevel the agent modeling plans describe the recipes for plan formation and execution they are a declarative representation of a reactive planner at the object level the domain tailoring the interaction with users in web stores we describe the user modeling and personalization techniques adopted in seta a prototype toolkit for the construction of adaptive web stores which customize the interaction with users the web stores created using seta suggest the items best fitting the customers needs and adapt the layout and the description of the store catalog to their preferences and expertise seta uses stereotypical information to handle the user models and applies personalization rules to dynamically generate the hypertextual pages presenting products the system adapts the graphical aspect length and terminology used in the descriptions to parameters like the user s receptivity expertise and interests moreover it maintains a model associated with each person the goods are selected for in this way multiple criteria can be applied for tailoring the selection of items to the preferences of their beneficiaries keywords user modeling personalized information presentation customization of web stores uso di piani di problem solving nel riconoscimento di piani e obiettivi in questo articolo si discute il ruolo dei piani di problem solving nell interpretazione dei dialoghi in linguaggio naturale per piano di problem solving si intende una descrizione dichiarativa dei passi del processo di pianificazione ed esecuzione di azioni linguistiche e di dominio l articolo mostra che una rappresentazione appropriata di questi piani e la base per modellare il comportamento cooperativo degli agenti che partecipano ad un dialogo i piani di problem solving sono parte di un architettura di agente in grado di cooperare con altri agenti 1 introduzione l analisi del linguaggio naturale ha un ruolo importante nello sviluppo di interfacce intelligenti infatti per rendere un sistema amichevole nei confronti dei suoi utenti e importante arricchirlo con una teoria del linguaggio che descrive le strategie comunicative adottate dalle persone per interagire inoltre e necessario esplicitare nel sistema i concetti basilari di razionalita e cooperativita l idea e routing documents according to style most research on automated text categorization has focused on determining the topic of a given text while topic is generally the main characteristic of an information need there are other characteristics that are useful for information retrieval in this paper we consider the problem of text categorization according to style for example in searching the web we may wish to automatically determine if a given page is promotional or informative was written by a native english speaker or not and so on learning to determine the style of a document is a dual to that of determining its topic in that those document features which capture the style of a document are precisely those which are independent of its topic we here define the features of a document to be the frequencies of each of a set of function words and parts of speech triples we then use machine learning techniques to classify documents we test our methods on four collections of downloaded newspaper and magazine articl optimal external memory interval management in this paper we present the external interval tree an optimal external memory data structure for answering stabbing queries on a set of dynamically maintained intervals the external interval tree can be used in an optimal solution to the dynamic interval management problem which is a central problem for object oriented and temporal databases and for constraint logic programming part of the structure uses a weight balancing technique for efficient worst case manipulation of balanced trees which is of independent interest the external interval tree as well as our new balancing technique have recently been used to develop several efficient external data structures efficient bulk operations on dynamic r trees in recent years there has been an upsurge of interest in spatial databases a major issue is how to manipulate efficiently massive amounts of spatial data stored on disk in multidimensional spatial indexes data structures construction of spatial indexes bulk loading has been studied intensively in the database community the continuous arrival of massive amounts of new data makes it important to update existing indexes bulk updating efficiently in this paper we present a simple yet efficient technique for performing bulk update and query operations on multidimensional indexes we present our technique in terms of the so called r tree and its variants as they have emerged as practically efficient indexing methods for spatial data our method uses ideas from the buffer tree lazy buffering technique and fully utilizes the available internal memory and the page size of the operating system we give a theoretical analysis of our technique showing that it is efficient both in terms of i o communication disk storage and internal computation time we also present the results of an extensive set of experiments showing that in practice our approach performs better than the previously best known bulk update methods with respect to update time and that it produces a better quality index in terms of query performance one important novel feature of our technique is that in most cases it allows us to perform a batch of updates and queries simultaneously to be able to do so is essential in environments where queries have to be answered even while the index is being updated and reorganized efficient learning of semi structured data from queries this paper studies the polynomial time learnability of the classes of ordered gapped tree patterns ogt and ordered gapped forests ogf under the into matching semantics in the query learning model of angluin the class ogt is a model of semi structured database query languages and a generalization of both the class of ordered unordered tree pattern languages and the class of non erasing regular pattern languages impact a platform for collaborating agents twork the impact server provides the infrastructure upon which different impact agents can interact to avoid a performance bottleneck multiple copies of the server can be replicated and scattered across the network impact agents a set of data objects can be represented in a wide variety of ways when building an application we d like to select a data structure that supports the application operations that are the most frequently executed the most critical or both so any definition of an agent must support such flexible choice of data structures and agentization must let us extend arbitrary data representations in impact an agent consists of any body of software code whatsoever with the associated wrapper figure 2 shows such an agent s architecture the software code the agent s code consists of two parts a set of data structures or data types manipulated by the agent for example if we are building a database agen cooperative multiagent robotic systems introduction teams of robotic systems at first glance might appear to be more trouble than they are worth why not simply build one robot that is capable of doing everything we need there are several reasons why two robots or more can be better than one ffl distributed action many robots can be in many places at the same time ffl inherent parallelism many robots can do many perhaps different things at the same time ffl divide and conquer certain problems are well suited for decomposition and allocation among many robots ffl simpler is better often each agent in a team of robots can be simpler than a more comprehensive single robot solution no doubt there are more reasons as well unfortunately there are also drawbacks in particular regarding coordination and elimination of interference the degree of difficulty imposed depends heavily upon the task and the communication and control strategies chosen making ldap active with the ltap gateway case study in providing telecom integration and enhanced services ldap lightweight directory access protocol directories are being rapidly deployed on the web they are currently used to store data like white pages information user profiles and network device descriptions these directories offer a number of advantages over current database technology in that they provide better support for heterogeneity and scalability however they lack some basic database functionality e g triggers transactions that is crucial for directory enabled networking den tasks like provisioning network services allocating resources reporting managing end to end security and offering mobile users customized features that follow them in order to address these limitations while keeping the simplicity and performance features of ldap directories unbundled and portable solutions are needed in this paper we discuss ldap limitations we faced while building an ldap meta directory that integrates data from legacy telecom systems and how ltap lightweight trigger access process a portable gateway that adds active functionality to ldap directories overcomes these limitations spatial cognition and neuro mimetic navigation a model of hippocampal place cell activity a computational model of hippocampal activity during spatial cognition and navigation tasks is presented the spatial representation in our model of the rat hippocampus is built on line during exploration via two processing streams an allothetic vision based representation is built by unsupervised hebbian learning extracting spatio temporal properties of the environment from visual input an idiothetic representation is learned based on internal movement related information provided by path integration on the level of the hippocampus allothetic and idiothetic representations are integrated to yield a stable representation of the environment by a population of localized overlapping ca3 ca1 place fields the hippocampal spatial representation is used as a basis for goal oriented spatial behavior we focus on the neural pathway connecting the hippocampus to the nucleus accumbens place cells drive a population of locomotor action neurons in the nucleus accumbens reward based learnin neuro mimetic navigation systems a computational model of the rat hippocampus we propose a bio inspired approach to autonomous navigation based on some of the components that rats use for navigation a spatial model of the environment is constructed by unsupervised hebbian learning the representation consists of a population of localized overlapping place elds modeling place cell activity in the rat hippocampus place elds are established by extracting spatio temporal properties of the environment from visual sensory inputs visual ambiguities are resolved by means of path integration reinforcement learning is applied to use place cell activity for goal oriented navigation experimental results obtained with a mobile khepera robot are presented keywords autonomous robots hippocampus place elds unsupervised learning reinforcement learning population vector coding path integration 1 introduction the complexity of the autonomous navigation task is inherent in the concept of autonomy ideally an autonomous agent should have a completely boticelli a single camera mobile robot using new approaches to range data fusion world modeling and navigation planning id a083 abstract boticelli is a mobile robot designed and built for testing new approaches in stereo vision world modeling data fusion map extraction reinforcement learning and navigation planning a single camera is used to capture depth information by taking advantage of camera movements the main thrust of the new approaches is to replace well known techniques that depend upon grids of points in space with techniques that use continuous piecewise linear functions these techniques scale well to large complex environments keywords mobile agents mapping and exploration reinforcement learning acknowledgements this research was supported mainly by the defence research establishment suffield contract w7702 6r594 001 to dendronic decisions limited we are very grateful to the scientific authority dr simon barton for his guidance travel was supported by the natural sciences and engineering research council of canada nserc help of kyle palmer in designing the multi efficient support for p http in cluster based web servers this paper studies mechanisms and policies for supporting http 1 1 persistent connections in cluster based web servers that employ content based request distribution we present two mechanisms for the efficient content based distribution of http 1 1 requests among the back end nodes of a cluster server a trace driven simulation shows that these mechanisms combined with an extension of the locality aware request distribution lard policy are effective in yielding scalable performance for http 1 1 requests we implemented the simpler of these two mechanisms back end forwarding measurements of this mechanism in connection with extended lard on a prototype cluster driven with traces from actual web servers confirm the simulation results the throughput of the prototype is up to four times better than that achieved by conventional weighted round robin request distribution in addition throughput with persistent connections is up to 26 better than without a layered approach towards domain authoring support this paper presents an approach to authoring support for web courseware based on a layered ontological paradigm the ontology based layers in the courseware authoring architecture serve as a basis for formal semantics and reasoning support in performing generic authoring tasks this approach represents an extension of our knowledge classification and indexing mechanism from a previously developed system aims aimed at supporting students while completing learning tasks in a web based learning training environment we propose the addition of two vertical layers in the system architecture author assisting layer and operational layer with the role of facilitating the creation of the ontological layers course ontology and domain ontology and of the educational metadata layer here we focus on the domain ontology creation process together with the support that the additional layers can provide within this process we exemplify our method by presenting a set of generic tasks related to conceptbased domain authoring and their ontological support an open electronic marketplace through agent based workflows moppet we propose an electronic marketplace architecture called moppet where the commerce processes in the marketplace are modeled as adaptable agent based workflows the higher level of abstraction provided by the workflow technology makes the customization of electronic commerce processes for different users possible agent based implementation on the other hand provides for a highly reusable component based workflow architecture as well as negotiation ability and the capability to adapt to dynamic changes in the environment agent communication is handled through knowledge query and manipulation language kqml a workflow based architecture also makes it possible for complete modeling of electronic commerce processes by allowing involved parties to be able to invoke already existing applications or to define new tasks and to restructure the control and data flow among the tasks to create custom built process definitions in the proposed architecture all data exchanges are realized through extensible markup language xml providing uniformity simplicity and a highly open and interoperable architecture metadata of activities are expressed through resource description framework rdf common business library cbl is used for achieving interoperability across business domains and domain specific document type definitions dtds are used for vertical industries we provide our own specifications for missing dtds to be replaced by the original specifications when they become available on the expressivity and complexity of temporal conceptual modelling the contribution of this paper is twofold on the one hand it introduces t dlr a novel temporal logic for temporal conceptual modelling motivated as the obvious generalisation of the successful dlr description logic tight decidability and complexity results are proved for t dlr and the monodic fragment of it t dlr moreover the decidability of conjunctive query containment under t dlr constraints is proved on the other hand the paper provides a formal semantic characterisation of all the important temporal conceptual modelling constructs for valid time representation as found in the literature to the best of our knowledge this is the first systematic formalisation of the constructs present in most temporal conceptual modelling systems this systematic characterisation as t dlr theories is an evidence of the adequacy of the t dlr temporal description logic for temporal conceptual modelling 1 introduction in this paper the novel t dlr temporal logic is introduced reasoning over conceptual schemas and queries in temporal databases this paper introduces a new logical formalism intended for temporal conceptual modelling as a natural combination of the wellknown description logic dlr and pointbased linear temporal logic with since and until the expressive power of the resulting dlrus logic is illustrated by providing a systematic formalisation of the most important temporal entity relationship data models appeared in the literature we define a query language where queries are nonrecursive datalog programs and atoms are complex dlrus expressions and investigate the problem of checking query containment under the constraints defined by dlrus conceptual schemas as well as the problems of schema satisfiability and logical implication although it is shown that reasoning in full dlrus is undecidable we identify the decidable in a sense maximal fragment dlr us by allowing applications of temporal operators to formulas and entities only but not to relation expressions we obtain the following hierarchy of complexity results a reasoning in dlr us with atomic formulas is exptime complete b satisfiability and logical implication of arbitrary dlr us formulas is expspace complete and c the problem of checking query containment of non recursive datalog queries under dlr us constraints is decidable in 2exptime a temporal description logic for reasoning over conceptual schemas and queries this paper introduces a new logical formalism intended for temporal conceptual modelling as a natural combination of the well known description logic dlr and point based linear temporal logic with since and until the expressive power of the resulting dlrus logic is illustrated by providing a characterisation of the most important temporal conceptual modelling constructs appeared in the literature we define a query language where queries are non recursive datalog programs and atoms are complex dlrus expressions and investigate the problem of checking query containment under the constraints defined by dlrus conceptual schemas i e dlrus knowledge bases as well as the problems of schema satisfiability and logical implication the robocup physical agent challenge phase i traditional ai research has not given due attention to the important role that physical bodies play for agents as their interactions produce complex emergent behaviors to achieve goals in the dynamic real world the robocup physical agent challenge provides a good testbed for studying how physical bodies play a signi cant role in realizing intelligent behaviors using the robocup framework kitano et al 95 in order for the robots to play a soccer game reasonably well a wide range of technologies needs to be integrated and a number of technical breakthroughs must be made in this paper we present three challenging tasks as the robocup physical agent challenge phase i 1 moving the ball to the speci ed area shooting passing and dribbling with no stationary or moving obstacles 2 catching the ball from an opponent or a teammate receiving goal keeping and intercepting and 3 passing the ball between two players the rst two are concerned with single agent skills while the third one is related to a simple cooperative behavior motivation for these challenges and evaluation methodology are given 1 learning significant locations and predicting user movement with gps wearable computers have the potential to act as intelligent agents in everyday life and assist the user in a variety of tasks depending on the context location is the most common form of context used by these agents to determine the user s task however another potential use is the creation of a predictive model of the user s future movements we present a system that automatically clusters gps data taken over an extended period of time into meaningful locations at multiple scales these locations are then incorporated into a markov model that can be consulted for use with a variety of applications in both single user and collaborative scenarios optimizing information agents by selectively materializing data we present an approach for optimizing the performance of information agents by materializing useful information a critical problem with information agents particularly those gathering and integrating information from web sources is a high query response time this is because the data needed to answer user queries is present across several differentweb sources and in several pages within a source and retrieving extracting and integrating the data is time consuming weaddress this problem by materializing useful classes of information and defining them as auxiliary data sources for the information agent the key challenge here is to identify the contentandschema of the classes of information that would be useful to materialize we present an algorithm that identifies such classes by analyzing patterns in user queries we describe an implementation of our approach and experiments in progress we also discuss other important problems that we will address in optimizing information agents paradigma agent implementation through jini one of the key problems of recent years has been the divide between theoretical work in agent based systems and its practical complement which have to a large extent developed along different paths the paradigma implementation framework has been designed with the aim of narrowing this gap it relies on an extensive formal agent framework implemented using recent advances in java technology specifically paradigma uses jini connectivity technology to enable the creation of on line communities in support of the development of agent based systems 1 introduction in a networked environment that is highly interconnected interdependent and heterogeneous we are faced with an explosion of information and available services that are increasingly hard to manage agent based systems can provide solutions to these problems as a consequence of their dynamics of social interaction communication and cooperation can be used to effectively model problem domains through the interaction of agent static and dynamic information organization with star clusters in this paper we present a system for static and dynamic information organization and show our evaluations of this system on trec data we introduce the off line and on line star clustering algorithms for information organization our evaluation experiments show that the off line star algorithm outperforms the single link and average link clustering algorithms since the star algorithm is also highly efficient and simple to implement we advocate its use for tasks that require clustering such as information organization browsing filtering routing topic tracking and new topic detection human factors in atc alarms and notifications design an experimental evaluation with the growing use of computerised working position alarm design on air traffic control displays is a concern as events to be notified increase in number and diversity safety requires visual notifications that can be efficiently detected and understood it also requires that no information on the radar display is obscured by the visual notifications we also need to design hierarchies of notifications from most severe to benign taking advantage of the current graphical capabilities of computers we have identified and explored various dimensions in visual alarm design we present in this paper an experimental evaluation in terms of detection time and precision of several of those dimensions opacity size temporal profile of animation and signal frequency from our results we conclude that opacity size and temporal profile of animation are well suited to introduce some nuances in the way we convey notifications on visual displays we also show that detection of a given signa theseus categorization by context introduction the traditional approach to document categorization is categorization by content since information for categorizing a document is extracted from the document itself in a hypertext environment like the web the structure of documents and the link topology can be exploited to perform what we call categorization by context attardi 98 the context surrounding a link in an html document is used for categorizing the document referred by the link categorization by context is capable of dealing also with multimedia material since it does not rely on the ability to analyze the content of documents categorization by context leverages on the categorization activity implicitly performed when someone places or refers to a document on the web by focusing the analysis to the documents used by a group of people one can build a catalogue tuned to the need of that group categorization by context is based on the following assumptions 1 hybrid tracking for augmented reality applying artificial intelligence to virtual reality intelligent virtual environments reearch into virtual environments on the one hand and artificial intelligence and artificial life on the other has largely been carried out by two different groups of people with different preoccupations and interests but some convergence is now apparent between the two fields applications in which activity independent of the user takes place involving crowds or other agents are beginning to be tackled while synthetic agents virtual humans and computer pets are all areas in which techniqes from the two fields require strong integration the two communities have much to learn from each other if wheels are not to be reinvented on both sides this paper reviews the issues arising from combining artificial intelligence and artificial life techniques with those of virtual environments to produce just such intelligent virtual environments the discussion is illustrated with examples that include environments providing knowledge to direct or assist the user rather than r the challenge of making augmented reality work outdoors this paper along with others we have yet to imagine acknowledgments tracking in unprepared environments for augmented reality systems many augmented reality applications require accurate tracking existing tracking techniques require prepared environments to ensure accurate results this paper motivates the need to pursue augmented reality tracking techniques that work in unprepared environments where users are not allowed to modify the real environment such as in outdoor applications accurate tracking in such situations is difficult requiring hybrid approaches this paper summarizes two 3dof results a real time system with a compass inertial hybrid and a non real time system fusing optical and inertial inputs we then describe the preliminary results of 5 and 6 dof tracking methods run in simulation future work and limitations are described matching in description logics preliminary results matching of concepts with variables concept patterns is a relatively new operation that has been introduced in the context of concept description languages description logics originally to help discard unimportant aspects of large concepts appearing in industrial strength knowledge bases this paper proposes a new approach to performing matching based on a concept centered normal form rather than the more standard structural subsumption normal form for concepts as a result matching can be performed in polynomial time using arbitrary concept patterns of the description language fl thus removing restrictions from previous work the paper also addresses the question of matching problems with additional side conditions which were motivated by practical experience 1 introduction the traditional inference problems for description logic dl systems like subsumption are now wellinvestigated this means that algorithms are available for solving the subsumption proble multidimensional data modeling for complex data systems for on line analytical processing olap considerably ease the process of analyzing business data and have become widely used in industry olap systems primarily employ multidimensional data models to structure their data however current multidimensional data models fall short in their ability to model the complex data found in some real world application domains the paper presents nine requirements to multidimensional data models each of which is exemplified by a real world clinical case study a survey of the existing models reveals that the requirements not currently met include support for many to many relationships between facts and dimensions built in support for handling change and time and support for uncertainty as well as different levels of granularity in the data the paper defines an extended multidimensional data model which addresses all nine requirements along with the model we present an associated algebra and outline how to implement the model using relational databases cost effective mobile agent planning for distributed information retrieval the number of agents and the execution time are two significant performance factors in mobile agent planning fewer agents cause lower network traffic and consume less bandwidth regardless of the number of agents used the execution time for a task must be kept minimal which means that use of the minimal number of agents must not impact on the execution time unfavorably as the population of the mobile agent application domain grows the importance of these two factors also increases learning network designs for asynchronous teams an asynchronous team a team is a network of agents workers and memories repositories for the results of work it is possible to design a teams to be effective in solving difficult computational problems the main design issues are what structure should the network have and what should be the complement of agents in the past the structure issue was resolved by intuition and experiment this paper describes a procedure by which good structures can be learned from experience the procedure is based on the use of regular expressions for encoding the capabilities of networks 1 introduction an asynchronous team a team is a problem solving architecture consisting of collections of agents and memories connected into a strongly cyclic directed network the memories form the nodes of the network the agents form the arcs figure 1 below shows such a network each memory holds a population of trial solutions the solutions are not necessarily solutions to the overall problem t autonomous helicopter control using reinforcement learning policy search methods many control problems in the robotics field can be cast as partially observed markovian decision problems pomdps an optimal control formalism finding optimal solutions to such problems in general however is known to be intractable it has often been observed that in practice simple structured controllers suffice for good sub optimal control and recent research in the artificial intelligence community has focused on policy search methods as techniques for finding sub optimal controllers when such structured controllers do exist traditional model based reinforcement learning algorithms make a certainty equivalence assumption on their learned models and calculate optimal policies for a maximumlikelihood markovian model in this work we consider algorithms that evaluate and synthesize controllers under distributions of markovian models previous work has demonstrated that algorithms that maximize mean reward with respect to model uncertainty leads to safer and more robust controll efficient phrase querying with an auxiliary index search engines need to evaluate queries extremely fast a challenging task given the vast quantities of data being indexed a significant proportion of the queries posed to search engines involve phrases in this paper we consider how phrase queries can be efficiently supported with low disk overheads previous research has shown that phrase queries can be rapidly evaluated using nextword indexes but these indexes are twice as large as conventional inverted files we propose a combination of nextword indexes with inverted files as a solution to this problem our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone and the space overhead is only 10 of the size of the inverted file further time savings are available with only slight increases in disk requirements analysis and optimisation of event condition action rules on xml xml is a now a dominant standard for storing and exchanging information with its increasing use in areas such as data warehousing and e commerce there is a rapidly growing need for rule based technology to support reactive functionality on xml repositories eventcondition action eca rules automatically perform actions in response to events and are a natural facility to support such functionality in this paper we study eca rules in the context of xml data we define a simple language for specifying eca rules on xml repositories the language is illustrated by means of some examples and its syntax and semantics are then specified more formally we then investigate methods for analysing and optimising these eca rules a task which has added complexity in this xml setting compared with conventional active databases 1 engineering a multi purpose test collection for web retrieval experiments past research into text retrieval methods for the web has been restricted by the lack of a test collection capable of supporting experiments which are both realistic and reproducible the 1 69 million document wt10g collection is proposed as a multi purpose testbed for experiments with these attributes in distributed ir hyperlink algorithms and conventional ad hoc retrieval wt10g was constructed by selecting from a superset of documents in such a way that desirable corpus properties were preserved or optimised these properties include a high degree of inter server connectivity integrity of server holdings inclusion of documents related to a very wide spread of likely queries and a realistic distribution of server holding sizes we conrm that wt10g contains exploitable link information using a site homepage nding experiment our results show that on this task okapi bm25 works better on propagated link anchor text than on full text keywords web retrieval link based ranking distributed information retrieval test collections 1 an event condition action language for xml xml repositories are now a widespread means for storing and exchanging information on the web as these repositories become increasingly used in dynamic applications such as e commerce there is a rapidly growing need for a mechanism to incorporate reactive functionality in an xml setting event condition action eca rules are a technology from active databases and are a natural method for supporting such functionality eca rules can be used for activities such as automatically enforcing document constraints maintaining repository statistics and facilitating publish subscribe applications an important question associated with the use of a eca rules is how to statically predict their run time behaviour in this paper we de ne a language for eca rules on xml repositories we theninvestigate methods for analysing the behaviour of a set of eca rules ataskwhich has added complexity in this xml setting compared with conventional active databases keywords event condition action rules xml xml repositories reactive functionality rule analysis 1 active databases and agent systems a comparison this paper examines active databases and agent systems comparing their purpose structure functionality and implementation our presentation is aimed primarily at an audience familiar with active database technology we show that they draw upon very similar paradigms in their quest to supply reactivity this presents opportunities for migration of techniques and formalisms between the two fields 1 fjbailey kemp dnk raog cs mu oz au 2 georgeff aaii oz au 3 appears in t sellis editor proceedings of the second international workshop on rules in database systems lecture notes in computer science 985 pages 342 356 athens greece 1995 1 introduction in recent times two technologies have become prominent in the database and artificial intelligence research communities an active database adb is a system which supplements traditional database functionality by reacting automatically to state changes both internal and external without user intervention an agent system a hallucinating faces in most surveillance scenarios there is a large distance between the camera and the objects of interest in the scene surveillance cameras are also usually set up with wide fields of view in order to image as much of the scene as possible the end result is that the objects in the scene normally appear very small in surveillance imagery it is generally possible to detect and track the objects in the scene however for tasks such as automatic face recognition and license plate reading resolution enhancement techniques are often needed although numerous resolution enhancement algorithms have been proposed in the literature most of them are limited by the fact that they make weak if any assumptions about the scene we propose an algorithm that can be used to learn a prior on the spatial distribution of the image gradient for frontal images of faces we proceed to show how such a prior can be incorporated into a super resolution algorithm to yield 4 8 fold improvements in resolution heuristic evaluation of groupware based on the mechanics of collaboration despite the increasing availability of groupware most systems are awkward and not widely used while there are many reasons for this a significant problem is that groupware is difficult to evaluate in particular there are no discount usability evaluation methodologies that can discover problems specific to teamwork in this paper we describe how we adapted nielsen s heuristic evaluation methodology designed originally for single user applications to help inspectors rapidly cheaply effectively identify usability problems within groupware systems specifically we take the mechanics of collaboration framework and restate it as heuristics for the purposes of discovering problems in shared visual work surfaces for distance separated groups 1 distributional clustering of words for text classification this paper applies distributional clustering pereira et al 1993 to document classification the approach clusters words into groups based on the distribution of class labels associated with each word thus unlike some other unsupervised dimensionality reduction techniques such as latent semantic indexing we are able to compress the feature space much more aggressively while still maintaining high document classification accuracy experimental results obtained on three real world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2 accuracy significantly better than latent semantic indexing deerwester et al 1990 class based clustering brown et al 1992 feature selection by mutual information yang and pederson 1997 or markovblanket based feature selection koller and sahami 1996 we also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clusteri a survey of factory control algorithms which can be implemented in a multi agent heterarchy dispatching scheduling and pull this paper has not seriously addressed the question of what new algorithms are inspired by the multi agent heterarchical paradigm except in two cases the market driven contract net uses a new form of forward backward scheduling called cost based forward backwards continuum scheduling which was inspired by the common agent concept of bidding also duffie s current work on developing agent based deterministic simulation would be a new form of deterministic simulation as it attempts to automate the human interaction normally required for successful implementation of such systems but in general we still have an open question of what new algorithms heterarchical agent architectures imply and what would be the performance of these new algorithms because a multi agent heterarchy is a distributed computing paradigm it would be worthwhile to investigate the communications overhead of these algorithms such evaluations are not generally reported such research would assure that not only can these algorithms be implemented in a multi agent heterarchy but that their implementation does not require excessive communications overhead 6 references 1 h hayashi the ims international collaborative program in 24th isir 1993 japan industrial robot association 2 national center for manufacturing sciences focus exceeding partner expectations ann arbor mi september 1994 3 s goldman and k preiss ed 21st century manufacturing enterprise strategy an industry led view bethlehem pa iacocca institute lehigh university 1991 4 s l goldman r n nagel and k preiss agile competitors and virtual organizations strategies for enriching the customer new york ny van nostrand reinhold 1995 5 p t kidd agile manufacturing forging new frontiers w lart flexible low power building blocks for wearable computers to ease the implementation of different wearable computers we developed a low power processor board named lart with a rich set of interfaces the lart supports dynamic voltage scaling so performance and power consumption can be scaled to match demands 59 221 mhz 106 640 mw high end wearables can be configured from multiple larts operating in parallel alternatively fpga boards can be used for dedicated data processing which reduces power consumption significantly communication in reactive multiagent robotic systems abstract multiple cooperating robots are able to complete many tasks more quickly and reliably than one robot alone communication between the robots can multiply their capabilities and e ectiveness but to what extent in this research the importance of communication in robotic societies is investigated through experiments on both simulated and real robots performance was measured for three di erent types of communication for three di erent tasks the levels of communication are progressively more complex and potentially more expensive to implement for some tasks communication can signi cantly improve performance but for others inter agent communication is apparently unnecessary in cases where communication helps the lowest level of communication is almost as e ective as the more complex type the bulk of these results are derived from thousands of simulations run with randomly generated initial conditions the simulation results help determine appropriate parameters for the reactive control system which was ported for tests on denning mobile robots mutaclp a language for temporal reasoning with multiple theories in this paper we introduce mutaclp a knowledge representation language which provides facilities for modeling and handling temporal information together with some basic operators for combining different temporal knowledge bases the proposed approach stems from two separate lines of research the general studies on meta level operators on logic programs introduced by brogi et al 6 8 and temporal annotated constraint logic programming taclp de ned by fr hwirth 14 in mutaclp atoms are annotated with temporal information which are managed via a constraint theory as in taclp mechanisms for structuring programs and combining separate knowledge bases are provided through meta level operators the language is given two different and equivalent semantics a top down semantics which exploits meta logic and a bottom up semantics based on an immediate consequence operator intention guided web sites a new perspective on adaptation recent years witnessed a rapid growth of multimedia technologies for offering information and services on the internet one of the many problems that are to be faced in this context is the great variety of possible users and the consequent need to adapt both the presentation of information and the interaction to the specific user s characteristics applications of quick combine for ranked query models in digital libraries queries are often based on the similarity of objects using several feature attributes like colors texture or full text searches such multi feature queries return a ranked result set instead of exact matches recently we presented a new algorithm called quick combine 5 for combining multi feature result lists guaranteeing the correct retrieval of the k top ranked results as benchmarks on practical data promise that we can dramatically improve performance we want to discuss interesting applications of quick combine in different areas the applications for the optimization in ranked query models are manifold generally speaking we believe that all kinds of federated searches can be supported like e g content based retrieval knowledge management systems or multi classifier combination integrating spatial information and image analysis one plus one makes ten photogrammetry and remote sensing have proven their efficiency for spatial data collection in many ways interactive mapping at digital workstations is performed by skilled operators which guarantees excellent quality in particular of the geometric data in this way worldwide acquisition of a large number of national gis databases has been supported and still a lot of production effort is devoted to this task in the field of image analysis it has become evident that algorithms for scene interpretation and 3d reconstruction of topographic objects which rely on a single data source cannot function efficiently research in two directions promises to be more successful multiple largely complementary sensor data like range data from laser scanners sar and panchromatic or multi hyper spectral aerial images have been used to achieve robustness and better performance in image analysis on the other hand given gis databases e g layers from topographic maps can be considered as vi space discretization for efficient human navigation there is a large body of research on motion control of legs in human models however they require specification of global paths in which to move a method for automatically computing a global motion path for a human in 3d environment of obstacles is presented object space is discretized into a 3d grid of uniform cells and an optimal path is generated between two points as a discrete cell path the grid is treated as graph with orthogonal links of uniform cost a search method is applied for path finding by considering only the cells on the upper surface of objects on which human walks a large portion of the grid is discarded from the search space thus boosting efficiency this is expected to be a higher level mechanism for various local foot placement methods in human animation keywords global navigation dynamic programming a graph search articulated body models 1 introduction human walking is a complex and well studied component of articulated body animation research t generating extraction based summaries from hand written summaries by aligning text spans human quality text summarization systems based on sentence extraction are difficult to design because documents can differ along several dimensions such as length writing style and lexical usage the lack of suitable corpora of extraction based summaries makes it difficult to evaluate and improve existing algorithms however there are a large number of hand written not extraction based summaries available for news wire stories this paper discusses our work on generating a corpus of approximately 25 000 extractionbased summaries from hand written summaries we discuss how text span alignment can be applied to this problem and how this problem differs from previous work on aligning parallel texts in addition we briefly analyze differences between handwritten and extracted summaries 1 introduction human quality text summarization systems based on sentence extraction are difficult to design and even more difficult to evaluate because documents can differ along several dimension reasoning agents in dynamic domains the paper discusses an architecture for intelligent agents based on the use of a prolog a language of logic programs under the answer set semantics a prolog is used to represent the agent s knowledge about the domain and to formulate the agent s reasoning tasks we outline how these tasks can be reduced to answering questions about properties of simple logic programs and demonstrate the methodology of constructing these programs keywords intelligent agents logic programming and nonmonotonic reasoning 1 introduction this paper is a report on the attempt by the authors to better understand the design of software components of intelligent agents capable of reasoning planning and acting in a changing environment the class of such agents includes but is not limited to intelligent mobile robots softbots immobots intelligent information systems expert systems and decision making systems the ability to design intelligent agents ia is crucial for such diverse tasks as an algebraic approach to static analysis of active database rules ing with credit is permitted to copy otherwise to republish to post on servers to redistribute to lists or to use any component of this work in other works requires prior specific permission and or a fee permissions may be requested from publications dept acm inc 1515 broadway new york ny 10036 usa fax 1 212 869 0481 or permissions acm org this is a preliminary release of an article accepted by acm transactions on database systems the definitive version is currently in production at acm and when released will supersede this version 2 delta e baralis and j widom 1 introduction an active database system is a conventional database system extended with a facility for managing active rules or triggers incorporating active rules into a conventional database system has raised considerable interest both in the scientific community and in the commercial world a number of prototypes that incorporate active rules into relational and object oriented database system tox the toronto xml engine abstract we present tox the toronto xml engine a repository for xml data and metadata which supports real and virtual xml documents real documents are stored as files or mapped into relational or object databases depending on their structuredness indices are defined according to the storage method used virtual documents can be remote documents defined as arbitrary weboql queries or views defined as queries over documents registered in the system the system catalog contains metadata for the documents especially their schemata used for query processing and optimization queries can range over both the catalog and the documents and multiple query languages are supported in this paper we describe the architecture and main of tox we present our indexing and storage strategies including two novel techniques and we discuss our query processing strategy the project started recently and is under active development 1 plans as situated action an activity theory approach to workflow systems within the community of cscw the notion and nature of workflow systems as prescriptions of human work has been debated and criticised based on the work of suchman 1987 the notion of situated action has often been viewed as opposed to planning work plans however do play an essential role in realising work based on experiences from designing a computer system that supports the collaboration within a hospital this paper discusses how plans themselves are made out of situated action and in return are realised in situ thus work can be characterised as situated planning this understanding is backed up by activity theory which emphasises the connection between plans and the contextual conditions for realising these plans in actual work introduction the issue of workflow systems has been addressed by several authors as ways of routing information objects among users and to specify automatic actions to be taken in that routing typically according to certain process models me a voxel based representation for evolutionary shape optimisation and keywords page title a voxel based representation for evolutionary shape optimisation abstract a voxel based shape representation when integrated with an evolutionary algorithm offers a number of potential advantage for shape optimisation topology need not be predefined geometric constraints are easily imposed and with adequate resolution any shape can be approximated to arbitrary accuracy however lack of boundary smoothness length of chromosome and inclusion of small holes in the final shape have been stated as problems with this representation this paper describes two experiments performed in an attempt to address some of these problems firstly a design problem with only a small computational cost of evaluating candidate shapes was used as a testbed for designing genetic operators for this shape representation secondly these operators were refined for a design problem using a more costly finite element evaluation it was concluded that the voxel representation can with careful design of genetic operators be useful in shape optimisation keywords shape optimisation evolutionary algorithms voxel representation 1 business suitability principles for workflow modelling by incorporating aspects of coordination and collaboration workflow implementations of information systems require a sound conceptualisation of business processing semantics traditionally the success of conceptual modelling techniques has depended largely on the adequacy of conceptualisation expressive power comprehensibility and formal foundation an equally important requirement particularly with the increased conceptualisation of business aspects is business suitability in this paper the focus is on the business suitability of workflow modelling for a commonly encountered class of operational business processing e g those of insurance claims bank loans and land conveyancing a general assessment is first conducted on some integrated techniques characterising well known paradigms structured process modelling object oriented modelling behavioural process modelling and business oriented modelling through this an insight into business suitability within the broader towards real scale business transaction workflow modelling while the specification languages of workflow management systems focus on process execution semantics the successful development of workflows relies on a fuller conceptualisation of business processing including process semantics for this a wellspring of modelling techniques paradigms and informal formal method extensions which address the analysis of organisational processing structures enterprise modelling and communication based on speech act theory is available however the characterisations indeed the cognition of workflows still appears coarse in this paper we provide the complementary empirical insight of a real scale business transaction workflow the development of the workflow model follows a set of principles which we believe address workflow modelling suitability through the principles advanced considerations including asynchronous as well as synchronous messaging temporal constraints and a service oriented perspective are motivated by illust online reconfiguration in replicated databases based on group communication use of group communication to support replication in database systems has proven to be an attractive alternative to traditional replica control schemes and various replica control protocols have been developed that use the ordering and reliability semantics of group communication primitives to simplify database system design and to improve performance although current solutions are able to mask site failures effectively they are unable to cope with recovery of failed sites merging of partitions or joining of new sites this paper addresses this important issue and proposes efficient solutions for online system reconfiguration providing new sites with a current state of the database without interrupting transaction processing in the rest of the system we present various alternatives that can match the needs of different operating environments we analyze the implications of long and complex reconfigurations on applications such as replicated databases and argue that their developement may be greatly simplified by extended forms of group communications three new algorithms for projective bundle adjustment with minimum parameters bundle adjustment is a technique used to compute the maximum likelihood estimate of structure and motion from image feature correspondences it practice large non linear systems have to be solved most of the time using an iterative optimization process starting from a sub optimal solution obtained by using linear methods the behaviour in terms of convergence and the computational cost of this process depend on the parameterization used to represent the problem i e of structure and motion on the non linear optimization of projective motion using minimal parameters i address the problem of optimizing projective motion over a minimal set of parameters most of the existing works overparameterize the problem feedbackbypass a new approach to interactive similarity query processing in recent years several methods have been proposed for implementing interactive similarity queries on multimedia databases common to all these methods is the idea to exploit user feedback in order to progressively adjust the query parameters and to eventually converge to an optimal parameter setting however all these methods also share the drawback to forget user preferences across multiple query sessions thus requiring the feedback loop to be restarted for every new query i e using default parameter values not only is this proceeding frustrating from the user s point of view but it also constitutes a significant waste of system resources in this paper we present feedbackbypass a new approach to interactive similarity query processing it complements the role of relevance feedback engines by storing and maintaining the query parameters determined with feedback loops over time using a wavelet based data structure the simplex tree for each query a favorable set of query parameters can be determined and used to either bypass the feedback loop completely for already seen queries or to start the search process from a near optimal configuration feedbackbypass can be combined well with all state of the art relevance feedback techniques working in high dimensional vector spaces its storage requirements scale linearly with the dimensionality of the query space thus making even sophisticated query spaces amenable experimen permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage the vldb copyright notice and the title of the publication and its date appear and notice is given that copying is by permission of the very large modeling and optimizing i o throughput of multiple disks on a bus in modern i o architectures multiple disk drives are attached to each i o controller a study of the performance of such architectures under i o intensive workloads has revealed a performance impairment that results from a previously unknown form of convoy behavior in disk i o in this paper we describe measurements of the read performance of multiple disks that share a scsi bus under a heavy workload and develop and validate formulas that accurately characterize the observed performance to within 12 on several platforms for i o sizes in the range 16 128 kb two terms in the formula clearly characterize the lost performance seen in our experiments we describe techniques to deal with the performance impairment via user level workarounds that achieve greater overlap of bus transfers with disk seeks and that increase the percentage of transfers that occur at the full bus bandwidth rather than at the lower bandwidth of a disk head experiments show bandwidth improvements of 10 20 when using these user level techniques but only in the case of large i os language sensitive text classification it is a traditional belief that in order to scale up to more effective retrieval and access methods modern information retrieval has to consider more the text content the modalities and techniques to fit this objectives are still under discussion more empirical evidence is required to determine the suitable linguistic levels for modeling each ir subtask e g information zoning parsing feature selection for indexing and the corresponding use of this information in this paper an original classification model sensitive to document syntactic information and characterized by a novel inference method is described extensive experimental evidence has been derived on real test data and also from well established academic test sets the results show that a significant improvement can be derived using the proposed inference model also the role of linguistic preprocessing seems to provide positive effects on the performance pos tagging and recognition of proper nouns received a specific experimental attention and provided significant effects on measured accuracy 1 reflective metalogical frameworks in computer science we speak of implementing a logic this is done in a programming language such as lisp called here the implementation language we also reason about the logic as in understanding how to search for proofs these arguments are expressed in the metalanguage and conducted in the metalogic of the object language being implemented we also reason about the implementation itself say to know it is correct this is done in a programming logic how do all these logics relate this paper considers that question and more we show that by taking the view that the metalogic is primary these other parts are related in standard ways the metalogic should be suitably rich so that the object logic can be presented as an abstract data type and it must be suitably computational or constructive so that an instance of that type is an implementation the data type abstractly encodes all that is relevant for metareasoning i e not only the term constructing functions but also the a glimpse into the future of id cyberspace is a complex dimension of both enabling and inhibiting data flows in electronic data networks current generation intrusion detection id systems are not technologically advanced enough to create the situational knowledge required to manage these networks next generation id system will fuse data combining both short term sensor data with long term knowledge databases to create cyberspace situational awareness this article offers a glimpse into the foggy crystal ball of future id systems before diving into the technical discussion we ask the reader to keep in mind the generic model of a datagram traversing the internet figure 1 illustrates an ip datagram moving in a store and forward environment from source to destination routed based on a destination address with a uncertain source address decrementing the datagram time to live ttl at every router hop 1 the datagram is routed through major internets and ip transit providers there is striking similarity between the transit of a datagram in the internet and an airplane through airspace future network management and air traffic control at a very high abstract level the concepts used to monitor objects in airspace apply to monitoring objects in networks the federal aviation administration faa divides airspace management into two distinct entities on the one hand local controllers guide aircraft into and out of the air space surrounding an airport their job is to maintain awareness of the location of all aircraft in their vicinity ensure proper separation identify threats to aircraft and manage the overall safety of passengers functionally this is similar to the role of network controllers who must control the environment within their administrative domains the network administrator must ensure the proper ports are open and the information is not delayed the collisions are kept to a minimum and the integrity of the delivery systems are not compromised this is naturally similar to the situational awareness required in current generation air traffic control atc e device an extensible active knowledge base system with multiple rule type support this paper describes e device an extensible active knowledge base system kbs that supports the processing of event driven production and deductive rules into the same active oodb system e device provides the infrastructure for the smooth integration of various declarative rule types such as production and deductive rules into an active oodb system that supports low level event driven rules only by a mapping each declarative rule into one event driven rule offering centralized rule selection control for correct run time behavior and conflict resolution and b using complex events to map the conditions of declarative rules and monitor the database to incrementally match those conditions e device provides the infrastructure for easily extending the system by adding a new rule types as subtypes of existing ones and b transparent optimizations to the rule matching network the resulting system is a flexible yet efficient kbs that gives the user the ability to express knowledge in a variety of high level forms for advanced problem solving in data intensive applications interbase kb integrating a knowledge base system with a multidatabase system for data warehousing this paper describes the integration of a multidatabase system and a knowledge base system to support landscapes on spaces of trees combinatorial optimization problems defined on sets of phylogenetic trees are an important issue in computational biology for instance the problem of reconstruction a phylogeny using maximum likelihood or parsimony approaches the collection of possible phylogenetic trees is arranged as a so called robinson graph by means of the nearest neighborhood interchange move the coherent algebra and spectra of robinson graphs are discussed in some detail as their knowledge is important for an understanding of the landscape structure we consider simple model landscapes as well as landscapes arising from the maximum parsimony problem focusing on two complementary measures of ruggedness the amplitude spectrum arising from projecting the cost functions onto the eigenspaces of the underlying graph and the topology of local minima and their connecting saddle points evaluating the novelty of text mined rules using lexical knowledge a data mining system may discover a large body of rules however relatively few of these may convey useful new knowledge to the user several metrics for evaluating the interestingness of mined rules have been proposed however most of these measure simplicity e g rule size certainty e g confidence or utility e g support another important aspect of interestingness is novelty does the rule represent an association that is currently unknown in this paper we present a new method of estimating the novelty of rules discovered by data mining methods using wordnet a lexical knowledge base of english words we have shown that novelty of a rule can be assessed by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule the more the average distance more is the novelty of the rule we present an experimental evaluation of this novelty metric by applying it to rules mined from book descriptions extracted from a the state of the art in distributed and dependable computing this report is dedicated to the memory of henrique fonseca using events for the scalable federation of heterogeneous components the thesis of this paper is that using our eventbased development principles components that were not designed to interoperate can be made to work together quickly and easily the only requirement is that each component must be made event based by adding an interface for registering interest in events and an interface for injecting actions a component notifies an event to a distributed client if the parameters of an event internal to the component match the parameters of a particular registration heterogeneous components can be federated using event based rules rules can respond to events from any component by injecting actions into any other component we show that the event paradigm is scalable by illustrating how event based components can be located worldwide using a federation of event brokers additionally we illustrate with 3 event based systems we have developed a component based multimedia system a multi user virtual worlds system and an augmented reality system for mobile users finally we show how the event paradigm is also scalable enough to allow event federation of entire systems not just single components we illustrate by showing how we have federated the operation of the 3 featured eventbased systems this enables for example real world mobile users to appear as avatars in the appropriate locations in the vr world and for these avatars to move in response to actual user movements programming by demonstration for information agents this article we will refer to the user in the female form while the agent will be referred to using male forms design of a component based augmented reality framework we propose a new approach to building augmented reality ar systems using a component based software framework this has advantages for all parties involved with ar systems our proposed framework consists of reusable distributed services for key subproblems of ar the middleware to combine them and an extensible software architecture infobeans configuration of personalized information assistants with the enormous amount of data contained in the www one of the crucial tasks a user has to face is the identification and aggregation of relevant pieces of information to satisfy her current information needs this paper presents an approach to the system supported configuration of individualized information services the programming by demonstration approach pursued by the infobeans releases the user from learning a programming language or dealing with technical subtleties the first version of this system will be released this fall keywords information assistants wrapper induction programming by demonstration information integration introduction the www provides an increasing amount of largely unrelated pieces of information like personal homepages as well as dedicated special purpose information systems like weather servers however creating useful and relevant information from ro oor hy sources to satisfy specific vqvo oovqhy needs is a largely unsupported cha trias trainable information assistants for cooperative problem solving software agents are intended to perform certain tasks on behalf of their users in many cases however the agent s competence is not sufficient to produce the desired outcome this paper presents an approach to cooperative problem solving in which an information agent and its user try to support each other in the achievement of a particular goal as a side effect the user can extend the agent s capabilities in a programming by demonstration dialog thus enabling it to autonomously perform similar tasks in the future 1 introduction software agents are intended to autonomously perform certain tasks on behalf of their users in many cases however the agent s competence might not be sufficient to produce the desired outcome instead of simply giving up and leaving the whole task to the user a much better alternative would be to precisely identify what the cause of the current problem is communicate it to another agent who can be expected to be able and willing to help and use th where are you pointing at a study of remote collaboration in a wearable videoconference system this paper reports on an empirical study aimed at evaluating the utility of a reality augmenting telepointer in a wearable videoconference system results show that using this telepointer a remote expert can effectively guide and direct a field worker s manual activities by analyzing verbal communication behavior and pointing gestures we were able to determine that experts overwhelmingly preferred pointing for guiding workers through physical tasks datablitz storage manager main memory database performance for critical applications introduction general purpose commercial disk based database systems though widely employed in practice have failed to meet the performance requirements of applications requiring short predictable response times and extremely high throughput rates main memory is the only technology capable of these characteristics datablitz 1 is a main memory storage manager product that supports the development of high performance and fault resilient applications requiring concurrent access to shared data in datablitz core algorithms for concurrency recovery index management and space management are optimized for the case that data is memory resident 2 datablitz architecture and features in this section we give a high level overview of the architecture and features of the datablitz storage manager product implemented at bell laboratories for more details see 1 direct access to data datablitz is designed to a supervised wrapper generation with lixto we illustrate basic features of the lixto wrapper generator such as the user and system interaction the capacious visual interface the marking and selecting procedures and the extraction tasks by describing the construction of a simple example program in the current lixto prototype 1 dab interactive haptic painting with 3d virtual brushes we present a novel painting system with an intuitive haptic interface which serves as an expressive vehicle for interactively creating painterly works we introduce a deformable 3d brush model which gives the user natural control of complex brush strokes the force feedback enhances the sense of realism and provides tactile cues that enable the user to better manipulate the paint brush we have also developed a bidirectional two layer paint model that combined with a palette interface enables easy loading of complex blends onto our 3d virtual brushes to generate interesting paint effects on the canvas the resulting system dab provides the user with an artistic setting which is conceptually equivalent to a real world painting environment several users have tested dab and were able to start creating original art work within minutes moral sentiments in multi agent systems we present a simulation of a society of agents where some of them have moral sentiments towards the agents that belong to the same social group using the iterated prisoner s dilemma as a metaphor for the social interactions besides the well understood phenomenon of short sighted self interested agents performing well in the short term but ruining their chances of such performance in the long run in a world of reciprocators the results suggest that where some agents are more generous than that these agents have a positive impact on the social group to which they belong without compromising too much their individual performance i e the group performance improves the inspiration for this project comes from a discussion on moral sentiments by m ridley we describe various simulations where conditions and parameters over determined dimensions were arranged to account for different types and compositions of societies further we indicate several lessons that arise from the analysis of the results and comparison of the different experiments we also relate this work to our previous anthropological approach to the adaptation of migrant agents and argue that allowing agents to possess suitably chosen emotions can have a decisive impact on multi agent systems this implies that some common notions of agent autonomy and related concepts should be reexamined instrumental interaction an interaction model for designing post wimp user interfaces this article introduces a new interaction model called instrumental interaction that extends and generalizes the principles of direct manipulation it covers existing interaction styles including traditional wimp interfaces as well as new interaction styles such as two handed input and augmented reality it defines a design space for new interaction techniques and a set of properties for comparing them instrumental interaction describes graphical user interfaces in terms of domain objects and interaction instruments interaction between users and domain objects is mediated by interaction instruments similar to the tools and instruments we use in the real world to interact with physical objects the article presents the model applies it to describe and compare a number of interaction techniques and shows how it was used to create a new interface for searching and replacing text keywords interaction model wimp interfaces direct manipulation post wimp interfaces instrumental oiled a reason able ontology editor for the semantic web ontologies will play a pivotal r ole in the semantic web where they will provide a source of precisely defined terms that can be communicated across people and applications oiled is an ontology editor that has an easy to use frame interface yet at the same time allows users to exploit the full power of an expressive web ontology language oil oiled uses reasoning to support ontology design facilitating the development of ontologies that are both more detailed and more accurate gripsee a gesture controlled robot for object perception and manipulation we have designed a research platform for a perceptually guided robot which also serves as a demonstrator for a coming generation of service robots in order to operate semi autonomously these require a capacity for learning about their environment and tasks and will have to interact directly with their human operators thus they must be supplied with skills in the fields of human computer interaction vision and manipulation gripsee is able to autonomously grasp and manipulate objects on a table in front of it the choice of object the grip to be used and the desired final position are indicated by an operator using hand gestures grasping is performed similar to human behavior the object is first fixated then its form size orientation and position are determined a grip is planned and finally the object is grasped moved to a new position and released as a final example for useful autonomous behavior we show how the calibration of the robot s image to world coordinate transform can be learned from experience thus making detailed and unstable calibration of this important subsystem superfluous the integration concepts developed at our institute have led to a flexible library of robot skills that can be easily recombined for a variety of useful behaviors jazz an extensible zoomable user interface graphics toolkit in java in this paper we investigate the use of scene graphs as a general approach for implementing two dimensional 2d graphical applications and in particular zoomable user interfaces zuis scene graphs are typically found in three dimensional 3d graphics packages such as sun s java3d and sgi s openinventor they have not been widely adopted by 2d graphical user interface toolkits to explore the effectiveness of scene graph techniques we have developed jazz a general purpose 2d scene graph toolkit jazz is implemented in java using java2d and runs on all platforms that support java 2 this paper describes jazz and the lessons we learned using jazz for zuis it also discusses how 2d scene graphs can be applied to other application areas keywords zoomable user interfaces zuis animation graphics user interface management systems uims pad jazz introduction today s graphical user interface gui toolkits contain a wide range of built in user interface objects also kno pad a zoomable graphical sketchpad for exploring alternate interface physics we describe pad a zoomable graphical sketchpad that we are exploring as an alternative to traditional window and icon based interfaces we discuss the motivation for pad describe the implementation and present prototype applications in addition we introduce an informational physics strategy for interface design and briefly contrast it with current design strategies we envision a rich world of dynamic persistent informational entities that operate according to multiple physics specifically designed to provide cognitively facile access and serve as the basis for design of new computationally based work materials 1 to appear in the journal of visual languages and computing pad a zoomable graphical sketchpad for exploring alternate interface physics 1 benjamin b bederson james d hollan computer science department university of new mexico albuquerque nm 87131 bederson cs unm edu hollan cs unm edu ken perlin jonathan meyer david bacon media research laboratory co statistical models for text segmentation abstract this paper introduces a new statistical approach to automatically partitioning text into coherent segments the approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text the models use two classes of features topicality features that use adaptive language models in a novel way to detect broad changes of topic and cue word features that detect occurrences of specific words which may be domain specific that tend to be used near segment boundaries assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains wall street journal news articles and television broadcast news story transcripts quantitative results on these domains are presented using a new probabilistically motivated error metric which combines precision and recall in a natural and flexible way this metric is used to make a quantitative assessment of the relative contributions of the different feature types as well as a comparison with decision trees and previously proposed text segmentation algorithms 1 causal models of mobile service robot behavior temporal projection the process of predicting what will happen when a robot executes its plan is essential for autonomous service robots to successfully plan their missions this paper describes a causal model of the behavior exhibited by the mobile robot rhino when running concurrent reactive plans for performing office delivery jobs the model represents aspects of robot behavior that cannot be represented by most action models used in ai planning it represents the temporal structure of continuous control processes several modes of their interferences and various kinds of uncertainty this enhanced expressiveness enables xfrm mcd92 bm94 a robot planning system to predict and therefore forestall various kinds of behavior flaws including missed deadlines whilst exploiting incidental opportunities the proposed causal model is experimentally validated using the robot and its simulator introduction temporal projection the process of predicting what will happen when a ro improving the registration precision by visual horizon silhouette matching a system for enhancing the situational awareness in an outdoor scenario by augmented reality ar techniques can utilize visual clues for improving registration precision if visible terrain silhouettes provide unique features to be matched with digital elevation map dem data the best match of a visually extracted silhouette with the dem silhouette provides camera observer orientation elevation and azimuth we have developed such a registration system which runs on a pc 200 mhz and is being ported to a wearable ar system 1 introduction and context 1 1 augmented reality in the past years augmented reality ar has gained significant attention due to rapid progress in several key areas wearable computing virtual reality rendering 2 ar technology provides means of intuitive information presentation for enhancing the situational awareness and perception by exploiting the natural and familiar human interaction modalities with the environment completely immersive a metaseek a content based meta search engine for images search engines are the most powerful resources for finding information on the rapidly expanding world wide web www finding the desired search engines and learning how to use them however can be very time consuming the integration of such search tools enables the users to access information across the world in a transparent and efficient manner these systems are called meta search engines the recent emergence of visual information retrieval vir search engines on the web is leading to the same efficiency problem this paper describes and evaluates metaseek a content based meta search engine used for finding images on the web based on their visual information metaseek is designed to intelligently select and interface with multiple on line image search engines by ranking their performance for different classes of user queries user feedback is also integrated in the ranking refinement we compare metaseek with a base line version of meta search engine which does not use the past performance of the different search engines in recommending target search engines for future queries mediacups experience with design and use of computer augmented everyday artefacts our view of ubiquitous computing is artefact centred in this view computers are considered as secondary artefacts that enable items of everyday use as networked digital artefacts this view is expressed in an artefact computing model and investigated in the mediacup project an evolving artefact computing environment the mediacup project provides insights into the augmentation of artefacts with sensing processing and communication capabilities and into the provision of an open infrastructure for information exchange among artefacts one of the artefacts studied is the mediacup itself an ordinary coffee cup invisibly augmented with computing and context awareness the mediacup and other computeraugmented everyday artefacts are connected through a network infrastructure supporting loosely coupled spatially defined communication keywords ubiquitous computing digital artefacts context awareness networking embedded systems mediacup 1 introduction computers are becoming ubi one way functions are essential for single server private information retrieval private information retrieval pir protocols allow a user to read information from a database without revealing to the server storing the database which information he has read kushilevitz and ostrovsky 23 construct based on the quadratic residuosity assumption a single server pir protocol with small communication complexity cachin micali and stadler 5 present a single server pir protocol with a smaller communication complexity based on the new phihiding assumption a major question addressed in the present work is what assumption is the minimal assumption necessary for the construction of single server private information retrieval protocols with small communication complexity we prove that if there is a 0 error pir protocol in which the server sends less than n bits then one way functions exist where n is the number of bits in the database that is even saving one bit compared to the naive protocol in which the entire database is sent already requires one way implementing a knowledge date a base knowledge based systems are very useful but can be dicult to design because of the complexity of the realworld knowledge they represent this paper compares the experiences of building the same knowledge base by hand in two dierent systems otter and clips the knowledge base considered is that of people s preferences towards others in the interests of nding a dating match finally this paper considers horn theorems and their impact on the usefulness of knowledge systems introduction because technology and automation are increasingly becoming a part of everyday life it is benecial to enable technology to understand its application area an obvious way of doing this is to implement and embed a knowledge base in an application however designing a good knowledge base is not trivial a good knowledge base needs to be general so it can be reused complete to avoid bad models and ecient in description and time this paper presents the authors experiences implement what can partitioning do for your data warehouses and data marts efficient query processing is a critical requirement for data warehousing systems as decision support applications often require minimum response times to answer complex ad hoc queries having aggregations multi ways joins over vast repositories of data this can be achieved by fragmenting warehouse data the data fragmentation concept in the context of distributed databases aims to reduce query execution time and facilitates the parallel execution of queries in this paper we propose a methodology for applying the fragmentation technique in a data warehouse dw star schema to reduce the total query execution cost we present an algorithm for fragmenting the tables of a star schema during the fragmentation process we observe that the choice of the dimension tables used in fragmenting the fact table plays an important role on overall performance therefore we develop a greedy algorithm in selecting best dimension tables we propose an analytical cost model for executing a set of olap queries on a fragmented star schema finally we conduct some experiments to evaluate the utility of fragmentation for efficiently executing olap queries key words data warehouses star schema fragmentation query optimization performance evaluation 1 how to monitor and control resource usage in mobile agent systems the mobile agent technology has already shown its advantages but at the same time has already remarked new problems currently limiting its diffusion in commercial environments a key issue is to control the operations that foreign mobile agents are authorized to perform on hosting execution environments it is necessary not only to rule the ma access to resources but also to control resource usage of admitted agents at execution time for instance to protect against possible denial of service attacks the paper presents a solution framework for the on line monitoring and control of java based ma platforms in particular it describes the design and implementation of mapi an on line monitoring component that we have integrated within the soma system the paper shows how to use mapi as the building block of a distributed monitoring tool that gives application and kernel level information about the state of mobile agents and their resource usage thus enabling the enforcement of management policies on ma resource consumption 1 multi item auctions for automatic negotiation available resources can often be limited with regard to the number of demands in this paper we propose an approach for solving this problem which consists of using the mechanisms of multi item auctions for allocating the resources to a set of software agents we consider the resource problem as a market in which there are vendor agents and buyer agents which trade on items representing the resources these agents use multi item auctions which are viewed here as a process of automatic negotiation and implemented as a network of intelligent software agents in this negotiation agents exhibit different acquisition capabilities which let them act differently depending on the current context or situation of the market for example the richer an agent is the more items it can buy i e the more resources it can acquire we present a model for this approach based on the english auction then we discuss experimental evidence of such a model model checking multiagent systems model checking is a very successful technique which has been applied in the design and verification of finite state concurrent reactive processes in this paper we show how this technique can be lifted to be applicable to multiagent systems our approach allows us to reuse the technology and tools developed in model checking to design and verify multiagent systems in a modular and incremental way and also to have a very efficient model checking algorithm 1 introduction model checking is a very successful automatic technique which has been devised for the design and verification of finite state reactive systems e g sequential circuit designs communication protocols and safety critical control systems see e g 2 there is evidence that model checking when applicable is far more successful than the other approaches to formal methods and verification e g first order or inductive theorem proving tableau based reasoning about modal satisfiability nowadays many very eff si designer a tool for intelligent integration of information si designer source integrator designer is a designer support tool for semi automatic integration of heterogeneous sources schemata relational object and semi structured sources it has been implemented within the momis project and it carries out integration following a semantic approach which uses intelligent description logics based techniques clustering techniques and an extended odmg odl language to represent schemata extracted integrated information starting from the sources descriptions local schemata si designer supports the designer in the creation of an integrated view of all the sources global schema which is expressed in the same language we propose si designer as a tool to build virtual catalogs in the e commerce environment 1 information integration the momis project demonstration ranted provided that the copies are not made or distributed for direct commercial advantage the vldb copyright notice and the title of the publication and its date appear and notice is given that copying is by permission of the very large data base endowment to copy otherwise or to republish requires a fee and or special permission from the endowment proceedings of the 26th vldb conference cairo egypt 2000 2 momis is a joint project among the universit a di modena e reggio emilia the universit a di milano and the universit a di brescia within the national research project interdata theme n 3 integration of information over the web coordinated by v de antonellis universit a di brescia 1 a common data model odm i 3 which is defined according to the odl i 3 language to describe source schemas for integration purposes odm i 3 and odl i 3 f12 24 designing storytelling technologies to encourage collaboration between young children we describe the iterative design of two collaborative storytelling technologies for young children kidpad and the klump we focus on the idea of designing interfaces to subtly encourage collaboration so that children are invited to discover the added benefits of working together this idea has been motivated by our experiences of using early versions of our technologies in schools in sweden and the uk we compare the approach of encouraging collaboration with other approaches to synchronizing shared interfaces we describe how we have revised the technologies to encourage collaboration and to reflect design suggestions made by the children themselves keywords children single display groupware sdg computer supported cooperative work cscw education computer supported collaborative learning cscl introduction collaboration is an important skill for young children to learn educational research has found that working in pairs or small groups can have beneficial effects on l a content based image meta search engine using relevance feedback search engines are the most powerful resources for finding information on the rapidly expanding world wide web finding the desired search engines and learning how to use them however can be very time consuming metasearch engines which integrate a group of such search tools enable users to access information across the world in a transparent and more efficient manner the recent emergence of visual information retrieval vir systems on the web is leading to the same efficiency problem this paper describes metaseek a meta search engine used for retrieving images based on their visual content on the web metaseek is designed to intelligently select and interface with multiple on line image search engines by ranking their performance for different classes of user queries user feedback is also integrated in the ranking refinement metaseek has been developed to explore the issues involved in querying large distributed on line visual information system sources we compare metasee using relevance feedback in contentbased image metasearch this article with a review of the issues in content based visual query then describe the current metaseek implementation we present the results of experiments that evaluated the implementation in comparison to a previous version of the system and a baseline engine that randomly selects the individual search engines to query we conclude by summarizing open issues for future research combinations of modal logics combining logics for modelling purposes has become a rapidly expanding enterprise that is inspired mainly by concerns about modularity and the wish to join together different kinds of information as any interesting real world system is a complex composite entity decomposing its descriptive requirements for design verification or maintenance purposes into simpler more restricted reasoning tasks is not only appealing but is often the only plausible way forward it would be an exaggeration to claim that we currently have a thorough understanding of combined methods however a core body of notions questions and results has emerged for an important class of combined logics and we are beginning to understand how this core theory behaves when it is applied outside this particular class in this paper we will consider the combination of modal including temporal logics identifying leading edge research that we and others have carried out such combined sys three ways to grow designs a comparison of evolved embryogenies for a design problem this paper explores the use of growth processes or embryogenies to map genotypes to phenotypes within evolutionary systems following a summary of the significant features of embryogenies the three main types of embryogenies in evolutionary computation are then identified and explained external explicit and implicit an experimental comparison between these three different embryogenies and an evolutionary algorithm with no embryogeny is performed the problem set to the four evolutionary systems is to evolve tessellating tiles in order to assess the scalability of the embryogenies the problem is increased in difficulty by enlarging the size of tiles to be evolved the results are surprising with the implicit embryogeny outperforming all other techniques by showing no significant increase in the size of the genotypes or decrease in accuracy of evolution as the scale of the problem is increased 1 introduction the use of computers to evolve solutions to problems has seen a dra compiling standard ml to java bytecodes ing with credit is permitted to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee request permissions from publications dept acm inc fax 1 212 869 0481 or permissions acm org compiling standard ml to java bytecodes nick benton andrew kennedy george russell persimmon it inc cambridge u k fnick andrew georgeg persimmon co uk abstract mlj compiles sml 97 into verifier compliant java bytecodes its features include type checked interlanguage working extensions which allow ml and java code to call each other automatic recompilation management compact compiled code and runtime performance which using a just in time compiling java virtual machine usually exceeds that of existing specialised bytecode interpreters for ml notable features of the compiler itself include whole program optimisation based on rewriting compilation of polymorphism by specialisation a novel monadic intermediate lang static management of integrity in object oriented databases design and implementation abstract in this paper we propose an efficient technique to statically manage integrity constraints in object oriented database programming languages we place ourselves in the context of a simplified database programming language close to o2 in which we assume that updates are undertaken by means of methods an important issue when dealing with constraints is that of efficiency a nave management of such constraints can cause a severe floundering of the overall system our basic assumption is that the run time checking of constraints is too costly to be undertaken systematically therefore methods that are always safe with respect to integrity constraints should be proven so at compile time the run time checks should only concern the remaining methods to that purpose we propose a new approach based on the use of predicate transformers combined with automatic theorem proving techniques to prove the invariance of integrity constraints under complex methods we then describe the current implementation of our prototype and report some experiments that have been performed with it on non trivial examples the counterpart of the problem of program verification is that of program correction static analysis techniques can also be applied to solve that problem we present a systematic approach to undertake the automatic correction of potentially unsafe methods however the advantages of the latter technique are not as clear as those of program verification we will therefore discuss some arguments for and against the use of method correction 1 mobile agents for information integration the large amount of information that is spread over the internet is an important resource for all people but also introduces some issues that must be faced the dynamism and the uncertainty of the internet along with the heterogeneity of the sources of information are the two main challanges for the today s technologies this paper proposes an approach based on mobile agents integrated in an information integration infrastructure mobile agents can significantly improve the design and the development of internet applications thanks to their characteristics of autonomy and adaptability to open and distributed environments such as the internet momis mediator environment for multiple information sources is an infrastructure for semi automatic information integration that deals with the integration and query of multiple heterogeneous information sources relational object xml and semi structured sources the aim of this paper is to show the advantage of the introduction in the momis infrastructure of intelligent and mobile software agents for the autonomous management and coordination of the integration and query processes over heterogeneous data sources 1 an intelligent approach to information integration information sharing from multiple heterogeneous sources is a challenging issue which ranges from database to ontology areas in this paper we propose an intelligent approach to information integration which takes into account semantic conflicts and contradictions caused by the lack of a common shared ontology our goal is to provide an integrated access to information sources allowing a user to pose a single query and to receive a single unified answer we propose a semantic approach for integration where the conceptual schema of each source is provided adopting a common standard data model and language description logics plus clustering techniques are exploited description logics is used to obtain a semi automatic generation of a common thesaurus to solve semantic heterogeneities and to derive a common ontology while clustering techniques are employed to build the global schema i e the unified view of the data to be used for query processing keywords intelligent info a semantic approach to information integration the momis project this paper we propose a semantic approach to the integration of heterogeneous information the approach follows the semantic paradigm in that conceptual schemata of an involved source are considered and a common data model odm i 3 and language odl i 3 are adopted to describe sharable information odm i 3 and odl i 3 are defined as a subset of the corresponding odmg 93 13 odm and odl a description logics ocdl object description language with constraints 6 is used as a intelligent techniques for the extraction and integration of heterogeneous information developing intelligent tools for the integration of information extracted from multiple heterogeneous sources is a challenging issue to effectively exploit the numerous sources available on line in global information systems in this paper we propose intelligent tool supported techniques to information extraction and integration which take into account both structured and semistructured data sources an object oriented language called odl i 3 derived from the standard odmg with an underlying description logics is introduced for information extraction odl i 3 descriptions of the information sources are exploited first to set a shared vocabulary for the sources information integration is performed in a semiautomatic way by exploiting odl i 3 descriptions of source schemas with a combination of description logics and clustering techniques techniques described in the paper have been implemented in the momis system based on a conventional mediator architecture keywords hetero semantic integration of semistructured and structured data sources this paper is to describe the momis 4 5 mediator environment for multiple information sources approach to the integration and query of multiple heterogeneous information sources containing structured and semistructured data momis has been conceived as a joint collaboration between university of milano and modena in the framework of the interdata national research project aiming at providing methods and tools for data management in internet based information systems like other integration projects 1 10 14 momis follows a semantic approach to information integration based on the conceptual schema or metadata of the information sources and on the following architectural elements i a common object oriented data model defined according to the odl i 3 language to describe source schemas for integration purposes the data model and odl i 3 have been defined in momis as subset of the odmg 93 ones following the proposal for a standard mediator language developed by the i collection synthesis the invention of the hyperlink and the http transmission protocol caused an amazing new structure to appear on the internet the world wide web with the web there came spiders robots and web crawlers which go from one link to the next checking web health ferreting out information and resources and imposing organization on the huge collection of information and dross residing on the net this paper reports on the use of one such crawler to synthesize document collections on various topics in science mathematics engineering and technology such collections could be part of a digital library focused crawls tunneling and digital libraries crawling the web to build collections of documents related to pre specified topics became an active area of research during the late 1990 s after crawler technology was developed for the benefit of search engines now web crawling is being seriously considered as an important strategy for building large scale digital libraries this paper considers some of the crawl technologies that might be exploited for collection building for example to make such collection building crawls more effective focused crawling was developed in which the goal was to make a best first crawl of the web we are using powerful crawler software to implement a focused crawl but use tunneling to overcome some of the limitations of a pure best first approach tunneling has been described by others as not only prioritizing links from pages according to the page s relevance score but also estimating the value of each link and prioritizing on that as well we add to this mix by devising a tunneling focused crawling strategy which evaluates the current crawl direction on the fly to determine when to terminate a tunneling activity results indicate that a combination of focused crawling and tunneling could be an e ective tool for building digital libraries text categorization and prototypes this document however in accordance with what we mentioned previously a member of a category does not necessarily have to have all the quintessential features of a category and this constitutes one of the big problems in choosing an algorithm for making a category representative data warehouse scenarios for model management model management is a framework for supporting meta data related applications where models and mappings are manipulated as first class objects using operations such as match merge applyfunction and compose to demonstrate the approach we show how to use model management in two scenarios related to loading data warehouses the case study illustrates the value of model management as a methodology for approaching meta data related problems it also helps clarify the required semantics of key operations these detailed scenarios provide evidence that generic model management is useful and very likely implementable panel is generic metadata management feasible dels such as invert and compose what is the role of an expression language that captures the semantics of models and mappings not only for design but also for run time execution does a generic approach offer any advantages for model manipulation areas of current interest such as data integration and xml if the skeptics are right that a generic approach to model management is unachievable pie in the sky are writers of metadata driven applications doomed forever to writing special purpose object at a time code for navigating their information structures if so what is the leverage that the database field can offer for these problems 2 panelists dr laura haas ibm research is working on a tool that can semi automatically produce mappings between two data representations she has been working on various aspects of data integration since starting the garlic project in 1994 a vision for management of complex models many problems encountered when building applications of database systems involve the manipulation of models by model we mean a complex structure that represents a design artifact such as a relational schema object oriented interface uml model xml dtd web site schema semantic network complex document or software configuration many uses of models involve managing changes in models and transformations of data from one model into another these uses require an explicit representation of mappings between models we propose to make database systems easier to use for these applications by making model and model mapping first class objects with special operations that simplify their use we call this capability model management in addition to making the case for model management our main contribution is a sketch of a proposed data model the data model consists of formal object oriented structures for representing models and model mappings and of high level algebraic operations on those structures such as matching differencing merging function application selection inversion and instantiation we focus on structure and semantics not implementation 1 augmenting reality in mobile substrates on the design of computer support for process control the paper investigates augmented reality as a perspective on the design of computer support for process control in a distributed environment based on empirical studies of work in a wastewater treatment plant three technical approaches on augmented reality augmenting the user the object of work and the environment are examined in terms of a collection of design scenarios we conclude that these approaches when used as metaphors rather than a consistent theoretical framework may inform design of mobile support for process control work keywords augmented reality process control mobile computing human computer interaction participatory design workplace studies brt keywords ab fa fc ga hd introduction advanced technical process settings such as modern wastewater treatment plants are characterised by being highly distributed and dynamic a possible strategy for supporting work in such settings is through the introduction of mobile technology in this paper dynamics in wastewater treatment a framework for understanding formal constructs in complex technical settings based on the study of unskilled work in a danish wastewater treatment plant the problem of formalisation of work is discussed and extended to technical processes five symmetrical levels of dynamics in complex technical work arrangements are proposed as a tool for understanding the limits of formalisation and for designing formal constructs in such settings the analysis is based on concepts of heterogeneity granularity of goals and motives and process and structure introduction an inevitable problem in the design of cscw systems is that work is not standing still rather work settings are dynamic routines evolve over time and unusual situations force deviations from the routine for this reason purely formal constructs and descriptions of work have proven inadequate when designing effective real world cscw systems suchman and wynn 1984 set off the debate about the role of formalism in cscw and their empirical studies clearly illustrated that there is more to office wo measuring the structural similarity among xml documents and dtds sources of xml documents are proliferating on the web and documents are more and more frequently exchanged among sources at the same time there is an increasing need of exploiting database tools to manage this kind of data an important novelty of xml is that information on document structures is available on the web together with the document contents this information can be used to improve document handling and to achieve more effective and efficient searches on documents however in such an heterogeneous environment as the web it is not reasonable to assume that xml documents that enter a source always conform to a predefined dtd present in the source definition and analysis of index organizations for object oriented database systems the efficient execution of queries in object oriented databases requires the design of specific indexing techniques to efficiently deal with predicates against nested attributes or against class inheritance hierarchies indexing techniques so far proposed can be classified into three groups inheritance indexing techniques whose goal is to support queries along inheritance hierarchies aggregation indexing techniques dealing with the efficient evaluation of nested predicates integrated techniques the aim of this paper is to analyze two techniques providing an integrated support the path index and the nested inherited index with respect to traditional techniques such as the multi index and the inherited multi index the analysis is performed assuming that multi valued attributes as well as instances with null attribute values are present in the database for this purpose the paper first presents the considered techniques an extension of the path index firstly defined in 6 an approach to classify semi structured objects several advanced applications such as those dealing with the web need to handle data whose structure is not known a priori such requirement severely limits the applicability of traditional database techniques that are based on the fact that the structure of data e g the database schema is known before data are entered into the database moreover in traditional database systems whenever a data item e g a tuple an object and so on is entered the application species the collection e g relation class and so on the data item belongs to collections are the basis for handling queries and indexing and therefore a proper classication of data items in collections is crucial in this paper we address this issue in the context of an extended object oriented data model we propose an approach to classify objects created without specifying the class they belong to in the most appropriate class of the schema that is the class closest to the object state in particular w a conceptual annotation approach to indexing in a web based information system all the specialists have agreed that the possibility of adding to multimedia www objects some sort of conceptual annotations describing their information content would greatly contribute to solve the problem of their intelligent indexing and retrieval we propose to associate with the web objects not the final conceptual annotation but a simple natural language nl caption in the form of short texts representing a general neutral description of their informational content the nl caption will then be converted into a conceptual annotation in nkrl narrative knowledge representation language making use of an automatic translation system like those we have implemented in the context of recent european projects 1 introduction many applications enabled by the www such as distance learning electronic commerce information gathering and filtering have strong need for tools supporting the effective retrieval of information it is today well recognized that an effective retriev enhancing the expressive power of the u datalog language u datalog has been developed with the aim of providing a set oriented logical update language guaranteeing update parallelism in the context of a datalog like language in u datalog updates are expressed by introducing constraints p x to denote insertion and p x to denote deletion inside datalog rules a u datalog program can be interpreted as a clp program in this framework a set of updates constraints is satisable if it does not represent an inconsistent theory that is it does not require the insertion and the deletion of the same fact this approach resembles a very simple form of negation however on the other hand u datalog does not provide any mechanism to explicitly deal with negative information resulting in a language with limited expressive power in this paper we provide a semantics based on stratication handling the use of negated atoms in u datalog programs and we show which problems arise in dening a compositional semantics klava a java framework for distributed and mobile applications highly distributed networks have now become a common infrastructure for a new kind of wide area distributed applications whose key design principle is network awareness namely the ability of dealing with dynamic changes of the network environment network aware computing has called for new programming languages that exploit the mobility paradigm as a basic interaction mechanism in this paper we present the architecture of klava an experimental java framework for distributed applications and code mobility we explain how klava implements code mobility by relying on java and show a few distributed applications that exploit mobile code and are programmed in klava keywords code mobility distributed applications network awareness language and middleware implementation tuple spaces java 1 x klaim and klava programming mobile code highly distributed networks have now become a common infrastructure for a new kind of wide area distributed applications whose key design principle is network awareness namely the ability to deal with dynamic changes of the network environment network aware computing has called for new programming languages that exploit the mobility paradigm as the basic interaction mechanism in this paper we present the klaim kernel language for agent interaction and mobility framework for programming mobile code applications namely the x klaim programming language and the java based run time system klava in particular we illustrate how klava handles mobile code finally an example is shown that is implemented using this framework symbolic representation of user defined time granularities in the recent literature on time representation an effort has been made to characterize the notion of time granularity and the relationships between granularities in order to have a common framework for their specification and to allow the interoperability of systems adopting different time granularities this paper considers the mathematical characterization of finite and periodical time granularities and it identifies a user friendly symbolic formalism which captures exactly that class of granularities this is achieved by a formal analysis of the expressiveness of well known symbolic representation formalisms 1 introduction there is a wide agreement in the ai and database community on the requirement for a data knowledge representation system of supporting standard as well as user defined time granularities examples of standard time granularities are days weeks months while user defined granularities may include businessweeks trading days working shifts school terms wi mia an ubiquitous multi agent web information system this paper gives a brief overview about ai methods and techniques we have developed for building ubiquitous web information systems these methods from areas of machine learning logic programming knowledge representation and multi agent systems are discussed in the context of our prototypical information system mia mia is a web information system for mobile users who are equipped with a pda palm pilot a cellular phone and a gps device or cellular wap phone it captures the main issues of ubiquitous computing location awareness anytime information access and pda technology 1 introduction nowadays the biggest but also the most chaotic and unstructured source of information is the world wideweb making this immense amount of information available for ubiquitous computing in daily life is a great challenge besides hardware issues for wireless ubiquitous computing that still are to be solved wireless communication blue tooth technologies wearable computing units integrat keyword searching and browsing in databases using banks with the growth of the web there has been a rapid increase in the number of users who need to access online databases without having a detailed knowledge of the schema or of query languages even relatively simple query languages designed for non experts are too complicated for them we describe banks a system which enables keyword based search on relational databases together with data and schema browsing banks enables users to extract information in a simple manner without any knowledge of the schema or any need for writing complex queries a user can get information by typing a few keywords following hyperlinks and interacting with controls on the displayed results banks models tuples as nodes in a graph connected by links induced by foreign key and other relationships answers to a query are modeled as rooted trees connecting tuples that match individual keywords in the query answers are ranked using a notion of proximity coupled with a notion of prestige of nodes based on inlinks similar to techniques developed for web search we present an efficient heuristic algorithm for finding and ranking query results 1 improved algorithms for topic distillation in a hyperlinked environment abstract this paper addresses the problem of topic distillation on the world wide web namely given a typ ical user query to find quality documents related to the query topic connectivity analysis has been shown to be useful in identifying high quality pages within a topic specific graph of hyperlinked documents the essence of our approach is to augment a previous connectivity anal ysis based algorithm with content analysis we identify three problems with the existing approach and devise al gorithms to tackle them the results of a user evaluation are reported that show an improvement of precision at 10 documents by at least 45 over pure connectivity anal ysis 1 a comparison of techniques to find mirrored hosts on the www we compare several algorithms for identifying mirrored hosts on the world wide web the algorithms operate on the basis of url strings and linkage data the type of information easily available from web proxies and crawlers identification of mirrored hosts can improve web based information retrieval in several ways first by identifying mirrored hosts search engines can avoid storing and returning duplicate documents second several new information retrieval techniques for the web make inferences based on the explicit links among hypertext documents mirroring perturbs their graph model and degrades performance third mirroring information can be used to redirect users to alternate mirror sites to compensate for various failures and can thus improve the performance of web browsers and proxies this work was presented at the workshop on organizing web space at the fourth acm conference on digital libraries 1999 we evaluated 4 classes of top down algorithms for detecting finding code on the world wide web a preliminary investigation to find out what kind of design structures programmers really use we need to examine a wide variety of programs unfortunately most program source code is proprietary and is unavailable for analysis the world wide web web potentially can provide a rich source of programs for study the freely available code on the web if in sufficient quality and quantity can provide a window into software design as it is practiced today in a preliminary study of source code availability on the web we estimate that 4 of urls contain object oriented source code and 9 of urls contain executable code either binary or class files this represents an enormous resource for program analysis we can with some risk of inaccuracy conservatively project our sampling results to the entire web our estimate is that the web contains at least 3 4 million files containing either java c or perl source code 20 3 million files containing c source code and 8 7 million files containing executable code keywords design source code analysis world wide web estimation code on the world wide web 1 dlv an overview the intelligent grounding the model generator and the model checker all of these modules perform a modular evaluation of their input according to various dependency graphs as de ned in 5 2 and try to detect and eciently handle special syntactic subclasses which in general yields a tremendous speedup supported by fwf austrian science funds under the project p11580 mat a query system for disjunctive deductive databases please address correspondence to this author the intelligent grounding takes an input program whose facts can be stored also in the tables of external relational databases and eciently generates a subset of the program instantiation that has exactly the same stable models as the full program but is much smaller in general for strati ed programs for example the grounding already computes the single stable model then the model generator is run on the ground output of the intelligent grounding it generates one candidate for a stable model a risk management in concurrent engineering in presence of intelligent agents contents 1 introduction 2 1 1 objective 3 1 2 requirements 3 1 3 proposal 3 1 4 working principle 4 1 5 discussion 5 2 current status 5 2 1 architecture 5 2 2 sdma as an agent 7 2 3 assumptions about the environment 7 2 4 sdma risk version 0 1 8 3 summary 8 a kqml specification for sd systems management in concurrent engineering using intelligent software agents intelligent software agents are used in frameworks where large number of experts need to interact in a project concurrently as in the projects taken up by the aerospace industry we describe one such framework and discuss an intelligent software agent to manage the systems design in such an environment what makes the problem interesting is the existence of other intelligent agents in the framework that are responsible for various other tasks as well as other human users our systems design management agent sdma uses its own domain knowledge to interact with the other agents and recommend strategies and policies we take one example task of systems design management risk management and discuss how it is performed by the sdma in detail 1 introduction using intelligent software agents within the concurrent engineering paradigm has received attention from several research groups the idea is to respond to the increased information and coordination demands of concurrent engineering p experiments on human robot communication with robota an imitative learning and communicating doll robot imitation 1 and communication behaviours are important means of interaction between humans and robots in experiments on robot teaching by demonstration imitation and communication behaviours can be used by the demonstrator to drive the robot s attention to the demonstrated task in a children game they play an important role to engage the interaction between the child and the robot and to stimulate the child s interest in this work we study how imitation skills can be used for teaching a robot a symbolic communication system to describe its actions and perceptions we report on experiments in which we study human robot interactions using a doll robot robota is a robot whose shape is similar to that of a doll and which has the capacity to learn imitate and communicate through simple phototaxis behaviour the robot can imitate mirror the arms and head s movements of a demonstrator the robot is controlled by a dynamical recurrent associative memory architecture drama wh a wearable spatial conferencing space wearable computers provide constant access to computing and communications resources in this paper we describe how the computing power of wearables can be used to provide spatialized 3d graphics and audio cues to aid communication the result is a wearable augmented reality communication space with audio enabled avatars of the remote collaborators surrounding the user the user can use natural head motions to attend to the remote collaborators can communicate freely while being aware of other side conversations and can move through the communication space in this way the conferencing space can support dozens of simultaneous users informal user studies suggest that wearable communication spaces may offer several advantages both through the increase in the amount of information it is possible to access and the naturalness of the interface 1 introduction one of the broad trends emerging in human computer interaction is the increasing portability of computing and communication fac a learning agent for wireless news access we describe a user interface for wireless information devices specifically designed to facilitate learning about users individual interests in daily news stories user feedback is collected unobtrusively to form the basis for a content based machine learning algorithm as a result the described system can adapt to users individual interests reduce the amount of information that needs to be transmitted and help users access relevant information with minimal effort keywords wireless intelligent information access news user modeling machine learning 1 introduction driven by the explosive growth of information available on the internet intelligent information access has become a central research area in computer science the 20 th century is commonly characterized as the information age and the sheer amount of information readily available today has created novel challenges numerous intelligent information agents software tools that provide personalized assistanc design and implementation of the j seal2 mobile agent kernel j seal2 is a secure portable and efficient execution environment for mobile agents the core of the system is a micro kernel fulfilling the same functions as a traditional operating system kernel protection communication domain termination and resource control this paper describes the key concepts of the j seal2 micro kernel and how they are implemented in pure java the olac metadata set and controlled vocabularies as language data and associated technologies proliferate and as the language resources community rapidly expands it has become difficult to locate and reuse existing resources are there any lexical resources for such and such a language what tool can work with transcripts in this particular format what is a good format to use for linguistic data of this type questions like these dominate many mailing lists since web search engines are an unreliable way to find language resources this paper describes a new digital infrastructure for language resource discovery based on the open archives initiative and called olac the open language archives community the olac metadata set and the associated controlled vocabularies facilitate consistent description and focussed searching we report progress on the metadata set and controlled vocabularies describing current issues and soliciting input from the language resources community 1 gtm the generative topographic mapping latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent or hidden variables a familiar example is factor analysis which is based on a linear transformations between the latent space and the data space in this paper we introduce a form of non linear latent variable model called the generative topographic mapping for which the parameters of the model can be determined using the em algorithm gtm provides a principled alternative to the widely used self organizing map som of kohonen 1982 and overcomes most of the significant limitations of the som we demonstrate the performance of the gtm algorithm on a toy problem and on simulated data from flow diagnostics for a multi phase oil pipeline gtm the generative topographic mapping 2 1 introduction many data sets exhibit significant correlations between the variables one way to capture such structure is to model the distribution of the data in term constraints in object oriented databases normal forms in relational database theory like 3nf or bcnf are dened by means of semantic contraints since for these constraints sound and complete axiomatisations exist and additionally for some of these constraints the implication problem is decidable computer aided database design is possible for relational data models object oriented database theory lacks such normal forms partly because neither a classication of semantic constraints nor sound and complete axiomatisations exist in this work we present three classes of semantic constraints for object oriented data models and show that these constraints have a sound and complete axiomatisation thus we prepare the grounds for normal forms in object oriented data models and subsequently for computer aided object oriented database design 1 introduction the theory of database design for relational data models identies a number of properties to characterise good database schemas these properties lead then to no decomposition of database classes under path functional dependencies and onto constraints based on f logic we specify an advanced data model with object oriented and logic oriented features that substantially extend the relational approach for this model we exhibit and study the counterpart to the well known decomposition of a relation scheme according to a nontrivial nonkey functional dependency for decomposing a class of a database schema the transformation of pivoting is used pivoting separates apart some attributes of the class into a newly generated class this new class is declared to be a subclass of the result class of the so called pivot attribute moreover the pivot attribute provides the link between the original class and the new subclass we identify the conditions for the result of pivoting being equivalent with its input the expressive power of path functional dependencies the validity of the path functional dependency between the pivot attribute and the transplanted attributes and the validity of the onto constraint guaranteeing that value powerview using information links and information views to navigate and visualize information on small displays powerview is a pda application designed to support people with situational information primarily during conversations and meetings with other people powerview was designed to address a number of issues in interface design concerning both information visualization and interaction on small mobile devices in terms of information visualization the system was required to provide the user with a single integrated information system that enabled quick access to related information once an object of interest had been selected in terms of interaction the system was required to enable easy and efficient information retrieval including single handed use of the device these problems were addressed by introducing information links and information views an evaluation of the application against the standard application suite bundle of the pda a casio cassiopeia e 11 proved the interfaces equivalent in usability even though the powerview application uses a novel interface par a probabilistic framework for matching temporal trajectories condensation based recognition of gestures and expressions the recognition of human gestures and facial expressions in image sequences is an important and challenging problem that enables a host of human computer interaction applications this paper describes a framework for incremental recognition of human motion that extends the condensation algorithm proposed by isard and blake eccv 96 human motions are modeled as temporal trajectories of some estimated parameters over time the condensation algorithm uses random sampling techniques to incrementally match the trajectory models to the multi variate input data the recognition framework is demonstrated with two examples the first example involves an augmented office whiteboard with which a user can make simple hand gestures to grab regions of the board print them save them etc the second example illustrates the recognition of human facial expressions using the estimated parameters of a learned model of mouth motion 1 introduction motion is intimately tied with our behavior we m executing query packs in ilp inductive logic programming systems usually send large numbers of queries to a database the lattice structure from which these queries are typically selected causes many of these queries to be highly similar as a consequence independent execution of all queries may involve a lot of redundant computation we propose a mechanism for executing a hierarchically structured set of queries a query pack through which a lot of redundancy in the computation is removed we have incorporated our query pack execution mechanism in the ilp systems tilde and warmr by implementing a new prolog engine ilprolog which provides support for pack execution at a lower level experimental results demonstrate significant efficiency gains our query pack execution mechanism is very general in nature and could be incorporated in most other ilp systems with similar efficiency improvements to be expected top down induction of first order logical decision trees this paper for instance the progol system muggleton 1995 has recently been extended with caching and other efficiency improvements cussens 1997 another direction of work is the use of sampling techniques see e g srinivasan 1998 sebag 1998 168 chapter 7 scaling up tilde top n optimization issues in mm databases introduction in multi media mm dbmss the usual way of operation in case of a mm retrieval query is to compute some ranking based on statistics and distances in feature spaces the mm objects are then sorted by descending relevance relative to the given query since users are limited in their capabilities of reviewing all objects in that ranked list only a reasonable top of say n objects is returned however this can turn out to be a quite time consuming process the first reason is that the number of objects i e documents in the dbms is usually very large 10 6 or even more from the information retrieval field it is known that usually half of all objects e g documents contains at least one query term so even considering only these objects might be very time consuming the same may hold for mm in general the problem of top n mm query optimization is to find techniques to limit the set of objects taken into consideration during the learning from labeled and unlabeled data using graph mincuts many application domains suffer from not having enough labeled training data for learning however large amounts of unlabeled examples can often be gathered cheaply as a result there has been a great deal of work in recent years on how unlabeled data can be used to aid classification we consider an algorithm based on finding minimum cuts in graphs that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data temporal statement modifiers a wide range of database applications manage time varying data many temporal query languages have been proposed each one the result of many carefully made yet subtly interacting design decisions in this article we advocate a different approach to articulating a set of requirements or desiderata that directly imply the syntactic structure and core semantics of a temporal extension of an arbitrary nontemporal query language these desiderata facilitate transitioning applications from a nontemporal query language and data model which has received only scant attention thus far the paper then introduces the notion of statement modifiers that provide a means of systematically adding temporal support to an existing query language statement modifiers apply to all query language statements for example queries cursor definitions integrity constraints assertions views and data manipulation statements we also provide a way to systematically add temporal support to an existing implementation the result is a temporal query language syntax semantics and implementation that derives from first principles we exemplify this approach by extending sql 92 with statement modifiers this extended language termed atsql is formally defined via a denotational semantics style mapping of path planning using lazy prm this paper describes a new approach to probabilistic roadmap planners prms the overall theme of the algorithm called lazy prm is to minimize the number of collision checks performed during planning and hence minimize the running time of the planner our algorithm builds a roadmap in the configuration space whose nodes are the user defined initial and goal configurations and a number of randomly generated nodes neighboring nodes are connected by edges representing paths between the nodes in contrast with prms our planner initially assumes that all nodes and edges in the roadmap are collision free and searches the roadmap at hand for a shortest path between the initial and the goal node the nodes and edges along the path are then checked for collision if a collision with the obstacles occurs the corresponding nodes and edges are removed from the roadmap our planner either finds a new shortest path or first updates the roadmap with new nodes and edges and then searches for a shortest path the above process is repeated until a collision free path is returned on current technology for information filtering and user profiling in agent based systems part i a perspective several current techniques and methods in information filtering and profiling are surveyed including state of the art technology various techniques currently used by large businesses and the academic stateof the art projects given the simplicity of the techniques currently applied in the field the further development and application of technology currently available in ai and algorithmics will yield significant improvements in both filtering and profiling results 1 introduction the terms information filtering and profiling are widely used here information filtering will refer to computer software systems which ffl split usually large data streams into useful and not useful components and direct the useful to interested users of particular interest are systems which recognize their users are different and split the data stream into separate possible overlapping streams which are directed at distinct users or groups of users typically data in the streams is c document categorization and query generation on the world wide web using webace we present webace an agent for exploring and categorizing documents on the world wide web based on a user profile the heart of the agent is an unsupervised categorization of a set of documents combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set the document categories are not given a priori we present the overall architecture and describe two novel algorithms which provide significant improvement over hierarchical agglomeration clustering and autoclass algorithms and form the basis for the query generation and search component of the agent we report on the results of our experiments comparing these new algorithms with more traditional clustering algorithms and we show that our algorithms are fast and scalable y authors are listed alphabetically 1 introduction the world wide web is a vast resource of information and services t emp a database driven electronic market place for business to business commerce on the internet electronic commerce systems for business to business commerce on the internet are still in their infancy the realization of internet electronic markets for business to business following a n suppliers m customers scenario is still unattainable with todays solutions comprehensive internet electronic commerce systems should provide for easy access to and handling of the system help to overcome di erences in time of business location language between suppliers and customers and at the same time should support the entire process of trading for business to business commerce in this paper we present a dbms based electronic commerce architecture and its prototypical implementation for business to business commerce according to a n suppliers mcustomers scenario business transactions within the electronic market are realized by a set of modular market services multiple physically distributed markets can be interconnected transparently to the users and form one virtually central market place the modeling and management of all market data in a dbms gives the system a solid basis for reliable consistent and secure trading on the market the generic and modular system architecture can be applied to arbitrary application domains the system is scalable and can cope with an increasing number of single markets participants and market data due to the possibility to replicate and distribute services and data and herewith to distribute data system and network load citeseer an autonomous web agent for automatic retrieval and identification of interesting publications published research papers available on the world wide web www or web are often poorly organized often exist in non text form e g postscript documents and increase in quantity daily significant amounts of time and effort are commonly needed to find interesting and relevant publications on the web we have developed a web based information agent that assists the user in the process of performing a scientific literature search given a set of keywords the agent uses web search engines and heuristics to locate and download papers the papers are parsed in order to extract information features such as the abstract and individually identified citations which are placed into an sql database the agent s web interface can be used to find relevant papers in the database using keyword searches or by navigating the links between papers formed by the citations links to both citing and cited publications can be followed in addition to simple browsing and keyword searches the agent a system for automatic personalized tracking of scientific literature on the web we introduce a system as part of the citeseer digital library project for automatic tracking of scientific literature that is relevant to a user s research interests unlike previous systems that use simple keyword matching citeseer is able to track and recommend topically relevant papers even when keyword based query profiles fail this is made possible through the use of a heterogenous profile to represent user interests these profiles include several representations including content based relatedness measures the citeseer tracking system is well integrated into the search and browsing facilities of citeseer and provides the user with great flexibility in tuning a profile to better match his or her interests the software for this system is available and a sample database is online as a public service a multi plane state machine agent model this paper presents a framework for implementing collaborative network agents agents are assembled dynamically from components into a structure described by a multi plane state machine model this organization lends itself to an elegant implementations of remote control collaboration checkpointing and mobility dening features of an agent system it supports techniques like agent surgery dicult to reproduce with other approaches the reference implementation for our model the bond agent system is distributed under an open source license and can be downloaded from http bond cs purdue edu 1 introduction the eld of agents is witnessing the convergence of researchers from several elds some see agents as a natural extension of the object oriented programming paradigm 14 15 one of the most popular books on articial intelligence reinterprets the whole eld in terms of agents 2 contemporary work on the theory of behavior provides the foundations for theoretical mo a clausal resolution method for ctl branching time temporal logic in this paper we extend our clausal resolution method for linear time temporal logics to a branching time framework thus we propose an efficient deductive method useful in a variety of applications requiring an expressive branching time temporal logic in ai the branching time temporal logic considered is computation tree logic ctl often regarded as the simplest useful logic of this class the key elements of the resolution method namely the normal form the concept of step resolution and a novel temporal resolution rule are introduced and justified with respect to this logic a completeness argument is provided together with some examples of the use of the temporal resolution method finally we consider future work in particular the extension of the method yet further to extended ctl ectl which is ctl extended with fairness operators and ctl the most powerful logic of this class we will also outline possible implementation of the approach by adapting techniques de the maximum clique problem contents 1 introduction 2 1 1 notations and definitions 3 2 problem formulations 4 2 1 integer programming formulations 5 2 2 continuous formulations 8 3 computational complexity 12 4 bounds and estimates 15 5 exact algorithms 19 5 1 enumerative algorithms 19 5 2 exact algorithms for the unweighted case 21 5 3 exact algorithms for the weighted case 25 6 heuristics 27 6 1 sequential greedy heuristics 28 6 2 local search heuristics 29 6 3 advanced search heuristics 30 6 3 1 simulated annealing 30 6 3 2 neural networks data mining for intelligent web caching the paper presents a vertical application of data warehousing and data mining technology intelligent web caching we introduce several ways to construct intelligent web caching algorithms that employ predictive models of web requests the general idea is to extend the lru policy of web and proxy servers by making it sensible to web access models extracted from web log data using data mining techniques two approaches have been studied in particular one based on association rules and another based on decision trees the experimental results of the new algorithms show substantial improvement over existing lrubased caching techniques in terms of hit rate i e the fraction of web documents directly retrieved in the cache we designed and developed a prototypical system which supports data warehousing of web log data extraction of data mining models and simulation of the web caching algorithms around an architecture that integrates the various phases in the knowledge discovery process the system supports a systematic evaluation and benchmarking of the proposed algorithms with respect to existing caching strategies 1 web log data warehousing and mining for intelligent web caching we introduce intelligent web caching algorithms that employ predictive models of web requests the general idea is to extend the lru policy of web and proxy servers by making it sensible to web access models extracted from web log data using data mining techniques two approaches have been studied in particular frequent patterns and decision trees the experimental results of the new algorithms show substantial improvement over existing lru based caching techniques in terms of hit rate we designed and developed a prototypical system which supports data warehousing of web log data extraction of data mining models and simulation of the web caching algorithms flattening an object algebra to provide performance algebraic transformation and optimization techniques have been the method of choice in relational query execution but applying them in oodbms has been difficult due to the complexity of object oriented query languages this paper demonstrates that the problem can be simplified by mapping a complex storage model to the flat binary model implemented by monet a state of theart database kernel we present a generic mapping scheme to flatten data models and study the case of a straightforward object oriented model we show how flattening enabled us to implement a full fledged query algebra on it using only a very limited set of simple operations the required primitives and query execution strategies are discussed and their performance is evaluated on the 1gb tpc d benchmark showing that our divide and conquer approach yields excellent results 1 introduction during the last decade relational database technology has grown towards industrial maturity and the attention of the research the legion system a novel approach to evolving heterogeneity for collective problem solving we investigate the dynamics of agent groups evolved to perform a collective task and in which the behavioural heterogeneity of the group is under evolutionary control two task domains are studied solutions are evolved for the two tasks using an evolutionary algorithm called the legion system a new metric of heterogeneity is also introduced which measures the heterogeneity of any evolved group behaviour it was found that the amount of heterogeneity evolved in an agent group is dependent of the given problem domain for the rst task the legion system evolved heterogeneous groups for the second task primarily homogeneous groups evolved we conclude that the proposed system in conjunction with the introduced heterogeneity measure can be used as a tool for investigating various issues concerning redundancy robustness and division of labour in the context of evolutionary approaches to collective problem solving 1 introduction investigations into heterogeneous a pushing reactive services to xml repositories using active rules push technology i e the ability of sending relevant information to clients in reaction to new events is a fundamental aspect of modern information systems xml is rapidly emerging as the widely adopted standard for information exchange and representation and hence several xml based protocols have been defined and are the object of investigation at w3c and throughout commercial organizations in this paper we propose the new concept of active xml rules for pushing reactive services to xml enabled repositories rules operate on xml documents and deliver information to interested remote users in reaction to update events occurring at the repository site the proposed mechanism assumes the availability of xml repositories supporting a standard xml query language such as xquery that is being developed by the w3c for the implementation of the reactive components it capitalizes on the use of standard dom events and of the soap interchange standard to enable the remote installation of active rules a simple protocol is proposed for subscribing and unsubscribing remote rules warehousing workflow data challenges and opportunities workflow management systems wfmss are software platforms that allow the definition execution monitoring and management of business processes wfmss log every event that occurs during process execution therefore workflow logs include a significant amount of information that can be used to analyze process executions understand the causes of high and low quality process executions and rate the performance of internal resources and business partners in this paper we present a packaged data warehousing solution coupled with hp process manager for collecting and analyzing workflow execution data we first present the main challenges involved in this effort and then detail the proposed approach 1 logics for databases and information systems temporal databases 34 3 2 2 relational database histories 36 3 3 temporal queries 36 3 3 1 abstract temporal query languages 37 3 3 2 expressive power 41 3 3 3 space efficient encoding of temporal databases 44 3 3 4 concrete temporal query languages 46 3 3 5 evaluation of abstract query languages using compilation 47 3 3 6 sql and derived temporal query languages 48 3 4 temporal integrity constraints 53 3 4 1 notions of constraint satisfaction 53 3 4 2 temporal integrity maintenance 54 3 4 3 temporal constraint checking 56 3 5 multidimensional time 58 3 5 1 why multiple temporal dimensions 59 3 5 2 abstract query languages for multi dimensional time 59 3 5 3 encoding of multi dimensional temporal databases 61 3 6 beyond first order temporal logic 62 3 7 conclusion 65 references 65 4 the role of deontic logic in the specification of information systems 71 j j ch meyer r j wieringa and f p m dignum 4 1 introduction soft constraints and deontic logic 72 4 1 1 integrity constrai the state of change a survey updates are a crucial component of any database programming language even the simplest database transactions such as withdrawal from a bank account require updates unfortunately updates are not accounted for by the classical horn semantics of logic programs and deductive databases which limits their usefulness in real world applications as a short term practical solution logic programming languages have resorted to handling updates using ad hoc operators without a logical semantics a great many works have been dedicated to developing logical theories in which the state of the underlying database can evolve with time many of these theories were developed with specific applications in mind such as reasoning about actions database transactions program verification etc as a result the different approaches have different strengths and weaknesses in this survey we review a number of these works discuss their application domains and highlight their strong and weak points query processing over device networks in the next decade millions of sensors and small scale mobile devices will integrate processors memory and communication capabilities networks of devices will be widely deployed for monitoring applications in these new applications users need to query very large collections of devices in an ad hoc manner most existing systems rely on a centralized system for collecting device data these systems lack flexibility because data is extracted in a predefined way also they do not scale to a large number of devices because large volumes of raw data are transferred in our new concept of a device database system distributed query execution techniques are applied to leverage the computing capabilities of devices and to reduce communication in this article we define an abstraction that allows us to represent a device network as a database and we describe how distributed query processing techniques are applied in this new context praveen seshradi is currently on leave at micr partial answers for unavailable data sources abstract many heterogeneous database system products and prototypes exist today they will soon be deployed in a wide variety of environments most existing systems suffer from an achilles heel they ungracefully fail in presence of unavailable data sources if some data sources are unavailable when accessed these systems either silently ignore them or generate an error this behavior is improper in environments where there is a non negligible probability that data sources cannot be accessed e g internet in case some data sources cannot be accessed when processing a query the complete answer to this query cannot be computed some work can however be done with the data sources that are available in this paper we propose a novel approach where in presence of unavailable data sources the answer to a query is a partial answer a partial answer is a representation of the work that has been done in case the complete answer to a query cannot be computed and of the work that remains to be done in order to obtain this complete answer the use of a partial answer is twofold first it contains an incremental query that allows to obtain the complete answer without redoing the work that has already been done second the application program can extract information from a partial answer through the use of a secondary query which we call a parachute query in this paper we present a framework for partial answers and we propose three algorithms for the evaluation of queries in presence of unavailable sources the construction of incremental queries and the evaluation of parachute queries 1 query processing in a device database system data types today s object relational and object oriented databases support abstract data type adt objects that are single attribute values encapsulating a collection of related data the critical feature of an adt that makes it suitable for representing devices is encapsulation note that there are natural parallels between devices and adts both adts and devices provide controlled access to encapsulated data and functionality through a well defined interface we build upon this observation by modeling each type of device in the network as an adt in the database an actual adt object in the database corresponds then to a physical device in the real world the public interface of the adt corresponds to the functionality supported by the device methods on the device are executed by sending requests to the device which evaluates the methods and answers with return values differences between the different device types are reflected by differences between the abstract dat a pattern approach to interaction design to create successful interactive systems user interface designers need to cooperate with developers and application domain experts in an interdisciplinary team these groups however usually miss a common terminology to exchange ideas opinions and values this paper presents an approach that uses pattern languages to capture this knowledge in software development hci and the application domain a formal domain independent definition of design patterns allows for computer support without sacrificing readability and pattern use is integrated into the usability engineering lifecycle as an example experience from building an award winning interactive music exhibit was turned into a pattern language which was then used to inform follow up projects and support hci education how statistics and prosody can guide a chunky parser introduction following the most common architecture of spoken dialog systems as shown in figure 1 the main task of linguistic processing is to yield a semantic representation of what the user said utterance user system answer word recognizer generator linguistic processor database base knowl control dialog figure 1 typical dialog system architecture these semantic representations are interpreted by the dialog module according to the dialog context and the system answer will be generated accordingly the system utterance depends on whether the system still needs certain information or if all necessary information has been given to accomplish its task in order to know when all required information has been provided the dialog this work was partly funded by the european community in the framework of the sqel project spoken queries in european languages copernicus project no 1634 the responsibility for the contents lies with bounded explanation and inductive refinement for acquiring control knowledge one approach to learning control knowledge from a problem solving trace consists of generating explanations for the local decisions made during the search process layout rules for graphical web documents the number of companies institutions and individuals competing for attention in the world wide web is growing exponentially this makes designing informative easy to grasp and visually appealing documents not only important for userfriendly information presentation but also the key to success for any information provider in this paper we present layout guidelines for textual and graphical static and dynamic 2 d and 3 d web documents which are drawn from fields as diverse as typography gestalt psychology architecture hypertext authoring and human computer interaction web documents are classified into five basic types and our layout rules are applied to each of these finally we show how currently evolving standards html 3 0 for text and still graphics java for 2 d animation and vrml for 3 d worlds support applying those rules 1 introduction whenever a new information conveying technology is invented it usually takes many years until authors develop new media that using java and corba for implementing internet databases we describe an architecture called webfindit that allows dynamic couplings of web accessible databases based on their content and interest we propose an implementation using www java jdbc and corba s orbs that communicate via the corba s iiop protocol the combination of these technologies offers a compelling middleware infrastructure to implement wide area enterprise applications in addition to a discussion of webfindit s core concepts and implementation architecture we also discuss an experience of using webfindit in a healthcare application 1 introduction the growth of the internet and the web increased dramatically the need for data sharing the web has brought a wave of new users and service providers to the internet it contains a huge quantity of heterogeneous information and services e g home pages online digital libraries product catalogs and so on bouguettaya et al 1998 the result is that the web is now accepted as the de facto support in all domains of li three dimensional optimization of supersonic inlets this paper presents the implementation of these new design techniques and their application to a mach 3 inlet case the significant improvements obtained using two different optimizers are presented and compared the results of these optimizations have been verified using a full reynolds averaged navier stokes solver all the following results are thoroughly analysed and placed into an industrial context 1 introduction partial order planning with concurrent interacting actions in order to generate plans for agents with multiple actuators agent teams or distributed controllers we must be able to represent and plan using concurrent actions with interacting effects this has historically been considered a challenging task requiring a temporal planner with the ability to reason explicitly about time we show that with simple modifications the strips action representation language can be used to represent interacting actions moreover algorithms for partial order planning require only small modifications in order to be applied in such multiagent domains we demonstrate this fact by developing a sound and complete partial order planner for planning with concurrent interacting actions pomp that extends existing partial order planners in a straightforward way these results open the way to the use of partial order planners for the centralized control of cooperative multiagent systems 1 introduction in order to construct plans for agents with mul convergence of gradient dynamics with a variable learning rate as multiagent environments become more prevalent we need to understand how this changes the agent based paradigm one aspect that is heavily affected by the presence of multiple agents is learning traditional learning algorithms have core assumptions such as markovian transitions which are violated in these environments rational and convergent learning in stochastic games this paper investigates the problem of policy learning in multiagent environments using the stochastic game framework which we briefly overview we introduce two properties as desirable for a learning agent when in the presence of other learning agents namely rationality and convergence we examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria we then contribute a new learning algorithm wolf policy hillclimbing that is based on a simple principle learn quickly while losing slowly while winning the algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges 1 multiagent learning using a variable learning rate learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies the optimal policy at any moment depends on the policies of the other agents this creates a situation of learning a moving target previous learning algorithms have one of two shortcomings depending on their approach they either converge to a policy that may not be optimal against the specific opponents policies or they may not converge at all in this article we examine this learning problem in the framework of stochastic games we look at a number of previous learning algorithms showing how they fail at one of the above criteria we then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings specifically we introduce the wolf principle win or learn fast for varying the learning rate we examine this technique theoretically proving convergence in self play on a restricted class of iterated matrix games we also present empirical results on a variety of more general stochastic games in situations of self play and otherwise demonstrating the wide applicability of this method key words multiagent learning reinforcement learning game theory 1 hybrid shipping architectures a survey recent advances in relational database systems include distributed systems that can choose to execute portions of query processing functionality at server or client sites a symmetric problem that has received little attention is the partitioning of client application functionality between client and server this report presents a survey of the literature related to both of these partitioning problems an introduction to 3 d user interface design three dimensional user interface design is a critical component of any virtual environment ve application in this paper we present a broad overview of 3 d interaction and user interfaces we discuss the effect of common ve hardware devices on user interaction as well as interaction techniques for generic 3 d tasks and the use of traditional 2 d interaction styles in 3 d environments we divide most userinteraction tasks into three categories navigation selection manipulation and system control throughout the paper our focus is on presenting not only the available techniques but also practical guidelines for 3 d interaction design and widely held myths finally we briefly discuss two approaches to 3 d interaction design and some example applications with complex 3 d interaction requirements we also present an annotated online bibliography as a reference companion to this article interaction techniques for common tasks in immersive virtual environments design evaluation and application 13 44 drew kessler for help with the sve toolkit the virtual environments group at georgia tech the numerous experimental subjects who volunteered their time dawn bowman iv table of contents introduction 1 1 1 motivation 1 1 2 definitions 4 1 3 problem statement 6 1 4 scope of the research 7 1 5 hypotheses 8 1 6 contributions information visualization human computer interaction and cognitive psychology domain visualizations digital libraries stand to benefit from technology insertions from the fields of information visualization human computer interaction and cognitive psychology among others however the current state of interaction between these fields is not well understood we use our knowledge visualization tool vxinsight towards a comprehensive topic hierarchy for news to date a comprehensive yahoo like hierarchy of topics has yet to be offered for the domain of news the yahoo approach of managing such a hierarchy hiring editorial staff to read documents and correctly assign them to topics is simply not practical in the domain of news far too many stories are written and made available online everyday while many machine learning methods exist for organising documents into topics these methods typically require a large number of labelled training examples before performing accurately when managing a large and ever changing topic hierarchy it is unlikely that there would be enough time to provide many examples per topic for this reason it would be useful to identify extra information within the domain of news that could be harnessed to minimise the number of labelled examples required to achieve reasonable accuracy to this end the notion of a semi labelled document is introduced these documents which are partially labelled by th grouplab at skigraph collaboration among distributed workgroup members is hampered by the lack of good tools to support informal interactions these tools either fail to provide teleawareness or enable smooth transitions into and out of informal interactions video media spaces always on video links have been proposed as a solution to this problem however the always on nature of video media spaces results in a conflict between the desire to provide awareness and the need to preserve privacy the present study examines distortion filtration applied to always on video as means of resolving this tension our discussions include the inter related concepts of informal interactions awareness and privacy and the treatment afforded by existing distributed collaboration support tools we then outline the present study where our goal is to understand the effect of distortion filtration on awareness and privacy keywords tele awareness telepresence privacy informal interaction video media spaces di logic programming multi agent systems a synergic combination for applications and semantics the paper presents an ongoing research project that uses logic programming linear logic programming and their related techniques for executable specifications and rapid prototyping of multi agent systems the mas paradigm is an extremely rich one and we believe that logic programming will play a very effective role in this area both as a tool for developing real applications and as a semantically well founded language for basing program analysis and proof of properties on lt bigwig gt a language for developing interactive web services lt bigwig gt is a high level programming language and a compiler for developing interactive web services the overall goal of the language design is to remove many of the obstacles that face current developers of web services in order to lower cost while increasing functionality and reliability the compiler translates programs into a conglomerate of lower level standard technologies such as cgi scripts html javascript and http authentication this paper describes the major facets of the language design and the techniques used in their implementation and compares the design with alternative web service technologies performance and memory access characterization of data mining applications this paper characterizes the performance and memoryaccess behavior of a decision tree induction program a previously unstudied application used in data mining and knowledge discovery in databases performance is studied via rsim an execution driven simulator for three uniprocessor models that exploit instruction level parallelism ilp to varying degrees several properties of the program are noted outof order dispatch and multiple issue provide a significant performance advantage 50 250 improvement in ipc for out of order versus in order and 5 120 improvement in ipc for four way issue versus singleissue multiple issue provides a greater performance improvement for larger l2 cache sizes when the program is limited by cpu performance out of order dispatch provides a greater performance improvement for smaller l2 cache sizes the program has a very small instruction footprint an 8 kb l1 instruction cache is sufficient to bring the instruction miss rate below 0 1 a smal pruning decision trees with misclassification costs we describe an experimental study of pruning methods for decision tree classifiers when the goal is minimizing loss rather than error in addition to two common methods for error minimization cart s cost complexity pruning and c4 5 s error based pruning we study the extension of cost complexity pruning to loss and one pruning variant based on the laplace correction we perform an empirical comparison of these methods and evaluate them with respect to loss we found that applying the laplace correction to estimate the probability distributions at the leaves was beneficial to all pruning methods unlike in error minimization and somewhat surprisingly performing no pruning led to results that were on par with other methods in terms of the evaluation criteria the main advantage of pruning was in the reduction of the decision tree size sometimes by a factor of ten while no method dominated others on all datasets even for the same domain different pruning mechanisms are better for characterization and parallelization of decision tree induction this paper examines the performance and memory access behavior of the c4 5 decision tree induction program a representative example of data mining applications for both uniprocessor and parallel implementations the goals of this paper are to characterize c4 5 in particular its memory hierarchy usage and to decrease the runtime of c4 5 by algorithmic improvement and parallelization performance is studied via rsim an execution driven simulator for three uniprocessor models that exploit instruction level parallelism to varying degrees this paper makes the following four contributions first it presents a complete characterization of the c4 5 decision tree induction program it was found that the with the exception of the input dataset the working set fits into an 8 kb data cache the instruction working set also fits into an 8 kb instruction cache for datasets smaller than the l2 cache performance is limited by accesses to main memory it was also found that multiple issue can mathematical programming for data mining formulations and challenges this paper is intended to serve as an overview of a rapidly emerging research and applications area in addition to providing a general overview motivating the importance of data mining problems within the area of knowledge discovery in databases our aim is to list some of the pressing research challenges and outline opportunities for contributions by the optimization research communities towards these goals we include formulations of the basic categories of data mining methods as optimization problems we also provide examples of successful mathematical programming approaches to some data mining problems keywords data analysis data mining mathematical programming methods challenges for massive data sets classification clustering prediction optimization to appear informs journal of compting special issue on data mining a basu and b golden guest editors also appears as mathematical programming technical report 98 01 computer sciences department university of wi an introduction to software agents ion and delegation agents can be made extensible and composable in ways that common iconic interface objects cannot because we can communicate with them they can share our goals rather than simply process our commands they can show us how to do things and tell us what went wrong miller and neches 1987 flexibility and opportunism because they can be instructed at the level of 16 bradshaw goals and strategies agents can find ways to work around unforeseen problems and exploit new opportunities as they help solve problems task orientation agents can be designed to take the context of the person s tasks and situation into account as they present information and take action adaptivity agents can use learning algorithms to continually improve their behavior by noticing recurrent patterns of actions and events toward agent enabled system architectures in the future assistant agents at the user interface and resource managing agents behind the scenes will increas on the knowledge requirements of tasks in order to successfully perform a task a situated system requires some information about its domain if we can understand what information the system requires we may be able to equip it with more suitable sensors or make better use of the information available to it these considerations have motivated roboticists to examine the issue of sensor design and in particular the minimal information required to perform a task we show here that reasoning in terms of what the robot knows and needs to know to perform a task is a useful approach for analyzing these issues we extend the formal framework for reasoning about knowledge already used in ai and distributed computing by developing a set of basic concepts and tools for modeling and analyzing the knowledge requirements of tasks we investigate properties of the resulting framework and show how it can be applied to robotics tasks 1 introduction the notion of computational complexity has had a profound effect on the development o style machines we approach the problem of stylistic motion synthesis by learning motion patterns from a highly varied set of motion capture sequences each sequence may have a distinct choreography performed in a distinct style learning identifies common choreographic elements across sequences the different styles in which each element is performed and a small number of stylistic degrees of freedom which span the many variations in the dataset the learned model can synthesize novel motion data in any interpolation or extrapolation of styles for example it can convert novice ballet motions into the more graceful modern dance of an expert the model can also be driven by video by scripts or even by noise to generate new choreography and synthesize virtual motion capture in many styles in proceedings of siggraph 2000 july 23 28 2000 new orleans louisiana usa this work may not be copied or reproduced in whole or in part for any commercial purpose permission to copy in whole o visual ranking of link structures extended abstract methods for ranking world wide web resources according to their position in the link structure of the web are receiving considerable attention because they provide the first effective means for search engines to cope with the explosive growth and diversification of the web antisocial bidding in repeated vickrey auctions in recent years auctions have become more and more important in the eld of multiagent systems as useful mechanisms for resource allocation and task assignment in many cases the vickrey second price sealed bid auction is used as a protocol that prescribes how the individual agents have to interact in order to come to an agreement the main reasons for choosing the vickrey auction are the low bandwidth and time consumption due to just one round of bidding and the existence of a dominant bidding strategy under certain conditions we show that the vickrey auction despite its theoretical benets is inappropriate if antisocial agents participate in the auction process more specically an antisocial attitude for economic agents that makes reducing the prot of competitors their main goal besides maximizing their own prot is introduced under this novel condition agents need to deviate from the dominant truth telling strategy this report presents a strategy for bidders exploring auction based leveled commitment contracting part i english type auctioning a key problem addressed in the area of multiagent systems is the automated assignment of multiple tasks to executing agents the automation of multiagent task assignment requires that the individual agents i use a common protocol that prescribes how they have to interact in order to come to an agreement and ii fix their final agreement in a contract that specifies the commitments resulting from the assignment on which they agreed this report describes a novel approach to automated task assignment in multiagent systems that is based on an auction based protocol and on leveled commitment contracting this approach is applicable in a broad range of realistic scenarios in which knowledge intensive negotiation among agents is not feasible and in which future environmental changes may require agents to breach their contracts 1 introduction the area of multiagent systems e g 5 8 16 which is concerned with systems composed of technical entities called agents that in task assignment in multiagent systems based on vickrey type auctioning and leveled commitment contracting a key problem addressed in the area of multiagent systems is the automated assignment of multiple tasks to executing agents the automation of multiagent task assignment requires that the individual agents i use a common protocol that prescribes how they have to interact in order to come to an agreement and ii x their nal agreement in a contract that species the commitments resulting from the assignment on which they agreed the work reported in this paper is part of a broader research eort aiming at the design and analysis of approaches to automated multiagent task assignment that combine auction protocols and leveled commitment contracts the primary advantage of such approaches is that they are applicable in a broad range of realistic scenarios in which knowledge intensive negotiation among agents is not feasible and in which unforeseeable future environmental changes may require agents to breach their contracts examples of standard auction protocols are the antisocial agents and vickrey auctions in recent years auctions have become more and more important in the field of multiagent systems as useful mechanisms for resource allocation and task assignment in many cases the vickrey second price sealed bid auction is used as a protocol that prescribes how the individual agents have to interact in order to come to an agreement we show that the vickrey auction despite its theoretical benefits is inappropriate if antisocial agents participate in the auction process more specifically an antisocial attitude for economic agents that makes reducing the profit of competitors their main goal besides maximizing their own profit is introduced under this novel condition agents need to deviate from the dominant truth telling strategy this paper presents a strategy for bidders in repeated vickrey auctions who are intending to inflict losses to fellow agents in order to be more successful not in absolute measures but relatively to the group of bidders the strategy is evaluated in a simple task allocation scenario cryptographic protocols for secure second price auctions in recent years auctions have become more and more important in the field of multiagent systems as useful mechanisms for resource allocation task assignment and last but not least electronic commerce in many cases the vickrey second price sealed bid auction is used as a protocol that prescribes how the individual agents have to interact in order to come to an agreement the main reasons for choosing the vickrey auction are the existence of a dominant strategy equilibrium the low bandwidth and time consumption due to just one round of bidding and the theoretical privacy of bids this paper specifies properties that are needed to ensure the accurate and secret execution of vickrey auctions and provides a classification of different forms of collusion we approach the two major security concerns of the vickrey auction the vulnerability to a lying auctioneer and the reluctance of bidders to reveal their private valuations we then propose a novel technique that allows to securely perform second price auctions this is achieved using the announcement of encrypted binary bidding lists on a blackboard top down bottom up and binary search techniques are used to interactively find the second highest bid step by step without revealing unnecessary information 1 vicious strategies for vickrey auctions ab c d 22 5 ab c 2lm 5 ab c 25 n8 28860 45710 5 ab c 37lu 5vtdw uxu t2ok 8 o 8 0 c y66tz c34 5x8 u 6 m3 c z 5 6 5 5x8 28810 u 6 m3 c m dw 0 40490 z u 2 u1 i 3bba 2cd5 8 n vu te n 122 8b 2m 8 37 te n 122 f c bg h ijk 8 o9p hq8g r6s o r6twkru qh o l rg vz c 528 n 1 vo 8 x y zzw8 122 2 o 8 x 5 vz 39 8 a 22 5 6 5 vz 39 5 37w 17590 33150 d u w 17590 33150 8 a 22 5 6 5 17590 33150 8 a semantics of disjunctive logic programs based on partial evaluation semantics and transformations in this paper we consider allowed disjunctive datalog programs over some fixed function free finite signature sigma in fact in the semantical part of this paper we consider only the ground instantiation of the programs because we claim that any sensible semantics should assign the same meaning to a program p and its instantiation ground p so the variables are seen only as a shorthand for denoting ground programs in a more compact way this means that in the semantical part we could as well have worked with propositional programs however in the computational part it would be very inefficient to compute first the ground instantiation of the given program here we make use of the allowedness condition every variable of the rule must occur also in a positive body literal this guarantees that in every rule application all variables are bound to a constant it is 5 true that in this way we again manage to consider only ground programs but computation of the semantics of autoepistemic belief theories recently one of the authors introduced a simple and yet powerful non monotonic knowledge representation framework called the autoepistemic logic of beliefs aeb theories in aeb are called autoepistemic belief theories every belief theory t has been shown to have the least static expansion t which is computed by iterating a natural monotonic belief closure operator psi t starting from t this way the least static expansion t of any belief theory provides its natural non monotonic semantics which is called the static semantics it is easy to see that if a belief theory t is finite then the construction of its least static expansion t stops after countably many iterations however a somewhat surprising result obtained in this paper shows that the least static expansion of any finite belief theory t is in fact obtained by means of a single iteration of the belief closure operator psi t although this requires t to be of a special form we also show that t can be always put in th on the equivalence of the static and disjunctive well founded semantics and its computation in recent years much work was devoted to the study of theoretical foundations of disjunctive logic programming and disjunctive deductive databases while the semantics of non disjunctive programs is fairly well understood the declarative and computational foundations of disjunctive logic programming proved to be much more elusive and difficult recently two new and promising semantics have been proposed for the class of disjunctive logic programs the first one is the static semantics static proposed by przymusinski and the other is the disjunctive well founded semantics d wfs proposed by brass and dix although the two semantics are based on very different ideas both of them have been shown to share a number of natural and intuitive properties in particular both preprint submitted to elsevier preprint 4 october 1999 of them extend the well founded semantics of normal logic programs nevertheless since the static semantics employs a much richer underlying language than the functional join processing inter object references are one of the key concepts of object relational and object oriented database systems in this work we investigate alternative techniques to implement inter object references and make the best use of them in query processing i e in evaluating functional joins we will give a comprehensive overview and performance evaluation of all known techniques for simple singlevalued as well as multi valued functional joins furthermore we will describe special order preserving functionaljoin techniques that are particularly attractive for decision support queries that require ordered results while most of the presentation of this paper is focused on object relational and object oriented database systems some of the results can also be applied to plain relational databases because index nested loop joins along key foreign key relationships as they are frequently found in relational databases are just one particular way to execute a functional join key words o evaluating functional joins along nested reference sets in object relational and object oriented databases previous work on functional joins was constrained in two ways 1 all approaches we know assume references being implemented as physical object identifiers oids and 2 most approaches are in addition limited to single valued reference attributes both are severe limitations since most object relational and all object oriented database systems do support nested reference sets and many object systems do implement references as location independent logical oids in this work we develop a new functional join algorithm that can be used for any realization form for oids physical or logical and is particularly geared towards supporting functional joins along nested reference sets the algorithm can be applied to evaluate joins along arbitrarily long path expressions which may include one or more reference sets the new algorithm generalizes previously proposed partition based pointer joins by repeatedly applying partitioning with interleaved re merging before evaluating the next fu automated servicing of agents agents need to be able to adapt to changes in their environment one way to achieve this is to service agents when needed a separate servicing facility an agent factory is capable of automatically modifying agents this paper discusses the feasibility of automated servicing agent factory generative migration of mobile agents in heterogeneous environments in most of today s agent systems migration of agents requires homogeneity in the programming language and or agent platform in which an agent has been designed in this paper an approach is presented with which heterogeneity is possible agents can migrate between non identical platforms and need not be written in the same language instead of migrating the code including data and state of an agent a blueprint of an agent s functionality is transferred an agent factory generates new code on the basis of this blueprint this approach of generative mobility not only has implications for interoperability but also for security as discussed in this paper compositional design and reuse of a generic agent model this paper introduces a formally specified design of a compositional generic agent model gam this agent model abstracts from specific application domains it provides a unified formal definition of a model for weak agenthood it can be re used as a template or pattern for a large variety of agent types and application domain types the model was designed on the basis of experiences in a number of application domains the compositional development method desire was used to design the agent model gam at a conceptual and logical level it serves as a unified precisely defined coneptual structure which can be refined by specialisation and instantiation to a large variety of other more specific agents to illustrate reuse of this agent model specialisation and instantiation to model co operative information gathering agents is described in depth moreover it is shown how gam can be used to describe in a unified and hence more comparable manner a large number of agent architectures from the literature regulating human robot interaction using emotions drives and facial expressions this paper presents a motivational system for an autonomous robot which is designed to regulate humanrobot interaction the mode of social interaction is that of a caretaker infant dyad where a human acts as the caretaker for the robot the robot s motivational system is designed to generate an analogous interaction for a robot human dyad as for an infantcaretaker dyad an infant s emotions and drives play a very important role in generating meaningful interactions with the caretaker bullowa 1979 similarly the learning task for the robot is to apply various communication skills acquired during social exchanges to manipulate the caretaker such that its drives are satisfied toward this goal the motivational system implements drives emotions and facial expressions the interaction is regulated specifically to promote a suitable learning environment although the details of the learning itself are beyond the scope of this paper this work represents an important step toward realiz economic markets as a means of open mobile agent systems mobile agent systems have gained popularity in use because they ease the application design process by giving software engineers greater flexibility although the value of any network is dependent on both the number of users and the number of sites participating in the network there is little motivation for systems to donate resources to arbitrary agents we propose to remedy the problem by imposing an economic market on mobile agent systems where agents purchase resources from host sites and sell services to users and other agents host sites accumulate revenues which are distributed to users to be used to launch more agents we argue for the use of markets to regulate mobile agent systems and discuss open issues in implementing market based mobile agent systems 1 introduction one of the more recent items in a network programmer s tool box is code mobility the technique is becoming more common in applications programming network management bpw98 video conferencing bpr98 so mobile agents in distributed information retrieval a mobile agent is an executing program that can migrate during execution from machine to machine in a heterogeneous network on each machine the agent interacts with stationary service agents and other resources to accomplish its task mobile agents are particularly attractive in distributed informationretrieval applications by moving to the location of an information resource the agent can search the resource locally eliminating the transfer of intermediate results across the network and reducing end toend latency in this chapter we rst discuss the strengths of mobile agents and argue that although none of these strengths are unique to mobile agents no competing technique shares all of them next after surveying several representative mobile agent systems we examine one speci c information retrieval application searching distributed collections of technical reports and consider how mobile agents can be used to implement this application e ciently and easily then we spend the bulk of the chapter describing two planning services that allow mobile agents to deal with dynamic network environments and information resources 1 planning algorithms that let an agent choose the best migration path through the network given its current task and the current network conditions and 2 planning algorithms that tell an agent howto observe achanging set of documents in a way that detects changes as soon as possible while minimizing overhead finally we consider the types of errors that can occur when information from multiple sources is merged and ltered and argue that the structure of a mobile agent application determines the extent to which these errors a ect the nal result 1 defining and combining symmetric and asymmetric similarity measures in this paper we present a framework for the definition of similarity measures using lattice valued functions we show their strengths particularly for combining similarity measures then we investigate a particular instantiation of the framework in which sets are used both to represent objects and to denote degrees of similarity the paper concludes by suggesting some generalisations of the findings 1 introduction there are many different ways of computing the similarity of object representations these include the feature based approach in which objects are represented by sets of features and similarity is based on feature commonality and difference e g 13 the geometric approach in which objects are represented by points in an n dimensional space usually specified by sets of pairs of attributes and atomic values and similarity is based on the inverse of the distance between objects in the space e g 12 and the structural approach which uses gr the anatomy of a large scale hypertextual web search engine in this paper we present google a prototype of a large scale search engine which makes heavy use of the structure present in hypertext google is designed to crawl and index the web efficiently and produce much more satisfying search results than existing systems the prototype with a full text and hyperlink database of at least 24 million pages is available at http google stanford edu to engineer a search engine is a challenging task search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms they answer tens of millions of queries every day despite the importance of large scale search engines on the web very little academic research has been done on them furthermore due to rapid advance in technology and web proliferation creating a web search engine today is very different from three years ago this paper provides an in depth description of our large scale web search engine the first such detailed public description we know of to date apart from the problems of scaling traditional search techniques to data of this magnitude there are new technical challenges involved with using the additional information present in hypertext to produce better search results this paper addresses this question of how to build a practical large scale system which can exploit the additional information present in hypertext also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want what can you do with a web in your pocket the amount of information available online has grown enormously over the past decade fortunately computing power disk capacity and network bandwidth have also increased dramatically it is currently possible for a university research project to store and process the entire world wide web since there is a limit on how much text humans can generate it is plausible that within a few decades one will be able to store and process all the human generated text on the web in a shirt pocket the web is a very rich and interesting data source in this paper we describe the stanford webbase a local repository of a significant portion of the web furthermore we describe a number of recent experiments that leverage the size and the diversity of the webbase first we have largely automated the process of extracting a sizable relation of books title author pairs from hundreds of data sources spread across the world wide web using a technique we call dual iterative pattern relation extraction second we have developed a global ranking of web pages called pagerank based on the link structure of the web that has properties that are useful for search and navigation third we have used pagerank to develop a novel search engine called google which also makes heavy use of anchor text all of these experiments rely significantly on the size and diversity of the webbase 1 enabling knowledge representation on the web by extending rdf schema recently there has been a wide interest in using ontologies on the web as a basis for this rdf schema rdfs provides means to define vocabulary structure and constraints for expressing metadata about web resources however formal semantics are not provided and the expressivity of it is not enough for full fledged ontological modeling and reasoning in this paper we will show how rdfs can be extended in such a way that a full knowledge representation kr language can be expressed in it thus enriching it with the required additional expressivity and the semantics of this language we do this by describing the ontology language oil as an extension of rdfs an important advantage of our approach is a maximal backward compatability with rdfs any meta data in oil format can still be partially interpreted by any rdfs only processor the oil extension of rdfs has been carefully engineered so that such a partial interpretation of oil meta data is still correct under the intended semantics of rdfs simply ignoring the oil specific portions of an oil document yields a correct rdf s document whose intended rdfs semantics is precisely a subset of the semantics of the full oil statements in this way our approach ensures maximal sharing of meta data on the web even partial interpretation of meta data by less semantically aware processors will yield a correct partial interpretation of the meta data we conclude that our method of extending is equally applicable to other kr formalisms 1 1 leveled commitment and trust in negotiation as agents become more autonomous agent negotiation and motivational attitudes such as commitment and trust become more important in this paper we consider the important choice in advanced negotiation applications whether negotiation parameters such as cardinality of interaction agent attitude and agent architectures are incorporated in the negotiation protocol or in the negotiation strategy only in the first case parameters are fixed and agents do not have to reason about them when they choose their strategy we define a dynamic deontic logic which can also be used for the second case because it models concepts like leveled commitment and trust for example it formalizes that violating commitments leads to a decrease in trustworthiness 1 introduction in advanced applications of multi agent systems agents interact more frequently deliberate more extensively and in general act more autonomously for example in electronic commerce agents are allowed to negotiate the boid architecture conflicts between beliefs obligations intentions and desires in this paper we introduce the so called beliefs obligations intentions desires or boid architecture it contains feedback loops to consider all eects of actions before committing to them and mechanisms to resolve conflicts between the outputs of its four components agent types such as realistic or social agents correspond to specific types of conflict resolution embedded in the boid architecture switch packet arbitration via queue learning in packet switches packets queue at switch inputs and contend for outputs the contention arbitration policy directly affects switch performance the best policy depends on the current state of the switch and current traffic patterns this problem is hard because the state space possible transitions and set of actions all grow exponentially with the size of the switch we present a reinforcement learning formulation of the problem that decomposes the value function into many small independent value functions and enables an efficient action selection 1 iadea a development environment architecture for building generic intelligent user interface agents the need exists in the work force for generic intelligent user interface agents to address the problem of increasing taskload that is overwhelming the human user interface agents could help alleviate user taskload by providing abstractions and intelligent assistance in a self contained software agent that communicates with the user through the existing user interface and also adapts to user needs and behaviors the benefits of a generic intelligent user interface agent environment is it can be applied to any highly interactive and information intensive software system from freight and parcel management systems to wall street financial investment and analysis we desire to address the two following difficulties with developing interface agents 1 the extensive number of existing computer systems makes it impractical to build these agents by hand for each system 2 any such agent must be compliant with existing user interface standards and business practices e g the united states utility theory based user models for intelligent interface agents an underlying problem of current interface agent research is the failure to adequately address effective and efficient knowledge representations and associated methodologies suitable for modeling the users interactions with the system these user models lack the representational complexity to manage the uncertainty and dynamics involved in predicting user intent and modeling user behavior a utility theory based approach is presented for effective user intent prediction by incorporating the ability to explicitly model users goals the uncertainty in the users intent in pursuing these goals and the dynamics of users behavior we present an interface agent architecture ciaa that incorporates our approach and discuss the integration of ciaa with three disparate domains a probabilistic expert system shell a natural language input database query system and a virtual space plane that are being used as test beds for our interface agent research keywords cognitive modeling active user interfaces for building decision theoretic systems knowledge elicitation acquisition continues to be a bottleneck to constructing decisiontheoretic systems methodologies and techniques for incremental elicitation acquisition of knowledge especially under uncertainty in support of users current goals is desirable this paper presents peski a probabilistic expert system development environment peski provides users with a highly interactive and integrated suite of intelligent knowledge engineering tools for decision theoretic systems from knowledge acquisition data mining and verification and validation to a distributed inference engine for querying knowledge peski is based on the concept of active user interfaces actuators to the human machine interface peski uses a number of techniques to reduce the inherent complexity of developing a cohesive realworld knowledge based system this is accomplished by providing multiple communication modes for human computer interaction and the use of a knowledge representation endowed with the ability to detect problems with the knowledge acquired and alert the user to these possible problems we discuss peski s use of these intelligent assistants to help users with the acquisition of knowledge especially in the presence of uncertainty managing time consistency for active data warehouse environments abstract real world changes are generally discovered delayed by computer systems the typical update patterns for traditional data warehouses on an overnight or even weekly basis enlarge this propagation delay until the information is available to knowledge workers typically traditional data warehouses focus on summarized data at some level rather than detail data for active data warehouse environments also detailed data about individual entities are required for checking the data conditions and triggering actions hence keeping data current and consistent in that context is not an easy task in this paper we present an approach for modeling conceptual time consistency problems and introduce a data model that deals with timely delays it supports knowledge workers to find out why or why not an active system responded to a certain state of the data therefore the model enables analytical processing of detail data enhanced by valid time based on a knowledge state at a specified instant of time all states that were not yet knowable to the system at that point in time are consistently ignored 1 hierarchical agent interface for animation asynchronous hierarchical agents ahas provide a vertically structured multilevel abstraction hierarchy in this paper we argue that this multilevel hierarchy is a convenient way to create a human agent interface at multiple levels of abstraction in this way the agent has several layers of specification input and visualization output which facilitates users with problem solving because such an interface parallels the hierarchical and iterative nature of human creative thought processes the aha interface presents an intuitive intimate interface which supports interactions on a scale from direct manipulation to delegation depending on the user s choice another feature of this interface is its two modes of interaction direct device interaction mouse clicking and interpretive command line or scripting mode this way agents can be forced to perform certain activities via mouse clicks direct control or they can be programmed via scripts on the fly we present example better living through geometry mark weiser described ubiquitous computing as invisible everywhere computing that does not live on a personal device of any sort but is in the woodwork everywhere 4 the easyliving project 1 is concerned with development of an architecture and technologies for ubiquitous computing environments which allow the dynamic aggregation of diverse i o devices into a single coherent user experience though the need for research in distributed computing perception and interfaces is widely recognized the importance of an explicit geometric world model for enhancing the user s experience of a ubiquitous computing system has not been well articulated this paper introduces three scenarios which benefit from geometric context and describes the easyliving geometric model algebraic models for contextual nets we extend the algebraic approach of meseguer and montanari from ordinary place transition petri nets to contextual nets covering both the collective and the individual token philosophy uniformly along the two interpretations of net behaviors two algebraic process semantics for contextual nets abstract we show that the so called petri nets are monoids approach initiated by meseguer and montanari can be extended from ordinary place transition petri nets to contextual nets by considering suitable nonfree monoids of places the algebraic characterizations of net concurrent computations we provide cover both the collective and the individual token philosophy uniformly along the two interpretations and coincide with the classical proposals for place transition petri nets in the absence of read arcs stholes a multidimensional workload aware histogram attributes of a relation are not typically independent multidimensional histograms can be an effective tool for accurate multiattribute query selectivity estimation in this paper we introduce stholes a workload aware histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density stholes histograms are built without examining the data sets but rather by just analyzing query results buckets are allocated where needed the most as indicated by the workload which leads to accurate query selectivity estimations our extensive experiments demonstrate that stholes histograms consistently produce good selectivity estimates across synthetic and real world data sets and across query workloads and in many cases outperform the best multidimensional histogram techniques that require access to and processing of the full data sets during histogram construction 1 evaluating top k queries over web accessible databases a query to a web search engine usually consists of a list of keywords to which the search engine responds with the best or top k pages for the query this top k query model is prevalent over multimedia collections in general but also over plain relational data for certain applications for example consider a relation with information on available restaurants including their location price range for one diner and overall food rating a user who queries such a relation might simply specify the user s location and target price range and expect in return the best 10 restaurants in terms of some combination of proximity to the user closeness of match to the target price range and overall food rating processing top k queries efficiently is challenging for a number of reasons one critical such reason is that in many web applications the relation attributes might not be available other than through external web accessible form interfaces which we will have to query repeatedly for a potentially large set of candidate objects in this article we study how to process top k queries efficiently in this setting where the attributes for which users specify target values might be handled by external autonomous sources with a variety of access interfaces we present a sequential algorithm for processing such queries but observe that any sequential top k query processing strategy is bound to require unnecessarily long query processing times since web accesses exhibit high and variable latency fortunately web sources can be probed in parallel and each source can typically process concurrent requests although sources may impose some restrictions on the type and number of probes that they are willing to accept we adapt our sequential query processing technique and introduce an efficient algorithm that maximizes source access parallelism to minimize query response time while satisfying source access constraints we evaluate adaptive navigation support in educational hypermedia the role of student knowledge level and the case for meta adaptation this paper provides a brief overview of main adaptive navigation support techniques and analyzes the results of most representative empirical studies of these techniques it demonstrates an evidence that different known techniques work most efficiently in different context in particular the studies summarized in the paper have provided evidence that users with different knowledge level of the subject may appreciate different adaptive navigation support technologies the paper argues that more empirical studies are required to help the developers of adaptive hypermedia systems in selecting most relevant adaptation technologies it also attempts to build a case for meta adaptive hypermedia systems ie systems that are able to adapt the very adaptation technology to the given user and context autonomous compliant motion the bayesian approach this paper gives an overview of the dierent levels of sensor processing complexity found in forcecontrolled tasks and explains which techniques from bayesian probability theory are appropriate to cope with uncertainties and missing information at each of the dierent levels the paper reduces all approaches for intelligent compliant motion sensor processing to a basic set of just four classes some of these algorithms have already been tested experimentally while others are still beyond the current state of theart the major contribution of this paper is to bring a clear structure to the eld which should eventually result in an easier integration of dierent research results and a more precise discussion about their relative merits and innovations 1 introduction force sensors are only one of many possible sensors that make robot controllers more autonomous i e make them work in an environment that need not be as modelled and structured as the current industrial settings i a coordination model for agents based on secure spaces shared space coordination models such as linda are ill suited for structuring applications composed of erroneous or insecure components this paper presents the secure object space model in this model a data element can be locked with a key and is only visible to a process that presents a matching key to unlock the element we give a precise semantics for secure object space operations and discuss an implementation in java for a mobile agent system an implementation of the semantics that employs encryption is also outlined for use in untrusted environments 1 introduction coordination languages based on shared data spaces have been around for over fifteen years researchers have often advocated their use for structuring distributed and concurrent systems because the mode of communication that they provide sometimes called generative communication is associative and uncoupled communication is associative in that processes do not explicitly name their communication part hypothesis testing for complex agents as agents approach animal like complexity evaluating them becomes as difficult as evaluating animals this paper describes the application of techniques for characterizing animal behavior to the evaluation of complex agents we describe the conditions that lead to the behavioral variability that requires experimental methods we then review the state of the art in psychological experimental design and analysis and show its application to complex agents we also discuss a specific methodological concern of agent research how the robots versus simulations debate interacts with statistical evaluation finally we make a specific proposal for facilitating the use of scientific method we propose the creation of a web site that functions as a repository for platforms suitable for statistical testing for results determined on those platforms and for the agents that have generated those results keywords agent performance complex systems behavioral indeterminacy repl scholonto an ontology based digital library server for research documents and discourse the internet is rapidly becoming the first place for researchers to publish documents but at present they receive little support in searching tracking analyzing or debating concepts in a literature from scholarly perspectives this paper describes the design rationale and implementation of scholonto an ontology based digital library server to support scholarly interpretation and discourse it enables researchers to describe and debate via a semantic network the contributions a document makes and its relationship to the literature the paper discusses the computational services that an ontology based server supports alternative user interfaces to support interaction with a large semantic network usability issues associated with knowledge formalization new work practices that could emerge and related work 2 1 introduction it is becoming standard practice for researchers to publish their documents on the internet or intranets via personal institutional and discipline spe adaptive combination of behaviors in an agent rchical structures ps97 often require specific manual preparations the shape of the structure factored representations sal00 are not scalable but restricted to a given environment number of objects size of environment 8 19 ecai 02 9 19 ecai 02 idea hypothesis a complex behavior is often guided by a set of basic motivations i e goals idea make use of basic behaviors associated with the basic motivations to recombine them in one complex behavior agent o1 o3 10 19 ecai 02 scene decomposition o2 agent o1 o2 o3 o2 o1 perceived objects o 1 o 2 et o 3 basic behaviors avoid holes b a hole push tiles b p hole tile behavior config pairs b a 2 11 19 ecai 02 basic behaviors example behavior pushing tile o 1 in hole o 2 o2 o1 o3 a basic generic behavior b is defined by a type of configuration t obj 1 invariant fourier wavelet descriptor for pattern recognition we present a novel set of descriptors for recognizing complex patterns such as roadsigns keys aircrafts characters etc given a pattern we first transform it to polar coordinate r using the centre of mass of the pattern as origin we then apply the fourier transform along the axis of polar angle and the wavelet transform along the axis of radius r the features thus obtained are invariant to translation rotation and scaling as an example we apply the method to a database of 85 printed chinese characters the result shows that the fourier wavelet descriptor is an efficient representation which can provide for reliable recognition feature extraction fourier transform invariant descriptor multiresolution analysis pattern recognition wavelet transform 1 introduction feature extraction is a crucial processing step for pattern recognition 15 some authors 5 gamma7 13 extract 1 d features from 2 d patterns the advantage of this approach is that we can save spa equality type and word constraints as a generalization of inclusion dependencies that are found in relational databases word constraints have been studied for semistructured data 6 as well as for an objectoriented model 10 in both contexts it is assumed that each data entity has a unique identity and two entities are equal if and only if they have the same identify in this setting the decidability of the implication and finite implication problems for word constraints has been established a question left open is whether these problems are still decidable in the context of an object oriented model m which supports complex values with nested structures and complex value equality this paper provides an answer to that question we characterize a schema in m in terms of a type constraint and an equality constraint and investigate the interaction between these constraints and word constraints we show that in the presence of equality and type constraint the implication and finite implication problems for path constraints on semistructured and structured data we present a class of path constraints of interest in connection with both structured and semi structured databases and investigate their associated implication problems these path constraints are capable of expressing natural integrity constraints that are not only a fundamental part of the semantics of the data but are also important in query optimization we show that in semistructured databases despite the simple syntax of the constraints their associated implication problem is r e complete and finite implication problem is co r e complete however we establish the decidability of the implication problems for several fragments of the path constraint language and demonstrate that these fragments suffice to express important semantic information such as inverse relationships and local database constraints commonly found in object oriented databases we also show that in the presence of types the analysis of path constraint implication becomes more delicate we demonstrate so salticus guided crawling for personal digital libraries in this paper we describe salticus a web crawler that learns from users web browsing activity salticus enables users to build a personal digital library by collecting documents and generalizing over the user s choices keywords personal digital library business intelligence web crawling document acquisition 1 bdi design principles and cooperative implementation a report on robocup agents this report discusses two major views on bdi deliberation for autonomous agents the first view is a rather conceptual one presenting general bdi design principles namely heuristic options decomposed reasoning and layered planning which enable bdi deliberation in realtime domains the second view is focused on the practical application of the design principles in robocup simulation league this application not only evaluates the usefulness in deliberation but also the usefulness in rapid cooperative implementation we compare this new approach which has been used in the vice world champion team at humboldt 98 to the old approach of at humboldt 97 and we outline the extensions for at humboldt 99 which are still under work conditions faced by deliberation in multi agent contexts differ significantly from the basic assumption of classical ai search and planning traditional game playing methods for example assume a static well known setting and a fixed round based interaction of exception handling in the spreadsheet paradigm exception handling is widely regarded as a necessity in programming languages today and almost every programming language currently used for professional software development supports some form of it however spreadsheet systems which may be the most widely used type of programming language today in terms of number of users using it to create programs spreadsheets have traditionally had only extremely limited support for exception handling spreadsheet system users range from end users to professional programmers and this wide range suggests that an approach to exception handling for spreadsheet systems needs to be compatible with the equational reasoning model of spreadsheet formulas yet feature expressive power comparable to that found in other programming languages forms 3 a first order visual language to explore the boundaries of the spreadsheet paradigm although detractors of functional programming sometimes claim that functional programming is too difficult or counterintuitive for most programmers to understand and use evidence to the contrary can be found by looking at the popularity of spreadsheets the spreadsheet paradigm a first order subset of the functional programming paradigm has found wide acceptance among both programmers and end users still there are many limitations with most spreadsheet systems in this paper we discuss language features that eliminate several of these limitations without deviating from the first order declarative evaluation model the language used to illustrate these features is a research language called forms 3 using forms 3 we show that procedural abstraction data abstraction and graphics output can be supported in the spreadsheet paradigm we show that with the addition of a simple model of time animated output and gui i o also become viable to demonstrate generality we also presen jack intelligent agents components for intelligent agents in java this paper is organised as follows section 2 introduces jack intelligent agents presenting the approach taken by aos to its design and outlining its major engineering characteristics the bdi model is discussed briefly in section 3 section 4 gives an outline of how to build an application with jack intelligent agents finally in section 5 we discuss how the use of this framework can be beneficial to both engineers and researchers for brevity we will refer to jack intelligent agents simply as jack on the expressiveness of event notification in data driven coordination languages javaspaces and tspaces are two coordination middlewares for distributed java programming recently proposed by sun and ibm respectively they extend the data driven coordination model of linda with an event notification mechanism a process can register interest in the incoming arrivals of a particular kind of data and receive communication of the occurrences of these events in 2 3 we introduce a process calculus l based on the linda coordination model and we prove that if processes are synchronous with the dataspace ordered interpretation l is turing powerful while this is not the case if they are asynchronous unordered interpretation here we introduce a new calculus ln obtained by extending l with the event noti cation mechanism and we prove two main results contrasting with what has been shown for l i ln is turing powerful also under the unordered interpretation and ii it allows a faithful encoding of the ordered semantics on top of the unordered one some thoughts on transiently shared dataspaces transiently shared dataspaces tsd recently introduced in the coordination infrastructure lime represent an emerging technology to enable the use of dataspaces for the coordination of mobile agents data sharing is allowed among agents running on hosts belonging to the same federation i e currently connected in this paper we present a formalization of the tsd technology in order to provide a framework for reasoning about systems based on this infrastructure in particular we concentrate on alternative design choices related to data migration host connectivity and reactive programming 1 augmenting buildings with infrared information we describe a building information and navigation system based on palm pilot pdas and a set of strong infrared transmitters located throughout a building the infrared senders stream localized data thus effectively augmenting areas of space with localized information this information can be perceived by just entering those areas with the pda in your hand we show that this form of augmentation of an environment can serve a multitude of purposes and requires neither the employment of classic 3d augmented reality nor to carry around wearable computers nor to wear head mounted displays 1 introduction the irreal system is a building information system that by its pure working principle can also be used as a navigation system within buildings it is based on the palm pilot family of pdas most of which feature a builtin infrared port located in the middle of the top end of the device so that it points away from the user when the device is being held in front of the user by using this seeing the whole in parts text summarization for web browsing on handheld devices we introduce five methods for summarizing parts of web pages on handheld devices such as personal digital assistants pdas or cellular phones each web page is broken into text units that can each be hidden partially displayed made fully visible or summarized the methods accomplish summarization by different means one method extracts significant keywords from the text units another attempts to find each text unit s most significant sentence to act as a summary for the unit we use information retrieval techniques which we adapt to the world wide web context we tested the relative performance of our five methods by asking human subjects to accomplish single page information search tasks using each method we found that the combination of keywords and single sentence summaries provides significant improvements in access times and number of pen actions as compared to other schemes exploiting geographical location information of web pages many information resources on the web are relevant primarily to limited geographical communities for instance web sites containing information on restaurants theaters and apartment rentals are relevant primarily to web users in geographical proximity to these locations in contrast other information resources are relevant to a broader geographical community for instance an on line newspaper may be relevant to users across the united states unfortunately the geographical scope of web resources is largely ignored by web search engines we make the case for identifying and exploiting the geographical location information of web sites so that web search engines can rank resources in a geographically sensitive fashion in addition to using more traditional information retrieval strategies in this paper we first consider how to compute the geographical location of web pages subsequently we consider how to exploit such information in one specific proof of concept appl coordinating adaptations in open service architectures an open service architecture osa is a software structure that makes an open set of information services available to an open set of users the world wide web constitutes the most outstanding example of an osa as of today an important feature of an osa is personalization i e adapting the user interface functionality and information of services to its users however designers of such a feature are facing many problems perhaps the biggest one being coordination if services fail to coordinate how they adapt to users chances are that the whole point of performing the adaptation i e helping the user is lost in this thesis i lay out a framework for describing and reasoning about adaptive systems in open service architectures with a special emphasis on coordination this framework is mainly meant for analysis and design but some of the ideas presented are also suitable as metaphors for implementations an implementation of an adaptive system that was designed using this fram proactive detection of distributed denial of service attacks using mib traffic variables a feasibility study in this paper we propose a methodology for utilizing network management systems for the early detection of distributed denial of service ddos attacks although there are quite a large number of events that are prior to an attack e g suspicious logons start of processes addition of new files sudden shifts in traffic etc in this work we depend solely on information from mib management information base traffic variables collected from the systems participating in the attack three types of ddos attacks were effected on a research test bed and mib variables were recorded using these datasets we show how there are indeed mib based precursors of ddos attacks this work was supported by the air force research laboratory rome ny usa under contract f30602 00 c 0126 to scientific systems company and by aprisma s university fellowship program 1999 2000 1 that render it possible to detect them before the target is shut down most importantly we describe how the relevant mi mars a programmable coordination architecture for mobile agents mobile agents represent a promising technology for the development of internet applications however mobile computational entities introduce peculiar problems w r t the coordination of the application components the paper outlines the advantages of linda like coordination models and shows how a programmable coordination model based on reactive tuple spaces can provide further desirable features for internet applications based on mobile agents accordingly the paper presents the design and the implementation of the mars coordination architecture for java based mobile agents mars defines linda like tuple spaces which can be programmed to react with specific actions to the accesses made by mobile agents xml dataspaces for mobile agent coordination this paper presents xmars a programmable coordination architecture for internet applications based on mobile agents in xmars agents coordinate both with each other and with their current execution environment through programmable xml dataspaces accessed by agents in a linda like fashion this suits very well the characteristics of the internet environment on the one hand it offers all the advantages of xml in terms of interoperability and standard representation of information on the other hand it enforces open and uncoupled interactions as required by the dynamicity of the environment and by the mobility of the application components in addition coordination in xmars is made more flexible and secure by the capability of programming the behaviour of the coordination media in reaction to the agents accesses an application example related to the management of on line academic courses shows the suitability and the effectiveness of the xmars architecture ke reactive tuple spaces for mobile agent coordination mobile active computational entities introduce peculiar problems in the coordination of distributed application components the paper surveys several coordination models for mobile agent applications and outlines the advantages of uncoupled coordination models based on reactive blackboards on this base the paper presents the design and the implementation of the mars system a coordination tool for java based mobile agents mars defines linda like tuple spaces that can be programmed to react with specific actions to the accesses made by mobile agents keywords mobile agents coordination reactive tuple spaces java www information retrieval 1 introduction traditional distributed applications are designed as a set of processes statically assigned to given execution environments and cooperating in a mostly network unaware fashion adl95 the mobile agent paradigm instead defines applications composed by network aware entities agents capable of changing their execution env superimposing codes representing hierarchical information in web directories in this article we describe how superimposed coding can be used to represent hierarchical information which is especially useful in categorized information retrieval systems for example web directories superimposed coding have been widely used in signature files in a rigid manner but our approach is more flexible and powerful the categorization is based on a directed acyclic graph and each document is assigned to one or more nodes using superimposed coding we represent the categorization information of each document in a signature in this paper we explain the superimposed coding theory and how this coding technique can be applied to more flexible environments furthermore we realize an exhaustive analysis of the important factors that have repercussions on the performance of the system finally we expose the conclusions obtained from this article computationally private information retrieval with polylogarithmic communication we present a single database computationally private information retrieval scheme with polylogarithmic communication complexity our construction is based on a new but reasonable intractability assumption which we call the phi hiding assumption phiha essentially the difficulty of deciding whether a small prime divides oe m where m is a composite integer of unknown factorization keywords integer factorization euler s function phi hiding assumption private information retrieval 1 introduction private information retrieval the notion of private information retrieval pir for short was introduced by chor goldreich kushilevitz and sudan cgks95 and has already received a lot of attention the study of pir is motivated by the growing concern about the user s privacy when querying a large commercial database the problem was independently studied by cooper and birman cb95 to implement an anonymous messaging service for mobile users ideally the pir problem consists a survey on knowledge compilation this paper we survey recent results in knowledge compilation of propositional knowledge bases we first define and limit the scope of such a technique then we survey exact and approximate knowledge compilation methods we include a discussion of compilation for non monotonic knowledge bases keywords knowledge representation efficiency of reasoning geovibe a visual interface to geographic digital library this paper explores the possibilities of visualizing document similarities and differences in both spatial and topical domains building on previous studies of geographical information retrieval and textual information retrieval ir systems we report on the development of an information browsing tool geovibe the system consists of two types of browsing windows geoview and vibeview that work in coordination for visual navigation in the document space geoview imposes a geographical order to the document space based on the idea of hypermaps where icons and footprints may be embedded in maps as the clickable hotspots linking to relevant documents context awareness in systems with limited resources mobile embedded systems often have strong limitations regarding available resources in this paper we propose a statistical approach which could scale down to microcontrollers with scarce resources to model simple contexts based on raw sensor data as a case study two experiments are provided where statistical modeling techniques were applied to learn and recognize different contexts based on accelerometer data we furthermore point out applications that utilize contextual information for power savings in mobile embedded systems accessing data integration systems through conceptual schemas data integration systems provide access to a set of heterogeneous autonomous data sources through a so called global or mediated view there is a general consensus that the best way to describe the global view is through a conceptual data model and that there are basically two approaches for designing a data integration system in the global as view approach one defines the concepts in the global schema as views over the sources whereas in the local as view approach one characterizes the sources as views over the global schema it is well known that processing queries in the latter approach is similar to query answering with incomplete information and therefore is a complex task on the other hand it is a common opinion that query processing is much easier in the former approach in this paper we show the surprising result that when the global schema is expressed in terms of a conceptual data model even a very simple one query processing becomes di cult in the global as view approach also we demonstrate that the problem of incomplete information arises in this case too and we illustrate some basic techniques for e ectively answering queries posed to the global schema of the data integration system 1 on the expressive power of data integration systems there are basically two approaches for designing a data integration system in the global as view gav approach one maps the concepts in the global schema to views over the sources whereas in the local as view lav approach one maps the sources into views over the global schema the goal of this paper is to relate the two approaches with respect to their expressive power automatic discovery of language models for text databases the proliferation of text databases within large organizations and on the internet makes it difficult for a person to know which databases to search given language models that describe the contents of each database a database selection algorithm such as gloss can provide assistance by automatically selecting appropriate databases for an information need current practice is that each database provides its language model upon request but this cooperative approach has important limitations this paper demonstrates that cooperation is not required instead the database selection service can construct its own language models by sampling database contents via the normal process of running queries and retrieving documents although random sampling is not possible it can be approximated with carefully selected queries this sampling approach avoids the limitations that characterize the cooperative approach and also enables additional capabilities experimental results demonstrate th containment of conjunctive regular path queries with inverse reasoning on queries is a basic problem both in knowledge representation and databases a fundamental form of reasoning on queries is checking containment i e verifying whether one query yields necessarily a subset of the result of another query query containment is crucial in several contexts such as query optimization knowledge base verification information integration database integrity checking and cooperative answering in this paper we address the problem of query containment in the context of semistructured knowledge bases where the basic querying mechanism namely regular path queries asks for all pairs of objects that are connected by a path conforming to a regular expression we consider conjunctive regular path queries with inverse which extend regular path queries with the possibility of using both the inverse of binary relations and conjunctions of atoms where each atom specifies that one regular path query with inverse holds between two v description logics for information integration information integration is the problem of combining the data residing at different heterogeneous sources and providing the user with a unified view of these data called mediated schema the mediated schema is therefore a reconciled view of the information which can be queried by the user it is the task of the system to free the user from the knowledge on where data are and how data are structured at the sources lossless regular views if the only information we have on a certain database is through a set of views the question arises of whether this is sufficient to answer completely a given query we say that the set of views is lossless with respect to the query if no matter what the database is we can answer the query by solely relying on the content of the views the question of losslessness has various applications for example in query optimization mobile computing data warehousing and data integration we study this problem in a context where the database is semistructured and both the query and the views are expressed as regular path queries the form of recursion present in this class prevents us from applying known results to our case unifying class based representation formalisms the notion of class is ubiquitous in computer science and is central in many formalisms for the representation of structured knowledge used both in knowledge representation and in databases in this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features such investigation leads us to propose a unifying framework in which we are able to capture the fundamental aspects of several representation languages used in different contexts the proposed formalism is expressed in the style of description logics which have been introduced in knowledge representation as a means to provide a semantically well founded basis for the structural aspects of knowledge representation systems the description logic considered in this paper is a subset of first order logic with nice computational characteristics it is quite expressive and features a novel combination of constructs that has not been studied before the distinguishing constructs are number restrictions which generalize existence and functional dependencies inverse roles which allow one to refer to the inverse of a relationship and possibly cyclic assertions which are necessary for capturing real world a multiagent approach for electronic travel planning in the last years the amount of information stored in internet has grown exponentially this article presents a new approach to cooperative problem solving that use the web as a source of data the architecture has been designed using two main artificial intelligence techniques multiagent system design and problem solving planning both are used to obtain a new architecture that dynamically obtains knowledge from internet the system uses two different types of agents planning agents and web agents planning agents pay attention to the user s queries and solve his her problems at a high level of abstraction web agents fill in the details obtaining the required information from internet different partial solutions given by the web agents while combined by the planning agent to obtain a detailed solution or solutions to the user queries travelplan a multiagent system to solve web electronic travel problems this paper presents travelplan a multiagent architecture to co operative work between different elements human and or software whose main goal is to recommend useful solutions in the electronic tourism domain to system users the system uses different types of intelligent autonomous agents whose main characteristics are cooperation negotiation learning planning and knowledge sharing the information used by the intelligent agents is heterogeneous and geographically distributed the main information source of the system is internet the web other information sources are distributed knowledge bases in the own system the process to obtain filter and store the information is performed automatically by agents this information is translated into a homogeneous format for high level reasoning in order to obtain different partial solutions partial solutions are reconstructed into a general solution or solutions to be presented to the user the system will recommend different solution mapweb cooperation between planning agents and web agents this paper presents mapweb multiagent planning in the web a multiagent system for cooperative work among dierent intelligent software agents whose main goal is to solve user planning problems using the information stored in the world wide web web mapweb is made of a heterogeneous mixture of intelligent agents whose main characteristics are cooperation reasoning and knowledge sharing the architecture of mapweb uses four types of agents useragents that are the bridge between the users and the system controlagents manager and coach agents that are responsible to manage the rest of agents planneragents that are able to solve planning problems and nally webagents whose aim is to retrieve represent and share information obtained from the web mapweb solves planning problems by means of cooperation between planneragents and webagents instead of trying the planneragent to solve the whole planning problem the planneragent focuses on a less restricted and therefore easier to solve problem what we call an abstract problem and cooperates with the webagents to validate and complete abstract solutions in order for cooperation to take place a common language and data structures have also been dened categories and subject descriptors h 3 5 online information services data sharing webbased services i 2 articial intelligence i 2 6 learning knowledge acquisition i 2 8 problem solving planning i 2 11 distributed articial intelligence intelligent agents multi agent systems web agents keywords information system agent architecture multi agent systems web agents intelligent agents planning 1 ewa learning in bilateral call markets this chapter extends the ewa learning model to bilateral call market games also known as the sealed bid mechanism in two person bargaining in these games a buyer and seller independently draw private values from commonly known distributions and submit bids if the buyer s bid is above the seller s they trade at the midpoint of the two bids otherwise they don t trade we apply ewa by assuming that players have value dependent bidding strategies and they partially generalize experience from one value cost condition to another in response to the incentives from nonlinear optimal bid functions the same learning model can be applied to other market institutions where subjects economize on learning by taking into consideration similarity between past experience and a new environment while still recognizing the difference in market incentives between them the chapter also presents a new application of ewa to a continental divide coordination game and reviews 32 earlier studies comparing ewa reinforcement and belief learning the application shows the advantages of a generalized adaptive model of behavior that includes elements of reinforcement belief based and direction learning as special cases at some cost of complexity for the benefit of generality and psychological appeal it is a good foundation to build upon to extend our understanding of adaptive behavior in more general games and market institutions in future work we should investigate the similarity parameters y and w to better characterize their magnitude and significance in different market institutions keywords experimental economics call markets sealed bid mechanism learning jel classification d44 d83 c92 august 2 2000 thanks to terry daniel for supplying data this research has been searching the unsearchable inducing serendipitous insights although no serious efforts seem to have been devoted yet to the theoretical and experimental study of the phenomenon the web is recognizably a well suited medium for information encountering the accidental discovery of information that is not sought for this is the very essence of serendipity the faculty of making fortunate and unexpected discoveries by accident this paper presents max a sof t ware agent that uses simple information retrieval techniques and heuristic search to wander on the internet and uncover useful and not sought for information that may stimulate serendipitous insights combining evolutionary algorithms with oblique decision trees to detect bent double galaxies decision trees have long been popular in classification as they use simple and easy to understand tests at each node most variants of decision trees test a single attribute at a node leading to axis parallel trees where the test results in a hyperplane which is parallel to one of the dimensions in the attribute space these trees can be rather large and inaccurate in cases where the concept to be learned is best approximated by oblique hyperplanes in such cases it may be more appropriate to use an oblique decision tree where the decision at each node is a linear combination of the attributes oblique decision trees have not gained wide popularity in part due to the complexity of constructing good oblique splits and the tendency of existing splitting algorithms to get stuck in local minima several alternatives have been proposed to handle these problems including randomization in conjunction with deterministic hill climbing and the use of simulated annealing in this paper we use using evolutionary algorithms to induce oblique decision trees this paper illustrates the application of evolutionary algorithms eas to the problem of oblique decision tree induction the objectives are to demonstrate that eas can find classifiers whose accuracy is competitive with other oblique tree construction methods and that at least in some cases this can be accomplished in a shorter time experiments were performed with a 1 1 evolution strategy and a simple genetic algorithm on public domain and artificial data sets the empirical results suggest that the eas quickly find competitive classifiers and that eas scale up better than traditional methods to the dimensionality of the domain and the number of instances used in training from markov random fields to associative memories and back spin glass markov random fields this paper we propose a fully connected energy function for markov random field mrf modeling which is inspired by spin glass theory sgt two major tasks in mrf modeling are how to define the neighborhood system for irregular sites and how to choose the energy function for a proper encoding of constraints the proposed energy function offers two major advantages that makes it possible to avoid mrf modeling problems in the case of irregular sites first full connectivity makes the neighborhood definition irrelevant and second the energy function is defined independently of the considered application a basic assumption in sgt is the infinite dimension of the configuration space in which the energy is defined the choice of a particular energy function which depends on the scalar product between configurations allows us to use a kernel function in the energy formulation this solves the problem of high dimensionality and makes it possible to use sgt results in an mrf framework we call this new model spin glass markov random field sg mrf experiments on textures and objects database show the correctness and effectiveness of the proposed model analysis and synthesis of agents that learn from distributed dynamic data sources abstract we propose a theoretical framework for specification and analysis of a class of learning problems that arise in open ended environments that contain multiple distributed dynamic data and knowledge sources we introduce a family of learning operators for precise specification of some existing solutions and to facilitate the design and analysis of new algorithms for this class of problems we state some properties of instance and hypothesis representations and learning operators that make exact learning possible in some settings we also explore some relationships between models of learning using different subsets of the proposed operators under certain assumptions 1 learning from distributed dynamic data many practical knowledge discovery tasks e g learning the behavior of complex computer systems from observations computer aided scientific discovery in bioinformatics present several new challenges in machine learning the data repositories in such applications tend to be very large physically distributed report on the conald workshop on learning from text and the web moo organization and presentation of documents in information retrieval systems gs hof collaborative filtering dvn lexicon learning gbgh query reformulation kk text generation rad and analysis of the statistical properties of text ma in short the state of the art in learning from text and the web is that a broad range of methods are currently being applied to many important and interesting tasks there remain numerous open research questions however broadly the goals of the work presented at the workshop fall into two overlapping categories i making textual information available in a structured format so that it can be used for complex queries and problem solving and ii assisting users in finding organizing and managing information represented in text sources as an example of research aimed at the former goal muslea minton and knoblock mmk have developed an approach to learning wrappers for semi structured web sources such as restau using decision trees to improve case based learning this paper shows that decision trees can be used to improve the performance of casebased learning cbl systems we introduce a performance task for machine learning systems called semi flexible prediction that lies between the classification task performed by decision tree algorithms and the flexible prediction task performed by conceptual clustering systems in semi flexible prediction learning should improve prediction of a specific set of features known a priori rather than a single known feature as in classification or an arbitrary set of features as in conceptual clustering we describe one such task from natural language processing and present experiments that compare solutions to the problem using decision trees cbl and a hybrid approach that combines the two in the hybrid approach decision trees are used to specify the features to be included in k nearest neighbor case retrieval results from the experiments show that the hybrid approach outperforms both the decision improving minority class prediction using case specific feature weights this paper addresses the problem of handling skewed class distributions within the case based learning cbl framework we rst present as a baseline an informationgain weighted cbl algorithm and apply it to three data sets from natural language processing nlp with skewed class distributions although overall performance of the baseline cbl algorithm is good we show that the algorithm exhibits poor performance on minority class instances we then present two cbl algorithms designed to improve the performance of minority class predictions each variation creates test case speci c feature weights by rst observing the path taken by the test case in a decision tree created for the learning task and then using pathspeci c information gain values to create an appropriate weight vector for use during case retrieval when applied to the nlp data sets the algorithms are shown to signi cantly increase the accuracy of minority class predictions while maintaining or improving overall classi cation accuracy 1 guest editors introduction machine learning and natural language introduction machine learning and natural language claire cardie cardie cs cornell edu department of computer science cornell university ithaca ny 14853 7501 raymond j mooney mooney cs utexas edu department of computer sciences taylor hall 2 124 university of texas austin tx 787121188 the application of machine learning techniques to natural language processing nlp has increased dramatically in recent years under the name of corpus based statistical or empirical methods however most of this research has been conducted outside the traditional machine learning research community this special issue attempts to bridge this divide by assembling an interesting variety of recent research papers on various aspects of natural language learning many from authors who do not generally publish in the traditional machine learning literature and presenting them to the readers of machine learning in the last five to ten y integrating case based learning and cognitive biases for machine learning of natural language this paper shows that psychological constraints on human information processing can be used effectively to guide feature set selection for case based learning of linguistic knowledge given as input a baseline case representation for a natural language learning task our algorithm selects the relevant cognitive biases for the task and then automatically modifies the representation in response to those biases by changing deleting and weighting features appropriately we apply the cognitive bias approach to feature set selection to four natural language learning problems and show that performance of the casebased learning algorithm improves significantly when relevant cognitive biases are incorporated into the baseline instance representation we argue that the cognitive bias approach offers new possibilities for case based learning of natural language it simplifies the process of instance representation design and in theory obviates the need for separate instance represent learning similarity space in this study we suggest a method to adapt an image retrieval system into a configurable one basically original feature space of a content based retrieval system is nonlinearly transformed into a new space where the distance between the feature vectors is adjusted by learning the transformation is realized by artificial neural network architecture a cost function is defined for learning and optimized by simulated annealing method experiments are done on the texture image retrieval system which use gabor filter features the results indicate that configured image retrieval system is significantly better than the original system 1 statistical phrases in automated text categorization in this work we investigate the usefulness of n grams for document indexing in text categorization tc we call n gram a set t k of n word stems and we say that t k occurs in a document d j when a sequence of words appears in d j that after stop word removal and stemming consists exactly of the n stems in t k in some order previous researches have investigated the use of n grams or some variant of them in the context of specific learning algorithms and thus have not obtained general answers on their usefulness for tc in this work we investigate the usefulness of n grams in tc independently of any specific learning algorithm we do so by applying feature selection to the pool of all grams n and checking how many n grams score high enough to be selected in the top grams we report the results of our experiments using several feature selection functions and varying values of performed on the reuters 21578 standard tc benchmark we also report results of making actual use of the selected n grams in the context of a linear classifier induced by means of the rocchio method conceptual linking ontology based open hypermedia this paper describes the attempts of the cohse project to define and deploy a conceptual open hypermedia service consisting of an ontological reasoning service which is used to represent a sophisticated conceptual model of document terms and their relationships a web based open hypermedia link service that can offer a range of different linkproviding facilities in a scalable and non intrusive fashion and integrated to form a conceptual hypermedia system to enable documents to be linked via metadata describing their contents and hence to improve the consistency and breadth of linking of www documents at retrieval time as readers browse the documents and authoring time as authors create the documents introduction concepts and metadata metadata is data that describes other data to enhance its usefulness the library catalogue or database schema are canonical examples for our purposes metadata falls into three broad categories catalogue information e g the artist hybrid coordination of reinforcement learning based behaviors for auv control this paper proposes a hybrid coordination method for behavior based control architectures the hybrid method takes in advantages of the robustness and modularity in competitive approaches as well as optimized trajectories in cooperative ones this paper will demonstrate the feasibility of this hybrid method with a 3d navigation application to an autonomous underwater vehicle auv the behaviors were learnt online by means of reinforcement learning q l learning was used extending the one step learning of the popular q learning to n steps realistic simulations were carried out results showed the good performance of the hybrid method on behavior coordination as well as on increasing and improving behavior learning concurrency in prolog using threads and a shared database concurrency in logic programming has received much attention in the past one problem with many proposals when applied to prolog is that they involve large modifications to the standard implementations and or the communication and synchronization facilities provided do not fit as naturally within the language model as we feel is possible in this paper we propose a new mechanism for implementing synchronization and communication for concurrency based on atomic accesses to designated facts in the shared database we argue that this model is comparatively easy to implement and harmonizes better than previous proposals within the prolog control model and standard set of built ins we show how in the proposed model it is easy to express classical concurrency algorithms and to subsume other mechanisms such as linda variable based communication or classical parallelism oriented primitives we also report on an implementation of the model and provide performance and resource consumption data 1 blobworld a system for region based image indexing and retrieval blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects each image is automatically segmented into regions blobs with associated color and texture descriptors querying is based on the attributes of one or two regions of interest rather than a description of the entire image in order to make large scale retrieval feasible we index the blob descriptions using a tree because indexing in the high dimensional feature space is computationally prohibitive we use a lower rank approximation to the high dimensional distance experiments show encouraging results for both querying and indexing 1 introduction from a user s point of view the performance of an information retrieval system can be measured by the quality and speed with which it answers the user s information need several factors contribute to overall performance the time required to run each individual query the quality precision recall of each i a genetic algorithm based solution for the problem of small disjuncts in essence small disjuncts are rules covering a small number of examples hence these rules are usually error prone which contributes to a decrease in predictive accuracy the problem is particularly serious because although each small disjuncts covers few examples the set of small disjuncts can cover a large number of examples this paper proposes a solution to the problem of discovering accurate small disjunct rules based on genetic algorithms the basic idea of our method is to use a hybrid decision tree genetic algorithm approach for classification more precisely examples belonging to large disjuncts are classified by rules produced by a decision tree algorithm while examples belonging to small disjuncts are classified by a new genetic algorithm particularly designed for discovering small disjunct rules 1 introduction in the context of the well known classification task of data mining the discovered knowledge is often expressed as a set of if then predict a hybrid decision tree genetic algorithm for coping with the problem of small disjuncts in data mining the problem of small disjuncts is a serious challenge for data mining algorithms in essence small disjuncts are rules covering a small number of examples due to their nature small disjuncts tend to be error prone and contribute to a decrease in predictive accuracy this paper proposes a hybrid decision tree genetic algorithm method to cope with the problem of small disjuncts the basic idea is that examples belonging to large disjuncts are classified by rules produced by a decision tree algorithm while examples belonging to small disjuncts whose classification is considerably more difficult are classified by rules produced by a genetic algorithm specifically designed for this task 1 introduction in the context of the well known classification task of data mining the discovered knowledge is often expressed as a set of if then rules since this kind of knowledge representation is intuitive for the user from a logical viewpoint typically the discovered rules ar a methodology for using intelligent agents to provide automated intrusion response this paper proposes a new methodology for adaptive automated intrusion response ir using software agents the majority of intrusion response systems irss react to attacks by generating reports or alarms this introduces a window of vulnerability between when an intrusion is detected and when action is taken to defend against the attack research by cohen indicates that the success of an attack is dependent on the time gap between detection and response if skilled attackers are given ten hours after they are detected and before a response they will be successful 80 of the time at thirty hours the attacker almost never fails 1 the proposed methodology addresses this window of vulnerability by providing an automated response to incidents using a heterogeneous collection of software agents these agents collaborate to protect the computer system against attack and adapt their response tactics until the system administrator can take an active role in the defense of the system improving business process quality through exception understanding prediction and prevention business process automation technologies are being increasingly used by many companies to improve the efficiency of both internal processes as well as of e services offered to customers in order to satisfy customers and employees business processes need to be executed with a high and predictable quality in particular it is crucial for organizations to meet the service level agreements slas stipulated with the customers and to foresee as early as possible the risk of missing slas in order to set the right expectations and to allow for corrective actions in this paper we focus on a critical issue in business process quality that of analyzing predicting and preventing the occurrence of exceptions i e of deviations from the desired or acceptable behavior we characterize the problem and propose a solution based on data warehousing and mining techniques we then describe the architecture and implementation of a tool suite that enables exception analysis prediction and prevention finally we show experimental results obtained by using the tool suite to analyze internal hp processes 1 improving the wang and mendel s fuzzy rule learning method by inducing cooperation among rules nowadays linguistic modeling lm is considered to be one of the most important areas of application for fuzzy logic it is accomplished by descriptive fuzzy rule based systems frbss whose most interesting feature is the interpolative reasoning they develop this characteristic plays a key role in the high performance of frbss and is a consequence of the cooperation among the fuzzy rules involved in the frbs a large quantity of automatic techniques has been proposed to generate these fuzzy rules from numerical data one of the most interesting families of techniques due to its simplicity and quickness is the ad hoc datadriven methods however its main drawback is the cooperation among the rules which is not suitably considered with the aim of facing up this drawback which makes the obtained models not to be as accurate as desired a new approach to improve the performance obtaining more cooperative rules is introduced in this paper following this appro a methodology to improve ad hoc data driven linguistic rule learning methods by inducing cooperation among rules within the linguistic modeling eld one of the most important applications of fuzzy rule based systems a family of ecient and simple methods guided by covering criteria of the data in the example set called ad hoc data driven methods has been proposed in the literature in the last few years their high performance in addition to their quickness and easy understanding have make them very suitable for learning tasks in this paper we are going to perform a double task analyzing these kinds of learning methods and introducing a new methodology to signicantly improve their accuracy keeping their descriptive power unalterable on the one hand a taxonomy of ad hoc data driven learning methods based on the way in which the available data is used to guide the learning will be made in this sense we will distinguish between two approaches the example based and the fuzzy grid based one whilst in the former each rule is obtained from a specic example in the latter the e different approaches to induce cooperation in fuzzy linguistic models under the cor methodology nowadays linguistic modeling is considered to be one of the most important areas of application for fuzzy logic it is accomplished by linguistic fuzzy rule based systems whose most interesting feature is the interpolative reasoning developed this characteristic plays a key role in their high performance and is a consequence of the cooperation among the involved fuzzy rules a new approach that makes good use of this aspect inducing cooperation among rules is introduced in this contribution the cooperative rules methodology one of its interesting advantages is its flexibility allowing it to be used with dierent combinatorial search techniques thus four specic metaheuristics are considered simulated annealing tabu search genetic algorithms and ant colony optimization their good performance is shown when solving a real world problem more than just another pretty face embodied conversational interface agents this article i describe some of the features of human human conversation that are being implemented in this new genre of embodied conversational agents then i describe an embodied conversational agent that is based on these features i argue that because conversation is such a primary skill for humans and such an early learned skill practiced in fact between infants and mothers who take turns cooing and burbling at one another and because the body is so well equipped to support conversation embodied conversational agents may turn out to be powerful ways for humans to interact with their computers however i claim that in order for embodied conversational agents to live up to their promise their implementations must be based on actual study of human human conversation and their architectures must reflect some of the intrinsic properties found there embodied conversational interfaces are not just computer interfaces represented by way of human or animal bodies and they are not just interfaces where those human or animal bodies are lifelike or believable in their actions and their reactions to human users embodied conversational interfaces are specifically conversational in their behaviors and specifically human like in the way they use their bodies in conversation that is embodied conversational agents may be defined as those that have the same properties as humans in face to face conversation including nudge nudge wink wink elements of face to face conversation for embodied conversational agents introduction only humans communicate using language and carry on conversations with one another and the skills of conversation have developed in humans in such a way as to exploit all of the unique affordances of the human body we make complex representational gestures with our prehensile hands gaze away and towards one another out of the corners of our centrally set eyes and use the pitch and melody of our voices to emphasize and clarify what we are saying perhaps because conversation is so defining of humanness and human interaction the metaphor of face to face conversation has been applied to human computer interface design for quite some time one of the early arguments for the utility of this metaphor gave a list of features of face to face conversation that could be applied fruitfully to human computer interaction including mixed initiative nonverbal communication sense of presence rules for transfer of control nickerson 1976 however although these feature simulated ship shock tests trials this paper contrasts the relative overall utilities of ship shock tests trials and simulations a list of advantages mack media lab autonomous conversational kiosk in this paper we describe an embodied conversational kiosk that builds on research in embodied conversational agents ecas and on information displays in mixed reality and kiosk format in order to display spatial intelligence ecas leverage people s abilities to coordinate information displayed in multiple modalities particularly information conveyed in speech and gesture mixed reality depends on users interactions with everyday objects that are enhanced with computational overlays we describe an implementation mack media lab autonomous conversational kiosk an eca who can answer questions about and give directions to the mit media lab s various research groups projects and people mack uses a combination of speech gesture and indications on a normal paper map that users place on a table between themselves and mack research issues involve users differential attention to hand gestures speech and the map and flexible architectures for embodied conversational agents that allow these modalities to be fused in input and generation fully embodied conversational avatars making communicative behaviors autonomous although avatars may resemble communicative interface agents they have for the most part not profited from recent research into autonomous embodied conversational systems in particular even though avatars function within conversational environments for example chat or games and even though they often resemble humans with a head hands and a body they are incapable of representing the kinds of knowledge that humans have about how to use the body during communication humans however do make extensive use of the visual channel for interaction management where many subtle and even involuntary cues are read from stance gaze and gesture we argue that the modeling and animation of such fundamental behavior is crucial for the credibility and effectiveness of the virtual interaction in chat by treating the avatar as a communicative agent we propose a method to automate the animation of important communicative behavior deriving from work in conversation and discourse theory b requirements for an architecture for embodied conversational characters in this paper we describe the computational and architectural requirements for systems which support real time multimodal interaction with an embodied conversational character we argue that the three primary design drivers are real time multithreaded entrainment processing of both interactional and propositional information and an approach based on a functional understanding of human face toface conversation we then present an architecture which meets these requirements and an initial conversational character that we have developed who is capable of increasingly sophisticated multimodal input and output in a limited application domain 1 introduction research in computational linguistics multimodal interfaces computer graphics and autonomous agents has led to the development of increasingly sophisticated autonomous or semi autonomous virtual humans over the last five years autonomous self animating characters of this sort are important for use in production animation interfa deliberate normative agents principles and architecture in this paper norms are assumed to be useful in agent societies it is claimed that not only following norms but also the possibility of intelligent norm violation can be useful principles for agents that are able to behave deliberately on the basis of explicitly represented norms are identified and an architecture is introduced using this agent architecture norms can be communicated adopted and used as meta goals on the agent s own processes as such they have impact on deliberation about goal generation goal selection plan generation and plan selection 1 introduction besides autonomy an important characteristic of agents is that they can react to a changing environment however if the protocols that they use to react to at least some part of the environment are fixed they have no ways to respond to impredictable changes for instance if an agent notices that another agent is cheating it cannot switch to another protocol to protect itself at least this is g prop ii global optimization of multilayer perceptrons using gas a general problem in model selection is to obtain the right parameters that make a model fit observed data for a multilayer perceptron mlp trained with backpropagation bp this means finding appropiate layer size and initial weights this paper proposes a method g prop genetic backpropagation that attempts to solve that problem by combining a genetic algorithm ga and bp to train mlps with a single hidden layer the ga selects the initial weights and changes the number of neurons in the hidden layer through the application of specific genetic operators g prop combines the advantages of the global search performed by the ga over the mlp parameter space and the local search of the bp algorithm the application of the g prop algorithm to several real world and benchmark problems shows that mlps evolved using g prop are smaller and achieve a higher level of generalization than other perceptron training algorithms such as quickpropagation or rprop and other evolutive algorithms s a logical framework for multi agent systems and joint attitudes we present a logical framework for reasoning about multi agent systems this framework uses giunchiglia et al s notion of a logical context to define a methodology for the modular specification of agents and systems of agents in particular the suggested methodology possesses important features from the paradigm of object oriented oo design we are particularly interested in the specification of agent behaviours via bdi theories i e theories of belief desire and intention we explore various issues arising from the bdi specification of systems of agents and illustrate how our framework can be used to specify bottom level agent behaviour via the specification of top level intentions or to reason about complex emergent behaviour by specifying the relationship between simple interacting agents 1 introduction the formal specification of autonomous reasoning agents has recently received much attention in the ai community particular under the paradigm of agent oriented progr revisiting rationality for agents with intentions formal frameworks for the specification of autonomous agents are commonly based on logics of intention and belief desirable properties for logics of intention are particularly non standard even more so than for logics of belief in this paper we address problems with existing logics of intention and belief by shifting to a non classical semantics making use of rantala s impossible or non normal worlds our framework invalidates the problematic properties of intention and by imposing certain constraints on the algebraic structure of the models we show that that many desirable properties can be obtained the non normal worlds framework provides a fine grained semantics and proves to be an extremely powerful and flexible tool for the logical specification of rational agent behaviour 1 introduction logics of belief desire and intention bdi have recently received much attention in the ai literature on the design of autonomous intelligent agents e g 2 3 6 the importanc generating and using state spaces of object oriented petri nets the article discusses the notion of state spaces of object oriented petri nets associated to the tool called pntalk and the role of identifiers of dynamically appearing and disappearing instances within these state spaces methods of working with identifiers based on sophisticated naming rules and mechanisms for abstracting names are described and compared some optimizations of state space generating algorithms for the context of object oriented petri nets are briefly mentioned as well key words petri nets object orientation state spaces formal analysis and verification 1 introduction methods of formal analysis and verification has been developed as an alternative to simulation approaches of examining properties of complex systems although we are not always able to fully verify the behaviour of a system even partial analysis or verification can reveal some errors which tend to be different from the ones found by simulation due to the different nature of formal analysis and a learning mobile robot theory simulation and practice this paper presents an implementation of the sins multi strategy learning controller for mobile robot navigation this controller uses low level reactive control that is modulated on line by a learning system based on case based reasoning and reinforcement learning the case based reasoning part captures regularities in the environment the reinforcement learning part gradually improves the acquired knowledge evaluation of the controller is presented in a real and in a simulated mobile robot 1 introduction how to specify behaviour in a robot has come a long way since the low level languages of assembly robotics lozano perez 1982 the classical ai approach to control 1 proved too slow and too fragile for the real world but showed that representations of the environment however difficult to maintain produce interesting behaviour in nouvelle ai e g brooks 1985 brooks 1991a brooks 1991b agents merely react to the current environmental situation posed limited integrating the document object model with hyperlinks for enhanced topic distillation and information extraction topic distillation is the process of finding authoritative web pages a comprehensive hubs which reciprocally endorse each other and are relevant to a given query hyperlink based topic distillation has been traditionally applied to a macroscopic web model where documents are nodes in a directed graph and hyperlinks are edges mas m kp models miss va lua44 clues such aba4 m na viga m paa els as templa m2 0 k inclusions whicha embedded in html palm using ma0kp takp consequently results of ma 6 1m2 distillakp atillakp have been deteriorakp inqua 1 ya s webpa0 a becoming more complex we propose a uniformfine gra k model for the web in which pa a represented by theirta trees aes caesm their document object models or doms am these dom trees ar interconnected by ordinam hyperlinks surprisingly ma 6 m2k distillakkp atillakk do not work in the finegra m scena 6 we present a new awm0pk1p suitak1 for the fine gra2k0 model it can dis aggregate hubs into coherent regions by segmenting their do trees utua endorsement between hubs as am0 1 m2k involve these regions rans tha single nodes representing complete hubs anecdotesae meatesmp ts using a 28 query 366000 document benchmark suite used in ea0 k4 topic distilla m2 reseai h reveal two benefits from the new am 0kk6m2 distillastion quati y improves a a by product of distillation is the aem14 y to extra0 relevat snippets from hubs which a nonly paym40 k relevant to the query accelerated focused crawling through online relevance feedback the organization of html into a tag tree structure which is rendered by browsers as roughly rectangular regions with embedded text and href links greatly helps surfers locate and click on links that best satisfy their information need can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen href target page w r t an information need based on information limited to the href source page such a capability would be of great interest in focused crawling and resource discovery because it can fine tune the priority of unvisited urls in the crawl frontier and reduce the number of irrelevant pages which are fetched and discarded automatic resource list compilation by analyzing hyperlink structure and associated text we describe the design prototyping and evaluation of arc a system for automatically compiling a list of authoritative web resources on any sufficiently broad topic the goal of arc is to compile resource lists similar to those provided by yahoo or infoseek the fundamental difference is that these services construct lists either manually or through a combination of human and automated effort while arc operates fully automatically we describe the evaluation of arc yahoo and infoseek resource lists by a panel of human users this evaluation suggests that the resources found by arc frequently fare almost as well as and sometimes better than lists of resources that are manually compiled or classified into a topic we also provide examples of arc resource lists for the reader to examine scalable feature selection classification and signature generation for organizing large text databases into hierarchical topic taxonomies we explore how to organize large text databases hierarchically by topic to aid better searching browsing and filtering many corpora such as internet directories digital libraries and patent databases are manually organized into topic hierarchies also called taxonomies similar to indices for relational data taxonomies make search and access more efficient however the exponential growth in the volume of on line textual information makes it nearly impossible to maintain such taxonomic organization for large fast changing corpora by hand we describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand and then updates the database with new documents as the corpus grows assigning topics to these new documents with high speed and accuracy to do this we use techniques from statistical pattern recognition to efficiently separate the feature words or discriminants from the noise words at each node of the taxonomy usi the order of things activity centred information access this paper focuses on the representation and access of web based information and how to make such a representation adapt to the activities or interests of individuals within a community of users the heterogeneous mix of information on the web restricts the coverage of traditional indexing techniques and so limits the power of search engines in contrast to traditional methods and in a way that extends collaborative filtering approaches the path model centres representation on usage histories rather than content analysis by putting activity at the centre of representation and not the periphery the path model concentrates on the reader not the author and the browser not the site we describe metrics of similarity based on the path model and their application in a url recommender tool and in visualising sets of urls keywords heterogeneous data activity indexing collaborative filtering information retrieval access and visualization 1 introduction as tim berners lee pointe quilt an xml query language for heterogeneous data sources the world wide web promises to transform human society by making virtually all types of information instantly available everywhere two prerequisites for this promise to be realized are a universal markup language and a universal query language the power and flexibility of xml make it the leading candidate for a universal markup language xml provides a way to label information from diverse data sources including structured and semi structured documents relational databases and object repositories several xml based query languages have been proposed each oriented toward a specific category of information quilt is a new proposal that attempts to unify concepts from several of these query languages resulting in a new language that exploits the full versatility of xml the name quilt suggests both the way in which features from several languages were assembled to make a new query language and the way in which quilt queries can combine information from diverse data sources into a query result with a new structure of its own approximate nearest neighbor queries revisited this paper proposes new methods to answer approximate nearest neighbor queries on a set of n points in d dimensional euclidean space for any fixed constant d a data structure with o 1 gammad 2 n log n preprocessing time and o 1 gammad 2 log n query time achieves approximation factor 1 for any given 0 1 a variant reduces the dependence by a factor of gamma1 2 for any arbitrary d a data structure with o d 2 n log n preprocessing time and o d 2 log n query time achieves approximation factor o d 3 2 applications to various proximity problems are discussed 1 introduction let p be a set of n point sites in d dimensional space ir d in the well known post office problem we want to preprocess p into a data structure so that a site closest to a given query point q called the nearest neighbor of q can be found efficiently distances are measured under the euclidean metric the post office problem has many applications within computational toward scalable learning with non uniform class and cost distributions a case study in credit card fraud detection many factors influence the performance of a learned classifier in this paper we study different methods of measuring performance based on a unified set of cost models and the effects of training class distribution with respect to these models observations from these effects help us devise a distributed multi classifier meta learning approach to learn in domains with skewed class distributions non uniform cost per error and large amounts of data one such domain is credit card fraud detection and our empirical results indicate that up to a certain degree of skewed distribution our approach can significantly reduce loss due to illegitimate transactions introduction inductive learning research has been focusing on devising algorithms that generate highly accurate classifiers many factors contribute to the quality of the learned classifier one factor is the class distribution in the training set using the same algorithm different training class distributions can generate classi a possible world semantics for disjunctive databases we investigate the fundamental problem of when a ground atom in a disjunctive database is assumed false there are basically two different approaches for inferring negative information for disjunctive databases they are minker s generalized closed world assumption gcwa and ross and topor s disjunctive database rule ddr a problem with the gcwa is that disjunctive clauses are sometimes interpreted exclusively even when they are intended for inclusive interpretation on the other hand the ddr always interprets disjunctive clauses inclusively we argue that neither approach is satisfactory whether a disjunctive clause is interpreted exclusively or inclusively should be specified explicitly negative information should then be inferred according to the stated intent of the disjunctive clauses a database semantics called pws is proposed to solve the aforementioned problem we also show that for propositional databases with no negative clauses the problem of determining data resource selection in distributed visual information systems with the advances in multimedia databases and the popularization of the internet it is now possible to access large image and video repositories distributed throughout the world one of the challenging problems in such an access is how the information in the respective databases can be summarized to enable an intelligent selection of relevant database sites based on visual queries this paper presents an approach to solve this problem based on image content based indexing of a metadatabase at a query distribution server the metadatabase records a summary of the visual content of the images in each database through image templates and statistical features characterizing the similarity distributions of the images the selection of the databases is done by searching the metadatabase using a ranking algorithm that uses query similarity to a template and the features of the databases associated with the template two selection approaches termed mean based and histogram based approaches global integration of visual databases different visual databases have been designed in various locations the global integration of such databases can enable users to access data across the world in a transparent manner in this paper we investigate an approach to the design and creation of an integrated information system which supports global visual query access to various visual databases over the internet specifically a metaserver including a hierarchical metadatabase a metasearch agent and a query manager is designed to support such an integration the metadatabase houses abstracted data about individual remote visual databases to support visual contentbased queries the abstracted data in the metadatabase reflect the semantics of each visual database the query manager extracts the feature contents from the queries the metasearch agent processes the queries by matching their feature contents with the metadata a list of relevant database sites is derived for efficient retrieval of the query in the selected dat approximate query translation across heterogeneous information sources in this paper we present a mechanism for approximately translating boolean query constraints across heterogeneous information sources achieving the best translation is challenging because sources support different constraints for formulating queries and often these constraints cannot be precisely translated for instance a query score 8 might be perfectly translated as rating 0 8 at some site but can only be approximated as grade a at another unlike other work our general framework adopts a customizable closeness metric for the translation that combines both precision and recall our results show that for query translation we need to handle interdependencies among both query conjuncts as well as disjuncts as the basis we identify the essential requirements of a rule system for users to encode the mappings for atomic semantic units our algorithm then translates complex queries by rewriting them in terms of the semantic units we show that un automatic i o hint generation through speculative execution aggressive prefetching is an effective technique for reducing the execution times of disk bound applications that is applications that manipulate data too large or too infrequently used to be found in file or disk caches while automatic prefetching approaches based on static analysis or historical access patterns are effective for some workloads they are not as effective as manually driven programmer inserted prefetching for applications with irregular or input dependent access patterns in this paper we propose to exploit whatever processor cycles are left idle while an application is stalled on i o by using these cycles to dynamically analyze the application and predict its future i o accesses our approach is to speculatively pre execute the application s code in order to discover and issue hints for its future read accesses coupled with an aggressive hint driven prefetching system this automatic approach could be applied to arbitrary applications and should be particularl a collective robotics application based on emergence and self organization this paper presents a collective robotics application which consists of making a pool of robots regroup objects that are distributed in their environment the innovative aspect in our approach rests on a system integrating operationally autonomous robots that make the global task achieved by virtue of emergence and self organization 1 introduction our work fits in the framework of bottom up artificial intelligence bottom up ai in short and more particularly in that of autonomous agents we are concerned with collective phenomena and their issues and more precisely the way to carry out solutions that allow a multi robot system to achieve global tasks by virtue of emergence and self organization our work is supported by two types of experiments namely those involving multi agent simulations and those involving real robots our paper precisely presents a collective robotics application which consists of making a pool of autonomous robots regroup objects that are distributed in thei okbc a programmatic foundation for knowledge base interoperability the technology for building large knowledge bases kbs is yet to witness a breakthrough so that a kb can be constructed by the assembly of prefabricated knowledge components knowledge components include both pieces of domain knowledge for example theories of economics or fault diagnosis and kb tools for example editors and theorem provers most of the current kb development tools can only manipulate knowledge residing in the knowledge representation system krs for which the tools were originally developed open knowledge base connectivity okbc is an application programming interface for accessing krss and was developed to enable the construction of reusable kb tools okbc improves upon its predecessor the generic frame protocol gfp in several signi cant ways okbc can be used with a much larger range of systems because its knowledge model supports an assertional view of a krs okbc provides an explicit treatment ofentities that are not frames and it has a much better way of controlling inference and specifying default values okbc can be used on practically any platform because it supports network transparency and has implementations for multiple programming languages in this paper we discuss technical design issues faced in the development of okbc highlight how okbc improves upon gfp and report on practical experiences in using it an algebraic compression framework for query results decision support applications in emerging environments require that sql query results or intermediate results be shipped to clients for further analysis and presentation these clients may use low bandwidth connections or have severe storage restrictions consequently there is a need to compress the results of a query for efficient transfer and client side access this paper explores a variety of techniques that address this issue instead of using a fixed method we choose a combination of compression methods that use statistical and semantic information of the query results to enhance the effect of compression to represent such a combination we present a framework of compression plans formed by composing primitive compression operators we also present optimization algorithms that enumerate valid compression plans and choose an optimal plan our experiments show that our techniques achieve significant performance improvement over standard compression tools like winzip 1 intro features real time adaptive feature learning and document learning for web search in this paper we report our research on building features an intelligent web search engine that is able to perform real time adaptive feature i e keyword and document learning not only does features learn from the user s document relevance feedback but also automatically extracts and suggests indexing keywords relevant to a search query and learns from the user s keyword relevance feedback so that it is able to speed up its search process and to enhance its search performance we design two efficient and mutual benefiting learning algorithms that work concurrently one for feature learning and the other for document learning features employs these algorithms together with an internal index database and a real time meta searcher so to perform adaptive real time learning to find desired documents with as little relevance feedback from the user as possible the architecture and performance of features are also discussed 1 introduction as the world wide web rapidly evo niagaracq a scalable continuous query system for internet databases continuous queries are persistent queries that allow users to receive new results when they become available while continuous query systems can transform a passive web into an active environment they need to be able to support millions of queries due to the scale of the internet no existing systems have achieved this level of scalability niagaracq addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures grouped queries can share the common computation tend to fit in memory and can reduce the i o cost significantly furthermore grouping on selection predicates can eliminate a large number of unnecessary query invocations our grouping technique is distinguished from previous group optimization approaches in the following ways first we use an incremental group optimization strategy with dynamic re grouping new queries are added to existing query groups without having to regroup already installed queries second we use a query split scheme that requires minimal changes to a general purpose query engine third niagaracq groups both change based and timer based queries in a uniform way to insure that niagaracq is scalable we have also employed other techniques including incremental evaluation of continuous queries use of both pull and push models for detecting heterogeneous data source changes and memory caching this paper presents the design of niagaracq system and gives some experimental results on the system s performance and scalability 1 websail from on line learning to web search in this paper we investigate the applicability of on line learning algorithms to the real world problem of web search consider that web documents are indexed using n boolean features we first present a practically efficient on line learning algorithm tw2 to search for web documents represented by a disjunction of at most k relevant features we then design and implement websail a real time adaptive web search learner with tw2 as its learning component websail learns from the user s relevance feedback in real time and helps the user to search for the desired web documents the architecture and performance of websail are also discussed detection and correction of conflicting concurrent data warehouse updates data integration over multiple heterogeneous data sources has become increasingly important for modern applications the integrated data is usually stored in materialized views to allow better access performance and high availability materialized view must be maintained after the data sources change in a loosely coupled environment such as the data grid the data sources are autonomous hence the source updates can be concurrent and cause erroneous maintenance results state of the art maintenance strategies apply compensating queries to correct such errors making the restricting assumption that all source schemata remain static over time however in such dynamic environments the data sources may change not only their data but also their schema query capabilities or semantics consequently either the maintenance queries or compensating queries would fail we now propose a novel solution that handles both concurrent data and schema changes first we analyze the concurrency between source updates and classify them into different classes of dependencies we then propose dyno a two pronged strategy composed of dependency detection and correction algorithms to handle these new classes of concurrency our techniques are not tied to specific maintenance algorithms nor to a particular data model to our knowledge this is the first comprehensive solution to the view maintenance concurrency problems in loosely coupled environments our experimental results illustrate that dyno imposes an almost negligible overhead on existing maintenance algorithms for data updates while now allowing for this extended functionality optimal anytime search for constrained nonlinear programming in this thesis we study optimal anytime stochastic search algorithms ssas for solving general constrained nonlinear programming problems nlps in discrete continuous and mixed integer space the algorithms are general in the sense that they do not assume differentiability or convexity of functions based on the search algorithms we develop the theory of ssas and propose optimal ssas with iterative deepening in order to minimize their expected search time based on the optimal ssas we then develop optimal anytime ssas that generate improved solutions as more search time is allowed our ssas for solving general constrained nlps are based on the theory of discrete con strained optimization using lagrange multipliers that shows the equivalence between the set of constrained local minima clmdn and the set of discrete neighborhood saddle points spdn to implement this theory we propose a general procedural framework for locating an spdn by incorporating genetic algorithms in the framework we evaluate new constrained search algorithms constrained genetic algorithm cga and combined constrained simulated annealing and genetic algorithm csaga txnwrap a transactional approach to data warehouse maintenance a data warehouse management system dwms maintains materialized views derived from one or more information sources iss under source changes much recent research has developed maintenance algorithms to achieve data warehouse consistency under source data updates typically by sending additional compensation based messages to information source space given the highly dynamic nature of modern distributed environments such as the www both source data or schema changes are likely to occur autonomously and even concurrently most previous solutions become invalid under this new requirement causing both wrong query results returned from is space or even complete rejection messages this paper now proposes to tackle this problem from a different angle by rephasing it as a global distributed transaction problem and develops a novel solution strategy that is not only handling this as of now unsolved problem of concurrent source shcmea changes but also added benefit is more efficient than previous solutions from the literature as foundation of our solution we encapsulate the complete data warehouse maintenance process as a dwms transaction we design a multiversion timestamp source wrapper materialization and associated concurrency control algorithm that guarantees a consistent view of the information source space data inside each dwms transaction thus removing the maintenance anomaly problem this integrated solution called txnwrap is proven to be correct and achieve complete consistency of data warehouse maintenance even under a mixture of concurrent data updates or schema changes txnwrap is complementary to previous maintenance algorithms of removing their concurrency considerations we have implemented txnwrap and plugged it into an existing dwms testbed learning based vision and its application to autonomous indoor navigation learning based vision and its application to autonomous indoor navigation by shaoyun chen adaptation is critical to autonomous navigation of mobile robots many adaptive mechanisms have been implemented ranging from simple color thresholding to complicated learning with artificial neural networks ann the major focus of this thesis lies in machine learning for vision based navigation two well known vision based navigation systems are alvinn and robin developed by carnegie mellon university and university of maryland respectively alvinn uses a two layer feedforward neural network while robin relies on a radial basis function network rbfn although current ann based methods have achieved great success in vision based navigation they have two major disadvantages 1 local minimum problem the training of either multilayer perceptron or radial basis function network can get stuck at poor local minimums 2 the flexibility problem after the system has been trained in certain r webmate a personal agent for browsing and searching the world wide web is developing very fast currently nding useful information on the web is a time consuming process in this paper we presentwebmate an agent that helps users to e ectively browse and search the web webmate extends the state of the art in web based information retrieval in manyways first it uses multiple tf idf vectors to keep track of user interests in di erent domains these domains are automatically learned bywebmate second webmate uses the trigger pair model to automatically extract keywords for re ning document search third during search the user can provide multiple pages as similarity relevance guidance for the search the system extracts and combines relevantkeywords from these relevant pages and uses them for keyword re nement using these techniques webmate provides e ective browsing and searching help and also compiles and sends to users personal newspaper by automatically spiding news sources wehave experimentally evaluated the performance of the system dynamic agents this paper we shall explain how dynamic behaviors are obtained and utilized through automatic action installation and inter agent communication we also describe intra agent communication between the carrier part and the action part of a dynamic agent and between di erent actions carried by the same dynamic agent we shall also discuss three triggering mechanisms for dynamic behavior modi cation planning based request driven and event based introducing a new advantage of crossover commonality based selection the commonality based crossover framework defines crossover as a two step process 1 preserve the maximal common schema of two parents and 2 complete the solution with a construction heuristic in these heuristic operators the first step is a form of selection this commonality based form of selection has been isolated in genie using random parent selection and a non elitist generational replacement scheme genie does not include fitness based selection however a theoretical analysis shows that ideal construction heuristics in genie can potentially converge to optimal solutions experimentally results show that the effectiveness of practical construction heuristics can be amplified by commonalitybased restarts overall it is shown that the commonality hypothesis is valid schemata common to above average solutions are indeed above average since common schemata can only be identified by multi parent operators commonality based selection is a unique advantage that crossover can enjoy over mutation 1 developing a context aware electronic tourist guide some issues and experiences in this paper we describe our experiences of developing and evaluating guide an intelligent electronic tourist guide the guide system has been built to overcome many of the limitations of the traditional information and navigation tools available to city visitors for example group based tours are inherently inflexible with fixed starting times and fixed durations and like most guidebooks are constrained by the need to satisfy the interests of the majority rather than the specific interests of individuals following a period of requirements capture involving experts in the field of tourism we developed and installed a system for use by visitors to lancaster the system combines mobile computing technologies with a wireless infrastructure to present city visitors with information tailored to both their personal and environmental contexts in this paper we present an evaluation of guide focusing on the quality of the visitors experience when using the system keywords mobile c a comparative study of version management schemes for xml documents the problem of managing multiple versions for xml documents and semistructured data is of significant interest in many db applications and web related services traditional document version control schemes such as rcs suffer from the following two problems at the logical level they conceal the structure of the documents by modeling them as sequences of text lines and storing a document s evolution as a line edit script at the physical level they can incur in severe storage or processing costs because of their inability to trade off storage with computation to solve these problems we propose version management strategies that preserve the structure of the original document and apply and extend db techniques to minimize storage and processing costs therefore we propose and compare three schemes for xml version management namely the usefulness based copy control the multiversion b tree and the partially persistent list method a common characteristic of these schemes is that they cluster data using the notion of page usefulness which by selectively copying current information from obsolete pages provides for fast version reconstruction with minimal storage overhead the cost and performance of these version management schemes are evaluated and compared through extensive analysis and experimentation version management of xml documents the problem of ensuring efficient storage and fast retrieval for multi version structured documents is important because of the recent popularity of xml documents and semistructured information on the web traditional document version control systems e g rcs which model documents as a sequence of lines of text and use the shortest edit script to represent version differences can be inefficient and they do not preserve the logical structure of the original document therefore we propose a new approach where the structure of the documents is preserved intact and their sub objects are timestamped hierarchically for efficient reconstruction of current and past versions our technique called the usefulness based copy control ubcc is geared towards efficient version reconstruction while using small storage overhead our analysis and experiments illustrate the effectiveness of the overall approach to version control for structured documents moreover ubcc can easily support multiple concurrent versions as well as partial document retrieval 1 efficient complex query support for multiversion xml documents managing multiple versions of xml documents represents a critical requirement for many applications also there has been much recent interest in supporting complex queries on xml data e g regular path expressions structural projections diff queries in this paper we examine the problem of supporting efficiently complex queries on multiversioned xml documents our approach relies on a scheme based on durable node numbers dnns that preserve the order among the xml tree nodes and are invariant with respect to updates using the document s dnns various complex queries are reduced to combinations of partial version retrieval queries we examine three indexing schemes to efficiently evaluate partial version retrieval queries in this environment a thorough performance analysis is then presented to reveal the advantages of each scheme efficient management of multiversion documents by object referencing traditional approaches to versioning semistructured information are edit based i e subsequent document versions are represented by using edit scripts this paper proposes a reference based version management scheme that preserves the logical structure of the evolving document through the use of object references by preserving the document structure among versions the new scheme facilitates more efficient query support in particular we examine queries involving projections and selections on the document versions as well as queries on the document evolution history moreover we show that the proposed scheme provides an effective representation of multiversioned xml documents both at the transport and exchange levels in fact with the reference based scheme a document s history can also be viewed and processed as yet another xml document furthermore we demonstrate the effectiveness of the new scheme at the storage level in particular the scheme is enhanced with a usefulness based page management policy that extends and adapts techniques used in transaction time databases to ensure efficient clustering of information among versions an extensive comparison of the reference based versioning against representations used in temporal databases and persistent object managers depicts the performance advantages of the new approach finally it should be noted that reference based versioning is applicable to other kinds of semistructured information besides xml documents and can be used to replace traditional version control schemes such as the edit based rcs and the timestamp based sccs linearly bounded reformulations of conjunctive databases extended abstract database reformulation is the process of rewriting the data and rules of a deductive database in a functionally equivalent manner a meta modeling approach to workflow management systems supporting exception handling workflow management systems wfmss facilitate the definition of structure and decomposition of business processes and assists in management of coordinating scheduling executing and monitoring of such activities most of the current wfmss are built on traditional relational database systems and or using an objectoriented database system for storing the definition and run time data about the workflows however a wfms requires advanced modeling functionalities to support adaptive features such as on line exception handling this article describes our advanced meta modeling approach using various enabling technologies such as object orientation roles rules active capabilities supported by an integrated environment the adome as a solid basis for a flexible wfms involving dynamic match making migrating workflows and exception handling copyright 1999 elsevier science ltd key words meta modeling object orientation workflow management match making exception handling workflo charm an i o driven execution strategy for high performance transaction processing the performance of a transaction processing system whose database is not completely memory resident critically depends on the amount of physical disk i o required this paper describes a high performance transaction processing system called charm which aims to reducing the concurrency control overhead by minimizing the performance impacts of disk i o on lock contention delay in existing transaction processing systems a transaction blocked by lock contention is forced to wait while the transaction currently holding the contended lock is performing physical disk i o a substantial portion of a transaction s lock contention delay is thus attributed to disk i os performed by other transactions charm implements a two stage transaction execution tste strategy which makes sure that all the data pages that a transaction needs be memory resident before it is allowed to lock database pages moreover charm supports an optimistic version of the tste strategy otste which furth estimating frequency of change many online data sources are updated autonomously and independently in this paper we make the case for estimating the change frequency of the data to improve web crawlers web caches and to help data mining we first identify various scenarios where different applications have different requirements on the accuracy of the estimated frequency then we develop several frequency estimators for the identified scenarios in developing the estimators we analytically show how precise effective the estimators are and we show that the estimators that we propose can improve precision significantly 1 introduction with the explosive growth of the internet many data sources are available online most of the data sources are autonomous and are updated independently of the clients that access the sources for instance popular news web sites such as cnn and ny times update their contents periodically whenever there are new developments also many online stores update the price availab synchronizing a database to improve freshness in this paper we study how to refresh a local copy of an autonomous data source to maintain the copy up to date as the size of the data grows it becomes more difficult to maintain the copy fresh making it crucial to synchronize the copy effectively we define two freshness metrics change models of the underlying data and synchronization policies we analytically study how effective the various policies are we also experimentally verify our analysis based on data collected from 270 web sites for more than 4 months and we show that our new policy improves the freshness very significantly compared to current policies in use parallel crawlers in this paper we study how we can design an effective parallel crawler as the size of the web grows it becomes imperative to parallelize a crawling process in order to finish downloading pages in a reasonable amount of time we first propose multiple architectures for a parallel crawler and identify fundamental issues related to parallel crawling based on this understanding we then propose metrics to evaluate a parallel crawler and compare the proposed architectures using 40 million pages collected from the web our results clarify the relative merits of each architecture and provide a good guideline on when to adopt which architecture 1 a randomized approach to planning biped locomotion with prescribed motions in this paper we present a new scheme for planning a natural looking locomotion of a human like biped figure given start and goal positions in a virtual environment our scheme finds a sequence of motions to move from the start position to the goal using a set of prescribed live captured motion clips our scheme consists of three parts roadmap construction roadmap search and motion generation we randomly sample a set of valid configurations of the biped figure for the environment to construct a directed graph called a roadmap that guides the locomotion of the figure every edge of the roadmap is attached with a live captured motion clip traversing the roadmap we obtain the sequence of footprints and that of motion clips we finally adapt the motion sequence to the constraints specified by the footprint sequence to obtain the locomotion a probabilistic approach to planning biped locomotion with prescribed motions typical high level directives for locomotion of human like characters are encountered frequently in animation scripts or interactive systems in this paper we present a new scheme for planning natural looking locomotion of a biped figure to facilitate rapid motion prototyping and task level motion generation given start and goal positions in a virtual environment our scheme gives a sequence of motions to move from the start to the goal using a set of live captured motion clips animating spatiotemporal constraint databases constraint databases provide a very expressive framework for spatiotemporal database applications however animating such databases is difficult because of the cost of constructing a graphical representation of a single snapshot of a constraint database we present a novel approach that makes the efficient animation of constraint databases possible the approach is based on a new construct parametric polygon we present an algorithm to construct the set of parametric polygons that represent a given linear constraint database we also show how to animate objects defined by parametric polygons analyze the computational complexity of animation and present empirical data to demonstrate the efficiency of our approach 1 introduction spatiotemporal databases have recently begun to attract broader interest 10 12 27 while the temporal 4 23 24 and spatial 14 28 database technologies are relatively mature their combination is far from straightforward in this conte control and datatypes using the view formalism we herein deal with mixed specification formalisms i e formalisms with both a static data types and a dynamic behaviour part our formalism is based on symbolic transition systems sts 9 that allow one to specify systems at an abstract level and to avoid state explosion sts are a kind of guarded finite state transition diagrams where states and transitions are labelled with open terms both dynamic and static parts of objects are specified in a unifying approach as formal structures that we call views these components interpretation structures use sts and we show how these may be derived from their view structures the system is structured by means of collections of objects with identities a temporal logic is used to glue the components altogether and expresses a generalized form of synchronous product 1 we then show how a view structure and its interpretation structure may be obtained the formalism is explained using a simplified phone service example keywor a distributed pi calculus with local areas of communication this paper introduces a process calculus designed to capture the phenomenon of names which are known universally but always refer to local information our system extends the calculus so that a channel name can have within its scope several disjoint local areas such a channel name may be used for communication within an area it may be sent between areas but it cannot itself be used to transmit information from one area to another areas are arranged in a hierarchy of levels distinguishing for example between a single application a machine or a whole network we give an operational semantics for the calculus and develop a type system that guarantees the proper use of channels within their local areas we illustrate with models of an internet service protocol and a pair of distributed agents 1 information visualization within a digital video library the informedia digital video library contains over a thousand hours of video consuming over a terabyte of disk space this paper summarizes the multimedia abstractions used to represent this video in prior systems and introduces the visualization techniques employed to browse and navigate multiple video documents at once keywords digital video library information visualization multimedia abstraction 1 introduction the informedia project at carnegie mellon university deals primarily with video the goal of the project is to enable full content search and retrieval from digital video libraries christel et al 1996 wactlar et al 1997 consider the task of trying to find a five minute video clip of interest from a library of a thousand hour long videotapes in the analog domain this task would be interminable and the frustrated user would probably walk away without completing the task simply digitizing the video will not make the job easier through the use of speech rec interactive maps for a digital video library the informedia digital video library contains over 1200 hours of video through automatic processing descriptors are derived for the video to improve library access a new extension to the video processing is the extraction of geographic references from these descriptors the operational library interface shows the geographic entities addressed in a given story highlighting the regions discussed at any point in the video through a map display synchronized with the video playback the map can also be used as a query mechanism allowing users to search the terabyte library for stories taking place in a selected area of interest 1 introduction the informedia project at carnegie mellon university investigates the utility of speech recognition image processing and natural language processing techniques for improving search and discovery in the video medium since 1994 the project has been digitizing in mpeg 1 format news video from cnn as well as documentary educational video fro integrity constraints and constraint logic programming it is shown that constraint logic is useful for evaluation of integrity constraints in deductive databases integrity constraints are represented as calls to a metainterpreter for negation as failure implemented as a constraint solver this procedure called lazy negationas failure yields an incremental evaluation it starts checking the existing database and each time an update request occurs simplified constraints are produced for checking the particular update and new constraints corresponding to specialized integrity constraints are generated for the updated database 1 introduction there is a relationship between integrity constraints in databases and the constraints of constraint logic programming going beyond the partial overlap of the names applied for these phenomena both concern conditions that should be ensured for systems of interdependent entities the different tuples in a database and the set of variables in a program execution state both relate to problems that e detecting intrusions using system calls alternative data models intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities in this paper we study one such observable sequences of system calls into the kernel of an operating system using system call data sets generated by several different programs each consisting both of normal and intrusive behavior we compare the ability of different data modeling methods to represent normal behavior accurately and to recognize intrusions we compare the following methods simple enumeration of observed sequences methods based on relative frequencies of different sequences a data mining technique and hidden markov models hmms all of the methods perform adequately with hmms giving the best overall results we discuss the factors affecting the performance of each method and conclude that for this particular problem weaker methods than hmms are likely sufficient 1 introduction in 1996 forrest and others introduced a simple in community webs c webs technological assessment and system architecture this paper our presentation mainly relies on examples taken from one of the potential c web applications namely c web portals for cultural communities conflict resolution in collaborative planning dialogues in a collaborative planning environment in which the agents are autonomous and heterogeneous it is inevitable that discrepancies in the agents beliefs result in conflicts during the planning process in such cases it is important that the agents engage in collaborative negotiation to resolve the detected conflicts in order to determine what should constitute their shared plan of actions and shared beliefs this paper presents a plan based model for conflict detection and resolution in collaborative planning dialogues our model specifies how a collaborative system should detect conflicts that arise between the system and its user during the planning process if the detected conflicts warrant resolution our model initiates collaborative negotiation in an attempt to resolve the conflicts in the agent s beliefs in addition when multiple conflicts arise our model identifies and addresses the most effective aspect in its pursuit of conflict resolution furthermore by captur dynamic update cube for range sum queries a range sum query is very popular and becomes important in finding trends and in discovering relationships between attributes in diverse database applications it sums over the selected cells of an olap data cube where target cells are decided by the specified query ranges the direct method to access the data cube itself forces too many cells to be accessed therefore it incurs a severe overhead the response time is very crucial for olap applications which need interactions with users in the recent dynamic enterprise environment data elements in the cube are frequently changed the response time is affected in such an environment by the update cost as well as the search cost of the cube in this paper we propose an efficient algorithm to reduce the update cost significantly while maintaining reasonable search efficiency by using an index structure called the tree in addition we propose a hybrid method to provide either an approximate result or a precise one to reduce the overall cost of queries it is useful for various applications that need a quick approximate answer rather than an accurate one such as decision support systems 1 coordination middleware for xml centric applications this paper focuses on coordination middleware for distributed ap plications based on active documents and xml technologies it in troduces the main concepts underlying active documents and xml then the paper goes into details about the problem of defining a suitable middleware architecture to effectively support coordina tion activities in applications including active documents and mo bile agents by specifically focusing on the role played by xml technologies in that context according to a simple taxonomy the characteristics of several middleware systems are analyzed and evaluated this analysis enables us to identify the advantages and the shortcoming of the different approaches and to identify the ba sic requirements of a middleware for xml centric applications 1 ant colony control for autonomous decentralized shop floor routing in this paper we introduce a new approach to autonomous decentralized shop floor routing our system which we call ant colony control ac 2 applies the analogy of a colony of ants foraging for food to the problem of dynamic shop floor routing in this system artificial ants use only indirect communication to make all shop routing decisions by altering and reacting to their dynamically changing common environment through the use of simulated pheromone trails for simple factory layouts we show that the emergent behavior of the colony is comparable to using the optimal routing strategy furthermore as the complexity of the factory layout is increased we show that the adaptive behavior of ac 2 evolves local decision making policies that lead to near optimal solutions from the standpoint of global performance 1 introduction the factory is a complex dynamical environment often plagued by unexpected events machines may break down an unexpected urgent job may suddenly be re wasp nests for self configurable factories agent based approaches to manufacturing scheduling and control have gained increasing attention in recent years such approaches are attractive because they offer increased robustness against the unpredictability of factory operations but the specification of local coordination policies that give rise to efficient global performance and effectively adapt to changing circumstances remains an interesting challenge in this paper we introduce a new approach to this coordination problem drawing on various aspects of a computational model of how wasp colonies coordinate individual activities and allocate tasks to meet the collective needs of the nest we focus specifically on the problem of configuring machines in a factory to best satisfy potentially changing product demands over time our system models the set of jobs queued in front of any given machine as a wasp nest wherein wasp like agents interact to form a social hierarchy and prioritize the jobs that they represent other was bringing information extraction out of the labs the pinocchio environment pinocchio is an environment for developing information extraction applications new applications and languages can be covered by just writing declarative resources information is represented uniformly throughout the architecture all the modules use the same input structure and the same type of declarative resources modules are implemented via the same basic processors and share a common environment for resource development and debugging the result is an environment easy to use with limited training and skills 1 introduction the exponential increase in the quantity of information held in digital archives has fueled growing interest in techniques for information extraction given that the vast majority of available information is textual e g web pages electronic newspapers agency news the role of information extraction from text ie is becoming more and more central an ie system extracts pieces of information that are salient to the user s needs the typical output is a learning to tag for information extraction from text learningpinocchio is an algorithm for adaptive information extraction it learns template filling rules that insert sgml tags into texts learningpinocchio is based on a covering algorithm that learns rules by bottom up generalization of instances in a tagged corpus it has been tested on three scenarios in informal domains in two languages italian and english experiments report excellent results with respect to the current state of the art 1 introduction by general agreement the main barriers to wide use and commercialization of ie are the difficulties in adapting systems to new applications and domains in the last years there has been increasing interest in applying machine learning to information extraction from text 13 3 9 1 11 in particular there is an increasing interest in the application of adaptive ie to web pages 12 10 and to informal domains rental ads email messages etc 15 8 2 for building fully automated systems this is due from one side to the lp 2 an adaptive algorithm for information extraction from web related texts lp 2 is an algorithm for adaptive information extraction from web related text that induces symbolic rules by learning from a corpus tagged with sgml tags induction is performed by bottom up generalisation of examples in a training corpus training is performed in two steps initially a set of tagging rules is learned then additional rules are induced to correct mistakes and imprecision in tagging shallow nlp is used to generalise rules beyond the flat word structure generalization allows a better coverage on unseen texts as it limits data sparseness and overfitting in the training phase in experiments on publicly available corpora the algorithm outperforms any other algorithm presented in literature and tested on the same corpora experiments also show a significant gain in using nlp in terms of 1 effectiveness 2 reduction of training time and 3 training corpus size in this paper we present the machine learning algorithm for rule induction in particular we focus on the nlp based generalisation and the strategy for pruning both the search space and the final rule set 1 finding text regions using localised measures we present a method based on statistical properties of local image neighbourhoods for the location of text in real scene images this has applications in robot vision and desktop and wearable computing the statistical measures we describe extract properties of the image which characterise text invariant to a large degree to the orientation scale or colour of the text in the scene the measures are employed by a neural network to classify regions of an image as text or non text we thus avoid the use of different thresholds for the various situations we expect including when text is too small to read or when the text plane is not fronto parallel to the camera we briefly discuss applications and the possibility of recovery of the text for optical character recognition 1 introduction automatic location and digitisation of text in arbitrary scenes where the text may or may not be fronto parallel to the viewing plane is an area of computer vision which has not yet been location and recovery of text on oriented surfaces we present a method for extracting text from images where the text plane is not necessarily fronto parallel to the camera initially we locate local image features such as borders and page edges we then use perceptual grouping on these features to find rectangular regions in the scene these regions are hypothesised to be pages or planes that may contain text edge distributions are then used for the assessment of these potential regions providing a measure of confidence it will be shown that the text may then be transformed to a fronto parallel view suitable for example for an ocr system or other higher level recognition the proposed method is scale independent of the size of the text we illustrate the algorithm using various examples keywords oriented text perspective recovery of text edge angle distribution 1 introduction location and recovery of text in a scene would be useful in the context of wearable computing desk computing or unguided robotic motion such a what do we want from a wearable user interface this document outlines the author s ultimate goal and suggests one technology that can be exploited by applications writers to make them fit neatly into future application integration frameworks exploiting redundancy in question answering our goal is to automatically answer brief factual questions of the form when was the battle of hastings or who wrote the wind in the willows since the answer to nearly any such question can now be found somewhere on the web the problem reduces to finding potential answers in large volumes of data and validating their accuracy we apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half the success of our approach depends on the idea that the volume of available web data is large enough to supply the answer to most factual questions multiple times and in multiple contexts a query is generated from a question and this query is used to select short passages that may contain the answer from a large collection of web data these passages are analyzed to identify candidate answers the frequency of these candidates within the passages is used to vote for the most likely answer the recognizing user s context from wearable sensors baseline system introduction we describe a baseline system for training and classifying natural situations it is a baseline system because it will provide the reference implementation of the context classifier against which we can compare more sophisticated machine learning techniques it should be understood that this system is a precursor to a system for understanding all types of observable context not just location we are less interested in obtaining high precision and recall rates than we are in obtaining appropriate model structures for doing higher order tasks like clustering and prediction on a user s life activities ii background there has been some excellent work on recognizing various kinds of user situations via wearable sensors starner 6 uses hmms and omnidirectional and directional cameras to determine the user s location in a building and current action during a physical game aoki also uses a head mounted directional camera to d sangam modeling transformations for integrating now and tomorrow today many application engineers struggle to not only publish their relational object or ascii file data on the web but to also integrate information from diverse sources often inventing and reinventing a suite of hard wired integration tools a model management system that supports the specification and manipulation of not only data models and schemata but also mappings between the different models in a generic manner has the promise of solving these issues however support for modeling and managing such mappings as objects remains an unsolved challenge in our work we propose a powerful middleware tool that successfully tackles this challenge for this we propose a graph theoretic framework that allows users to explicitly model mappings between different data models as well as re structuring within one data model our map metamodel is based on a set of re usable mapping constructs that can in principle be applied on any data model described in our framework in our work we have tested these operators for xml and relational model mappings using the description of maps at the model level mappings between specific application schemas and transformations of associated application data can be automated by our framework our framework guarantees the correctness of the map of the generated transformation code of the output data model and of the generated application schemas based on the correctness criteria for the map metamodel in this paper we also introduce the model management system that we are developing to realize our proposed map modeling theory with we show not only the feasibility of our approach but also demonstrate the re usability and the ease of end to end development of modeling strategies to further illustrate our ide combining content based and collaborative filters in an online newspaper the explosive growth of mailing lists web sites and usenet news demands effective filtering solutions collaborative filtering combines the informed opinions of humans to make personalized accurate predictions content based filtering uses the speed of computers to make complete fast predictions in this work we present a new filtering approach that combines the coverage and speed of content filters with the depth of collaborative filtering we apply our research approach to an online newspaper an as yet untapped opportunity for filters useful to the wide spread news reading populace we present the design of our filtering system and describe the results from preliminary experiments that suggest merits to our approach optimizatizing the performance of schema evolution sequences more than ever before schema transformation is a prevalent problem that needs to be addressed to accomplish for example the migration of legacy systems to the newer oodb systems the generation of structured web pages from data in database systems or the integration of systems with different native data models such schema transformations are typically composed of a sequence of schema evolution operations the execution of such sequences can be very timeintensive possibly requiring many hours or even days and thus effectively making the database unavailable for unacceptable time spans while researchers have looked at the deferred execution approach for schema evolution in an effort to improve availability of the system to the best of our knowledge ours is the first effort to provide a direct optimization strategy for a sequence of changes in this paper we propose heuristics for the iterative elimination and cancellation of schema evolution primitives as well as for the merging of an ounce of prevention is worth a pound of cure formal verification for consistent database evolution consistency of a database is as an important property that must be preserved at all times in most oodb systems today application code can directly access and alter both the data as well as the structure of the database as a consequence application code can potentially violate the integrity of the database in terms of the invariants of the data model the user specified application constraints and even the referential integrity of the objects themselves a common form of consistency management in most databases today is to encode constraints at the system level e g foreign keys or at the trigger based level e g user constraints and to perform transaction rollback on discovery of any violation of these constraints however for programs that alter the structure as well as the objects in a database such as an extensible schema evolution program roll backs are expensive and add to the already astronomical cost of doing schema evolution in this paper pre execution generating a topically focused virtualreality internet surveys highlight that internet users are frequently frustrated by failing to locate useful information and by difficulty in browsing anarchically linked web structures we present a new internet browsing application called vr net that addresses these problems it first identifies semantic domains consisting of tightly interconnected web page groupings the second part populates a 3d virtual world with these information sources representing all relevant pages plus appropriate structural relations users can then easily browse through around a semantically focused virtual library 1 introduction the internet is a probably the most significant global information resource ever created allowing access to an almost unlimited amount of information in this paper we describe two inter related difficulties suffered by internet users and their combined influence on web use we then introduce an integrated search and browse solution tool that directly tackles both issues we also examin privacy preserving distributed data mining em there is a simple distributed solution that provides a degree of privacy to the individual sites an example association rule could be received f lu shot and age 50 implies hospital admission where at least 5 of insured meet all the criteria support and at least 30 of those meeting the flu shot and age criteria actually require hospitalization confidence there are algorithms to e ciently find all association rules with a minimum level of support we can easily extend this to the distributed case using the following lemma if a rule has support k valid the data approach figure 1 data warehouse approach to distributed data mining globally it must have support k on at least one of the individual sites a distributed algorithm for this would work as follows request that each site send all rules with support at least k for e using ldap directory caches this paper we consider the problem of reusing cached ldap directory entries for answering declarative ldap queries we develop a suite of query transformations that capture the semantics of ldap queries and design a sound and complete algorithm for determining whether a conjunctive ldap query is cacheanswerable using positive query templates we demonstrate the practicality of our algorithm for real applications with a preliminary performance evaluation based on sample queries from a directory enabled application at at t labs cache answerability is related to the problem of finding complete rewritings of a query using views see e g 8 a key conceptual difference arises due to the fact that we do not seek a rewriting of the original query in terms of the views in the semantic cache description but want to evaluate the original query against the cached directory instance further the differences of the ldap data model and query language make the previous results inapplicable for details see section 2 efficient goal directed bottom up evaluation of logic programs this paper introduces a new strategy for the efficient goal directed bottomup evaluation of logic programs instead of combining a standard bottomup evaluation strategy with a magic set transformation the evaluation strategy is specialized for the application to magic set programs which are characterized by clause bodies with a high degree of overlapping the approach is similar to other techniques which avoid re computation by maintaining and reusing partial solutions to clause bodies however the overhead is considerably reduced as these are maintained implicitly by the underlying prolog implementation the technique is presented as a simple meta interpreter for goal directed bottom up evaluation no magic set transformation is involved as the dependencies between calls and answers are expressed directly within the interpreter the proposed technique has been implemented and shown to provide substantial speed ups in applications of semantic based program analysis based on bottom up widening pos for efficient and scalable groundness analysis of logic programs the domain of positive boolean functions pos is by now well established for the analysis of the variable dependencies that arise within logic programs analyses based on pos that use binary decision diagrams have been shown to be efficient for a wide range of practical programs however independent of the representation assuming that p np an unwidened pos analysis can never come with any efficiency guarantees because of its potentially explosive behaviour this paper proposes a simple widening for goal dependent groundness analysis of logic programs that guarantees scalability and efficiency experimental results indicate that the widening induces only a very small loss of precision developing haptic and visual perceptual categories for reaching and grasping with a humanoid robot properties of the human embodiment sensorimotor apparatus and neurological structure participate directly in the growth and development of cognitive processes against enormous worst case complexity it is our position that relationships between morphology and perception over time lead to increasingly comprehensive models that describe the agent s relationship to the world we are applying insight derived from neuroscience neurology and developmental psychology to the design of advanced robot architectures to investigate developmental processes we have begun to approximate the human sensorimotor configuration and to engage sensory and motor subsystems in developmental sequences many such sequences have been documented in studies of infant development so we intend to bootstrap cognitive structures in robots by emulating some of these growth processes that bear an essential resemblance to the human morphology in this paper we will show two related examples in which a humanoid robot determines the models and representations that govern its behavior the first is a model that captures the dynamics of a haptic exploration of an object with a dextrous robot hand that supports skillful grasping the second example constructs constellations of visual features to predict relative hand object postures that lead reliably to haptic utility the result is a rst step in a trajectory toward associative visual haptic categories that bounds the incremental complexity of each stage of development treating constraints as objectives for single objective evolutionary optimization this paper presents a new approach to handle constraints using evolutionary algorithms the new technique treats constraints as objectives and uses a multiobjective optimization approach to solve the re stated single objective optimization problem the new approach is compared against other numerical and evolutionary optimization techniques in several engineering optimization problems with different kinds of constraints the results obtained show that the new approach can consistently outperform the other techniques using relatively small sub populations and without a significant sacrifice in terms of performance an updated survey of evolutionary multiobjective optimization techniques state of the art and future trends this paper reviews some of the most popular evolutionary multiobjective optimization techniques currently reported in the literature indicating some of their main applications their advantages disadvantages and degree of aplicability finally some of the most promising areas of future research are briefly discussed 1 introduction since the pioneering work of rosenberg in the late 1960s regarding the possibility of using genetic based search to deal with multiple objectives 1 this new area of research now called evolutionary multi objective optimization or emoo for short has grown considerably as indicates the notable increment mainly in the last 15 years of technical papers in international conferences and peer reviewed journals special sessions in international conferences and interest groups in the internet 1 multiobjective optimization is with no doubt a very important research topic both for scientists and engineers not only because of the multiobjective nature feature selection in web applications using roc inflections and power set pruning a basic problem of information processing is selecting enough features to ensure that events are accurately represented for classification problems while simultaneously minimizing storage and processing of irrelevant or marginally important features to address this problem feature selection procedures perform a search through the feature power set to find the smallest subset meeting performance requirements major restrictions of existing procedures are that they typically explicitly or implicitly assume a fixed operating point and make limited use of the statistical structure of the feature power set we present a method that combines the neyman pearson design procedure on finite data with the directed set structure of the receiver operating curves on the feature subsets to determine the maximal size of the feature subsets that can be ranked in a given problem the search can then be restricted to the smaller subsets resulting in significant reductions in computational learning concepts by interaction this paper presents a theory of how robots may learn concepts by interacting with their environment in an unsupervised way first categories of activities are learned then abstractions over those categories result in concepts the meanings of concepts are discussed robotic systems that learn categories of activities and concepts are presented introduction if machines could acquire conceptual knowledge with the same facility as humans then ai would be much better off there s no denying the dream of a machine that knows roughly what we know organized roughly as we organize it with roughly the same values and motives as we have it makes sense then to ask how this knowledge is acquired by humans and how might it be acquired by machines i focus on the origins of conceptual knowledge the earliest distinctions and classes the first efforts to carve the world at its joints one reason is just the desire to get to the bottom of or in this case the beginning of anything equix a search and query language for xml equix is a search language for xml that combines the power of querying with the simplicity of searching requirements for such languages are discussed and it is shown that equix meets the necessary criteria both a graph based abstract syntax and a formal concrete syntax are presented for equix queries in addition the semantics is defined and an evaluation algorithm is presented the evaluation algorithm is polynomial under combined complexity equix combines pattern matching quantification and logical expressions to query both the data and meta data of xml documents the result of a query in equix is a set of xml documents a dtd describing the result documents is derived automatically from the query 1 learning to match and cluster entity names introduction information retrieval is in large part the study of methods for assessing the similarity of pairs of documents document similarity metrics have been used for many tasks including ad hoc document retrieval text classification yc1994 and summarization gc1998 ssmb1997 another problem area in which similarity metrics are central is record linkage e g ka1985 where one wishes to determine if two database records taken from different source databases refer to the same entity for instance one might wish to determine if two database records from two different hospitals each containing a patient s name address and insurance information refer to the same person as another example one might wish to determine if two bibliography records each containing a paper title list of authors and journal name refer to the same publication in both of these examples and in many other practical cases most of the record fields integration of heterogeneous databases without common domains using queries based on textual similarity most databases contain name constants like course numbers personal names and place names that correspond to entities in the real world previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization however in many cases this assumption does not hold determining if two name constants should be considered identical can require detailed knowledge of the world the purpose of the user s query or both in this paper we reject the assumption that global domains can be easily constructed and assume instead that the names are given in natural language text we then propose a logic called whirl which reasons explicitly about the similarity of local names as measured using the vector space model commonly adopted in statistical information retrieval we describe an efficient implementation of whirl and evaluate it experimentally on data extracted from the world wide web we show that whir joins that generalize text classification using whirl whirl is an extension of relational databases that can perform soft joins based on the similarity of textual identifiers these soft joins extend the traditional operation of joining tables based on the equivalence of atomic values this paper evaluates whirl on a number of inductive classification tasks using data from the world wide web we show that although whirl is designed for more general similaritybased reasoning tasks it is competitive with mature inductive classification systems on these classification tasks in particular whirl generally achieves lower generalization error than c4 5 ripper and several nearest neighbor methods whirl is also fast up to 500 times faster than c4 5 on some benchmark problems we also show that whirl can be efficiently used to select from a large pool of unlabeled items those that can be classified correctly with high confidence introduction consider the problem of exploratory analysis of data obtained from the internet assuming that o the missing link a probabilistic model of document content and hypertext connectivity we describe a joint probabilistic model for modeling the contents and inter connectivity of document collections such as sets of web pages or research paper archives the model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics furthermore the relationships between topics is mapped out in order to build a predictive model of link content among the many applications of this approach are information retrieval and search topic identification query disambiguation focused web crawling web authoring and bibliometric analysis qualitative spatial representation and reasoning an overview the paper is a overview of the major qualitative spatial representation and reasoning techniques we survey the main aspects of the representation of qualitative knowledge including ontological aspects topology distance orientation and shape we also consider qualitative spatial reasoning including reasoning about spatial change finally there is a discussion of theoretical results and a glimpse of future work the paper is a revised and condensed version of 33 34 keywords qualitative spatial reasoning ontology the text is in a slightly di erent format from the fi format cohn the genia project corpus based knowledge acquisition and information extraction from genome research papers we present an outline of the genome information acquisition genia project for automatically extracting biochemical information from journal papers and abstracts genia will be available over the internet and is designed to aid in information extraction retrieval and visualisation and to help reduce information overload on researchers the vast repository of papers available online in databases such as medline is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the internet 1 introduction in the context of the global research effort to map the human genome the genome informatics extraction project genia genia 1999 aims to support such research by automatically extracting information from biochemical papers and their abstracts such as those available from medline medline 1999 written by domain specialists the vast repository of research papers which are the results does zooming improve image browsing we describe an image retrieval system we built based on a zoomable user interface zui we also discuss the design results and analysis of a controlled experiment we performed on the browsing aspects of the system the experiment resulted in a statistically significant difference in the interaction between number of images 25 75 225 and style of browser 2d zui 3d the 2d and zui browser systems performed equally and both performed better than the 3d systems the image browsers tested during the experiment include cerious software s thumbs plus trivista technology s simple landscape and photo goround and our zoomable image browser based on pad keywords evaluation controlled experiment image browsers retrieval systems real time computer graphics zoomable user interfaces zuis multiscale interfaces pad introduction in the past two decades with the emergence of faster computers the declining cost of memory the popularity of digital cameras online archives toward computer based support of meta cognitive skills a computational framework to coach self explanation this paper we describe how these solutions have been implemented in a computer tutor that coaches self explanation within andes a tutoring system for newtonian physics we also present the results of a formal study to evaluate the usability and effectiveness of the system finally we discuss some hypotheses to explain the obtained results based on the analysis of the data collected during the study introduction research on intelligent tutoring systems its has been increasingly affecting education while for many years its remained confined to research labs today they have started moving into the classroom showing their effectiveness for learning and influencing the structure of traditional curricula koedinger anderson h mark 1995 however existing its still target only a limited part of the learning process they generally focus on teaching problem solving and domain specific cognitive skills the long term goal of our research is to explore innovat xml conceptual modeling using uml the extensible markup language xml is increasingly finding acceptance as a standard for storing and exchanging structured and semi structured information with its expressive power xml enables a great variety of applications relying on such structures notably product catalogs digital libraries and electronic data interchange edi as the data schema an xml document type definition dtd is a means by which documents and objects can be structured currently there is no suitable way to model dtds conceptually our approach is to model dtds and thus classes of documents on the basis of uml unified modeling language we consider uml to be the connecting link between software engineering and document design i e it is possible to design object oriented software together with the necessary xml structures for this reason we describe how to transform the static part of uml i e class diagrams into xml dtds the major challenge for the transformation is to defi corporate memory management through agents the comma project corporate memory management through agents aims at developing an open agent based platform for the management of a corporate memory by using the most advanced results on the technical the content and the user interaction level we focus here on methodologies for the set up of multi agent systems requirement engineering and knowledge acquisition approaches 1 introduction how to improve access share and reuse of both internal and external knowledge in a company how to improve newcomers learning and integration in a company how to enhance technology monitoring in a company knowledge management km aims at solving such problems different research communities offer partial solutions for supporting km the integration of results from these different research fields seems to be a promising approach this is the motivation of the comma ist project funded by the european commission which started february 2000 the main objective is to implement and data integration services introduction with the prevalence of the network technology and the internet access to data independent of its physical storage location has become highly facilitated this further has enabled users to access a multitude of data sources that are related in some way and to combine the returned data to come up with useful information which is not physically stored in a single place for instance a person who has the intension of buying a car can query several car dealer web sites and then compare the results he can further query a data source which provides information about car reviews to help his decision about the cars he liked as another example imagine a company which has several branches in di erent cities each branch has its own local database recording its sales whenever global decisions about the company have to be made each branch database must be queried and the results must be combined on the other hand contacting data sources individually and then combining eventscope amplifying human knowledge and experience via intelligent robotic systems and information interaction the eventscope program develops publicly accessible reality browsers that display both archived and updating representations of remote environments derived from on site robotic sensors the interface encourages collaborative work within a community of users public exploration of real remote sites presents a variety of interface issues addressed by eventscope including time delay public exploration via a single robot and communication between geographically separate users from diverse backgrounds merging public interface with educational and contextual information extends the notion of interface to remote reality library eventscope is a nasa and private foundationfunded project based at carnegie mellon university 1 introduction publicly funded earth and planetary exploration is conducted to increase knowledge of our universe the public traditionally accesses this knowledge passively through the media however the development of the web and of robotic remote sensing tech reality browsing using information interaction and robotic autonomy for planetary exploration reality browsing is a framework that enables distributed control of a team of planetary robots in it prioritized user queries are serviced in a hierarchical data structure consisting of an internet accessible world model data archives on the remote robots and finally a multiple robot planner that coordinates query directed searches this paper introduces the reality browser concept and outlines important research issues required for implementation policy controlled mobility the mobility of software components seems an interesting solution for the deployment of web services and applications in the internet global infrastructure and also in mobile ad hoc networks the network infrastructure already supports several forms of code mobility to make possible to dynamically reconfigure bindings between code fragments and locations where they are to be executed however more work is still to be done to facilitate the specification and the enforcement of the mobility behaviour of software components the traditional approach to embed the migration strategy into the component at design time can not suit the dynamicity of the new network scenarios mobile components should instead be enhanced with the possibility to adapt their mobility behaviour to evolving application and environment conditions and to react to unforeseen events to reach this goal the paper proposes the adoption of policy based systems to abstract away the specification of migration strategies from the component code this approach permits to change the mobility behaviour of components without intervention on the component code we have experienced the dynamicity and flexibility of the proposed approach in the framework obtained by integrating a policy based management system in a mobile agent environment keywords mobility migration policies adaptation mobile agents reconfiguration 1 speaker tracking in broadcast audio material in the framework of the thisl project in this paper we present a first approach to build an automatic system for broadcast news speaker based segmentation based on a chop and recluster method this system is developed in the framework of the thisl project a metric based segmentation is used for the chop procedure and different distances have been investigated the recluster procedure relies on a bottom up clustering of segments obtained beforehand and represented by non parametricmodels various hierarchical clustering schemes have been tested some experiments on bbc broadcast news recordings show that the system can detect real speaker changes with high accuracy mean error 0 7s and fair false alarm rate mean false alarm rate 5 5 the recluster procedure can produce homogeneous clusters but it is not already robust enough to tackle too complex classification tasks 1 introduction thisl thematic indexing of spoken language 1 is an esprit long term research project that is investigating the development introspective multistrategy learning constructing a learnung strategy under reasoning failure officer praised dog for barking at object enables detect drugs out fk initiates retrieval 5 6 missing figure 10 forgetting to fill the tank with gas a actual intention e expectation q question c context i index g goal tank out of gas tank full tank low fill tank should have filled up with gas when tank low expectation what action to do key g goal i index c context q question e expectation a actual intention results at store connections with related concepts other learning goals take multiple arguments for instance a knowledge differentiation goal cox ram 1995 is a goal to determine a change in a body of knowledge such that two items are separated conceptually in contrast a knowledge reconciliation goal cox ram 1995 is one that seeks to merge two items that were mistakenly considered separate entities both expansion goals and reconciliation goals may include or spawn a knowledge organization goal ram 1993 that seeks to reorganize the existing knowledge so that it is made available to the reasoner at the appropriate time as well as modify the structure or content of a concept itself such reorganization of knowledge affects the conditions under which a particular piece of knowledge is retrieved or the kinds of indexes associated with an item in memory a model independent source code repository software repositories used to support program development and maintenance invariably require an abstract model of the source code this requirement restricts the repository user to the analyses and queries supported by the data model of the repository in this work we present a software repository system based on an existing information retrieval system for structured text source code is treated as text augmented with supplementary syntactic and semantic information both the source text and supplementary information can then be queried to retrieve elements of the code no transformations are necessary to satisfy the requirements of a database storage model as a result the system is free of many of the limitations imposed by existing systems 1 introduction in order to store computer source code in a database the source code must be abstracted in some manner so that it satisfies the requirements of the data model supported by the chosen database system databases applied to t goal directed adaptive behavior in second order neural networks the maxson family of architectures the paper presents a neural network architecture maxson based on second order connections that can learn a multiple goal approach avoid task using reinforcement from the environment it also enables an agent to learn vicariously from the successes and failures of other agents the paper shows that maxson can learn certain spatial navigation tasks much faster than traditional q learning as well as learn goal directed behavior increasing the agent s chances of long term survival the paper shows that an extension of maxson v maxson enables agents to learn vicariously and this improves the overall survivability of the agent population multiple goal q learning issues and functions this paper addresses the concerns of agents using reinforcement learning to learn to achieve multiple simultaneous goals it proves that an algorithm based on acting upon the maximal goal at any one time will in many cases not not produce the maximal expected utility for the agent the paper then examines the type of function approximator necessary for the agent s reinforcement learning system and concludes that a bi linear function is the best compromise between expressive power and speed of learning second order networks for wall building agents this paper describes robust neurocontrollers for groups of agents that perform construction tasks they enable agents to balance multiple goals perform sequences of actions and survive while building walls corridors intersections and briar patches wearable computing and the remembrance agent this paper gives an overview of the field of wearable computing it covers the key differences between wearables and other portable computers and discusses issues with the design and application for wearables there then follows a specific example the wearable remembrance agent a proactive memory aid the paper concludes with discussion of future directions for research and applications inspired by using the prototype 1 introduction this scenario may sound some way off in practical terms but it is not all the technologies needed to support it are available it is the aim of this paper to describe some of these technologies in more detail and give an indication of their current status the paper will start by describing features available in wearable computers that are not available in current laptops or personal digital assistants pdas it will then go on to describe a number of other general application areas for wearables and current wearable technologies and design needs this is followed by a description of the remembrance agent ra a wearable memory aid that continually reminds the wearer of potentially relevant information based on the wearer s current physical and virtual context finally the paper discusses extensions that are being added to the current prototype system 2 what are wearable computers the fuzzy definition of a wearable computer is that it is a computer that is always with the user is comfortable and easy to keep and use and is as unobtrusive as clothing however this smart clothing definition is unsatisfactory when the details are considered most importantly it does not convey how a wearable computer is any different from a very small palm top a more specific definition is that wearable computers have many of the followin server selection on the world wide web we evaluate server selection methods in a web environment modeling a digital library which makes use of existing web search servers rather than building its own index the evaluation framework portrays the web realistically in several ways its search servers index real web documents are of various sizes cover different topic areas and employ different retrieval methods selection is based on statistics extracted from the results of probe queries submitted to each server we evaluate published selection methods and a new method for enhancing selection based on expected search server effectiveness results show cori to be the most effective of three published selection methods cori selection steadily degrades with fewer probe queries causing a drop in early precision of as much as 0 05 one relevant document out of 20 modifying cori selection based on an estimation of expected effectiveness disappointingly yields no significant improvement in effectiveness however modifying cor effective site finding using link anchor information link based ranking methods have been described in the literature and applied in commercial web search engines however according to recent trec experiments they are no better than traditional content based methods we conduct a different type of experiment in which the task is to find the main entry point of a specific web site in our experiments ranking based on link anchor text is twice as effective as ranking based on document content even though both methods used the same bm25 formula we obtained these results using two sets of 100 queries on a 18 5 million docu ment set and another set of 100 on a 0 4 million document set this site finding effectiveness begins to explain why many search engines have adopted link methods it also opens a rich new area for effectiveness improvement where traditional methods fail learning to extract symbolic knowledge from the world wide web the world wide web is a vast source of information accessible to computers but understandable only to humans the goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the world wide web such a knowledge base would enable much more e ective retrieval of web information and promote new uses of the web to support knowledge based inference and problem solving our approach istodevelop a trainable information extraction system that takes two inputs the rst is an ontology that de nes the classes e g company person employee product and relations e g employed by produced by ofinterest when creating the knowledge base the second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations given these inputs the system learns to extract information from other pages and hyperlinks on the web this paper describes our general approach several machine learning algorithms for this task and promising initial results with a prototype system that has created a knowledge base describing university people courses and research projects diffusion snakes using statistical shape knowledge we present a novel extension of the mumford shah functional that allows to incorporate statistical shape knowledge at the computational level of image segmentation our approach exhibits various favorable properties non local convergence robustness against noise and the ability to take into consideration both shape evidence in given image data and knowledge about learned shapes in particular the latter property distinguishes our approach from previous work on contour evolution based image segmentation experimental results conrm these properties ruling agent motion in structured environments the design and development of cooperative internet applications based on mobile agents require appropriate modelling of both the physical space where agents roam and the conceptual space of mobile agent interaction the paper discusses how an open internet based organisation network can be modelled as a hierarchical collection of locality domains where agents can dynamically acquire information about resource location and availability according to their permissions it also analyses the issue of how agent motion can be ruled and constrained within a structured environment by means of an appropriate coordination infrastructure 1 introduction mobile agents are a promising technology for the design and development of cooperative applications on the internet 3 5 12 13 due to their capability of autonomously roaming the internet mobile agents can move locally to the resources they need let them be users data or services and there interact with them this can p roadrunner towards automatic data extraction from large web sites the paper investigates techniques for extracting data from html sites through the use of automatically generated wrappers to automate the wrapper generation and the data extraction process the paper develops a novel technique to compare html pages and generate a wrapper based on their similarities and differences experimental results on real life data intensive web sites confirm the feasibility of the approach 1 datafoundry information management for scientific data data warehouses and data marts have been successfully applied to a multitude of commercial business applications they have proven to be invaluable tools by integrating information from distributed heterogeneous sources and summarizing this data for use throughout the enterprise although the need for information dissemination is as vital in science as in business working warehouses in this community are scarce because traditional warehousing techniques don t transfer to scientific environments there are two primary reasons for this difficulty first schema integration is more difficult for scientific databases than for business sources because of the complexity of the concepts and the associated relationships while this difference has not yet been fully explored it is an important consideration when determining how to integrate autonomous sources second scientific data sources have highly dynamic data representations schemata when a data source participating in a warehouse automatic generation of warehouse mediators using an ontology engine data warehouses created for dynamic scientific environments such as genetics face significant challenges to their long term feasibility one of the most significant of these is the high frequency of schema evolution resulting from both technological advances and scientific insight failure to quickly incorporate these modifications will quickly render the warehouse obsolete yet each evolution requires significant effort to ensure the changes are correctly propagated datafoundry utilizes a mediated warehouse architecture with an ontology infrastructure to reduce the maintenance requirements of a warehouse among other things the ontology is used as an information source for automatically generating mediators the programs that transfer data between the data sources and the warehouse the identification definition and representation of the metadata required to perform this task are the primary contributions of this work 1 introduction the datafoundry research project at llnl s meta data based mediator generation mediators are a critical component of any data warehouse they transform data from source formats to the warehouse representation while resolving semantic and syntactic conflicts the close relationship between mediators and databases requires a mediator to be updated whenever an associated schema is modified failure to quickly perform these updates significantly reduces the reliability of the warehouse because queries do not have access to the most current data this may result in incorrect or misleading responses and reduce user confidence in the warehouse unfortunately this maintenance may be a significant undertaking if a warehouse integrates several dynamic data sources this paper describes a meta data framework and associated software designed to automate a significant portion of the mediator generation task and thereby reduce the effort involved in adapting to schema changes by allowing the dba to concentrate on identifying the modifications at a high level instead of r the evolutionary unfolding of complexity we analyze the population dynamics of a broad class of tness functions that exhibit epochal evolution a dynamical behavior commonly observed in both natural and artificial evolutionary processes in which long periods of stasis in an evolving population are punctuated by sudden bursts of change our approach statistical dynamics combines methods from both statistical mechanics and dynamical systems theory in a way that offers an alternative to current landscape models of evolutionary optimization we describe the population dynamics on the macroscopic level of tness classes or phenotype subbasins while averaging out the genotypic variation that is consistent with a macroscopic state metastability in epochal evolution occurs solely at the macroscopic level of the fitness distribution while a balance between selection and mutation maintains a quasistationary distribution of fitness individuals diffuse randomly through selectively neutral subbasins in genotype space sudden innovations occur when through this diffusion a genotypic portal is discovered that connects to a new subbasin of higher fitness genotypes in this way we identify innovations with the unfolding and stabilization of a new dimension in the macroscopic state space the architectural view of subbasins and portals in genotype space clarifies how frozen accidents and the resulting phenotypic constraints guide the evolution to higher complexity preserving contextual navigation in hypermedia querying navigating query results is a highly volatile task usually requiring much effort from the users while not providing a firm reference point for further queries or refinement many users accessing traditional search engines are confronted with lengthy pages of hypertext links through which relevant information must be found accessing each link can bring a user closer or farther from the information they seek in either case the context between the query results and the current web page can be lost delaunay mm addresses these issues by creating virtual documents through which all query results are displayed in a meaningful and organized fashion and that reference the originating web page thus providing semantic browsing and hypermedia navigation without loss of context keywords digital library querying hypermedia querying context preservation introduction with an increase in the number of users daily the world wide web has become one of the most indispensable technologie a user interface for distributed multimedia database querying with mediator supported refinement the delaunay mm system supports an interactive customizable interface for querying multimedia distributed databases like digital libraries through this interface users select virtual document styles that cater the display of query results to their needs while also offering transparent pre and post query refinement and nested querying delaunay mm s virtual documents preserve context by maintaining a single customizable interface for result viewing the advanced transparent query features rely on mediation to provide adept access to information in this paper we present the framework for delaunay mm its architecture the user interface and results of the first usability study 1 introduction with an increase in the number of users daily the world wide web has become an indispensable technology with increasingly diverse user populations and available technologies the value of reliable searching and information navigation mechanisms is becoming more significant the peerware core middleware support for peer to peer and mobile systems the pervasiveness of computer networks together with the availability of wireless links are steering distributed systems towards scenarios where computing is increasingly decentralized decoupled and dynamically reconfigurable the popularity of and demand for applications that exploit mobile and peer to peer interactions is a symptom of such change nevertheless by and large these applications are being built in an ad hoc manner and often with architectures that by sticking to the traditional client server paradigm do not fully capture and support the peculiar requirements of the new scenario lineage tracing for general data warehouse transformations data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information during the integration process source data typically undergoes a series of transformations which may vary from simple algebraic operations or aggregations to complex data cleansing procedures in a warehousing environment the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived we formally define the lineage tracing problem in the presence of general data warehouse transformations and we present algorithms for lineage tracing in this environment our tracing procedures take advantage of known structure or properties of transformations when present but also work in the absence of such information our results can be used as the basis for a lineage tracing tool in a general warehousing setting and also can guide the design of data warehouses that enable efficient lineage tracing 1 practical lineage tracing in data warehouses we consider the view data lineage problem in a warehousing environment for a given data item in a materialized warehouse view we want to identify the set of source data items that produced the view item we formalize the problem and present a lineage tracing algorithm for relational views with aggregation based on our tracing algorithm we propose a number of schemes for storing auxiliary views that enable consistent and efficient lineage tracing in a multisource data warehouse we report on a performance study of the various schemes identifying which schemes perform best in which settings based on our results we have implemented a lineage tracing package in the whips data warehousing system prototype at stanford with this package users can select view tuples of interest then efficiently drill down to examine the source data that produced them 1 introduction data warehousing systems collect data from multiple distributed sources integrate the information as materialized v relational representations that facilitate learning given a collection of objects in the world along with some relations that hold among them a fundamental problem is how to learn denitions of some relations and concepts of interest in terms of the given relations these denitions might be quite complex and inevitably might require the use of quanti ed expressions attempts to use rst order languages for these purposes are hampered by the fact that relational inference is intractable and consequently so is the problem of learning relational denitions this work develops an expressive relational representation language that allows the use of propositional learning algorithms when learning relational denitions the representation serves as an intermediate level between a raw description of observations in the world and a propositional learning system that attempts to learn denitions for concepts and relations it allows for hierarchical composition of relational expressions that can be evaluated ecientl automatic discrimination among languages based on prosody alone the development of methods for the automatic identification of languages is motivated both by speech based applications intended for use in a multi lingual environment and by theoretical questions of cross linguistic variation and similarity we evaluate the potential utility of two prosodic variables f 0 and amplitude envelope modulation in a pairwise language discrimination task discrimination is done using a novel neural network which can successfully attend to temporal information at a range of timescales both variables are found to be useful in discriminating among languages and confusion patterns in general reflect traditional intonational and rhythmic language classes the methods employed allow empirical determination of prosodic similarity across languages die entwicklung von methoden zur automatischen sprachidentifikation wird motiviert sowohl durch sprach basierte anwendungen die zum einsatz in einer mehrsprachigen umgebung bestimmt sind als auch durch theoretisch language identification from prosody without explicit features most current language identification lid systems make little or no use of prosodic information despite the importance of prosody in lid by humans the greatest obstacle has been that of finding an appropriate feature set which captures linguistically relevant prosodic information the only system to attempt lid entirely on the basis of prosodic variables uses a set of over 200 features which are selected and combined in a task specific manner 12 we apply a novel recurrent neural network model to the task of pairwise discrimination among languages network inputs are limited to delta f 0 and the first difference of the band limited amplitude envelope initial results are based on all pairwise combinations of english german japanese mandarin and spanish with 90 speakers per language keywords language identification recurrent neural networks prosody 1 prosody and language identification most current approaches to automatic language identification use some form of segment re developing language processing components with gate a user guide contents 1 introduction 3 1 1 how to use this text 4 1 2 context 5 1 3 overview 6 1 4 structure of the book 11 1 5 further reading 12 2 how to 14 2 1 download gate 14 2 2 install and run gate 14 2 3 d f configure gate 16 2 4 build gate 17 2 5 d f create a new creole resource 18 2 6 f11 experience with a language engineering architecture three years of gate gate the general architecture for text engineering aims to provide a software infrastructure for researchers and developers working in the area of natural language processing a version of gate has now been widely available for three years in this paper we review the objectives which motivated the creation of gate and the functionality and design of the current system we discuss the strengths and weaknesses of the current system identify areas for improvement and present plans for implementing these improvements introduction we think that if you re researching human language processing you should probably not be writing code to ffl store data on disk ffl display data ffl load processor modules and data stores into processes ffl initiate and administer processes ffl divide computation between client and server ffl pass data between processes and machines a domain specific software architecture dssa for language processing should do all this for you you will have t loglinear models for first order probabilistic reasoning recent work on loglinear models in probabilistic constraint logic programming is applied to first order probabilistic reasoning probabilities are defined directly on the proofs of atomic formulae and by marginalisation on the atomic formulae themselves we use stochastic logic programs slps composed of labelled and unlabelled definite clauses to define the proof probabilities we have a conservative extension of first order reasoning so that for example there is a one one mapping between logical and random variables we show how in this framework inductive logic programming ilp can be used to induce the features of a loglinear model from data we also compare the presented framework with other approaches to first order probabilistic reasoning keywords loglinear models constraint logic programming inductive logic programming 1 introduction a framework which merges first order logical and probabilistic inference in a theoretically sound and applicable manner promises ma genetic algorithms based systems for conceptual engineering design this paper we try to integrate methods of preferences and scenarios with genetic algorithms used to perform multi objective optimisation the goal is to make a system that will be able to work together with the designer during the conceptual design phase where interaction and designer knowledge are sometimes more important than accuracy module optimisation constraint handling module fuzzy rules handling module jres a resource accounting interface for java with the spread of the internet the computing model on server systems is undergoing several important changes recent research ideas concerning dynamic operating system extensibility are finding their way into the commercial domain resulting in designs of extensible databases and web servers in addition both ordinary users and service providers must deal with untrusted downloadable executable code of unknown origin and intentions across the board java has emerged as the language of choice for internet oriented software we argue that in order to realize its full potential in applications dealing with untrusted code java needs a flexible resource accounting interface the design and prototype implementation of such an interface jres is presented in this paper the interface allows to account for heap memory cpu time and network resources consumed by individual threads or groups of threads jres allows limits to be set on resources available to threads and it can invoke a feature relevance heuristic for indexing and compressing large case bases this paper reports results with igtree a formalism for indexing and compressing large case bases in instance based learning ibl and other lazy learning techniques the concept of information gain entropy minimisation is used as a heuristic feature relevance function for performing the compression of the case base into a tree igtree reduces storage requirements and the time required to compute classifications considerably for problems where current ibl approaches fail for complexity reasons moreover generalisation accuracy is often similar for the tasks studied to that obtained with information gain weighted variants of lazy learning and alternative approaches such as c4 5 although igtree was designed for a specific class of problems linguistic disambiguation problems with symbolic nominal features huge case bases and a complex interaction between sub regularities and exceptions we show in this paper that the approach has a wider applicability when generalising i the eel programming language and internal concurrency in logic agents this paper describes work done on creating the logic programming language eel the language is designed for implementing agents with a behaviour based concurrent internal architecture the paper also suggests a new such architecture which improves on the ones currently available it gives examples of how parts of that architecture are implemented in eel and comments that the agent oriented programming paradigm currently contains two different metaphors for concurrency eel s event based approach to process communication and process initiation introduces an explicit representation of state to a logic program a new declarative approach to object states is demonstrated as a part of the object oriented implementation of the suggested agent architecture as well as being a programming language eel is a formalism which is well suited for logic based machine learning of behaviour and interaction this paper briefly outlines the scope for using such learning to improve on exist antitonic logic programs we propose a framework which extends antitonic logic programs 3 to an arbitrary complete bilattice of truth values where belief and doubt are explicitly represented based on fitting s ideas this framework allows a precise definition of important operators found in logic programming such as explicit negation and the default negation in particular it leads to a natural integration of explicit negation with the default negation through the coherence principle 20 according to this principle the explicit negation entails the default negation we then define coherent answer sets and the paraconsistent well founded model semantics generalizing paraconsistent semantics for logic programs for instance wfsxp 2 our framework is an extension of antitonic logic programs in the most cases and is general enough to capture probabilistic deductive databases possibilistic logic programming hybrid probabilistic logic programs and fuzzy logic programming thus we have a powerful mathematical formalism for dealing with default reasoning paraconsistency and uncertainty we illustrate its adumbration of paraconsistency with an embedding of wfsxp into our framework semantic approaches to structuring and querying web sites in order to pose effective queries to web sites some form of site data model must be implicitly or explicitly shared by users many approaches try to compensate for the lack of such a common model by considering the hypertextual structure of web sites unfortunately this structure has usually little to do with data semantics in this paper a different technique is proposed that allows for both navigational and logical conceptual description of web sites the data model is based on wg log a query language based on the graph oriented database model of good gys94 and g log par95 which allows the description of data manipulation primitives via sets of graph s the wg log description of a web site schema is lexically based on standard hypermedia design languages thus allowing for easy schema generation by current hypermedia authoring environments the use of wg log for queries allows graphic query construction with respect to both the navigational and the logical parts of schema structuring and querying the web through graph oriented languages in order to pose effective queries to web sites some form of site data model must be implicitly or explicitly shared by users many approaches try to compensate for the lack of such a common model by considering the hypertextual structure of web sites unfortunately this structure has usually little to do with data semantics in this paper a different technique is proposed that allows for both navigational and data model description of web sites while allowing for graphical queries the data model is based on wglog a description and query language based on the graph oriented object database model of good gys94 and g log par95 allowing description of data manipulation primitives via graph transformations wg log description of the navigational part of a web site schema is lexically based on standard hypermedia design languages thus allowing for easy schema generation by current hypermedia authoring environments the use of wg log for queries allows graphic query construction complexity and expressive power of logic programming this paper surveys various complexity and expressiveness results on different forms of logic programming the main focus is on decidable forms of logic programming in particular propositional logic programming and datalog but we also mention general logic programming with function symbols next to classical results on plain logic programming pure horn clause programs more recent results on various important extensions of logic programming are surveyed these include logic programming with different forms of negation disjunctive logic programming logic programming with equality and constraint logic programming 1 computing science department uppsala university box 311 s 751 05 uppsala sweden email dantsin pdmi ras ru 2 institut und ludwig wittgenstein labor fur informationssysteme technische universitat wien treitlstra e 3 a 1040 wien austria e mail eiter kr tuwien ac at 3 institut und ludwig wittgenstein labor fur informationssysteme technische universitat rule discovery from time series we consider the problem of finding rules relating patterns in a time series to other patterns in that series or patterns in one series to patterns in another series a simple example is a rule such as a period of low telephone call activity is usually followed by a sharp rise in call volume examples of rules relating two or more time series are if the microsoft stock price goes up and intel falls then ibm goes up the next day and if microsoft goes up strongly for one day then declines strongly on the next day and on the same days intel stays about level then ibm stays about level our emphasis is in the discovery of local patterns in multivariate time series in contrast to traditional time series analysis which largely focuses on global models thus we search for rules whose conditions refer to patterns in time series however we do not want to define beforehand which patterns are to be used rather we want the patterns to be formed from the data in t a case for parallelism in data warehousing and olap in recent years the database community has experienced a tremendous increase in the availability of new technologies to support efficient storage and retrieval of large volumes of data namely data warehousing and on line analytical processing olap products efficient query processing is critical in such an environment yet achieving quick response times with olap queries is still largely an open issue in this paper we propose a solution approach to this problem by applying parallel processing techniques to a warehouse environment we suggest an efficient partitioning strategy based on the relational representation of a data warehouse i e star schema furthermore we incorporate a particular indexing strategy dataindexes to further improve query processing times and parallel resource utilization and propose a preliminary parallel star join strategy 1 introduction in recent years there has been an explosive growth in the use of databases for decision support this phenome bringing up robots or the psychology of socially intelligent robots from theory to implementation we discuss robotic experiments in a framework based on theories in developmental psychology 1 introduction piaget s theory of cognitive development has strongly influenced many approaches in artificial intelligence and agent research his theory has been challenged from various directions and recent experiments have confirmed vygotsky s belief in the essential role of social interaction and teaching as a scaffolding mechanism which is important for the child in order to reach higher levels of competence and control based on current skills hereby concepts are not taught directly but through social interaction the child s experiences are re arranged a shared understanding develops between the child and its interaction partner a piagetean viewpoint sees language as a product of the cognitive development of mental representations while vygotsky believes that the sole primary function of language is communication with peers and adults and that language develops exactly in this cont studying robot social cognition within a developmental psychology framework this paper discusses two prominent theories of cognitive development and relates them to experiments in social robotics the main difference between these theories lies in the different views on the relationship between a child and its social environment a the child as a solitary thinker piaget and b the child in society vygotsky we discuss the implications this has on the design of socially intelligent agents focusing on robotic agents we argue that the framework proposed by vygotsky provides a promising research direction in autonomous agents we give examples of implementations in the area of social robotics which support our theoretical considerations more specifically we demonstrate how a teacher learner setup can be used to teach a robot a proto language the same control architecture is also used for a humanoid doll robot which can interact with a human by imitation another experiment addresses dynamic coupling of movements between a human and a mobile robot here agent based social simulation with coalitions in social reasoning there is a growing belief that the agents cognitive structures play a central role on the enhancement of predicative capacities of decision making strategies this paper analyses and simulates the construction of cognitive social structures in the process of decision making with multiple actors in this process it is argued that the agent s rational choices may be assessed by its motivations according to different patterns of social interactions we first construct an abstract model of social dependence between agents and define a set of social structures that are easily identifiable according to potential interactions we then carry out a set of experiments at micro social levels of analysis where the agents cognitive structures are explicitly represented these experiments indicate that different social dependence structures imply distinct structural patterns of negotiation proposals which appear to have diverse patterns of complexity in the search space it is subsequently shown that this observation emerges as an issue of ambiguity in the regulation of different decision making criteria relative to motivation oriented and utility oriented choices in the scope of this ambiguity we finally make some conjectures relative to further analytical and empirical analysis around the relation between patterns of complexity of social structures and decision making topical locality in the web experiments and observations most web pages are linked to others with related content this idea combined with another that says that text in and possibly around html anchors describe the pages to which they point is the foundation for a usable world wide web in this paper we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the web in particular we find that the likelihood of linked pages having similar textual content to be high the similarity of sibling pages increases when the links from the parent are close together titles descriptions and anchor text represent at least part of the target page and that anchor text may be a useful discriminator among unseen child pages these results present the foundations necessary for the success of many web systems including search engines focused crawlers linkage analyzers and intelligent web agents 1 introduction most web pages are linked to others with related content applying parallelism to improve genetic algorithm based design optimization introduction the abundance of powerful workstations makes course grained parallelization an obvious enhancement to many optimization techniques including genetic algorithms gol89 dm97 while initial modifications have been made to gado genetic algorithm for design optimization ras98 rhg97 such changes have not been carefully analyzed for potential impacts on quality more generally parallelization has the potential to improve ga performance through the use of alternative models of computation parallelism can certainly reduce the total elapsed clock time for a solution but as a change in model of computation either real or simulated it can change the number of simulator calls and even make new solutions achievable the effects of parallelization on gado were investigated during my summer internship at the center for computational design 2 objectives since a straightforward parallelized implementation already existed my first tasks were to ana discoweb applying link analysis to web search how often does the search engine of your choice produce results that are less than satisfying generating endless links to irrelevant pages even though those pages may contain the query keywords how often are you given pages that tell you things you already know while the search engines and related tools continue to make improvements in their information retrieval algorithms for the most part they continue to ignore an essential part of the web the links we have found that link analysis can have significant contributions to web page retrieval from search engines to web community discovery and to the measurement of web page influence it can help to rank results and find high quality index hub link pages that contain links to the best sites on the topic of interest our work is based on research from ibm s clever project 7 4 6 stanford s google 3 and the web archaeology research 2 1 at compaq s systems research center these research teams have demonstrated some of the contributions that link analysis can make in the web in our work we have attempted to generalize and improve upon these approaches just as in citation analysis of published works the most influential documents on the web will have many other documents recommending pointing to them this idea underlies all link analysis efforts from the straightforward technique of counting the number of incoming edges to a page to the deeper eigenvector analysis used in our work and in those projects mentioned above it turns out that the identification of high quality web pages reduces to a sparse eigenvalue of the adjacency matrix business process coordination state of the art trends and open issues over the past decade there has been a lot of work in developing middleware for integrating and automating enterprise business processes today with the growth in e commerce and the blurring of enterprise boundaries there is renewed interest in business process coordination especially for inter organizational processes this paper provides a historical perspective on technologies for intra and interenterprise business processes reviews the state of the art and exposes some open research issues we include a discussion of process based coordination and event rule based coordination and corresponding products and standards activities we provide an overview of the rather extensive work that has been done on advanced transaction models for business processes and of the fledgling area of business process intelligence 1 finding related pages in the world wide web when using traditional search engines users have to formulate queries to describe their information need this paper discusses a different approach toweb searching where the input to the search process is not a set of query terms but instead is the url of a page and the output is a set of related web pages a related web page is one that addresses the same topic as the original page for example www washingtonpost com is a page related to www nytimes com since both are online newspapers we describe two algorithms to identify related web pages these algorithms use only the connectivity information in the web i e the links between pages and not the content of pages or usage information we haveimplemented both algorithms and measured their runtime performance to evaluate the e ectiveness of our algorithms we performed a user study comparing our algorithms with netscape s what s related service 12 our study showed that the precision at 10 for our two algorithms are 73 better and 51 better than that of netscape despite the fact that netscape uses both content and usage pattern information in addition to connectivity information a multi agent system for emergent process management a multi agent system manages emergent business processes the agents in this system all have the same generic architecture the generic agent architecture is a three layer bdi hybrid multi agent architecture the architecture copes with plans whose goals develop and mutate the agents in the system choose their course of action on the basis of estimates of the likelihood of a choice leading to success and on estimates of the time cost and value of making a choice 1 introduction emergent processes are business processes they are distinct from production workflows 1 emergent processes are opportunistic in nature whereas production workflows are routine emergent processes are inherently distributed and involve asynchronous parallel work what amounts to a satisfactory conclusion of an emergent process is not generally known until the process is well advanced further the tasks involved in an emergent process are typically not predefined and emerge as the process deve verbal and nonverbal discourse planning this paper we first describe our enriched discourse generator explaining the 2 sets of rules trigger and regulation we have added we also review the di erent types of gaze communicative acts finally we present the variables defining the context and how they modify the computation of the display of the communicative acts extending a multi agent system for genomic annotation the explosive growth in genomic and soon expression and proteomic data exemplified by the human genome project is a fertile domain for the application of multi agent information gathering technologies furthermore hundreds of smaller profile yet still economically important organisms are being studied that require the efficient and inexpensive automated analysis tools that multiagent approaches can provide in this paper we give a progress report on the use of the decaf multi agent toolkit to build reusable information gathering systems for bioinformatics we will briefly summarize why bioinformatics is a classic application for information gathering how decaf supports it and recent extensions underway to support new analysis paths for genomic information 1 a multi agent system for automated genomic annotation massive amounts of raw data are currently being generated by biologists while sequencing organisms outside of the largest high profile projects such as the human genome project most of this raw data must be analyzed through the piecemeal application of various computer programs and searches of various public web databases due to the inexperience and lack of training both the raw data and any valuable derived knowledge will remain generally unavailable except in published textual forms multi agent information gathering systems have a lot to contribute to these efforts even at the current state of the art we have used decaf a multi agent system toolkit based on retsina and taems to construct a prototype multi agent system for automated annotation and database storage of sequencing data for herpes viruses the resulting system eliminates tedious and always out of date hand analyses makes the data and annotations available for other researchers or agent systems and provides a level of query processing beyond even some high profile web sites coordination assistance for mixed human and computational agent systems in many application areas such as concurrent engineering software development hospital scheduling manufacturing scheduling and military planning individuals are responsible for an agenda of tasks and face choices about the best way to locally handle each task in what order to do tasks and when to do them such decisions are often hard to make because of coordination problems individual tasks are related to the tasks of others in complex ways and there are many sources of uncertainty no one has a complete view of the task structure at arbitrary levels of detail the situation may be changing dynamically and no one is entirely sure of the outcomes of all of their actions the focus of this paper is the development of support tools for distributed cooperative work by groups collaborative teams of human and computational agents we will discuss the design of a set of distributed autonomous computer programs agents that assist people in coordinating their activities by environment centered analysis and design of coordination mechanisms environment centered analysis and design of coordination mechanisms may 1995 keith s decker b s carnegie mellon university m s rensselaer polytechnic institute ph d university of massachusetts amherst directed by professor victor r lesser committee professor paul r cohen professor john a stankovic professor douglas l anderton coordination as the act of managing interdependencies between activities is one of the central research issues in distributed artificial intelligence many researchers have shown that there is no single best organization or coordination mechanism for all environments problems in coordinating the activities of distributed intelligent agents appear in many domains the control of distributed sensor networks multi agent scheduling of people and or machines distributed diagnosis of errors in local area or telephone networks concurrent engineering software agents for information gathering the design of coordination mechanisms for groups of compu coordinating human and computer agents in many application areas individuals are responsible for an agenda of tasks and face choices about the best way to locally handle each task in what order to do tasks and when to do them such decisions are often hard to make because of coordination problems individual tasks are related to the tasks of others in complex ways and there are many sources of uncertainty no one has a complete view of the task structure at arbitrary levels of detail the situation may be changing dynamically and no one is entirely sure of the outcomes of all of their actions the focus of this paper is the development of support tools for distributed cooperative work by groups collaborative teams of human and computational agents we will discuss the design of a set of distributed autonomous computer programs agents that assist people in coordinating their activities by helping them to manage their agendas we describe several ongoing implementations of these ideas including 1 simulated agen a database approach for modeling and querying video data indexing video data is essential for providing content based access in this paper we consider how database technology can offer an integrated framework for modeling and querying video data as many concerns in video e g modeling and querying are also found in databases databases provide an interesting angle to attack many of the problems from a video applications perspective database systems provide a nice basis for future video systems more generally database research will provide solutions to many video issues even if these are partial or fragmented from a database perspective video applications provide beautiful challenges next generation database systems will need to provide support for multimedia data e g image video audio these data types require new techniques for their management i e storing modeling querying etc hence new solutions are significant this paper develops a data model and a rule based query language for video content based indexing a totally ordered broadcast and multicast algorithms a comprehensive survey total order multicast algorithms constitute an important class of problems in distributed systems especially in the context of fault tolerance in short the problem of total order multicast consists in sending messages to a set of processes in such a way that all messages are delivered by all correct destinations in the same order however the huge amount of literature on the subject and the plethora of solutions proposed so far make it difficult for practitioners to select a solution adapted to their specific problem as a result naive solutions are often used while better solutions are ignored this paper proposes a classification of total order multicast algorithms based on the ordering mechanism of the algorithms and describes a set of common characteristics e g assumptions properties with which to evaluate them in this classification more than fifty total order broadcast and multicast algorithms are surveyed the presentation includes asynchronous algorithms optimizing neural networks for time series prediction in this paper we investigate the effective design of an appropriate neural network model for time series prediction based on an evolutionary approach in particular the breeder genetic algorithms are considered to face contemporaneously the optimization of i the design of a neural network architecture and ii the choice of the best learning method the effectiveness of the approach proposed is evaluated on a standard benchmark for prediction models the mackey glass series 1 introduction the main motivation for time series research is to provide a prediction when a mathematical model of a phenomenon is either unknown or incomplete a time series consists of measurements or observations of the previous outcomes of a phenomenon that are made sequentially over time if these consecutive observations are dependent on each other then it is possible to attempt a prediction clearly it is supposed that the process is somehow predictable the time series prediction problems are usually simulating the evolution of 2d pattern recognition on the cam brain machine an evolvable hardware tool for building a 75 million neuron artificial brain this paper presents some simulation results of the evolution of 2d visual pattern recognizers to be implemented very shortly on real hardware namely the cam brain machine cbm an fpga based piece of evolvable hardware which implements a genetic algorithm ga to evolve a 3d cellular automata ca based neural network circuit module of approximately 1 000 neurons in about a second i e a complete run of a ga with 10 000s of circuit growths and performance evaluations up to 65 000 of these modules each of which is evolved with a humanly specified function can be downloaded into a large ram space and interconnected according to humanly specified artificial brain architectures this ram containing an artificial brain with up to 75 million neurons is then updated by the cbm at a rate of 130 billion ca cells per second such speeds will enable real time control of robots and hopefully the birth of a new research field that we call brain building the first such artif building an artificial brain using an fpga based cam brain machine this paper reports on recent progress made in atr s attempt to build a 10 000 evolved neural net module artificial brain to control the behaviors of a life sized robot kitten 1 introduction this paper presents progress in atr s artificial brain cam brain project the broad aim of atr s artificial brain project is to build grow evolve an artificial brain containing a billion artificial neurons by the year 2001 the basic ideas of the cam brain project are as follows use cellular automata ca as the foundation upon which to grow and evolve neural network circuits with user defined functionality the state of each cellular automata cell can be stored in one or two bytes of ram since nowadays it is possible to have a gigabyte of ram in one s workstation there is a huge space in which to store the ca cell states more than enough to contain an artificial brain the next consideration in the cam brain project was how to evolve these neural networks quickly enough for brain buildin evolving an optimal de convolution function for the neural net modules of atr s artificial brain project this paper reports on efforts to evolve an optimum de convo lution function to be used to convert analog to binary signals spike trains and vice versa for the binary input output signals of the neural net circuit modules evolved at electronic speeds by the so called cam brain machine cbm of atr s artificial brain project 1 2 3 the cbm is an fpga based piece of hardware which will be used to evolve tens of thousands of cellular automata based neural network circuits or modules at electronic speeds in about a second each which are then downloaded into humanly architected artificial brains in a large ram space 2 3 since state of the art programmable fpgas constrained us to use 1 bit binary signaling in our neural model the codi 1bit model 4 an efficient de convolution technique is needed to convert digital signals to analog and vice versa so that evolutionary engineers ees who evolve the many modules can think it terms of analog signals when they need to rath latency dependent fitness in evolutionary multithreaded web agents the world wide web creates opportunities for search systems using adaptive distributed agents this paper presents a threaded implementation of infospiders a client based system that uses an evolving population of intelligent agents to browse the web at query time we consider different fitness functions based on network resource consumption and show that taxing agents in proportion to latency results in better efficiency without penalties in the quality of the retrieved documents the tool is available to the public as a java applet an incremental interpreter for high level programs with sensing like classical planning the execution of high level agent programs requires a reasoner to look all the way to a final goal state before even a single action can be taken in the world this deferral is a serious problem in practice for large programs furthermore the problem is compounded in the presence of sensing actions which provide necessary information but only after they are executed in the world to deal with this we propose characterize formally in the situation calculus and implement in prolog a new incremental way of interpreting such high level programs and a new high level language construct which together and without loss of generality allow much more control to be exercised over when actions can be executed we argue that such a scheme is the only practical way to deal with large agent programs containing both nondeterminism and sensing introduction in de giacomo lesperance levesque 1997 it was argued that when it comes to providing high level control to multi layer methods and the optimal optimizer multi layer methods are methods that act on several layers simultaneously examples of multi layer methods are found in multi agent systems global and per agent behavior in learning e g boosting bias tuning in self adaptive methods such as evolution strategies in hybrid approaches and in optimization e g multiple runs result pooling we give a formal definition of what a multi layer method is we discuss the relationship with the no free lunch theorem to show that such a thing as the optimal optimizer exists and how multi layer methods can be used to approximate it probabilistic temporal databases i algebra dyreson and snodgrass have drawn attention to the fact that in many temporal database applications there is often uncertainty present about the start time of events the end time of events the duration of events etc when the granularity of time is small e g milliseconds a statement such as packet p was shipped sometime during the first 5 days of january 1998 leads to a massive amount of uncertainty 5 theta 24 theta 60 theta 60 theta 1000 possibilities as noted in 53 past attempts to deal with uncertainty in databases have been restricted to relatively small amounts of uncertainty in attributes dyreson and snodgrass have taken an important first step towards solving this problem in this paper we first introduce the syntax of temporal probabilistic tp relations and then show how they can be converted to an explicit significantly more space consuming form called annotated relations we then present a theoretical annotated temporal algebra tata being e a multiagent architecture for fuzzy modeling in this paper a hybrid learning system that combines different fuzzy modeling techniques is being investigated in order to implement the different methods we propose the use of intelligent agents which collaborate by means of a multiagent architecture dlove using constraints to allow parallel processing in multi user virtual reality in this paper we introduce dlove a new paradigm for designing and implementing distributed and nondistributed virtual reality applications using one way constraints dlove allows programs written in its framework to be executed on multiple computers for improved performance it also allows easy specification and implementation of multi user interfaces dlove hides all the networking aspects of message passing among the machines in the distributed environment and performs the needed network optimizations as a result a user of dlove does not need to understand parallel and distributed programming to use the system he or she needs only be able to use the serial version of the user interface description language parallelizing the computation is performed by dlove without modifying the interface description internet agents for telemedicine services telemedicine can be viewed as the telematic support for collaboration among distant medical professionals which cooperate on shared resources of various kind in light of this attention to telematics and informatics concepts particularly oriented towards collaboration should be paid in particular the recently appeared agent paradigm seems suitable for the analysis design and development of telemedicine services because of its committment to intercommunication and sharing of resources the present paper is aimed at introducing the agent paradigm from the theoretical basis up to the technological issues and at describing an agent based approach to telemedicine specifically applied to telepathology applications the described system is based on an agent based tool james namely an agent model and template implemented using java which has been used to implement a prototype multipurpose telepathology application based on a federated agency architecture analysis and design using mase and agenttool this paper provides an overview of the work being done at the air force institute of technology on the multiagent systems engineering methodology and the associated agenttool environment our research is focused on discovering methods and techniques for engineering practical multiagent systems it uses the abstraction provided by multiagent systems for developing intelligent distributed software systems specifying agent behavior as concurrent tasks approved for public release distribution unlimited software agents are currently the subject of much research in many interrelated fields while much of the agent community has concentrated on building exemplar agent systems defining theories of agent behavior and inter agent communications there has been less emphasis on defining the techniques required to build practical agent systems while many agent researchers refer to tasks performed by roles within a multiagent system few really define the what they mean by tasks we believe that the definition of tasks is critical in order to completely define what an agent within a multiagent system tasks not only define the types of internal processing an agent must do but also how interactions with other agents relate to those internal processes in this report we define concurrent tasks which specify a single thread of control that defines a task that the agent can perform and integrates inter agent as well as intra agent interactions we typically think of concurrent tasks as defining how a role decides what actions to take not necessarily what the agent does this is an important distinction when talking about agents since hard coding specific behavior may not be the ideal case often agents incorporate the concept of plans and planning to analysis and design of multiagent systems using hybrid coordination media over the last few years two advances in agent oriented software engineering have had a significant impact the first is the identification of interaction and coordination as the central focus of multiagent systems design and the second is the realization that the multiagent organization is distinct from the individual agents that populate the system also the evolution of new more powerful hybrid coordination models which combine data centered and control centered coordination approaches have given us the capability to model and implement the rules that govern organizations independently from the individual agents in the system this paper investigates how to combine the power of these hybrid coordination capabilities with the concept of organizational rules using traditional conversation based approaches to designing multiagent systems 1 multiagent systems engineering a methodology and language for designing agent systems this paper overviews mase and provides a high level introduction to one critical component used within mase the agent modeling language details on the agent definition language and detailed agent design are left for a future paper programmable agents for flexible qos management in ip networks network programmability seems to be a promising solution to network management and quality of service qos control software mobile agents technology is boosting the evolution toward application level control of network functionalities code may be deployed in the network dynamically and on demand for the benefit of applications or application classes agents support a dynamic distribution of control and management functions across networks thus increasing flexibility and efficiency we propose to use mobile agent technology to overcome some of the problems inherent in current internet technology we focus our attention to qos monitoring being locally significant in network subdomains and realize a qos management strategy in response to variations of user customer of application requirements and of the network state we describe our experience and the results obtained from our testbed where software agents are instantiated executed migrated and suspended in order to implement flexible qos management in ip networks augmented workspace the world as your desktop we live in a three dimensional world and much of what we do and how we interact in the physical world has a strong spatial component unfortunately most of our interaction with the virtual world is two dimensional we are exploring the extension of the 2d desktop workspace into the 3d physical world using a stereoscopic see through head mounted display we have built a prototype that enables us to overlay virtual windows on the physical world this paper describes the augmented workspace which allows a user to pos ition windows in a 3d work area keywords ubiquitous computing cooperative buildings human computer interaction physical space context awareness visualization 1 introduction in our daily lives much of what we do and how we interact has a strong spatial component your calendar is on a wall or on a certain part of your desk and sticky notes are placed on walls and whiteboards yet as an increasing portion of our work is done on computers a large m an annotation tool for web browsers and its applications to information retrieval with bookmark programs current web browsers provide a limited support to personalize the web we present a new web annotation tool which uses the document object model level 2 and dynamic html to deliver a system where speed and privacy are important issues we report on several experiments showing how annotations improve document access and retrieval by providing user directed document summaries preliminary results also show that annotations can be used to produce user directed document clustering and classification introduction current web browsers provide a limited support to personalize the web namely bookmarks in two recent surveys users report that the use of bookmarks is among the three main problems they have when using the internet abrams 1998 cockburn 1999 we see several problems when using bookmarks the number of bookmarks grows linearly with time abrams 1998 and it becomes a real challenge for many users to organize this amount of data by storing the the luce coordination technology for mas design and development on the internet internet based multi agent systems call for new metaphors abstractions methodologies and enabling techologies specifically tailored to agent oriented engineering while coordination models define the framework to manage the space of agent interaction ruling social behaviours and accomplishing social tasks their impact on system design and development calls for an effective coordination technology this paper presents luce a coordination technology that integrates java prolog and the notion of logic tuple centre a programmable coordination medium into a coherent framework the power of the luce coordination technology is first discussed in general then shown in the context of a simple yet significant system a tictactoe game among intelligent software agents and human players on the internet from tuple spaces to tuple centres a tuple centre is a linda like tuple space whose behaviour can be programmed by means of transactional reactions to the standard communication events this paper defines the notion of tuple centre and shows the impact of its adoption as a coordination medium for a distributed multi agent system on both the system design and the overall system efficiency three companions for data mining in first order logic three companion systems claudien icl and tilde are presented they use a common representation for examples and hypotheses each example is represented by a relational database this contrasts with the classical inductive logic programming systems such as progol and foil it is argued that this representation is closer to attribute value learning and hence more natural furthermore the three systems can be considered first order upgrades of typical data mining systems which induce association rules classification rules or decision trees respectively 1 three companions for first order data mining three companion systems claudien icl and tilde are presented they use a common representation for examples and hypotheses each example is represented by a relational database this contrasts with the classical inductive logic programming systems such as progol and foil it is argued that this representation is closer to attribute value learning and hence more natural furthermore the three systems can be considered first order upgrades of typical data mining systems which induce association rules classification rules or decision trees respectively 1 introduction typical data mining algorithms employ a limited attribute value representation where each example consists of a single tuple in a relational database this representation is inadequate for problem domains that require reasoning about the structure of the domain such as e g in bio chemistry 7 natural language processing 14 this paper presents three companion systems where each example corresponds to a compressive computation in analog vlsi motion sensors we introduce several different focal plane analog vlsi motion sensors developed in the past we show how their pixel parallel architecture can be used to extract low dimensional information from a higher dimensional data set as an example we present an algorithm and corresponding experiments to compute the focus of expansion focus of contraction and the axis of rotation from natural visual input a fully integrated system for real time computation of these quantities is proposed as well in computer simulations it is shown that the direction of motion vector field is best suited to perform the algorithm even at high noise levels 1 analog vlsi motion sensors in the past the computer vision communityhas invested much effort into developing motion detection algorithms for a critical review see bfb94 implementing these algorithms in real time systems proved challenging for computational reasons additionally it has been realized that a motion vector field is useful mainly as star chime customizable hyperlink insertion and maintenance engine for software engineering environments source code browsing is an important part of program comprehension browsers expose semantic and syntactic relationships such as between object references and definitions in gui accessible forms these relationships are derived using tools which perform static analysis on the original software documents implementing such browsers is tricky program comprehension strategies vary and it is necessary to provide the right browsing support analysis tools to derive the relevant crossreference relationships are often difficult to build tools to browse distributed documents require extensive coding for the gui as well as for data communications therefore there are powerful motivations for using existing static analysis tools in conjunction with www technology to implement browsers for distributed software projects the chime framework provides a flexible customizable platform for inserting html links into software documents using information generated by existing software analysis tools using the chime specification language and a simple retargetable database interface it is possible to quickly incorporate a range of different link insertion tools for software documents into an existing legacy software development environment this enables tool builders to offer customized browsing support with a well known gui this paper describes the chime architecture and describes our experience with several re targeting efforts of this system 1 needles in a haystack plan recognition in large spatial domains involving multiple agents while plan recognition research has been applied to a wide variety of problems it has largely made identical assumptions about the number of agents participating in the plan the observability of the plan execution process and the scale of the domain we describe a method for plan recognition in a real world domain involving large numbers of agents performing spatial maneuvers in concert under conditions of limited observability these assumptions differ radically from those traditionally made in plan recognition and produce a problem which combines aspects of the fields of plan recognition pattern recognition and object tracking we describe our initial solution which borrows and builds upon research from each of these areas employing a pattern directed approach to recognize individual movements and generalizing these to produce inferences of large scale behavior introduction plan recognition the problem of inferring goals intentions or future actions given observations of knowledge modeling state of the art a major characteristic of developments in the broad field of artificial intelligence ai during the 1990s has been an increasing integration of ai with other disciplines a number of other computer science fields and technologies have been used in developing intelligent systems starting from traditional information systems and databases to modern distributed systems and the internet this paper surveys knowledge modeling techniques that have received most attention in recent years among developers of intelligent systems ai practitioners and researchers the techniques are described from two perspectives theoretical and practical hence the first part of the paper presents major theoretical and architectural concepts design approaches and research issues the second part discusses several practical systems applications and ongoing projects that use and implement the techniques described in the first part finally the paper briefly covers some of the most recent results in the fields of intelligent manufacturing systems intelligent tutoring systems and ontologies 2 1 a plan fusion algorithm for multi agent systems we introduce an algorithm for cooperative planning in multi agent systems the algorithm enables the agents to combine fuse their plans in order to increase their joint profits a computational resources and skills framework is developed for representing the planned activities of an agent under time constraints using this resource skill framework we present an ecient polynomial time algorithm that fuses the plans of a group of agents in such a way that their joint profits improve the framework and the algorithm are illustrated using a simplified example from the freight transport domain 1 introduction recently much attention has been given to the topic of cooperation and cooperative planning in multiagent systems usually the starting point for research on this problem is the observation that there exist classes of problems that cannot be solved by a single agent in isolation but require several agents to work together in an interactive way coordinating their plans and sharing knowledge discovery from client server databases the subject of this paper is the implementation of knowledge discovery in databases specifically we assess the requirements for interfacing tools to client server database systems in view of the architecture of those systems and of knowledge discovery processes we introduce the concept of a query frontier of an exploratory process and propose a strategy based on optimizing the current query frontier rather than individual knowledge discovery algorithms this approach has the advantage of enhanced genericity and interoperability we demonstrate a small set of query primitives and show how one example tool the well known decision tree induction algorithm c4 5 can be rewritten to function in this environment 1 introduction relational databases are the current dominant database technology in industry and many organizations have collected large amounts of data in so called data warehouses expressly for the purpose of decision support and data mining in general the data must distributed and disappearing user interfaces in f expression will often dominate the pure functional aspect to a user we are already witnessing this transition with mobile phones novel computational artifacts based on these technologies may become invisible or reside in the background in several different ways truly invisible when the actual computer and its interface is almost totally integrated with an environment that is familiar to the user the user interaction model is implicit and perhaps even unnoticeable to the user such a system truly resides in the background of the user s attention at all times transparent here the invisible ui is not invisible in the literal sense rather it is transparent in the same sense as a very familiar tool is transparent to its user to the point where it almost acts as an extension of one s body every distributed and disappearing user interfaces in ubiquitous computing keywords context awareness disappearing user interfaces the conference assistant combining context awareness with wearable computing we describe the conference assistant a prototype mobile context aware application that assists conference attendees we discuss the strong relationship between context awareness and wearable computing and apply this relationship in the conference assistant the application uses a wide variety of context and enhances user interactions with both the environment and other users we describe how the application is used and the context aware architecture on which it is based 1 introduction in human human interaction a great deal of information is conveyed without explicit communication but rather by using cues these shared cues or context help to facilitate grounding between participants in an interaction 3 we define context to be any information that can be used to characterize the situation of an entity where an entity can be a person place or physical or computational object in human computer interaction there is very little shared context between the human and the co co clustering documents and words using bipartite spectral graph partitioning both document clustering and word clustering are important and well studied problems by using the vector space model a document collection may be represented as a word document matrix in this paper we present the novel idea of modeling the document collection as a bipartite graph between documents and words using this model we pose the clustering probliem as a graph partitioning problem and give a new spectral algorithm that simultaneously yields a clustering of documents and words this co clustrering algorithm uses the second left and right singular vectors of an appropriately scaled word document matrix to yield good bipartitionings in fact it can be shown that these singular vectors give a real relaxation to the optimal solution of the graph bipartitioning problem we present several experimental results to verify that the resulting co clustering algoirhm works well in practice and is robust in the presence of noise concept decompositions for large sparse text data using clustering abstract unlabeled document collections are becoming increasingly common and available mining such data sets represents a major contemporary challenge using words as features text documents are often represented as high dimensional and sparse vectors a few thousand dimensions and a sparsity of 95 to 99 is typical in this paper we study a certain spherical k means algorithm for clustering such document vectors the algorithm outputs k disjoint clusters each with a concept vector that is the centroid of the cluster normalized to have unit euclidean norm as our first contribution we empirically demonstrate that owing to the high dimensionality and sparsity of the text data the clusters produced by the algorithm have a certain fractal like and self similar behavior as our second contribution we introduce concept decompositions to approximate the matrix of document vectors these decompositions are obtained by taking the least squares approximation onto the linear subspace spanned by all the concept vectors we empirically establish that the approximation errors of the concept decompositions are close to the best possible namely to truncated singular value decompositions as our third contribution we show that the concept vectors are localized in the word space are sparse and tend towards orthonormality in contrast the singular vectors are global in the word space and are dense nonetheless we observe the surprising fact that the linear subspaces spanned by the concept vectors and the leading singular vectors are quite close in the sense of small principal angles between them in conclusion the concept vectors produced by the spherical k means component based algebraic specification and verification in cafeobj we present a formal method for component based system specification and verification which is based on the new algebraic specification language cafeobj which is a modern successor of obj incorporating several new developments in algebraic specification theory and practice we first give an overview of the main features of cafeobj including its logical foundations and then we focus on the behavioural specification paradigm in cafeobj surveying the object oriented cafeobj specification and verification methodology based on behavioural abstraction the last part of this paper further focuses on a component based behavioural specification and verification methodology which features high reusability of both specification code and verification proof scores this methodology constitutes the basis for an industrial strength formal method around cafeobj 1 introduction cafeobj whose definition is given by 7 is a modern successor of the obj language incorporating several ne a comparative study of classification based personal e mail filtering this paper addresses personal e mail filtering by casting it in the framework of text classification modeled as semi structured documents email messages consist of a set of fields with predefined semantics and a number of variable length free text fields while most work on classification either concentrates on structured data or free text the work in this paper deals with both of them to perform classification a naive bayesian classifier was designed and implemented and a decision tree based classifier was implemented the design considerations and implementation issues are discussed using a relatively large amount of real personal e mail data a comprehensive comparative study was conducted using the two classifiers the importance of different features is reported results of other issues related to building an effective personal e mail classifier are presented and discussed it is shown that both classifiers can perform filtering with reasonable accuracy while the decision tree based classifier outperforms the bayesian classifier when features and training size are selected optimally for both a carefully designed naive bayesian classifier is more robust 1 toward learning based web query processing in this paper we describe a novel web query processing approach with learning capabilities under this approach user queries are in the form of keywords and search engines are employed to find urls of web sites that might contain the required information the first few urls are presented to the user for browsing meanwhile the query processor learns both the information required by the user and the way that the user navigates through hyperlinks to locate such information with the learned knowledge it processes the rest urls and produces precise query results in the form of segments of web pages without user involvement the preliminary experimental results indicate that the approach can process a range of web queries with satisfactory performance the architecture of such a query processor techniques of modeling html pages and knowledge for query processing are discussed experiments on the effectiveness of the approach the required knowledge and the training strategies are presented antnet distributed stigmergetic control for communications networks this paper introduces antnet a novel approach to the adaptive learning of routing tables in communications networks antnet is a distributed mobile agents based monte carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems antnet s agents concurrently explore the network and exchange collected information the communication among the agents is indirect and asynchronous mediated by the network itself this form of communication is typical of social insects and is called stigmergy we compare our algorithm with six state of the art routing algorithms coming from the telecommunications and machine learning elds the algorithms performance is evaluated over a set of realistic testbeds we run many experiments over real and arti cial ip datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal tra c distributions results are very encouraging antnet showed superior performance under all the experimental conditions with respect to its competitors we analyze the main characteristics of the algorithm and try to explain the reasons for its superiority 1 mocsyn multiobjective core based single chip system synthesis in this paper we present a system synthesis algorithm called mocsyn which partitions and schedules embedded system specifications to intellectual property cores in an integrated circuit given a system specification consisting of multiple periodic task graphs as well as a database of core and integrated circuit characteristics mocsyn synthesizes real time heterogeneous single chip hardware software architectures using an adaptive multiobjective genetic algorithm that is designed to escape local minima the use of multiobjective optimization allows a single system synthesis run to produce multiple designs which trade off different architectural features integrated circuit price power consumption and area are optimized under hard real time constraints mocsyn differs from previous work by considering problems unique to single chip systems it solves the problem of providing clock signals to cores composing a system on a chip it produces a bus structure which balances ease of layo vrml with constraints in this paper we discuss the benefits of extending vrml by constraints and present a new way based on prototypes and scripting to implement this extension our approach is easy to use extensible and it considerably increases the expressivity of vrml our implementation supports one way equational and finite domain constraints we demonstrate the use of these constraints by means of several examples finally we argue that in the long run constraints should become an integral part of vrml cr categories i 3 7 computer graphics three dimensional graphics and realism virtual reality d 3 3 programming languages language constructs and features constraints keywords vrml animation programming 1 approximate statistical tests for comparing supervised classification learning algorithms this paper reviews five approximate statistical tests for determining whether one learning algorithm out performs another on a particular learning task these tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists type i error two widely used statistical tests are shown to have high probability of type i error in certain situations and should never be used these tests are a a test for the difference of two proportions and b a paired differences t test based on taking several random train test splits a third test a paired differences t test based on 10 fold cross validation exhibits somewhat elevated probability of type i error a fourth test mcnemar s test is shown to have low type i error the fifth test is a new test 5x2cv based on 5 iterations of 2 fold cross validation experiments show that this test also has acceptable type i error the paper also measures the power ability to detect algorit towards socially sophisticated bdi agents we present an approach to social reasoning that integrates prior work on norms and obligations with the bdi approach to agent architectures norms and obligations can be used to increase the eficiency of agent reasoning and their explicit representation supports reasoning about a wide range of behaviour types in a single framework we propose a modified bdi interpreter loop that takes norms and obligations into account in an agent s deliberation web genre visualization web users vary widely in terms of their expertise on the topics for which they search the amount of detail they seek etc unfortunately today s one size fits all web search services do not cater to such individual preferences for example it is difficult to query for documents that give extensive detail but assume modest prior expertise we describe how shallow text classification techniques can be used to sort the documents returned by web search services according to genre dimensions such as level of expertise and amount of detail and propose a simple visualization interface that helps users rapidly find appropriate documents keywords document genre information retrieval visualisation text classification shallow linguistic processing motivation consider two users seeking information about pearson correlation coefficient alice is writing a data analysis program and needs a web page to remind her of the equations bob a teacher wants to point his pupils to an overview that isn t bogged down in equations a probabilistic model for dimensionality reduction in information retrieval and filtering dimension reduction methods such as latent semantic indexing lsi when applied to semantic spaces built upon text collections improve information retrieval information filtering and word sense disambiguation a new dual probability model based on similarity concepts is introduced to explain the observed success semantic associations can be quantitatively characterized by their statistical significance the likelihood semantic dimensions containing redundant and noisy information can be separated out and should be ignored because their contribution to the overall statistical significance is negative giving rise to lsi lsi is the optimal solution of the model the peak in likelihood curve indicates the existence of an intrinsic semantic dimension the importance of lsi dimensions follows the zipf distribution indicating that lsi dimensions represent latent concepts document frequency of words follow the zipf distribution and the number of distinct words follows log normal distribution experiments on four standard document collections both confirm and illustrate the results and concepts presented here active rule analysis and optimisation in the rock roll deductive object oriented database active database systems provide facilities that monitor and respond to changes of relevance to the database active behaviour is normally described using event condition action rules eca rules and a number of systems have been developed based upon different data models that support such rules however experience using active databases shows that while such systems are powerful and potentially useful in many applications they are hard to program and liable to exhibit poor performance at runtime this document addresses both of these issues by examining both analysis and optimisation of active rules in the context of a powerful active database system it is shown how rule analysis methods developed for straightforward active rule languages for relational data models can be extended to take account of rich event description languages and more powerful execution models it is also shown how the results of analyses can be exploited by rule optimisers and that multiple quer a formal specification of dmars the procedural reasoning system prs is the best established agent architecture currently available it has been deployed in many major industrial applications ranging from fault diagnosis on the space shuttle to air traffic management and business process control the theory of prs like systems has also been widely studied within the intelligent agents research community the beliefdesire intention bdi model of practical reasoning that underpins prs is arguably the dominant force in the theoretical foundations of rational agency despite the interest in prs and bdi agents no complete attempt has yet been made to precisely specify the behaviour of real prs systems this has led to the development of a range of systems that claim to conform to the prs model but which differ from it in many important respects our aim in this paper is to rectify this omission we provide an abstract formal model of an idealised dmars system the most recent implementation of the prs using html formatting to aid in natural language processing on the world wide web because of its magnitude and the fact that it is not computer understandable the world wide web has become a prime candidate for automatic natural language tasks this thesis argues that there is information in the layout of a web page and that by looking at the html formatting in addition to the text on a page one can improve performance in tasks such as learning to classify segments of documents a rich representation for web pages the html struct tree is described a parsing algorithm for creating struct trees is presented as well as a set of experiments that use struct trees as a feature set for learning to extract a company s name and location from its web pages through these experiments we found that it is useful to consider the layout of a page for these tasks contents 1 acknowledgements 2 2 motivations 3 3 related work 3 4 html struct trees 4 4 1 the representation 4 4 2 the parsing algorithm experiences in the use of mobile agents for developing distributed applications introduction the recent development of telecommunication networks has contributed to the success of distributed systems and has stimulated the research for a new generation of applications such as the access to remote databases the web and e commerce traditional programming in distributed systems has been based on the well known client server paradigm an alternative to such traditional mechanisms has recently been spreading and is based on the use of environments that give a sort of code mobility 2 by this term we mean the possibility to change dynamically at run time the binding between the software components of an application and their physical location within a network of computers code mobility is the main feature on which mobile agent systems are based a mobile agent can be deemed as a software module able to autonomously perform the task assigned by the user by moving if necessary from a node of the network to the other in order to collect the informa coordinating mobile agents by means of communicators this paper proposes a coordination model for both static and mobile agents based on abstract structures called communicators entities which handle agent dialogue performed through acl speech act exchanging such structures are designed based on a need to model agent dialogue in a human like style offering a set of coordination primitives of general validity able to provide both a direct and indirect interaction model since a communicator handles messages exchanged within a well defined multi agent application it is fully programmable i e it is possible to specify what messages can be exchanged and how these message have to be handled in detail a communicator performs a syntactic and semantic routing allowing the exchange and forwarding of a message according to the programmed coordination laws impact a platform for heterogenous agents this report as well as the allocated flight route of the plane 3 1 action base 131 chapter 3 actions and agent programs multi agent systems ushuaia oct 2000 example 3 3 store example revisited multi agenten systeme coalition formation 4 4 payoff division overview 98 4 contract nets coalition formation 98 1 chapter 4 contract nets coalition formation multi agenten systeme vu ss 00 4 1 general contract nets how to distribute tasks global market mechanisms implementations use a single centralized mediator announce bid award cycle distributed negotiation we need the following 1 define a task allocation problem in precise terms 2 define a formal model for making bidding and awarding decisions 4 1 general contract nets 99 chapter 4 contract nets coalition formation multi agenten systeme vu ss 00 definition 4 1 task allocation problem a task allocation problem is given by 1 a set of tasks t 2 a set of agents a a a 3 a cost function cost i i i 2 t r stating the costs that agent i i i incurs by handling some tasks and 4 the initial allocation of tasks t init 1 1 1 t init a a a where t i i i a a a t init i i meta agent programs there are numerous applications where an agent a needs to reason about the beliefs of another agent as well as about the actions that other agents may take in 21 the concept of an agent program is introduced and a language within which the operating principles of an agent can be declaratively encoded on top of imperative data structures is dened in this paper we rst introduce certain belief data structures that an agent needs to maintain then we introduce the concept of a meta agent program map that extends the framework of 21 19 so as to allow agents to perform metareasoning we build a formal semantics for maps and show how this semantics supports not just beliefs agent a may have about agent b s state but also beliefs about agents b s beliefs about agent c s actions beliefs about b s beliefs about agent c s state and so on finally we provide a translation that takes any map as input and converts it into an agent program such that there is a one one correspondence between the semantics of the map and the semantics of the resulting agent program this correspondence allows an implementation of maps to be built on top of an implementation of agent programs nonmonotonic reasoning towards efficient calculi and implementations places to stay on the move software architectures for mobile user interfaces architectural design has an important effect on usability most notably on temporal properties this paper investigates software architecture options for mobile user interfaces in particular those for collaborative systems one of the new features of mobile systems as compared with fixed networks is the connection point to the physical network the point of presence pop which forms an additional location for code and data this allows architectures that bring computation closer to the users hence reducing feedback and feedthrough delays a consequence of using pops is that code and data have to be mobile within the network leading to potential security problems keywords mobile computing collaborative work cscw software architecture clientserver introduction at first sight it seems that software architectures are about the internals of system design and not a necessary concern for the user interface however the merging of computing and communication systems and the maturin resolution based proof for multi modal temporal logics of knowledge temporal logics of knowledge are useful in order to specify complex systems in which agents are both dynamic and have information about their surroundings we present a resolution method for propositional temporal logic combined with multi modal s5 and illustrate its use on examples this paper corrects a previous proposal for resolution in multi modal temporal logics of knowledge keywords temporal and modal logics non classical resolution theorem proving 1 introduction combinations of logics have been useful for specifying and reasoning about complex situations for example multi agent systems 21 24 accident analysis 15 and security protocols 18 for example logics to formalise multi agent systems often incorporate a dynamic component representing change of over time an informational component to capture the agent s knowledge or beliefs and a motivational component for notions such as goals wishes desires or intentions often temporal or dynamic logic is used for background readings for collection synthesis efficiently ordering query plans for data integration interface to a multitude of data sources given a user query formulated in this interface the system translates it into a set of query plans each plan is a query formulated over the data sources and specifies a way to access sources and combine data to answer the user query benchmarking optimization software with cops 1 introduction 1 testing methods 2 1 largest small polygon 3 2 distribution of electrons on a sphere 5 3 hanging chain 7 4 shape optimization of a cam 9 5 isometrization of ff pinene 11 6 marine population dynamics 13 7 flow in a channel 16 8 robot arm 18 9 particle steering 21 10 goddard rocket 23 11 hang glider 26 12 catalytic cracking of gas oil 29 13 methanol to hydrocarbons 31 14 catalyst mixing 33 15 elastic plastic torsion 35 16 journal bearing 37 17 minimal surface with obstacle 39 acknowledgments 41 references 41 ii benchmarking optimization software with cops by elizabeth d dolan and jorge j mor e abstract we describe version 2 0 of the cops set of nonlinearly constrained optimization problems we have added new problems as well as streamlined and improved most of the problems we also provide a comparison of the lancelot loqo minos and snopt solvers on these problems introduction the cops 5 test set provides a modest selection of difficult nonlinearly constrai mining the link structure of the world wide web the world wide web contains an enormous amount of information but it can be exceedingly difficult for users to locate resources that are both high in quality and relevant to their information needs we develop algorithms that exploit the hyperlink structure of the www for information discovery and categorization the construction of high quality resource lists and the analysis of on line hyperlinked communities 1 introduction the world wide web contains an enormous amount of information but it can be exceedingly difficult for users to locate resources that are both high in quality and relevant to their information needs there are a number of fundamental reasons for this the web is a hypertext corpus of enormous size approximately three hundred million web pages as of this writing and it continues to grow at a phenomenal rate but the variation in pages is even worse than the raw scale of the data the set of web pages taken as a whole has almost no unifying structure wi mining high speed data streams many organizations today have more than very large databases they have databases that grow without limit at a rate of several million records per day mining these continuous data streams brings unique opportunities but also new challenges this paper describes and evaluates vfdt an anytime system that builds decision trees using constant memory and constant time per example vfdt can incorporate tens of thousands of examples per second using o the shelf hardware it uses hoe ding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner we study vfdt s properties and demonstrate its utility through an extensive set of experiments on synthetic data we apply vfdt to mining the continuous stream of web access data from the whole university of washington main campus metacost a general method for making classifiers cost sensitive research in machine learning statistics and related fields has produced a wide variety of algorithms for classification however most of these algorithms assume that all errors have the same cost which is seldom the case in kdd prob lems individually making each classification learner costsensitive is laborious and often non trivial in this paper we propose a principled method for making an arbitrary classifier cost sensitive by wrapping a cost minimizing procedure around it this procedure called metacost treats the underlying classifier as a black box requiring no knowledge of its functioning or change to it unlike stratification metacost is applicable to any number of classes and to arbitrary cost matrices empirical trials on a large suite of benchmark databases show that metacost almost always produces large cost reductions compared to the cost blind classifier used c4 5rules and to two forms of stratification further tests identify the key components of metacost and those that can be varied without substantial loss experiments on a larger database indicate that metacost scales well maintaining transitive closure of graphs in sql it is common knowledge that relational calculus and even sql are not expressive enough to express recursive queries such as the transitive closure in a real database system one can overcome this problem by storing a graph together with its transitive closure and maintaining the latter whenever updates to the former occur this leads to the concept of an incremental evaluation system or ies much is already known about the theory of ies but very little has been translated into practice the purpose of this paper is to ll in this gap by providing a gentle introduction to and an overview of some recent theoretical results on ies the introduction is through the translation into sql of three interesting positive maintenance results that have practical importance the maintenance of the transitive closure of acyclic graphs of undirected graphs and of arbitrary directed graphs interestingly these examples also allow ustoshow the relationship between power and cost in the incremental maintenance of database queries 1 al log integrating datalog and description logics we presentanintegrated system for knowledge representation called al log based on description logics and the deductive database language datalog al log embodies two subsystems called structural and relational the former allows for the de nition of structural knowledge about classes of interest concepts and membership relation between objects and classes the latter allows for the de nition of relational knowledge about objects described in the structural component the interaction between the two components is obtained by allowing constraints within datalog clauses thus requiring the variables in the clauses to range over the set of instances of a speci ed concept we propose a method for query answering in al log based on constrained resolution where the usual deduction procedure de ned for datalog is integrated with a method for reasoning on the structural knowledge keywords description logics deductive databases datalog object based knowledge representation query a a novel user interface for group collaboration flexible user interfaces that can be customized to meet the needs of the task at hand are particularly important for real time group collaboration this paper presents the user interface of the disciple distributed system for collaborative information processing and learning system for synchronous groupware along with the multimodal human computer interface enhancement disciple supports sharing of javabeans compliant components 17 i e beans and applets which at runtime get imported into the shared workspace and can be interconnected into more complex components as a result importing various components allows user tailoring of the human computer interface we present a software architecture for customization of both grouplevel and application level interfaces the applicationlevel interface includes a management system for sharing multiple modalities across concurrent applications this multimodal management system is loadable on demand yet strongly embedded in the disciple f tic a toolkit for validation in formal language learning quite often heuristics and common sense suggest directions for improving well known learning algorithms however it seems not an easy task to verify that the modifications are indeed helpful this is made more complicated through various additional influences inherent in different application domains in order to obtain a faithful impression of phenomena that are intrinsic to the algorithms the role of specific domains should be minimized our validation toolkit tic allows to explore the behaviour of various algorithms for learning formal languages this is a well examined and standardized application domain tic is operated by interactive as well as automatic control motivation and introduction today a lot of different learning approaches and algorithms do exist there are classical as well as brand new approaches and all of them come in many versions and refinements on the one hand this indicates a desirable improvement of methods but on the other hand it good examples in learning containment decision lists this paper by our very specific approaches and results in a very particular setting we intend to go a small step towards a better understanding and partial answering of questions like above seeking a foundation for context aware computing context aware computing is generally associated with elements of the ubiquitous computing program and the opportunity to distribute computation and interaction through the environment rather than concentrating it at the desktop computer however issues of context have also been important in other areas of hci research i argue that the scope of context based computing should be extended to include not only ubiquitous computing but also recent trends in tangible interfaces as well as work on sociological investigations of the organization of interactive behavior by taking a view of contextaware computing that integrates these different perspectives we can begin to understand the foundational relationships that tie them all together and that provide a framework for understanding the basic principles behind these various forms of embodied interaction in particular i point to phenomenology as a basis for the development of a new framework for design and evaluation of context aware mental states recognition from communication effective and useful communication requires the agents being able to foresee the effects of their utterances over the addressee s mental state however referring to the classical speech act theory it seems to us that the idea of predicting such effects is rather optimistic since they are not really completely a priori foreseeable by the speaker along with some obvious main effects there are other side effects which might be regarded as the result of some kind of plausible inference particularly abduction performed by the hearerhimself79 the received communication and over its own actual mental state which can be differentfromen one expected by the speaker its image may be incorrect and incomplete ofthe81 24 s mental state in this paper we explore the idea that if and asfar17 it is possible 1 to formalize in a declarative manner the mental state ofan502 22165 agent 1 2 to postulate a correlation between a speaker s mental state and his uttering a certainsent updating mental states from communication in order to perform effective communication agents must be able to foresee the effects of their utterances on the addressee s mental state in this paper we investigate on the update of the mental state of an hearer agent as a consequence of the utterance performed by a speaker agent given an agent communication language with a stripslike semantics we propose a set of criteria that allow to bind the speaker s mental state to its uttering of a certain sentence on the basis of these criteria we give an abductive procedure that the hearer can adopt to partially recognize the speaker s mental state that led to a specific utterance this procedure can be adopted by the hearer to update its own mental state and its image of the speaker s mental state 1 introduction in multi agent systems communication is necessary for the agents to cooperate and coordinate their activities or simply to avoid interfering with one another if agents are not designed with embedded pre compiled an information gathering agent for querying web search engines information gathering agents have attracted much attention of late as a new application they are attractive because the need for intelligent assistance in navigating the world wide web and large databases is acute information agents provide an open ended and complex yet easily accessible environment in which ideas from many areas can be integrated we have developed an information gathering agent called savvysearch for intelligently searching multiple search engines on the web savvysearch tracks responses from existing search engines to manage resource usage and submit queries only to the most appropriate search engines to implement savvysearch we adapted simple ideas from machine learning information retrieval and planning and tested two issues in the designs can search engine selection knowledge be acquired to improve performance do users find that high quality results are being returned early within the limited parallelism provided by savvysearch current results indicate experiences with selecting search engines using metasearch search engines are among the most useful and high profile resources on the internet the problem of finding information on the internet has been replaced with the problem of knowing where search engines are what they are designed to retrieve and how to use them this paper describes and evaluates savvysearch a meta search engine designed to intelligently select and interface with multiple remote search engines the primary meta search issue examined is the importance of carefully selecting and ranking remote search engines for user queries we studied the efficacy of savvysearch s incrementally acquired meta index approach to selecting search engines by analyzing the effect of time and experience on performance we also compared the meta index approach to the simpler categorical approach and showed how much experience is required to surpass the simple scheme 1 introduction search engines are powerful tools for assisting the otherwise unmanageable task of navigating the rapidly ex speeding up relational reinforcement learning through the use of an incremental first order decision tree learner relational reinforcement learning rrl is a learning technique that combines standard reinforcement learning with inductive logic programming to enable the learning system to exploit structural knowledge about the application domain using text elements by context to display search results in information retrieval systems model and research results information retrieval systems display search results by various methods this paper focuses on a model for displaying a list of search results by means of textual elements that utilize a new information unit that replaces the currently used information unit the paper includes a short description of several studies that support the model 1 introduction because of the growth in the number and scope of global databases a special approach is required to locating information from the perspective of the user interface the internet as it exists today is an outstanding example of a broad base unfocused database most internet search engines display their information as a serially ordered list of results with a partial attempt at ranking the results in most cases this list includes the document title url and at times the first few lines of the document the information as currently displayed to the user is incomplete and insufficiently focused on the search query this requi algorithm for documents ranking drmr preliminary results in the framework of a study which investigated implementation of a model for displaying search results the possibility of ranking documents that appear in a list of search results was examined the purpose of this paper is to present the concept of using mutual references between documents as a tool for ranking documents and to present the findings of a study that investigated the applicability of the concept keywords documents ranking mutual references displaying search results list text retrieval systems 1 designing a digital library for young children an intergenerational partnership as more information resources become accessible using computers our digital interfaces to those resources need to be appropriate for all people however when it comes to digital libraries the interfaces have typically been designed for older children or adults therefore we have begun to develop a digital library interface developmentally appropriate for young children ages 5 10 years old our prototype system we now call querykids offers a graphical interface for querying browsing and reviewing search results this paper describes our motivation for the research the design partnership we established between children and adults our design process the technology outcomes of our current work and the lessons we have learned keywords children digital libraries information retrieval design techniques education applications participatory design cooperative inquiry intergenerational design team zoomable user interfaces zuis the need for research a growing body of k designing pets a personal electronic teller of stories in robots for kids morgan kaufmann san francisco ca druin a and hendler j eds page 2 who or what is pets figure 1 pets spaceship and mypets software what does pets do pets is a personal electronic teller of stories a robotic story telling environment for elementary school age children druin et al 1999a the pets kit contains a box of fuzzy stuffed animal parts and an authoring application on a personal computer figures 1 and 3 children can build a robotic animal or pet by connecting animal parts such as torso head paws ears and wings after they construct their pet they can write and tell stories using the my pets software just as the robotic animal is constructed from discrete components my pets is also constructive this application enables children to create emotions draw emotive facial expressions name their robotic companion and compile a library of stori the role of children in the design of new technology this paper suggests a framework for understanding the roles that children can play in the technology design process particularly in regards to designing technologies that support learning each role user tester informant and design partner has been defined based upon a review of the literature and my lab s own research experiences this discussion does not suggest that any one role is appropriate for all research or development needs instead by understanding this framework the reader may be able to make more informed decisions about the design processes they choose to use with children in creating new technologies this paper will present for each role a historical overview research and development methods as well as the strengths challenges and unique contributions associated with children in the design process using a case base of surfaces to speed up reinforcement learning this paper demonstrates the exploitation of certain vision processing techniques to index into a case base of surfaces the surfaces are the result of reinforcement learning and represent the optimum choice of actions to achieve some goal from anywhere in the state space this paper shows how strong features that occur in the interaction of the system with its environment can be detected early in the learning process such features allow the system to identify when an identical or very similar task has been solved previously and to retrieve the relevant surface this results in an orders of magnitude increase in learning rate 1 introduction one important research issue for case based learning is its combination with other learning methods as aamodt and plaza 1 point out generally the machine learning community aims to produce a coherent framework where each learning method fulfills a specific and distinct role in the system this paper discusses one such approach combinin categorization of software errors that led to security breaches a set of errors known to have led to security breaches in computer systems was analyzed the analysis led to a categorization of these errors after examining several proposed schemes for the categorization of software errors a new scheme was developed and used this scheme classifies errors by their cause the nature of their impact and the type of change or fix made to remove the error the errors considered in this work are found in a database maintained by the coast laboratory the categorization is the first step in the investigation of the effectiveness of various measures of code coverage in revealing software errors that might lead to security breaches 1 introduction we report the outcome of an effort to categorize errors in software that are known to have led to security breaches the set of errors used in this study came from a database of errors developed in the coast laboratory 10 several existing schemes for the categorization of software errors were evaluated for vulnerability testing of software system using fault injection we describe an approach for testing a software system for possible security flaws traditionally security testing is done using penetration analysis and formal methods based on the observation that most security flaws are triggered due to a flawed interaction with the environment we view the security testing problem as the problem of testing for the fault tolerance properties of a software system we consider each environment perturbation as a fault and the resulting security compromise a failure in the toleration of such faults our approach is based on the well known technique of fault injection environment faults are injected into the system under test and system behavior observed the failure to tolerate faults is an indicator of a potential security flaw in the system an environment application interaction eai fault model is proposed eai allows us to decide what faults to inject based on eai we present a security flaw classification scheme this scheme was used to classify 142 security flaws in a vulnerability database this classification revealed that 91 of the security flaws in the database are covered by the eai model augmented reality which augmentation for which reality in this paper we first present a brief review of approaches used for studying and designing augmented reality ar systems the variety of approaches and definitions in ar requires classification we define two intrinsic characteristics of ar systems task focus and nature of augmentation based on these two characteristics we identify four classes of ar systems in addition our op a s notation provides a complementary characterization method based on interaction using op a s an ar system is modeled as a set of components that communicate with each other one crucial type of op a s component is the adapter that establishes a bridge between the real world and the virtual world by defining a classification scheme we aim at providing a better understanding of the paradigm of ar and at laying the foundations of future design principles according to the class of systems keywords classification interaction characterization introduction one of the recent design goals in human com classification space for augmented surgery an augmented reality case study one of the recent design goals in human computer interaction has been to extend the sensorymotor capabilities of computer systems to combine the real and the virtual in order to assist the user in his environment such systems are called augmented reality ar although ar systems are becoming more prevalent we still do not have a clear understanding of this interaction paradigm in this paper we propose opas as a generic framework for classifying existing ar systems computer assisted medical interventions cami for which the added value of ar has been demonstrated by experience are discussed in light of opas we illustrate opas using our system casper computer assisted pericardial puncture a cami system which assists in surgical procedures pericardial punctures keywords augmented surgery cami augmented reality classification space 1 introduction the term augmented reality ar appears in the literature usually in conjunction with the term virtual reality vr th computational intelligence methods and data understanding abstract experts in machine learning and fuzzy system frequently identify understanding the data with the use of logical rules reasons for inadequacy of crisp and fuzzy rule based explanations are presented an approach based on analysis of probabilities of classification p ci x as a function of the size of the neighborhood of the given case x is presented probabilities are evaluated using monte carlo sampling or for some models using analytical formulas coupled with topographically correct visualization of the data in this neighborhood this approach applicable to any classifiers gives in many cases a better evaluation of the new data than rule based systems two real life examples of such interpretation are presented 1 introduction classification and prediction are the two most common applications of computational intelligence ci methods i e methods designed for solving problems that are effectively non algorithmic although sometimes classification by a black box is sufficient if it is reliable domain experts use software to help them to understand eliminators and classifiers classification may not be reliable for several reasons noise in the data insufficient input information overlapping distributions and sharp definition of classes faced with k possibilities a decision support system may still be useful in such cases if instead of classification elimination of improbable classes is done eliminators may be constructed using classifiers assigning new cases to a pool of several classes instead of just one winning class elimination may be done with the help of several classifiers using modified error functions a real life medical example is presented illustrating the usefulness of elimination optimization and interpretation of rule based classifiers abstract machine learning methods are frequently used to create rule based classifiers for continuous features linguistic variables used in conditions of the rules are defined by membership functions these linguistic variables should be optimized at the level of single rules or sets of rules assuming the gaussian uncertainty of input values allows to increase the accuracy of predictions and to estimate probabilities of different classes detailed interpretation of relevant rules is possible using probabilistic confidence intervals a real life example of such interpretation is given for personality disorders the approach to optimization and interpretation described here is applicable to any rule based system 1 introduction in many applications rule based classifiers are created starting from machine learning fuzzy logic or neural network methods 1 3 if the number of rules is relatively small and accuracy is sufficiently high such classifiers are an optimal choice because the reasons for their decisions are easily verified crisp logical rules are desirable minimal distance neural methods a general framework for minimal distance methods is presented radial basis functions rbfs and multilayer perceptrons mlps neural networks are included in this framework as special cases new versions of minimal distance methods are formulated a few of them have been tested on a real world datasets obtaining very encouraging results optimization and global minimization methods suitable for neural networks neural networks are usually trained using local gradient based procedures such methods frequently find suboptimal solutions being trapped in local minima optimization of neural structures and global minimization methods applied to network cost functions have strong influence on all aspects of network performance recently genetic algorithms are frequently combined with neural methods to select best architectures and avoid drawbacks of local minimization methods many other global minimization methods are suitable for that purpose although they are used rather rarely in this context this paper provides a survey of such global methods including some aspects of genetic algorithms contents 1 introduction 2 2 monte carlo and its improvements 4 3 simulated annealing and its variants 6 3 1 adaptive simulated annealing 7 3 2 alopex optimizing search by showing results in context we developed and evaluated seven interfaces for integrating semantic category information with web search results list interfaces were based on the familiar ranked listing of search results sometimes augmented with a category name for each result category interfaces also showed page titles and or category names but re organized the search results so that items in the same category were grouped together visually our user studies show that all category interfaces were more effective than list interfaces even when lists were augmented with category names for each result the best category performance was obtained when both category names and individual page titles were presented either alone is better than a list presentation but both together provide the most effective means for allowing users to quickly examining search results these results provide a better understanding of the perceptual and cognitive factors underlying the advantage of category groupings and provide some practical guidance to web search interface designers keywords user interface world wide web search user study usability text categorization focus in context handling temporal grouping and pattern matching queries in a temporal object model this paper presents a language for expressing temporal pattern matching queries and a set of temporal grouping operators for structuring histories following calendar based criteria pattern matching queries are shown to be useful for reasoning about successive events in time while temporal grouping may be either used to aggregate data along the time dimension or to display histories the combination of these capabilities allows to express complex queries involving succession in time and calendar based conditions simultaneously these operators are embedded into the tempos temporal data model and their use is illustrated through examples taken from a geographical application the proposal has been validated by a prototype on top of the o 2 dbms keywords temporal databases temporal object model temporal query language pattern matching queries temporal grouping calendar granularity o 2 1 introduction in most modern dbms time is one of the primitive datatypes provided for d a sequence based object oriented model for video databases structuration annotation and composition are amidst the most crucial modeling issues that video editing and querying in the context of a database entail in this paper we propose a sequence based object oriented data model that addresses them in an unified yet orthogonal way this orthogonality allows to capture the interactions between these three aspects i e annotations may be attached to any level of video structuration and the composition operators preserve the structurations and annotations of the argument videos the proposed model reuses concepts stemming from temporal databases so that operators defined in this latter setting may be used to query it accordingly the query language for video databases proposed in this paper is a variant of a temporal extension of odmg s oql the main components of the proposal have been formalized and implemented on top of an object oriented dbms keywords video databases sequence databases object oriented databases odmg r esum development and evaluation of clustering techniques for finding people typically in a large organisation much expertise and knowledge is held informally within employees own memories when employees leave an organisation many documented links that go through that person are broken and no mechanism is usually available to overcome these broken links this matchmaking problem is related to the problem of finding potential work partners in a large and distributed organisation this paper reports a comparative investigation into using standard information retrieval techniques to group employees together based on their web pages this information can hopefully be subsequently used to redirect broken links to people who worked closely with a departed employee or used to highlight people say in different departments who work on similar topics the paper reports the design and positive results of an experiment conducted at ris national laboratory comparing four different ir searching and clustering approaches using real users we multi agent systems by incremental gradient reinforcement learning situated with local and scalable perceptions have identical capabilities are possibly heterogeneous cooperate do not directly communicate each agent learns its behavior on its own 11 22 ijcai 01 bloc merging the problem reward 3 if blocs are merged actions n w e s perceptions dir agent 4 dir yellow bloc dir blue bloc near yellow bloc near blue bloc total 1024 4 mdp with 2 agents and 2 cubes for an 8 8 world 15 248 024 states 12 22 ijcai 01 an agent learns a policy but the o a is not markovian convergence is not assured stochastic policies should perform better sjj94 1b 1a a r b r b r a r 13 22 ijcai 01 multi agent framework each agent considers other agents as part of the environment all agents learn therefore evolve unpredictable transitions q learning baxter s gradient descent a rank aggregation methods for the web we consider the problem of combining ranking results from various sources in the context of the web the main applications include building meta search engines combining ranking functions selecting documents based on multiple criteria and improving search precision through word associations wedevelop a set of techniques for the rank aggregation problem and compare their performance to that of well known methods a primary goal of our work is to design rank aggregation techniques that can effectively combat spam a serious problem in web searches experiments show that our methods are simple efficient and effective keywords rank aggregation ranking functions metasearch multi word queries spam 1 flow analysis for verifying specifications of concurrent and distributed software this paper presents flavers a finite state verification approach that analyzes whether concurrent or sequential programs satisfy user defined correctness properties in contrast to other finite state verification techniques flavers is based on algorithms with low order polynomial bounds on the running time flavers achieves this efficiency at the cost of precision users however can improve the precision of the results by selectively and judiciously incorporating additional semantic information into the analysis problem the flavers analysis approach has been implemented for programs written in ada we report on an empirical study of the performance of applying the flavers ada tool set to a collection of multi tasking ada programs this study indicates that sufficient precision for proving program properties can be achieved and that the cost for such analysis grows as a low order polynomial in the size of the program 1 introduction the application of distributed and concurrent pr patterns in property specifications for finite state verification model checkers and other finite state verification tools allow developers to detect certain kinds of errors automatically nevertheless the transition of this technology from research to practice has been slow while there are a number of potential causes for reluctance to adopt such formal methods we believe that a primary cause is that practitioners are unfamiliar with specification processes notations and strategies in a recent paper we proposed a pattern based approach to the presentation codification and reuse of property specifications for finite state verification since then we have carried out a survey of available specifications collecting over 500 examples of property specifications we found that most are instances of our proposed patterns furthermore we have updated our pattern system to accommodate new patterns and variations of existing patterns encountered in this survey this paper reports the results of the survey and the current status of our pattern syste capturing and querying multiple aspects of semistructured data motivated to a large extent by the substantial and growing prominence of the world wide web and the potential benefits that may be obtained by applying database concepts and techniques to web data management new data models and query languages have emerged that contend with web data these models organize data in graphs where nodes denote objects or values and edges are labeled with single words or phrases nodes are described by the labels of the paths that lead to them and these descriptions serve as the basis for querying this paper proposes an extensible framework for capturing and querying meta data properties in a semistructured data model properties such as temporal aspects of data prices associated with data access quality ratings associated with the data and access restrictions on the data are considered specifically the paper defines an extensible data model and an accompanying query language that provides new facilities for matching slicing col efficiently supporting temporal granularities abstract granularity is an integral feature of temporal data for instance a person s age is commonly given to the granularity of years and the time of their next airline flight to the granularity of minutes a granularity creates a discrete image in terms of granules of a possibly continuous time line we present a formal model for granularity in temporal operations that is integrated with temporal indeterminacy or don t know when information we also minimally extend the syntax and semantics of sql 92 to support mixed granularities this support rests on two operations scale and cast that move times between granularities e g from days to months we demonstrate that our solution is practical by showing how granularities can be specified in a modular fashion and by outlining a time and space efficient implementation the implementation uses several optimization strategies to mitigate the expense of accommodating multiple granularities index terms calendar granularity indeterminacy sql 92 temporal database tsql2 1 detecting traffic problems with ilp expert systems for decision support have recently been successfully introduced in road transport management these systems include knowledge on traffic problem detection and alleviation the paper describes experiments in automated acquisition of knowledge on traffic problem detection the task is to detect road sections where a problem has occured critical sections from sensor data it is necessary to use inductive logic programming ilp for this purpose as relational background knowledge on the road network is essential in this paper we apply three state of the art ilp systems to learn how to detect traffic problems and compare their performance to the performance of a propositional learning system on the same problem 1 introduction expert systems for decision support have recently been successfully introduced in road transport management some of the proposals in this direction are trys 5 kits 4 and artist 9 from a general perspective the goal of a real time traffi achieving workflow adaptability by means of reflection belief in the importance of business processes has triggered considerable interest in the workflow systems that automate these processes however of the two competing management philosophies that promulgate business processes business process reengineering proposes radical change whereas continuous process improvement places much greater emphasis on adaptability the former school is somewhat discredited whereas the latter school seems more likely to endure thus making the flexibility and evolution of workflows an issue of increasing importance in this paper we present a programmable object oriented metalevel framework which aims to reveal the processes of assembling and coordinating the tasks that make up business processes this is achieved by isolating four key facets state behaviour location and coordination in particular we open up the general process of task coordination and specification allowing for extensions in a planned way by suitable manipulation of coordin systematic output modification in a 2d user interface toolkit in this paper we present a simple but general set of techniques for modifying output in a 2d user interface toolkit we use a combination of simple subclassing wrapping and collusion between parent and output objects to produce arbitrary sets of composable output transformations the techniques described here allow rich output effects to be added to most if not all existing interactors in an application without the knowledge of the interactors themselves this paper explains how the approach works discusses a number of example effects that have been built and describes how the techniques presented here could be extended to work with other toolkits we address issues of input by examining a number of extensions to the toolkit input subsystem to accommodate transformed graphical output our approach uses a set of hooks to undo output transformations when input is to be dispatched keywords user interface toolkits output rendering interactors drawing effects introduction the shape of the web and its implications for searching the web with the rapid growth of the number of web pages designing a search engine that can retrieve high quality information in response to a user query is a challenging task automated search engines that rely on keyword matching usually return too many low quality matches and they take a long time to run it is argued in the literature that link following search methods can substantially increase the search quality provided that these methods use an accurate assumption about useful patterns in the hyperlink topology of the web recent work in the field has focused on detecting identi able patterns in the web graph and exploiting this information to improve the performance of search algorithms we survey relevant work in this area and comment on the implications of these patterns for other areas such as advertisement and marketing intelligent driving agents the agent approach to tactical driving in autonomous vehicles and traffic simulation computer traffic simulation is important for making new traffic control strategies microscopic traffic simulators can model traffic flow in a realistic manner and are ideal for agent based vehicle control in this paper we describe a model of a reactive agent that is used to control a simulated vehicle the agent is capable of tactical level driving and has different driving styles to ensure fast reaction times the agent s driving task is divided in several competing and reactive behaviour rules the agent is implemented and tested in a prototype traffic simulator program the simulator consists of an urban environment with multi lane roads intersections traffic lights light controllers and vehicles every vehicle is controlled by a driving agent and all agents have individual behaviour settings preliminary experiments have shown that the agents exhibit human like behaviour ranging from slow and careful to fast and aggressive driving behaviour accurate and fast proximity queries between polyhedra using convex surface decomposition the need to perform fast and accurate proximity queries arises frequently in physically based modeling simulation animation real time interaction within a virtual environment and game dynamics the set of proximity queries include intersection detection tolerance verification exact and approximate minimum distance computation and disjoint contact determination specialized data structures and algorithms have often been designed to perform each type of query separately we present a unified approach to perform any of these queries seamlessly for general rigid polyhedral objects with boundary representations which are orientable 2 manifolds the proposed method involves a hierarchical data structure built upon a surface decomposition of the models furthermore the incremental query algorithm takes advantage of coherence between successive frames it has been applied to complex benchmarks and compares very favorably with earlier algorithms and systems 1 atomi automated reconstruction of topographic objects from aerial images using vectorized map information the project atomi is a co operation between the federal office of topography l t and eth zurich the aim of atomi is to update vector data of road centerlines and building roof outlines from 1 25 000 maps fitting it to the real landscape improve the planimetric accuracy to 1m and derive height information one representative height for each building with 1 2 m accuracy this update should be achieved by using image analysis techniques developed at eth zurich and digital aerial imagery the whole procedure should be implemented as a stand alone software package able to import and export data as used at l t it should be quasi operational fast and the most important reliable we do not aim at full automation ca 80 completeness is a plausible target the paper will present in detail the aims input data strategy and general methods used in atomi we will also present an overview of the results achieved up to now and problems faced in building and road reconstruction more de information passing and belief revision in multi agent systems we define a programming language for multi agent systems in which agents interact with a common environment and cooperate by exchanging their individual beliefs on the environment in handling the information they acquire the agents employ operations to expand remove and update their individual belief bases the overall framework which generalizes traditional concurrent programming concepts is parameterized by an information system of constraints such a system is used to represent the environment as well as the beliefs of the agents we give the syntax of the programming language and develop an operational semantics in terms of a transition system 1 introduction and syntax a lot of effort has been made in the development of programming languages for multi agent systems that cover typical agent concepts as beliefs desires intentions commitments speech acts communication cooperation and so on however in our opinion most of the concurrency aspects of the existing multi a a data model and algebra for probabilistic complex values we present a probabilistic data model for complex values more precisely we introduce probabilistic complex value relations which combine the concept of probabilistic relations with the idea of complex values in a uniform framework we elaborate a model theoretic definition of probabilistic combination strategies which has a rigorous foundation on probability theory we then define an algebra for querying database instances which comprises the operations of selection projection renaming join cartesian product union intersection and difference we prove that our data model and algebra for probabilistic complex values generalizes the classical relational data model and algebra moreover we show that under certain assumptions all our algebraic operations are tractable we finally show that most of the query equivalences of classical relational algebra carry over to our algebra on probabilistic complex value relations hence query optimization techniques for class on the difference of horn theories in this paper we consider computing the difference between two horn theories this problem may arise for example if we take care of a theory change in a knowledge base in general the difference of horn theories is not horn therefore we consider horn approximations of the difference in terms of horn cores i e weakest horn theories included in the difference and the horn envelope i e the strongest horn theory containing the difference which have been proposed and analyzed extensively in the literature we study the problem under the familiar representation of horn theories by horn cnfs as well as under the recently proposed model based representation in terms of the characteristic models for all problems and representations polynomial time algorithms or proofs of intractability for the propositional case are provided thus our work gives a complete picture of the tractability intractability frontier in the propositional horn theories extension of the relational algebra to probabilistic complex values we present a probabilistic data model for complex values more precisely we introduce probabilistic complex value relations which combine the concept of probabilistic relations with the idea of complex values in a uniform framework we then de ne an algebra for querying database instances which comprises the operations of selection projection renaming join cartesian product union intersection and difference we finally show that most of the query equivalences of classical relational algebra carry over to our algebra on probabilistic complex value relations hence query optimization techniques for classical relational algebra can easily be applied to optimize queries on probabilistic complex value relations first order representation of stable models turi 1991 introduced the important notion of a constrained atom an atom with associated equality and disequality constraints on its arguments a set of constrained atoms is a constrained interpretation we investigate how non ground representations of both the stable model semantics and the well founded semantics may be obtained through turi s approach the practical implication of this is that the wellfounded model or the set of stable models may be partially pre computed at compile time resulting in the association of each predicate symbol in the program to a constrained atom algorithms to create such models are presented both for the well founded case and the case of stable models query processing reduces to checking whether each atom in the query is true in a stable model resp well founded model this amounts to showing the atom is an instance of one of some constrained atom whose associated constraint is solvable various related complexity results are explored and the impacts of these results are discussed from the point of view of implementing systems that incorporate the stable and well founded semantics instructable and adaptive web agents that learn to retrieve and extract information we present a system for rapidly and easily building instructable and selfadaptive web agents for information retrieval and information extraction tasks our wisconsin adaptive web assistant wawa constructs a web agent by accepting user preferences in form of instructions and adapting the agent s behavior as it encounters new information wawa has two neural networks that are responsible for the adaptive capabilities of its agents user provided instructions are compiled into these neural networks and are modified via training examples users can create these training examples by rating pages that are retrieved by wawa but more importantly our system uses techniques from reinforcement learning to internally create its own examples users can also provide additional instruction throughout the life of an agent we evaluate wawa on a home page finder agent and a seminarannouncement extractor agent keywords web mining instructable and adaptive software agents machine learning task models intentions and agent conversation policies it is possible to define conversation policies such as communication or dialogue protocols that are based strictly on what messages and respectively what performatives may follow each other while such an approach has many practical applications such protocols support only local coherence in a conversation lengthy message exchanges require some infrastructure to lend them global coherence recognition of agent intentions about the joint task is essential for this global coherence but there are further mechanisms needed to ensure that both local and global coherence are jointly maintained this paper presents a general yet practical approach to designing managing and engineering agents that can do simple run time intention recognition without creating complex multi state protocols in this approach we promote developing abstract task models and designing conversation policies in terms of such models an implemented agent assistant based on these ideas is brie implementing incremental code migration with xml we demonstrate how xml and related technologies can be used for code mobility at any granularity thus overcoming the restrictions of existing approaches by not fixing a particular granularity for mobile code we enable complete programs as well as individual lines of code to be sent across the network we define the concept of incremental code mobility as the ability to migrate and add remove or replace code fragments i e increments in a remote program the combination of fine grained and incremental migration achieves a previously unavailable degree of flexibility we examine the application of incremental and fine grained code migration to a variety of domains including user interface management application management on mobile thin clients for example pdas and management of distributed documents keywords incremental code migration xml technologies 1 introduction the increasing popularity of java and the spread of webbased technologies are contributing to a growing software engineering and middleware a roadmap the construction of a large class of distributed systems can be simplified by leveraging middleware which is layered between network operating systems and application components middleware resolves heterogeneity and facilitates communication and coordination of distributed components state of the practice middleware products enable software engineers to build systems that are distributed across a localarea network state of the art middleware research aims to push this boundary towards internet scale distribution adaptive systems middleware for dependable and wireless systems the challenge for software engineering research is to devise notations techniques methods and tools for distributed system construction that systematically build and exploit the capabilities that middleware products deliver now and in the future incremental code mobility with xml we demonstrate how xml and related technologies can be used for code mobility at any granularity thus overcoming the restrictions of existing approaches by not fixing a particular granularity for mobile code we enable complete programs as well as individual lines of code to be sent across the network we define the concept of incremental code mobility as the ability to migrate and add remove or replace code fragments i e increments in a remote program the combination of fine grained and incremental mobility achieves a previously unavailable degree of flexibility we examine the application of incremental and fine grained code mobility to a variety of domains including user interface management application management on mobile thin clients for example pdas and management of distributed documents keywords incremental code mobility xml technologies 1 introduction the increasing popularity of java and the spread of webbased technologies are contributing to a growing int task oriented software understanding the main factors that affect software understanding are the complexity of the problem solved by the program the program text the user s mental ability and experience and the task being performed this paper describes a planning approach solution to the software understanding problem that focuses on the user s task and expertise first user questions about software artifacts have been studied and the most commonly asked questions are identified these questions are organized into a question model and procedures for answering them are developed then the patterns in user questions while performing certain tasks have been studied and these patterns are used to build generic task models the explanation system uses these task models in several ways the task model along with a user model is used to generate explanations tailored to the user s task and expertise in addition the task model allows the system to provide explicit task support in its interface keywords software explan sics marketspace an agent based market infrastructure we present a simple and uniform communication framework for an agent based market infrastructure the goal of which is to enable automation of consumer goods markets distributed over the internet the framework consists of an information model for participant interests and an interaction model that defines a basic vocabulary for advertising searching negotiating and settling deals the information model is based on structured documents representing contracts and representations of constrained sets of contracts called interests the interaction model is asynchronous message communication in a speech act based language similar to but simpler than kqml 7 and fipa acl 8 we also discuss integration of an agent based market infrastructure with the web 1 to each and everyone an agent augmenting web based commerce with agents internet has evolved from an information space to a market space with thousands potentially millions of electronic storefronts auctions and other commercial services this creates great opportunities but is not without problems one major problem is the difficulty of finding relevant offers another problem is coping with the multitude of different styles of web based user interfaces to different marketplaces yet another problem is how to automate routine tasks in such an environment we present one possible solution to these problems an agent based market infrastructure in which agents support all users and services helps customers and commercial sites find matching interests and if desired negotiate and close deals the infrastructure is entirely open and decentralized each participant has an agent that acts in the interest of its owner interaction is entirely symmetric any participant can play any role on a market in this paper we present an integration of such an infrastructure sics marketspace with the web personal assistant agents help users in their interaction with services and are able to handle routine tasks off line agent enabled services are able to adapt to the interests of their users even on their first visit and are provided with a mechanism to take the first initiative push in a highly focused manner keywords agent based markets software agents worldwide web electronic commerce personal assistants 1 generic agent framework for internet information systems for effective internet database services it is essential that the information requirements of regular users can be met without the typical delays currently experienced using internet browsers and the world wide web we use cooperating agents to manage both client and server caches thereby bringing significant performance improvements the caching and prefetching of information is based on both user and application profiles and agents communicate to ensure the currency of client caches according to specific application requirements various forms of agents can be installed on the server and client sides to provide value added services to both casual and regular users all component agents are instantiations and or specialisations of a generic agent we describe how a specific internet brokering system for engineering product data has been constructed using our general framework for the development of internet information systems 1 introduction with the development of world wide web query by trace visual predicate specification in spatio temporal databases in this paper we propose a visual interface for the specification of predicates to be used in queries on spatio temporal databases the approach is based on a visual specification method for temporally changing spatial situations this extends existing concepts for visual spatial query languages which are only capable of querying static spatial situations we outline a preliminary user interface that supports the specification on an intuitive and easily manageable level and we describe the design of the underlying visual language the visual notation can be used directly as a visual query interface to spatio temporal databases or it can provide predicate specifications that can be integrated into textual query languages leading to heterogeneous languages key words spatio temporal queries visual predicate specification visual database interface 1 introduction spatio temporal databases deal with spatial objects that change over time for example they move or they grow cars boosting applied to word sense disambiguation in this paper schapire and singer s adaboost mh boosting algorithm is applied to the word sense disambiguation wsd problem initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses naive bayes and exemplar based approaches which represent state of the art accuracy on supervised wsd in order to make boosting practical for a real learning domain of thousands of words several ways of accelerating the algorithm by reducing the feature space are studied the best variant which we call lazyboosting is tested on the largest sense tagged corpus available containing 192 800 examples of the 191 most frequent and ambiguous english words again boosting compares favourably to the other benchmark algorithms 1 introduction word sense disambiguation wsd is the problem of assigning the appropriate meaning sense to a given word in a text or discourse this meaning is distinguishable from other senses potentially attributable on the portability and tuning of supervised word sense disambiguation systems this report describes a set of experiments carried out to explore the portability of alternative supervised word sense disambiguation algorithms the aim of the work is threefold firstly studying the performance of these algorithms when tested on a different corpus from that they were trained on secondly exploring their ability to tune to new domains and thirdly demonstrating empirically that the lazyboosting algorithm outperforms state of the art supervised wsd algorithms in both previous situations keywords word sense disambiguation machine learning natural language processing portability and tuning of nlp systems 1 introduction word sense disambiguation wsd is the problem of assigning the appropriate meaning sense to a given word in a text or discourse where this meaning is distinguishable from other senses potentially attributable to that word as an example table 1 shows the definition of two senses of the word age 1 and an example sentence 2 for eac machine learning for intelligent processing of printed documents a paper document processing system is an information system component which transforms information on printed or handwritten documents into a computer revisable form in intelligent systems for paper document processing this information capture process is based on knowledge of the speci c layout and logical structures of the documents this article proposes the application of machine learning techniques to acquire the speci c knowledge required byan intelligent document processing system named wisdom that manages printed documents such as letters and journals knowledge is represented by means of decision trees and rst order rules automatically generated from a set of training documents in particular an incremental decision tree learning system is applied for the acquisition of decision trees used for the classi cation of segmented blocks while a rst order learning system is applied for the induction of rules used for the layout based classi cation and understanding of d computers seeing people this paper we present methods that give machines the ability to see people interpret their actions and interact with them we present the motivating factors behind this work examples of how such computational methods are developed and their applications the basic reason for providing machines the ability to see people really depends on the task we are associating with a machine an industrial vision system aimed at extracting defects on an assembly line need not know anything about people similarly a computer used for email and text writing need not see and perceive the users gestures and expressions however if our interest is to build intelligent machines that can work with us support our needs and be our helpers than it maybe required for these machines to know more about who they are supporting and helping if our computers are to do more then support our text based needs like writing papers spreadsheets and communicating via email perhaps take on a role of being a personal assistant then the ability to see a person is essential such an ability to perceive people is something that we take for granted in our everyday interactions with each other at present our model of a machine or more specifically of a computer is something that is placed in the corner of the room it is deaf dumb and blind having no sense of the environment that it is in or of the person that is near it we communicate with this computer using a coded sequence of tappings on a keyboard imagine a computer that knows that you are near it that you are looking at it knows who you are and what you are trying to do imagine a machine that can interpret a video signal based on who is in the scene and what they are doing such abilities in a computer are hard to imagine unless it has using linear classifiers in the integration of user modeling and text content analysis in the personalization of a web based spanish news service nowadays many newspapers and news agencies offer personalized information access services and moreover there is a growing interest in the improvement of these services in this paper we present a methodology useful to improve the intelligent personalization of news services and the way it has been applied to a spanish relevant newspaper abc our methodology integrates textual content analysis tasks and machine learning techniques to achieve an elaborated user model which represents separately short term needs and long term multi topic interests the characterization of a user s interests includes his preferences about structure newspaper sections content and information delivery a wide coverage and non specific domain classification of topics and a personal set of keywords allow the user to define his preferences about content machine learning techniques are used to obtain an initial representation of each category of the topic classification finally we introduce some details about the mercurio system which is being used to implement this methodology for abc we describe our experience and an evaluation of the system in comparison with other commercial systems the dc tree a fully dynamic index structure for data warehouses many companies have recognized the strategic importance of the knowledge hidden in their large databases and have built data warehouses typically updates are collected and applied to the data warehouse periodically in a batch mode e g over night then all derived information such as index structures has to be updated as well the standard approach of bulk incremental updates to data warehouses has some drawbacks first the average runtime for a single update is small but the total runtime for the whole batch of updates may become rather large second the contents of the data warehouse is not always up to date in this paper we introduced the dc tree a fully dynamic index structure for data warehouses modeled as a data cube this new index structure is designed for applications where the above drawbacks of the bulk update approach are critical the dc tree is a hierarchical index structure similar to the x tree exploiting the concept hierarchies typically defined for the using multi strategy learning to improve planning efficiency and quality itr a framework for environment aware massively distributed computing physical environment in real time and the need to reason about emerging aggregate properties as opposed to individual component behavior in this research we propose to develop theory methods and tools for massively distributed environment aware computing more succinctly referred to as swarm computing the state of swarm computing today is similar to that of sequential computing in the early 1950s developers painstakingly produce swarm programs by designing and programming the actions of individual devices and converge on an acceptable program through extensive simulation and experimentation in the pre compiler era skeptical programmers believed that a mechanical process could not possibly produce code of comparable quality to that produced by highly skilled machine coders and that the cost of machine time is high enough to outweigh any possible savings in programmer effort the state of swarm programming today is similar devices are still expensive enough an jacob project documentation the jacob software system has been built as part of the jacob project which is a pilot project of the virtual reality valley twente initiative the jacob project investigates the application of virtual reality techniques and involves the design and construction of an animated agent in a 3 dimensional virtual environment the project focuses on software engineering aspects multimodal interaction and the use of agent technology in the current version of the jacob system an agent called jacob teaches the user the towers of hanoi game interaction takes place through natural language and manipulations of objects in the virtual environment the purpose of this report is to provide information needed for further development of the jacob system it describes a number of technical details including the file and directory structure the software architecture the event handling mechanism and the integration of the dialogue management system 3 table of contents 1 introduction selecting and materializing horizontally partitioned warehouse views data warehouse views typically store large aggregate tables based on a subset of dimension attributes of the main data warehouse fact table aggregate views can be stored as 2 n subviews of a data cube with n attributes methods have been proposed for selecting only some of the data cube views to materialize in order to speed up query response time accommodate storage space constraint and reduce warehouse maintenance cost this paper proposes a method for selecting and materializing views which selects and horizontally fragments a view recomputes the size of the stored partitioned view while deciding further views to select 2001 elsevier science b v all rights reserved keywords data warehouse views fragmentation performance benet 1 introduction decision support systems dss used by business executives require analyzing snapshots of departmental databases over several periods of time departmental databases of the same organization e g a bank may be stored on dier representing school timetabling in a disjunctive logic programming language in this paper we show how school timetabling problems with preferences originating from didactical organisational and personal considerations can be represented in a highly declarative and natural way using an extension of disjunctive datalog by strong and weak integrity constraints 1 introduction almost all people have come across school timetabling during their lives for a long time almost all school timetables were created manually a timeconsuming task which often yielded suboptimal schedules in the last thirty years however systems have been designed which automate timetable creation timetabling in general is the problem of finding suitable combinations of two or more types of resources which have to be at the same place during several discrete periods of time and which have to satisfy various additional constraints some of these constraints are strict while some are not the latter express preferences or desiderata in the case of school timetabling problems these using database optimization techniques for nonmonotonic reasoning in this paper a program rewriting technique for disjunctive datalog is proposed which descends from query optimization techniques in relational algebra and reduces the size of the ground instantiation of a program in many cases as a consequence the time for generating the ground instantiation and the time for subsequent operations on the ground program is shortened a part of this technique has already been implemented as a preprocessing step in the disjunctive deductive database system dlv using a recently published application we show that a signicant reduction of the ground program size and a tremendous overall speedup can be achieved 1 introduction dlv is a disjunctive deductive database system implementing the consistent answer sets semantics like similar systems in the area of non monotonic reasoning 6 the kernel modules of dlv operate on a ground instantiation of the input program i e a program that does not contain any variables but is semantically equivalent t hermes a notification service for digital libraries the high publication rate of scholarly material makes searching and browsing an inconvenient way to keep oneself up todate instead of being the active part in information access researchers want to be notified whenever a new paper in one s research area is published amplifying reality many novel applications take on the task of moving the personal computer away from the desktop with the approach to merge digital information with physical space and objects these new applications have given rise to a plethora of notions and terms used to classify them we introduce amplified reality as a concept complementary to that of augmented reality to amplify reality is to enhance the publicly available properties of persons and physical objects by means of using wearable or embedded computational resources the differences between the two concepts are discussed and examples of implementations are given the reason for introducing this term is to contribute to the terminology available to discuss already existing applications but also to open up for a discussion of interesting design implications keywords amplified reality augmented reality ubiquitous computing wearable computing embedded vs superimposed properties private vs public 1 breaking away f the bubblebadge a wearable public display we are exploring the design space of wearable computers by designing public wearable computer displays this paper describes our first prototype the bubblebadge by effectively turning the wearer s private display inside out the bubblebadge transforms the wearable computing concept by making digital information public rather than private user tests showed that the device introduces a new way to interact with information providing devices suggesting that it would be valuable to explore the concept further keywords wearable computers interaction technology public displays introduction a wearable computer is defined as a continuously running augmenting and mediating computational device 2 wearable computers are usually highly private since both input and output is controlled and seen only by the user who is effectively hiding behind a hand held keyboard and a head mounted display but while wearable computing can be a powerful tool for the single user there is usuall on splitting and cloning agents embedded with cloning mechanisms an agent can balance its own loads by discharging computing tasks to its clones when it is over loaded in addition it s more reasonable to transfer the smarter smaller clones of an agent rather than the bulky agent itself in mobile computing in this paper a simple bdi agent model is formally established using this model the semantics of constructing new agents by inheritance and self identifying behavior of existing agents are precisely de ned four kinds of cloning mechanisms are identi ed the properties of each cloning mechanism and the relationships in between are studied and some implementation issues are also discussed computing iceberg queries efficiently many applications compute aggregate functions such as count sum over an attribute or set of attributes to find aggregate values above some specified threshold we call such queries iceberg queries because the number of above threshold results is often very small the tip of an iceberg relative to the large amount of input data the iceberg such iceberg queries are common in many applications including data warehousing information retrieval market basket analysis in data mining clustering and copy detection we propose efficient algorithms to evaluate iceberg queries using very little memory and significantly fewer passes over data as compared to current techniques that use sorting or hashing we present an experimental case study using over three gigabytes of web data to illustrate the savings obtained by our algorithms 1 introduction in this paper we develop efficient execution strategies for an important class of queries that we call iceberg queries an iceberg query translingual visual speech synthesis audio driven facial animation is an interesting and evolving technique for human computer interaction based on an incoming audio stream a face image is animated with full lip synchronization this requires a speech recognition system in the language in which audio is provided to get the time alignment for the phonetic sequence of the audio signal however building a speech recognition system is data intensive and is a very tedious and time consuming task we present a novel scheme to implement a language independent system for audio driven facial animation given a speech recognition system for just one language in our case english the method presented here can also be used for text to audio visual speech synthesis 1 applying temporal databases to geographical data analysis this paper reports an experience in which a temporal database was used to analyze the results of a survey on human behaviors and displacements in a ski resort this survey was part of a broader study about the use of the resort s infrastructure based on the time geography methodology as such the presented experience may be seen as an attempt to implement some concepts of the time geography using temporal database technology throughout the paper some shortcomings of current temporal data models regarding human displacements analysis are pointed out and possible solutions are briefly sketched keywords temporal databases time geography human displacements analysis 1 initialization of iterative refinement clustering algorithms iterative refinement clustering algorithms e g k means em converge to one of numerous local minima it is known that they are especially sensitive to initial conditions we present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the modes of a distribution the refined initial starting condition leads to convergence to better local minima the procedure is applicable to a wide class of clustering algorithms for both discrete and continuous data we demonstrate the application of this method to the expectation maximization em clustering algorithm and show that refined initial points indeed lead to improved solutions refinement run time is considerably lower than the time required to cluster the full database the method is scalable and can be coupled with a scalable clustering algorithm to address the large scale clustering in data mining 1 background clustering has been formulated in var computing the median with uncertainty we consider a new model for computing with uncertainty it is desired to compute a function f x1 xn where x1 xn are unknown but guaranteed to lie in specified intervals i1 in it is possible to query the precise value of any x j at a cost c j the goal is to pin down the value of f to within a precision ffi at a minimum possible cost we focus on the selection function f which returns the value of the kth smallest argument we present optimal offline and online algorithms for this problem 1 introduction consider the following model for computing with uncertainty we wish to compute a function f x1 xn over n real valued arguments the values of the variables x1 xn are not known in advance however we are provided with real intervals i1 in along with a guarantee that for each j x j 2 i j furthermore it is possible to query the true value x j of each x j at a cost c j the goal is to pin down the value of f into an i advances in analogy based learning false friends and exceptional items in pronunciation by paradigm driven analogy when looked at from a multilingual perspective grapheme to phoneme conversion is a challenging task fraught with most of the classical nlp vexed questions bottle neck problem of data acquisition pervasiveness of exceptions difficulty to state range and order of rule application proper treatment of context sensitive phenomena and long distance dependencies and so on the hand crafting of transcription rules by a human expert is onerous and time consuming and yet for some european languages still stops short of a level of correctness and accuracy acceptable for practical applications we illustrate here a self learning multilingual system for analogy based pronunciation which was tested on italian english and french and whose performances are assessed against the output of both statistically and rule based transcribers the general point is made that analogy based self learning techniques are no longer just psycholinguistically plausible models but competitive tools combining the advantages of using language independent self learning tractable algorithms with the welcome bonus of being more reliable for applications than traditional text to speech systems a new heuristic for optimizing large queries there is a number of oodb optimization techniques proposed recently such as the translation of path expressions into joins and query unnesting that may generate a large number of implicit joins even for simple queries unfortunately most current commercial query optimizers are still based on the dynamic programming approach of system r and cannot handle queries of more than ten tables there is a number of recent proposals that advocate the use of combinatorial optimization techniques such as iterative improvement and simulated annealing to deal with the complexity of this problem these techniques though fail to take advantage of the rich semantic information inherent in the query specification such as the information available in query graphs which gives a good handle to choose which relations to join each time this paper presents a polynomial time algorithm that generates a good quality order of relational joins it can also be used with minor modifications to sort oodb a query unnesting in object oriented databases there is already a sizable body of proposals on oodb query optimization one of the most challenging problems in this area is query unnesting where the embedded query can take any form including aggregation and universal quantification although there is already a number of proposed techniques for query unnesting most of these techniques are applicable to only few cases we believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations including aggregation and quantification in the same way this paper presents a new query unnesting algorithm that generalizes many unnesting techniques proposed recently in the literature our system is capable of removing any form of query nesting using a very simple and efficient algorithm the simplicity of the system is due to the use of the monoid comprehension calculus as an intermediate form for oodb queries the monoid comprehension calculus treats op voodoo a visual object oriented database language for odmg oql this paper presents a simple and effective visual language to express odmg oql queries the language is expressive enough to allow most types of query nesting aggregation universal and existential quantifications group by and sorting and at the same time is uniform and very simple to learn and use our visual language is strongly typed in the sense that queries constructed in our system are always type correct in addition there is sufficient type information displayed by the system that guides every stage of the query construction the main difference between our language and other related visual query languages is that we use only one generic visual construct called a template instead of inventing a new one for each oql syntactic feature 1 introduction query and data visualizations are key components of many commercial relational database systems such as microsoft access and paradox object oriented databases offer excellent opportunities for visual data browsing b the importance of being mobile some social consequences of wearable augmented reality systems what are the consequences of mobility for augmented reality this brief paper explores some of the issues that i believe will be raised by the development and future commonplace adoption of mobile wearable augmented reality systems these include social influences on tracking accuracy the importance of appearance and comfort an increase in collaborative applications integration with other devices and implications for personal privacy 1 introduction over the past decade the reality of mobile computing has begun to embrace the potential of wearable computing in the process several researchers have attempted to clarify what distinguishes wearable computing from mobile computing rhodes 10 suggests five criteria for wearable systems portable while operational needing minimal manual input sensitive to the user s surrounding environment always on and able to attract the user s attention even when not actively in use mann 8 cites three desirable properties for wearabl wearing it out first steps toward mobile augmented reality systems introduction over the past decade there has been a ground swell of activityintwo elds of user interface research augmented reality and wearable computing augmentedreality 1 refers to the creation of virtual environments that supplement rather than replace the real world with additional information this is accomplished through the use of see through displays that enrich the user s view of the world byoverlaying visual auditory and even haptic material on what she experiences visual augmented reality systems typically but not exclusively employ head tracked head worn displays these either use half silvered mirror beam splitters to re ect small computer displays optically combining them with a view of the real world or use opaque displays fed by electronics that merge imagery captured by head worn cameras with synthesized graphics wearable computing moves computers o the desktop and onto the user s body made possible through the glove talkii a neural network interface which maps gestures to parallel formant speech synthesizer controls glove talkii is a system which translates hand gestures to speech through an adaptive interface hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer the mapping allows the hand to act as an artificial vocal tract that produces speech in real time this gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume currently the best version of glovetalkii uses several input devices including a cyberglove a contactglove a 3 space tracker and a foot pedal a parallel formant speech synthesizer and 3 neural networks the gesture to speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network the gating network and the consonant network are trained with examples from the user the vowel network implements a fixed user defined relationship between hand position and vowel sound and does not require any training workshop on intelligent information integration iii 99 ontobroker or how to enable intelligent access to the www the world wide web www is currently one of the most important electronic information sources however its query interfaces and the provided reasoning services are rather limited ontobroker consists of a number of languages and tools that enhance query access and inference service in the www it provides languages to annotate web documents with ontological information to represent ontologies and to formulate queries the tool set of ontobroker allows us to access information and knowledge from the web and to infer new knowledge with an inference engine based on techniques from logic programming this article provides several examples that illustrate these languages and tools and the kind of service that is provided we also discuss the bottlenecks of our approach that stem from the fact that the applicability of ontobroker requires two time consuming activities 1 developing shared ontologies that reflect the consensus of a group of web users and 2 annotating we logical update queries as open nested transactions the rule based update language ultra has been designed for the specification of complex database updates in a modular fashion the logical semantics of update goals is based on update request sets which correspond to deferred basic updates in the database the declarative character of the logical semantics leaves much freedom for various evaluation strategies among them a top down resolution which can be naturally mapped onto a system of nested transactions in this paper we extend this operational model as follows not only the basic operations are performed and committed independently from the toplevel transaction but also complex operations defined by update rules this leads to an open nested transaction hierarchy which allows to exploit the semantical properties of complex operations to gain more concurrency on the other hand high level compensation is necessary and meta information must be provided by the programmer we present the key elements of this combi on the role of bdi modelling for integrated control and coordinated behavior in autonomous agents this paper describes an architecture for controlling and coordinating autonomous agents building on previous work addressing reactive and deliberative control methods the proposed multi layered hybrid architecture allows a rationally bounded goal directed agent to reason predictively about potential conflicts by constructing knowledge level models which explain other agents observed behaviors and hypothesize their beliefs desires and intentions at the same time it enables the agent to operate autonomously to react promptly to changes in its real time environment and to coordinate its actions effectively with other agents a principal aim of this research is to understand the role different functional capabilities play in constraining an agent s behavior under varying environmental conditions to this end an experimental testbed has been constructed comprising a simulated multi agent world in which a variety of agent configurations and behaviors have been investigated a numbe approximate nearest neighbor searching in multimedia databases in this paper we develop a general framework for approximate nearest neighbor queries we categorize the current approaches for nearest neighbor query processing based on either their ability to reduce the data set that needs to be examined or their ability to reduce the representation size of each data object we first propose modifications to wellknown techniques to support the progressive processing of approximate nearest neighbor queries a user may therefore stop the retrieval process once enough information has been returned we then develop a new technique based on clustering that merges the benefits of the two general classes of approaches our cluster based approach allows a user to progressively explore the approximate results with increasing accuracy we propose a new metric for evaluation of approximate nearest neighbor searching techniques using both the proposed and the traditional metrics we analyze and compare several techniques with a detailed performance evaluation we demonstrate the feasibility and efficiency of approximate nearest neighbor searching we perform experiments on several real data sets and establish the superiority of the proposed cluster based technique over the existing techniques for approximate nearest neighbor searching 1 constrained nearest neighbor queries in this paper we introduce the notion of constrained nearest neighbor queries cnn and propose a series of methods to answer them this class of queries can be thought of as nearest neighbor queries with range constraints although both nearest neighbor and range queries have been analyzed extensively in previous literature the implications of constrained nearest neighbor queries have not been discussed due to their versatility cnn queries are suitable to a wide range of applications from gis systems to reverse nearest neighbor queries and multimedia applications we develop methods for answering cnn queries with different properties and advantages we prove the optimality with respect to i o cost of one of the techniques proposed in this paper the superiority of the proposed technique is shown by a performance analysis 1 introduction two dimensional range queries are used frequently in various applications such as spatial databases sam89 gg98 and geographic infor detection and tracking of facial features in video sequences this work presents a real time system for detection and tracking of facial features in video sequences such system may be used in visual communication applications such as teleconferencing virtual reality intelligent interfaces humanmachine interaction surveillance etc we have used a statistical skin color model to segment face candidate regions in the image the presence or absence of a face in each region is verified by means of an eye detector based on an efficient template matching scheme once a face is detected the pupils nostrils and lip corners are located and these facial features are tracked in the image sequence performing real time processing 1 tracking facial features using gabor wavelet networks this work presents a new method for automatic facial feature tracking in video sequences in this method a discrete face template is represented as a linear combination of continuous 2d odd gabor wavelet functions the weights and 2d parameters position scale and orientation of each wavelet are determined optimally so that the maximum of image information is preserved for a given number of wavelets we have used this representation to achieve effective facial feature tracking that is robust to homogeneous illumination changes and affine deformations of the face image moreover the tracking approach considers the overall geometry of the face being robust to facial feature deformations such as eye blinking and smile the number of wavelets in the representation may be chosen with respect to the available computational resources even allowing real time processing 1 combining inductive and deductive inference in knowledge management tasks this paper indicates how different logic programming technologies can underpin an architecture for distributed knowledge management in which higher throughput in information supply is achieved by a semi automated solution to the more challenging problem of knowledge creation the paper first proposes working definitions of the notions of data knowledge and information in purely logical terms and then shows how existing technologies can be combined into an inference engine referred to as a knowledge information and data engine kide integrating inductive and deductive capabilities the paper then briefly introduces the notion of virtual organizations and uses the set up stage of virtual organizations to exemplify the value adding potential of kides in knowledge management contexts constructing qualitative event models automatically from video input we describe an implemented technique for generating event models automatically based on qualitative reasoning and a statistical analysis of video input using an existing tracking program which generates labelled contours for objects in every frame the view from a fixed camera is partitioned into semantically relevant regions based on the paths followed by moving objects the paths are indexed with temporal information so objects moving along the same path at different speeds can be distinguished using a notion of proximity based on the speed of the moving objects and qualitative spatial reasoning techniques event models describing the behaviour of pairs of objects can be built again using statistical methods the system has been tested on a traffic domain and learns various event models expressed in the qualitative calculus which represent human observable events the system can then be used to recognise subsequent selected event occurrences or unusual behaviours 1 introduction d the string b tree a new data structure for string search in external memory and its applications we introduce a new text indexing data structure the string b tree that can be seen as a link between some traditional external memory and string matching data structures in a short phrase it is a combination of b trees and patricia tries for internal node indices that is made more effective by adding extra pointers to speed up search and update operations consequently the string b tree overcomes the theoretical limitations of inverted files b trees prefix b trees suffix arrays compacted tries and suffix trees string b trees have the same worst case performance as b trees but they manage unbounded length strings and perform much more powerful search operations such as the ones supported by suffix trees string b trees are also effective in main memory ram model because they improve the online suffix tree search on a dynamic set of strings they also can be successfully applied to database indexing and software duplication a fuzzy beam search rule induction algorithm this paper proposes a fuzzy beam search rule induction algorithm for the classification task the use of fuzzy logic and fuzzy sets not only provides us with a powerful flexible approach to cope with uncertainty but also allows us to express the discovered rules in a representation more intuitive and comprehensible for the user by using linguistic terms such as low medium high rather than continuous numeric values in rule conditions the proposed algorithm is evaluated in two public domain data sets 1 introduction this paper addresses the classification task in this task the goal is to discover a relationship between a goal attribute whose value is to be predicted and a set of predicting attributes the system discovers this relationship by using known class examples and the discovered relationship is then used to predict the goal attribute value or the class of unknown class examples there are numerous rule induction algorithms for the classification task however when cyborgs meet building communities of cooperating wearable agents this paper keywords wearable computing personal agents 1 introduction our modern world society is characterized by an ever increasing ubiquity pervasiveness of electronic communication technologies like phone and email despite this fact most human interactions still occur when we physically meet other people every day we encounter a large number of people friends colleagues and strangers alike at places like coffee shops grocery stores and offices we interact with people to trade news tell stories gossip or exchange goods and services often we use these situations 1 to pursue our own goals for example we purchase items coordinate schedules or make other arrangements when we meet other people wearable computers provide a chance to augment such human every day interactions and to advance cooperation why features of wearables always on always active senses environment proactive ability to support user during every day life ability to act as use raw a relational algebra for the web the main idea underlying the paper is to extend the relational algebra such that it becomes possible to process queries against the world wide web these extensions are minor in that we tried to keep them at the domain level additionally to the known domains int bool float string we introduce three new domains to deal with urls html documents or fragments thereof and path expressions over these domains we define several functions that are accessible from the algebra within the subscripts of the relational operators the approach allows us to reuse the operators of the relational algebra without major modifications indead the only extension necessary is the introduction of a map operator further two modifications to the scan and the indexscan are necessary finally the indexscan which has the functionality of a typical meta search engine is capable of computing a unified rank based on the tuple order provided by the underlying search engines 1 introduction the web 2 w distributed repositories of highly expressive reusable ontologies we describe an ongoing project to develop technology that will support collaborative construction and effective use of distributed large scale repositories of highly expressive reusable ontologies we are focusing on developing a distributed server architecture for ontology construction and use representation formalisms that remove key barriers to expressing essential knowledge in and about ontologies ontology construction tools and tools for obtaining domain models for use in applications from large scale ontology repositories we are building on the results of the darpa knowledge sharing effort specifically by using the knowledge interchange format kif as a core representation language and the ontolingua system as a core ontology development environment in order to enable distributed ontology repositories and services we are developing a distributed server architecture for ontology construction and use based on ontology servers which provide access via a network api to the contents of ontologies and to information derivable from the contents by a general purpose reasoner ontology servers will be analogous to data base servers and will provide services including configuration gathering user interface design requirements for social computing design for cooperation is a challenge as designers we note that as we are moving towards the final years of this century several areas have achieved significant breakthroughs among them it is easy to perceive that areas of computing and telecommunications have had an impact of paramount importance to society as a whole these technologies have allowed an increasing integration of research fields people of various backgrounds and abilities as well as made the interaction of different cultures possible as a result we have been living in the internet era with a very large number of web sites which can be visited queried and played with that constitutes what we call social computing application examples are digital libraries health care information systems physics collaboratories and web based entertainments like interactive web games within this context we are concerned with the user interface design requirements gathering for such systems in that sense we present a prot active perception and map learning for robot navigation this paper describes a simulated on line mapping system for robot navigation this system allows the autonomous creation of topological maps enhanced with metrical information provided by internal odometry and external vision and sonars sensors within such maps the robot s position is represented and calculated probabilistically according to algorithms that are inspired by hidden markov models the visual system is very simple and does not allow reliable recognition of speci c places but used jointly with odometry sonar recordings and an active perception system it allows reliable localization even when the robot starts exploring its environment and when it is passively translated from one place to another advantages and drawbacks of the current system are discussed together with means to remediate the latter incremental evolution of neural controllers for navigation in a 6 legged robot this paper describes how the sgoce paradigm has been used within the context of a minimal simulation strategy to evolve neural networks controlling locomotion and obstacle avoidance in a 6 legged robot such controllers have been first evolved through simulation and then successfully downloaded on the real robot adaptable and adaptive information provision for all users including disabled and elderly people due to the tremendously increasing popularity of the world wide web hypermedia is going to be the leading online information medium for some years to come and will most likely become the standard gateway for citizens to the information highway already today visitors of web sites are generally heterogeneous and have different needs and this is likely to increase in the future the aim of the avanti project is to cater hypermedia information to these individual needs by adapting the content and the presentation of web pages to each individual user the special needs of elderly and disabled users are also partly considered a model of the characteristics of user groups individual users and usage environments and a domain model are exploited in the adaptation process one aim of this research is to verify that adaptation and user modeling techniques that were hitherto mostly used for catering interactive software systems to able bodied users also prove useful for adaptation to users with special needs another original aspect is the development of a network wide user modeling server that can concurrently accommodate the user modeling needs of several applications and several instances of an application within a distributed computing environment fact or fiction content classification for digital libraries the world wide web www is a vast repository of information much of which is valuable but very often hidden to the user the anarchic nature of the www presents unique challenges when it comes to information extraction and categorization we view the www as a valuable resource for the gathering of information for digital libraries in this paper we will describe the process of extracting and classifying information from the www for the purpose of integrating it into digital libraries our eorts focus on ways to automatically classify news articles according to whether they present opinions or reported facts we describe and evaluate a system in development that automatically classies and recommends web news articles from sports and politics domains 1 genre classification and domain transfer for information filtering the world wide web is a vast repository of information but the sheer volume makes it difficult to identify useful documents we identify document genre is an important factor in retrieving useful documents and focus on the novel document genre dimension of subjectivity learning with deictic representation most reinforcement learning methods operate on propositional representations of the world state such representations are often intractably large and generalize poorly using a deictic representation is believed to be a viable alternative they promise generalization while allowing the use of existing reinforcement learning methods yet there are few experiments on learning with deictic representations reported in the literature in this paper we explore the effectiveness of two forms of deictic representation and a na ve propositional representation in a simple blocks world domain we find empirically that the deictic representations actually worsen performance we conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects 1 user modeling in human computer interaction a fundamental objective of human computer interaction research is to make systems more usable more useful and to provide users with experiences fitting their specific background knowledge and objectives the challenge in an information rich world is not only to make information available to people at any time at any place and in any form but specifically to say the right thing at the right time in the right way designers of collaborative humancomputer systems face the formidable task of writing software for millions of users at design time while making it work as if it were designed for each individual user only known at use time user modeling research has attempted to address these issues in this article i will first review the objectives progress and unfulfilled hopes that have occurred over the last ten years and illustrate them with some interesting computational environments and their underlying conceptual frameworks a special emphasis is given to high functionali intelligent agents in virtual enterprises decreasing innovation cycles changing market situations as well as growing specialisation in individual market segments demand new ways of economic thinking increasingly forcing enterprises into cooperations sometimes even with direct competitors presently discussed and designated as the corporate and cooperation model of the future is the so called virtual enterprise in this paper we advocate the use of intelligent agents as a useful metaphor and as a software engineering methodology for the design and the operation of virtual enterprises we focus on how agents can support the cooperative process of setting up virtual enterprises through the internet by performing tasks such as presentation information retrieval and extraction and the participation in auctions in electronic markets this paper does not describe completed research it rather offers a perspective of the high potential of agent based technology for one of tomorrow s key industrial areas by presenting the main objectives of the new research project ave build it an intuitive design tool based on direct object manipulation this paper means human action in a world graspable interfaces establishing design principles phd research plan for morten fjeld topic design of tangible user interfaces designing graspable groupware for co located planning and configuration tasks this paper shows some of the vital steps in the design process of a graspable groupware system activity theory is the theoretical foundation for our research our design philosophy is based on the tradition of augmented reality ar which enriches natural communication with virtual features another important part of our design philosophy is the use of coinciding action and perception spaces we developed groupware for layout planning and configuration tasks called the build it system this system enables users grouped around a table to cooperate in the design manipulation of a virtual setting thus supporting colocated instead of distributed interaction rauterberg et al 1997a 1997b 1998 fjeld et al 1998a the multi user nature of build it overcomes a serious drawback often seen with cscw tools namely that they are based on single user applications grudin 1988 we believe that co location is an indispensable factor for the early stage of a complex planning process input and output however can be prepared and further developed off line fjeld et al 1998b using any conventional cad system exploring brick based camera control introduction build it is a planning tool based on computer vision technology with a capacity for complex planning and composition tasks rauterberg et al 1997 the system enables users grouped around a table to interact in a virtual scene using real bricks to select and manipulate objects in the scene fig 1 left a plan view of the scene is projected onto the table a perspective view of the scene called side view is projected on the wall the plan view contains a storage space with originals allowing users to select new objects object selection is done by putting a brick at the object position once selected objects can be positioned rotated and fixed by simple brick manipulation fig 1 right they are de selected and stay put when the brick is covered or removed objects brought back to the storage space are deleted figure 1 build it offers a plan view for combined action and perception and a f1 abduction and induction essays on their relation and integration bibliography charles s peirce society 22 2 145 164 anderson d 1987 creativity and the philosophy of c s peirce volume 27 of philosophy library martinus nijhoff andreasen t and christiansen h 1996 counterfactual exceptions in deductive database queries in proceedings of the twelfth european conference on artificial intelligence pages 340 344 i ii bibliography andreasen t and christiansen h 1998 a practical approach to hypothetical database queries in freitag b decker h kifer m and voronkov a editors transactions and change in logic databases volume 1472 of lecture notes in computer science berlin springer verlag angluin d 1980 inductive inference of formal languages from positive data information and control 45 117 135 angluin d frazier m and pitt l 1992 learning conjunctions of horn clauses f11 strongly typed inductive concept learning in this paper we argue that the use of a language with a type system together with higher order facilities and functions provides a suitable basis for knowledge representation in inductive concept learning and in particular illuminates the relationship between attribute value learning and inductive logic programming ilp individuals are represented by closed terms tuples of constants in the case of attribute value learning arbitrarily complex terms in the case of ilp to illustrate the point we take some learning tasks from the machine learning and ilp literature and represent them in escher a typed higher order functional logic programming language being developed at the university of bristol we argue that the use of a type system provides better ways to discard meaningless hypotheses on syntactic grounds and encompasses many ad hoc approaches to declarative bias 1 motivation and scope inductive concept learning consists of finding mappings of individuals or objects a first order approach to unsupervised learning this paper deals with learning first order logic rules from data lacking an explicit classification predicate consequently the learned rules are not restricted to predicate definitions as in supervised inductive logic programming first order logic offers the ability to deal with structured multi relational knowledge possible applications include first order knowledge discovery induction of integrity constraints in databases multiple predicate learning and learning mixed theories of predicate definitions and integrity constraints one of the contributions of our work is a heuristic measure of confirmation trading off satisfaction and novelty of the rule the approach has been implemented in the tertius system the system performs an optimal best first search finding the k most confirmed hypotheses it can be tuned to many different domains by setting its parameters and it can deal either with individual based representations as in propositional learning or with general logi agents with complex plans design and implementation of casa we describe the design of casa an agent specification language that builds on the formal agent specification approach agentspeak l and extends it by concepts from concurrent logic programming with casa it is possible to design agents with complex behavior patterns like speculative computations and parallel executed strategies the design of multi agent systems composed of casa agents is supported by providing predefined message structures and integrating an existing agent communication framework efficient identification of web communities we dene a community on the web as a set of sites that have more links in either direction to members of the community than to non members members of such a community can be eciently identied in a maximum ow minimum cut framework where the source is composed of known members and the sink consists of well known non members a focused crawler that crawls to a xed depth can approximate community membership by augmenting the graph induced by the crawl with links to a virtual sink node the effectiveness of the approximation algorithm is demonstrated with several crawl results that identify hubs authorities web rings and other link topologies that are useful but not easily categorized applications of our approach include focused crawlers and search engines automatic population of portal categories and improved ltering categories and subject descriptors h 2 8 database management database applications data mining h 3 3 information storage and retrieval information towards uml based analysis and design of multi agent systems the visual modeling facilities of the uml do not provide sufficient means to support the design of multi agent systems in this paper we are investigating the development phases of requirements analysis design and code generation for multi agent systems in the requirements analysis phase we are using extended use case diagrams to identify agents and their relationship to the environment in the design phase we are using stereotyped class and object diagrams to model different agent types and their related goals and strategies while these diagrams define the static agent system architecture dynamic agent behavior is modeled in statecharts with respect to the bdi 1 agent approach concerning code generation we show how the used diagrams can be taken to generate code for casa our executable agent specification language that is integrated into an existing multi agent framework 1 extracting query modifications from nonlinear svms when searching the www users often desire results restricted to a particular document category ideally a user would be able to filter results with a text classifier to minimize false positive results however current search engines allow only simple query modifications to automate the process of generating effective query modifications we introduce a sensitivity analysis based method for extracting rules from nonlinear support vector machines the proposed method allows the user to specify a desired precision while attempting to maximize the recall our method performs several levels of dimensionality reduction and is vastly faster than searching the combination feature space moreover it is very effective on real world data self organization and identification of web communities the vast improvement in information access is not the only advantage resulting from the increasing percentage of hyperlinked human knowledge available on the web additionally much potential exists for analyzing interests and relationships within science and society however the web s decentralized and unorganized nature hampers content analysis millions of individuals operating independently and having a variety of backgrounds knowledge goals and cultures author the information on the web despite the web s decentralized unorganized and heterogeneous nature our work shows that the web self organizes and its link structure allows efficient identification of communities this self organization is significant because no central authority or process governs the formation and structure of hyperlinks barrier trees of degenerate landscapes the heights of energy barriers separating two macro states are useful for estimating transition frequencies in non degenerate landscapes the decomposition of a landscape into basins surrounding local minima connected by saddle points is straightforward and yields a useful definition of macro states in this work we develop a rigorous concept of barrier trees for degenerate landscapes we present a program that efficiently computes such barrier trees and apply it to two well known examples of landscapes keywords fitness landscape potential energy surface energy barrier saddle points degenerate states dedicated to peter schuster on the occasion of his 60th birthday a layered approach to nlp based information retrieval a layered approach to information retrieval permits the inclusion of multiple search engines as well as multiple databases with a natural language layer to convert english queries for use by the various search engines the nlp layer incorporates morphological analysis noun phrase syntax and semantic expansion based on wordnet 1 introduction this paper describes a layered approach to information retrieval and the natural language component that is a major element in that approach the layered approach packaged as intermezzo tm was deployed in a pre product form at a government site the nlp component has been installed with a proprietary ir engine photofile flank martin balogh and rothey 1995 flank garfield and norkin 1995 at several commercial sites including picture network international pni simon and schuster and john deere intermezzo employs an abstraction layer to permit simultaneous querying of multiple databases a user enters a query into a clien graded learning for object detection our goal is to detect all instances of a generic object class such as a face in greyscale scenes the design of the algorithm is motivated by computational efficiency the search is coarse to fine in both the exploration of poses and the representation of the object class starting from training examples we recursively learn a hierarchy of spatial arrangements of edge fragments graded by their size sparsity the arrangements have no a priori semantic or geometric interpretation instead they are selected to be decomposable each can be split into two correlated subarrangements each of which can be further divided etc as a result the probability of an arrangement of size k appearing on an object instance decays slowly with k we demonstrate this both theoretically and in experiments in which detection means finding a sufficient number of arrangements of various sizes 1 introduction starting with a training set of examples of a generic object class e g face our goal integrating keyword search into xml query processing due to the popularity of the xml data format several query languages for xml have been proposed specially devised to handle data whose structure is unknown loose or absent while these languages are rich enough to allow for querying the content and structure of an xml document a varying or unknown structure can make formulating queries a very difficult task we propose an extension to xml query languages that enables keyword search at the granularity of xml elements that helps novice users formulate queries and also yields new optimization opportunities for the query processor we present an implementation of this extension on top of a commercial rdbms we then discuss implementation choices and performance results keywords xml query processing full text index 1 introduction there is no doubt that xml is rapidly becoming one of the most important data formats it is already used for scientific data e g dna sequences in linguistics e g the treebank database at the u a performance evaluation of alternative mapping schemes for storing xml data in a relational database xml is emerging as one of the dominant data formats for data processing on the internet to query xml data query languages likexql lorel xml ql or xml gl have been proposed in this paper we study how xml data can be stored and queried using a standard relational database system for this purpose we present alternative mapping schemes to store xml data in a relational database and discuss how xml ql queries can be translated into sql queries for every mapping scheme we present the results of comprehensive performance experiments that analyze the tradeo s of the alternative mapping schemes in terms of database size query performance and update performance while our discussion is focussed on xml and xml ql the results of this paper are relevant for most semi structured data models and most query languages for semi structured data 1 introduction it has become clear that not all applications are met by the relational object relational or object oriented data models query optimization in the presence of limited access patterns 1 introduction the goal of a query optimizer of a database system is to translate a declarative query expressed on a logical schema into an imperative query execution plan that accesses the physical storage of the data and applies a sequence of relational operators in building query execution plans traditional relational query optimizers try to find the most efficient method for accessing the necessary data when possible a query optimizer will use auxiliary data structures such as an index on a file in order to efficiently retrieve a certain set of tuples in a relation however when such structures do not exist or are not useful for the given query the alternative of scanning the entire relation always exists the existence of the fall back option to perform a complete scan is an important assumption in traditional query optimization several recent query processing applications have the common characteristic that it is not always possible to perform complete scans on the data instead the query optimization problem is complicated by the fact that there are only limited access patterns to the data one such typtex inductive typological text classification by multivariate statistical analysis for nlp systems tuning evaluation the increasing use of methods in natural language processing nlp which are based on huge corpora require that the lexical morphosyntactic and syntactic homogeneity of texts be mastered we have developed a methodology and associate tools for text calibration or profiling within the elra benchmark called contribution to the construction of contemporary french corpora based on multivariate analysis of linguistic features we have integrated these tools within a modular architecture based on a generic model allowing us on the one hand flexible annotation of the corpus with the output of nlp and statistical tools and on the other hand retracing the results of these tools through the annotation layers back to the primary textual data this allows us to justify our interpretations 1 introduction natural language processing nlp is increasingly dependent on corpus based methods the availability of corpora is no longer a problem as huge and annotated corpora are now readily avail a data model and data structures for moving objects databases we consider spatio temporal databases supporting spatial objects with continuously changing position and extent termed moving objects databases we formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi component regions with holes the data model is given as a collection of data types and operations which can be plugged as attribute types into any dbms data model e g relational or object oriented to obtain a complete model and query language a particular novel concept is the sliced representation which represents a temporal development as a set of units where unit types for spatial and other data types represent certain simple functions of time we also show how the model can be mapped into concrete physical data structures in a dbms environment 1 introduction a wide and increasing range of database applications has to deal with spatial objects whose position and or extent changes over time active markov localization for mobile robots localization is the problem of determining the position of a mobile robot from sensor data most existing localization approaches are passive i e they do not exploit the opportunity to control the robot s effectors during localization this paper proposes an active localization approach the approach is based on markov localization and provides rational criteria for 1 setting the robot s motion direction exploration and 2 determining the pointing direction of the sensors so as to most efficiently localize the robot furthermore it is able to deal with noisy sensors and approximative world models the appropriateness of our approach is demonstrated empirically using a mobile robot in a structured office environment key words robot position estimation autonomous service robots 1 introduction to navigate reliably in indoor environments a mobile robot must know where it is over the last few years there has been a tremendous scientific interest in algorithms for estimating position estimation for mobile robots in dynamic environments for mobile robots to be successful they have to navigate safely in populated and dynamic environments while recent research has led to a variety of localization methods that can track robots well in static environments we still lack methods that can robustly localize mobile robots in dynamic environments in which people block the robot s sensors for extensive periods of time or the position of furniture may change this paper proposes extensions to markov localization algorithms enabling them to localize mobile robots even in densely populated environments two different filters for determining the believability of sensor readings are employed these filters are designed to detect sensor readings that are corrupted by humans or unexpected changes in the environment the technique was recently implemented and applied as part of an installation in which a mobile robot gave interactive tours to visitors of the deutsches museum bonn extensive empirical tests involving datasets recorded during peak traffic hours in the museum demonstrate that this approach is able to accurately estimate the robot s position in more than 98 of the cases even in such highly dynamic environments the gateway system uniform web based access to remote resources exploiting our experience developing the webflow system we designed the gateway system to provide seamless and secure access to computational resources at asc msrc the gateway follows our commodity components strategy and it is implemented as a modern three tier system tier 1 is a highlevel front end for visual programming steering run time data analysis and visualization that is built on top of the web and oo commodity standards distributed object based scalable and reusable web server and object broker middleware forms tier 2 back end services comprise tier 3 in particular access to high performance computational resources is provided by implementing the emerging standard for metacomputing api 1 introduction the last few years have seen the growing power and capability of commodity computing and communication technologies largely driven by commercial distributed information systems all of them can be abstracted to a three tier model with largely independent clients c sentinel a multiple engine information retrieval and visualization system we describe a prototype information retrieval system sentinel under development at harris corporation s information systems division sentinel is a fusion of multiple information retrieval technologies integrating n grams a vector space model and a neural network training rule one of the primary advantages of sentinel is its 3 dimenstional visualization capability that is based fully upon the mathematical representation of information within sentinel this 3 dimensional visualization capability provides users with an intuitive understanding with relevance feedback query refinement techniques that can be better utilized resulting in higher retrieval accuracy precision a general framework for evolving schemata support in this paper a semantic approach for the specication and the management of databases with evolving schemata is introduced it is shown how a general object oriented model for schema versioning and evolution can be formalised how the semantics of schema change operations can be dened how interesting reasoning tasks can be supported based on an encoding in description logics 1 introduction the problems of schema evolution and versioning arose in the context of long lived database applications where stored data were considered worth surviving changes in the database schema 23 according to a widely accepted terminology 19 a database supports schema evolution if it permits modications of the schema without the loss of extant data in addition it supports schema versioning if it allows the querying of all data through user denable version interfaces for the sake of brevity schema evolution can be considered as a special case of schema versioning where only the curr the i com tool for intelligent conceptual modelling in this paper we present i com a tool for intelligent conceptual modelling i com allows for the specification of multiple eer diagrams and inter and intra schema constraints complete logical reasoning is employed by the tool to verify the specification infer implicit facts and manifest any inconsistencies 1 introduction i com is a tool supporting the conceptual design phase of an information system and in particular of an integration information system such as a data warehouse the tool is an evolution of part of the conceptual modelling demonstrators suite jarke et al 2000 developed within the european esprit long term research data warehouse quality dwq project jarke et al 1999 i com adopts an extended entity relationship eer conceptual data model enriched with multidimensional aggregations and interschema constraints i com is fully integrated with a very powerful description logics reasoning server which acts as a background inference engine the co generating accurate rule sets without global optimization the two dominant schemes for rule learning c4 5 and ripper both operate in two stages first they induce an initial rule set and then they refine it using a rather complex optimization stage that discards c4 5 or adjusts ripper individual rules to make them work better together in contrast this paper shows how good rule sets can be learned one rule at a time without any need for global optimization we present an algorithm for inferring rules by repeatedly generating partial decision trees thus combining the two major paradigms for rule generation creating rules from decision trees and the separate and conquer rule learning technique the algorithm is straightforward and elegant despite this experiments on standard datasets show that it produces rule sets that are as accurate as and of similar size to those generated by c4 5 and more accurate than ripper s moreover it operates efficiently and because it avoids postprocessing does not suffer the extremely slow performance on pathological example sets for which the c4 5 method has been criticized naive bayes for regression abstract despite its simplicity the naive bayes learning scheme performs well on most classification tasks and is often significantly more accurate than more sophisticated methods although the probability estimates that it produces can be inaccurate it often assigns maximum probability to the correct class this suggests that its good performance might be restricted to situations where the output is categorical it is therefore interesting to see how it performs in domains where the predicted value is numeric because in this case predictions are more sensitive to inaccurate probability estimates this paper shows how to apply the naive bayes methodology to numeric prediction i e regression tasks by modeling the probability distribution of the target value with kernel density estimators and compares it to linear regression locally weighted linear regression and a method that produces model trees decision trees with linear regression functions at the leaves although we exhibit an artificial dataset for which naive bayes is the method of choice on real world datasets it is almost uniformly worse than locally weighted linear regression and model trees the comparison with linear regression depends on the error measure for one measure naive bayes performs similarly while for another it is worse we also show that standard naive bayes applied to regression problems by discretizing the target value performs similarly badly we then present empirical evidence that isolates naive bayes independence assumption as the culprit for its poor performance in the regression setting these results indicate that the simplistic statistical assumption that naive bayes makes is indeed more restrictive for regression than for classification proceedings of the 6th international workshop on deductive databases and logic the integration of concepts from logic and deduction into databases and knowledge bases has created the field of deductive databases logic programming provides a powerful declarative language for accessing and maintaining knowledge in databases techniques from relational databases and automated deduction are useful for achieving efficient retrieval and reasoning in large knowledge bases thus deductive databases can be used for building intelligent information systems the contributions in this proceedings of the sixth international workshop on deductive databases and logic programming ddlp 98 are grouped into four sessions theoretical aspects applications datalog extensions and semantics and a demo session 3 4 contents preface 7 schedule of presentations 11 theoretical aspects 13 nieves r brisaboa agustin gonzales hector j hernandez and jose r parama chasing programs in datalog 13 francois bry norbert eisinger heribert schuetz and sunna torge sic satisfiabil information extraction with hmms and shrinkage hidden markov models hmms are a powerful probabilistic tool for modeling time series data and have been applied with success to many language related tasks such as part of speech tagging speech recognition text segmentation and topic detection this paper describes the application of hmms to another language related task information extraction the problem of locating textual sub segments that answer a particular information need in our work the hmm state transition probabilities and word emission probabilities are learned from labeled training data as in many machine learning problems however the lack of su cient labeled training data hinders the reliability of the model the key contribution of this paper is the use of a statistical technique called shrinkage that signi cantly improves parameter estimation of the hmm emission probabilities in the face of sparse training data in experiments on seminar announcements and reuters acquisitions articles shrinkage is shown to r pkdd 98 tutorial on scalable high performance data mining with parallel processing contents 1 introduction 2 overview of 7 different approaches for speeding up data mining in large databases 3 an overview of parallel processing for data mining 4 parallel rule induction 5 parallel instance based learning 6 parallel genetic algorithms 7 parallel neural networks 8 conclusions introduction problem how to perform efficient data mining in very large databases natural solution parallelism performance issues any sequential data mining algorithm o n parallelism reduces this lower bound to o n p n no of tuples p no of processors cost benefit issues many data warehouses are already implemented on cost effective parallel database servers 2 overview of 7 different approaches for speeding up data mining in large databases data oriented approaches 1 sampling reduces number of tuples 2 attribute selection reduces number of attributes 3 discretization reduces number of values of attributes which in evaluating database selection techniques a testbed and experiment we describe a testbed for database selection techniques and an experiment conducted using this testbed the testbed is a decomposition of the trec tipster data that allows analysis of the data along multiple dimensions including collection based and temporal based analysis we characterize the subcollections in this testbed in terms of number of documents queries against which the documents have been evaluated for relevance and distribution of relevant documents we then present initial results from a study conducted using this testbed that examines the effectiveness of the ggloss approach to database selection the databases from our testbed were ranked using the ggloss techniques and compared to the ggloss ideal l baseline and a baseline derived from trec relevance judgements we have examined the degree to which several ggloss estimate functions approximate these baselines our initial results confirm that the ggloss estimators are excellent predictors of the ideal l ranks but effective and efficient automatic database selection we examine a class of database selection algorithms that require only document frequency information the cori algorithm is an instance of this class of algorithms in previous work we showed that cori is more effective than ggloss when evaluated against a relevance based standard in this paper we introduce a family of other algorithms in this class and examine components of these algorithms and of the cori algorithm to begin identifying the factors responsible for their performance we establish that the class of algorithms studied here is more effective and efficient than ggloss and is applicable to a wider variety of operational environments in particular this methodology is completely decoupled from the database indexing technology so is as useful in heterogeneous environments as in homogeneous environments 1 introduction database or collection selection 2 6 10 8 13 14 15 9 is a fundamental problem in distributed searching given an environment containing many dat self bounding learning algorithms most of the work which attempts to give bounds on the generalization error of the hypothesis generated by a learning algorithm is based on methods from the theory of uniform convergence these bounds are a priori bounds that hold for any distribution of examples and are calculated before any data is observed in this paper we propose a different approach for bounding the generalization error after the data has been observed a self bounding learning algorithm is an algorithm which in addition to the hypothesis that it outputs outputs a reliable upper bound on the generalization error of this hypothesis we first explore the idea in the statistical query learning framework of kearns 10 after that we give an explicit self bounding algorithm for learning algorithms that are based on local search 1 introduction most of the work on the sample complexity of learning is based on uniform convergence theory and attempts to give uniform a priori bounds a uniform a priori bound is a guar estimating mixture models of images and inferring spatial transformations using the em algorithm presented at the ieee conference on computer vision and pattern recognition ft collins co june 1999 mixture modeling and clustering algorithms are effective simple ways to represent images using a set of data centers however in situations where the images include background clutter and transformations such as translation rotation shearing and warping these methods extract data centers that include clutter and represent dierent transformations of essentially the same data taking face images as an example it would be more useful for the dierent clusters to represent dierent poses and expressions instead of cluttered versions of dierent translations scales and rotations by including clutter and transformation as unobserved latent variables in a mixture model we obtain a new transformed mixture of gaussians which is invariant to a specied set of transformations we show how a linear time em algorithm can be used to t this model by jointly estimating a mixture mo multivariate information bottleneck the information bottleneck method is an unsupervised non parametric data organization technique given a joint distribution p a b this method constructs a new variable t that extracts partitions or clusters over the values of a that are informative about b the information bottleneck has already been applied to document classification gene expression neural code and spectral analysis in this paper we introduce a general principled framework for multivariate extensions of the information bottleneck method this allows us to consider multiple systems of data partitions that are inter related our approach utilizes bayesian networks for specifying the systems of clusters and what information each captures we show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations we also present a general framework for iterative algorithms for constructing solutions and apply it to several examples learning probabilistic relational models a large portion of real world data is stored in commercial relational database systems in contrast most statistical learning methods work only with quot flat quot data representations thus to apply these methods we are forced to convert our data into a flat form thereby losing much of the relational structure present in our database this paper builds on the recent work on probabilistic relational models prms and describes how to learn them from databases prms allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects although prms are significantly more expressive than standard models such as bayesian networks we show how to extend well known statistical methods for learning bayesian networks to learn these models we describe both parameter estimation and structure learning the automatic induction of the dependency structure in a model moreover we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets we present experimental results on both real and synthetic relational databases 1 digital libraries a generic classification and evaluation scheme evaluation of digital libraries dls is essential for further development in this area whereas previous approaches were restricted to certain facets of the problem we argue that evaluation of dls should be based on a broad view of the subject area for this purpose we develop a new description scheme using four major dimensions data collection system technology users and usage for each of these dimensions we describe the major attributes using this scheme existing dl test beds can be characterised for this purpose we have performed a survey by means of a questionnaire which is now continued by setting up a dl meta library hyspirit a probabilistic inference engine for hypermedia retrieval in large databases hyspirit is a retrieval engine for hypermedia retrieval integrating concepts from information retrieval ir and deductive databases the logical view on ir models retrieval as uncertain inference for which we use probabilistic reasoning since the expressiveness of classical ir models is not sufficient for hypermedia retrieval hyspirit is based on a probabilistic version of datalog in hypermedia retrieval different nodes may contain contradictory information thus we introduce probabilistic four valued datalog in order to support fact queries as well as contentbased retrieval hyspirit is based on an open world assumption but allows for predicate specific closed world assumptions for performing efficient retrieval on large databases our system provides access to external data we demonstrate the application of hyspirit by giving examples for retrieval on images structured documents and large databases 1 introduction due to the advances in hardware processing of multimed a search engine for 3d models as the number of 3d models available on the web grows there is an increasing need for a search engine to help people find them unfortunately traditional text based search techniques are not always effective for 3d data in this paper we investigate new shape based search methods the key challenges are to develop query methods simple enough for novice users and matching algorithms robust enough to work for arbitrary polygonal models we present a web based search engine system that supports queries based on 3d sketches 2d sketches 3d models and or text keywords for the shape based queries we have developed a new matching algorithm that uses spherical harmonics to compute discriminating similarity measures without requiring repair of model degeneracies or alignment of orientations it provides 46 245 better performance than related shape matching methods during precision recall experiments and it is fast enough to return query results from a repository of 20 000 models in under a second the net result is a growing interactive index of 3d models available on the web i e a google for 3d models is machine colour constancy good enough this paper presents a negative result current machine colour constancy algorithms are not good enough for colour based object recognition this result has surprised us since we have previously used the better of these algorithms successfully to correct the colour balance of images for display colour balancing has been the typical application of colour constancy rarely has it been actually put to use in a computer vision system so our goal was to show how well the various methods would do on an obvious machine colour vision task namely object recognition although all the colour constancy methods we tested proved insufficient for the task we consider this an important finding in itself in addition we present results showing the correlation between colour constancy performance and object recognition performance and as one might expect the better the colour constancy the better the recognition rate 1 introduction we set out to show that machine colour constancy had matured to usability engineering for virtual environments through a framework of usability characteristics the goal of much work in virtual environments ves to date has been to produce innovative visual aural and haptic technology until recently there has been very little user centered usability focused research in ves however there is beginning to be at least some awareness of the need for usability engineering within the ve community mostly addressing particular parts of the ve usability space this paper motivates the need for usability engineering methods specifically for ves and describes a framework of usability characteristics for ves it gives a detailed example of use of the framework and supplemental ve usability resources in design and evaluation of a navigation metaphor for a real world battlefield visualization ve application our goal is to increase awareness of the need for ve usability through this framework which in turn will be used to produce usability engineering methods for development of ves keywords virtual environments virtual reality usability usabil information extraction beyond document retrieval in this paper we give a synoptic view of the growth text processing technology of information extraction ie whose function is to extract information about a pre specified set of entities relations or events from natural language texts and to record this information in structured representations called templates here we describe the nature of the ie task review the history of the area from its origins in ai work in the 1960 s and 70 s till the present discuss the techniques being used to carry out the task describe application areas where ie systems are or are about to be at work and conclude with a discussion of the challenges facing the area what emerges is a picture of an exciting new text processing technology with a host of new applications both on its own and in conjunction with other technologies such as information retrieval machine translation and data mining design principles for resource management systems for intelligent spaces the idea of ubiquitous computing and smart environments is no longer a dream and has long become a serious area of research and soon this technology will start entering our every day lives there are two major obstacles that prevent this technology from spreading first di erent smart spaces are equipped with very di erent kinds of devices e g a projector vs a computer monitor vs a tv set second multiple applications running in a space at the same time inevitably contend for those devices and other scarce resources the underlying software in a smart space needs to provide tools for self adaptivity in that it shields the rest of the software from the physical constraints of the space and that it dynamically adjusts the allocation of scarce resources as the number and priorities of active tasks change managing periodically updated data in relational databases a stochastic modeling approach recent trends in information management involve the periodic transcription of data onto secondary devices in a networked environment and the proper scheduling of these transcriptions is critical for efficient data management to assist in the scheduling process we are interested in modeling data obsolescence that is the reduction of consistency over time between a relation and its replica the modeling is based on techniques from the field of stochastic processes and provides several stochastic models for content evolution in the base relations of a database taking referential integrity constraints into account these models are general enough to accommodate most of the common scenarios in databases including batch insertions and life spans both with and without memory as an initial proof of concept of the applicability of our approach we validate the insertion portion of our model framework via experiments with real data feeds we also discuss a set of transcription protocols which make use of the proposed stochastic model following the paths of xml data an algebraic framework for xml query evaluation this paper introduces an algebraic framework for expressing and evaluating queries over xml data it presents the underlying assumptions of the framework describes the input and output of the algebraic operators and defines these operators and their semantics it evaluates the framework with regard to other proposed xml query algebras examples show that this framework is flexible enough to capture queries expressed in quilt one of the dominant xml query languages we have used this algebra in the context of an internet query engine in which it is used to formulate logical plans for xml ql queries we define equivalence rules that provide opportunities for optimization and give example cases that point out the usefulness of these rules 1 an extensible framework for data cleaning data integration solutions dealing with large amounts of data have been strongly required in the last few years besides the traditional data integration problems e g schema integration local to global schema mappings three additional data problems have to be dealt with 1 the absence of universal keys across dierent databases that is known as the object identity problem 2 the existence of keyboard errors in the data and 3 the presence of inconsistencies in data coming from multiple sources dealing with these problems is globally called the data cleaning process in this work we propose a framework which oers the fundamental services required by this process data transformation duplicate elimination and multi table matching these services are implemented using a set of purposely designed macro operators moreover we propose an sql extension for specifying each of the macro operators one important feature of the framework is the ability of explicitly includ improving data cleaning quality using a data lineage facility the problem of data cleaning which consists of removing inconsistencies and errors from original data sets is well known in the area of decision support systems and data warehouses however for some applications existing etl extraction transformation loading and data cleaning tools for writing data cleaning programs are insufficient one important challenge with them is the design of a data flow graph that effectively generates clean data a generalized difficulty is the lack of explanation of cleaning results and user interaction facilities to tune a data cleaning program this paper presents a solution to handle this problem by enabling users to express user interactions declaratively and tune data cleaning programs 1 the erlangen spoken dialogue system evar a state of the art information retrieval system in this paper we present an overview of the spoken dialogue system evar that was developed at the university of erlangen in january 1994 it became accessible over telephone line and could answer inquiries in the german language about german intercity train connections it has since been continuously improved and extended including some unique features such as the processing of out of vocabulary words and a flexible dialogue strategy that adapts to the quality of the recognition of the user input in fact several different versions of the system have emerged i e a subway information system train and flight information systems in different languages and an integrated multilingual and multifunctional system which covers german and 3 additional languages in parallel current research focuses on the introduction of stochastic models into the semantic analysis on the direct integration of prosodic information into the word recognition process on the detection of user emotion a a multi agent architecture for distributed corporate memories this paper presents an approach to design a multi agent system managing a corporate memory in the form of a distributed semantic web and describes the resulting architecture clustering large datasets in arbitrary metric spaces clustering partitions a collection of objects into groups called clusters such that similar objects fall into the same group similarity between objects is defined by a distance function satisfying the triangle inequality this distance function along with the collection of objects describes a distance space in a distance space the only operation possible on data objects is the computation of distance between them all scalable algorithms in the literature assume a special type of distance space namely a k dimensional vector space which allows vector operations on objects we present two scalable algorithms designed for clustering very large datasets in distance spaces our first algorithm bubble is to our knowledge the first scalable clustering algorithm for data in a distance space our second algorithm bubble fm improves upon bubble by reducing the number of calls to the distance function which may be computationally very expensive both algorithms make only a single scan ov classification with sparse grids using simplicial basis functions recently we presented a new approach 20 to the classification problem arising in data mining it is based on the regularization network approach but in contrast to other methods which employ ansatz functions associated to data points we use a grid in the usually high dimensional feature space for the minimization process to cope with the curse of dimensionality we employ sparse grids 52 thus only o h 1 n n d 1 instead of o h d n grid points and unknowns are involved here d denotes the dimension of the feature space and hn 2 n gives the mesh size we use the sparse grid combination technique 30 where the classification problem is discretized and solved on a sequence of conventional grids with uniform mesh sizes in each dimension the sparse grid solution is then obtained by linear combination the method computes a nonlinear classifier but scales only linearly with the number of data points and is well suited for data mining applications where the amount of data is very large but where the dimension of the feature space is moderately high in contrast to our former work where d linear functions were used we now apply linear basis functions based on a simplicial discretization this allows to handle more dimensions and the algorithm needs less operations per data point we further extend the method to so called anisotropic sparse grids where now different a priori chosen mesh sizes can be used for the discretization of each attribute this can improve the run time of the method and the approximation results in the case of data sets with different importance of the attributes we describe the sparse grid combination technique for the classification problem give implementational details and discuss the complexity of the algorithm it turns out that the method scales linearly with the number of given data points finally we report on the quality of the classifier built by our new method on data sets with up to 14 dimensions we show that our new method achieves correctness rates which are competitive to those of the best existing methods learning task models for collagen for an application independent collaborative tool a key step is to develop a detailed task model for a particular domain this is a time consuming and dicult task and seems to require a fairly advanced knowledge of ai representations for plans goals and recipes this paper discusses some preliminary ideas for making it easier to construct and evolve task models either through interaction with a human domain expert through machine learning or in a mixed initiative system introduction an important trend in recent work on human computer interaction and user modeling has been to view humancomputer interaction as a kind of collaboration in this approach the human user and the computer often personied as an agent coordinate their actions toward achieving shared goals collagen is an application independent collaboration manager based on the sharedplan theory of task oriented collaborative discourse grosz sidner 1990 rich sidner 1998 lesh rich sidner 199 reinforcement learning for visual servoing of a mobile robot a novel reinforcement learning algorithm is applied to a visual servoing task on a real mobile robot there is no requirement for camera calibration an actuator model or a knowledgeable teacher the controller learns from a critic which gives a scalar reward the learning algorithm handles continuously valued states and actions and can learn from good and bad experiences including data gathered while performing unrelated behaviours and from historical data experimental results are presented 1 introduction visual servoing consists of moving some part of a robot to a desired position using visual feedback hutchinson et al 1996 it is a basic building block for purposeful robot behaviours such as foraging target pursuit and landmark based navigation some degree of calibration is generally required to achieve visual servoing this calibration can be a time consuming and error prone process in this work we show that reinforcement based learning can eliminate the ca neurobotics lab research learning vision and sonar recognition with mobile robots this article provides an overview of research projects undertaken in the neurobotics laboratory at boston university we focus on applications of neural networks and other biomimetic techniques in sensory processing navigation and other tasks using mobile robots these applications share some central themes the inclusion of minimal assumptions about the robots and the environment cross validation of modules on a variety of robotics platforms and environments and real time operation using real robots keywords mobile robots looming mobile robots robot learning neural networks artmap sensor fusion 1 introduction the neurobotics laboratory was founded in 1996 with the goal of applying neural networks and other biomimetic techniques to the control and guidance of wheeled mobile robot research in the lab covers various problems in the general area of autonomous mobile robotics with an emphasis on navigation and control using biomimetic algorithms that operate in real time wi agents supporting information integration the miks framework during past years we have developed the momis mediator environment for multiple information sources system for the integration of data from structured and semi structured data sources in this paper we propose a new system miks mediator agent for integration of knowledge sources which enriches the momis architecture exploiting the intelligent and mobile agent features 1 motivation the web explosion both at internet and intranet level has transformed the electronic information system from single isolated node to an entry point into a worldwide network of information exchange and business transactions one of the main challenges for the designers of the e commerce infrastructures is the information sharing retrieving data located in different sources thus obtaining an integrated view to overcome any contradiction or redundancy during past years we have developed the momis mediator environment for multiple information sources system for the integration of data from struc adding some smartness to devices and everyday things in mobile computing context awareness indicates the ability of a system to obtain and use information on aspects of the system environment to implement contextawareness mobile system components have to be augmented with the ability to capture aspects of their environment recent work has mostly considered locationawareness and hence augmentation of mobile artifacts with locality in this paper we discuss augmentation of mobile artifacts with diverse sets of sensors and perception techniques for awareness of context beyond location we report experience from two projects one on augmentation of mobile phones with awareness technologies and the other on embedding of awareness technology in everyday non digital artifacts fast file access for fast agents mobile agents are a powerful tool for coordinating general purpose distributed computing where the main goal is high performance in this paper we demonstrate how the inherent mobility of agents may be exploited to achieve fast file access which is necessary for most general purpose applications we present a file system for mobile agents based exclusively on local disks of the participating workstations the mobility of agents allows us to make all file operations local which significantly reduces access time we also demonstrate how code files and special system files can be handled efficiently in a localdisk based environment 1 the belief desire intention model of agency introduction within the atal community the belief desire intention bdi model has come to be possibly the best known and best studied model of practical reasoning agents there are several reasons for its success but perhaps the most compelling are that the bdi model combines a respectable philosophical model of human practical reasoning originally developed by michael bratman 1 a number of implementations in the irma architecture 2 and the various prs like systems currently available 7 several successful applications including the now famous fault diagnosis system for the space shuttle as well as factory process control systems and business process management 8 and finally an elegant abstract logical semantics which have been taken up and elaborated upon widely within the agent research community 14 16 however it could be argued that the bdi model is now becoming somewhat dated the principles of the architecture were established in the mid 1980s virpi a high level toolkit for interactive scientific visualization in virtual reality research areas that require interactive visualization of simulation data data mining on symbolic knowledge extracted from the web information extractors and classifiers operating on unrestricted unstructured texts are an errorful source of large amounts of potentially useful information especially when combined with a crawler which automatically augments the knowledge base from the world wide web at the same time there is much structured information on the world wide web wrapping the web sites which provide this kind of information provide us with a second source of information possibly less up to date but reliable as facts we give a case study of combining information from these two kinds of sources in the context of learning facts about companies we provide results of association rules propositional and relational learning which demonstrate that data mining can help us improve our extractors and that using information from two kinds of sources improves the reliability of data mined rules 1 introduction the world wide web has become a significant source of information most of this computer retri using error correcting codes for text classification this paper explores in detail the use of error correcting output coding ecoc for learning text classifiers we show that the accuracy of a naive bayes classifier over text classification tasks can be significantly improved by taking advantage of the error correcting properties of the code we also explore the use of different kinds of codes namely error correcting codes random codes and domain and data specific codes and give experimental results for each of them the ecoc method scales well to large data sets with a large number of classes experiments on a real world data set show a reduction in classification error by up to 66 over the traditional naive bayes classifier we also compare our empirical results to semitheoretical results and find that the two closely agree 1 introduction text classification is the problem of grouping text documents into classes or categories for the purpose of this paper we define classification as categorizing documents in combining labeled and unlabeled data for text classification with a large number of categories a major concern with supervised learning techniques for text classification is that they often require a large number of labeled examples to learn accurately one way to reduce the amount of labeled data required is to develop algorithms that can learn effectively from a small number of labeled examples augmented with a large number of unlabeled examples in this paper we develop a framework to incorporate unlabeled data in the error correcting output coding ecoc setup by decomposing multiclass problems into multiple binary problems and then use co training to learn the individual binary classification problems we show that our method is especially useful for classification tasks involving a large number of categories where co training doesn t perform very well by itself and when combined with ecoc outperforms several other algorithms that combine labeled and unlabeled data for text classification 1 using error correcting codes for efficient text classification with a large number of categories we investigate the use of error correcting output codes ecoc for efficient text classification with a large number of categories and propose several extensions which improve the performance of ecoc ecoc has been shown to perform well for classification tasks including text classification but it still remains an under explored area in ensemble learning algorithms we explore the use of error correcting codes that are short minimizing computational cost but result in highly accurate classifiers for several real world text classification problems our results also show that ecoc is particularly effective for highprecision classification in addition we develop modifications and improvements to make ecoc more accurate such as intelligently assigning codewords to categories according to their confusability and learning the decoding combining the decisions of the individual classifiers in order to adapt to different datasets to reduce the need for labeled training data we develop a framework for ecoc where unlabeled data can be used to improve classification accuracy this research will impact any area where efficient classification of documents is useful such as web portals information filtering and routing especially in open domain applications where the number of categories is usually very large and new documents and categories are being constantly added and the system needs to be very efficient continuous time hierarchical reinforcement learning hierarchical reinforcement learning rl is a general framework which studies how to exploit the structure of actions and tasks to accelerate policy learning in large domains prior work in hierarchical rl such as the maxq method has been limited to the discrete time discounted reward semi markov decision process smdp model this paper generalizes the maxq method to continuous time discounted and average reward smdp models we describe two hierarchical reinforcement learning algorithms continuous time discounted reward maxq and continuous time average reward maxq we apply these algorithms to a complex multiagent agv scheduling problem and compare their performance and speed with each other as well as several well known agv scheduling heuristics 1 agent interoperation across multagent system boundaries recently the number of autonomous agents and multiagent systems mas that have been developed by different developers has increased despite efforts for the creation of standards eg in communication languages registration protocols etc it is clear that at least in the near term heterogeneous agents and mass will be prevalent therefore mechanisms that allow agents and or mass to interoperate and transact are needed in this paper we report on a case study and lessons learned of an interoperator agent we developed we discuss requirements for interoperation mechanisms resulting challenges and our design decisions and implementation of the retsina oaa interoperator scalability issues for query routing service discovery in this paper we discuss the relationship between mediatorbased systems for service discovery in multi agent systems and the technique of query routing used for resource discovery in distributed information systems we then construct a model of the query routing task which we use to examine the complexity and scalability characteristics of a number of commonly encountered architectures for resource or service discovery data stream quality towards parallel and perhaps the single most critical factor that limits today s utility of knowledge discovery in databases kdd is scalability scalability is the main consequence of ever increasing collections of mass data whereas data mining algorithms impose limits by their inherent computational complexity one line of attack is to reduce the amount of data during preprocessing that is by exploring the raw data for gib steps toward an expert level bridge playing program this paper describes goren in a box gib the first bridge playing program to approach the level of a human expert we give a basic overview of the algorithms used describe their strengths and weaknesses and present the results of experiments comparing gib to both human opponents and earlier programs introduction of all the classic games of skill only card games and go have yet to see the appearance of serious computer challengers in go this appears to be because the game is fundamentally one of pattern recognition as opposed to search the brute force techniques that have been so successful in the development of chess playing programs have failed almost utterly to deal with go s huge branching factor indeed the arguably strongest go program in the world was beaten by janice kim in the aaai 97 hall of champions after kim had given the program a monumental 25 stone handicap card games appear to be different perhaps because they are games of imperfect information or perhaps annotate a web based knowledge management support system for document collections difficulties with web based full text information retrieval ir systems include spurious matches manually intensive document sifting and the absence of communication or coordination between users furthermore it is not clear how to best design a computer supported collaborative work cscw system which possesses coordination mechanisms in the inherently stateless web environment document collections in an intranet are a particularly interesting problem electronic documents contain the potential for improving information throughput and knowledge transfer in organizations the emergence of intranet infrastructure creates new ir demands as all users can easily publish documents online the associated problems include inefficient discovery and proliferation of documents regardless of quality which in turn implies the mismanagement of the authors expertise this thesis investigates how to design an ir system to support km in organizations i propose an architecture and implement a n multi agent architectures as organizational structures a multi agent system mas is an organization of coordinated autonomous agents that interact in order to achieve particular possible common goals considering real world organizations as an analogy this paper proposes architectural styles for mas which adopt concepts from organizational theories the styles are modeled using the i framework which o ers the notions of actor goal and actor dependency and specified in formal tropos they are evaluated with respect to a set of software quality attributes such as predictability or adaptability in addition we conduct a comparative study of organizational and conventional software architectures using the mobile robot control example from the software engineering literature the research is conducted in the context of tropos a comprehensive software system development methodology beyond predictive accuracy what this paper presents a number of such criteria and discusses the impact they have on meta level approaches to model selection development and characterization of an acoustic rangefinder localization is important to wearable and embedded applications at many levels for example it might be used to implement physical user interfaces which detect body language or the user s positioning of objects in the environment this paper presents the initial results of a prototype implementation of an acoustic rangender unlike other published work in this area this design uses a sliding correlator to detect the acoustic signal this technique signicantly improves performance in obstructed and noisy environments ongoing work towards a more compact implementation and towards new protocols for establishing coordinate systems is also described 1 introduction when the members of our research group are brainstorming applications for our soon to be indispensible ad hoc networked sensor technology we often run into the same brick wall how do we do anything that is physically motivated and context aware without localization virtually every application we propose degenerates t the tropos software development methodology processes models and diagrams abstract tropos is a novel agent oriented software development methodology founded on two key features i the notions of agent goal plan and various other knowledge level concepts are fundamental primitives used uniformly throughout the software development process and ii a crucial role is assigned to requirements analysis and specification when the system to be is analyzed with respect to its intended environment this paper provides a first detailed account of the tropos methodology in particular we describe the basic concepts on which tropos is founded and the types of models one builds out of them we also specify the analysis process through which design flows from external to system actors through a goal analysis and delegation in addition we provide an abstract syntax for tropos diagrams and other linguistic constructs 1 community search assistant this paper describes a new software agent the community search assistant which recommends related searches to users of search engines the community search assistant enables communities of users to search in a collaborative fashion all queries submitted by the community are stored in the form of a graph links are made between queries that are found to be related users can peruse the network of related queries in an ordered way following a path from a first cousin to a second cousin to a third cousin etc to a set of search results the first key idea behind the use of query graphs is that the determination of relatedness depends on the documents returned by the queries not on the actual terms in the queries themselves the second key idea is that the construction of the query graph transforms single user usage of information networks e g search into collaborative usage all users can tap into the knowledge base of queries submitted by others introduction on case based learnability of languages case based reasoning is deemed an important technology to alleviate the bottleneck of knowledge acquisition in artificial intelligence ai in case based reasoning knowledge is represented in the form of particular cases with an appropriate similarity measure rather than any form of rules the case based reasoning paradigm adopts the view that an ai system is dynamically changing during its life cycle which immediately leads to learning considerations within the present paper we investigate the problem of case based learning of indexable classes of formal languages prior to learning considerations we study the problem of case based representability and show that every indexable class is case based representable with respect to a fixed similarity measure next we investigate several models of case based learning and systematically analyze their strengths as well as their limitations finally the general approach to case based learnability of indexable classes of form learning in case based classification algorithms while symbolic learning approaches encode the knowledge provided by the presentation of the cases explicitly into a symbolic representation of the concept e g formulas rules or decision trees case based approaches describe learned concepts implicitly by a pair cb d i e by a set cb of cases and a distance measure d given the same information symbolic as well as the case based approach compute a classification when a new case is presented this poses the question if there are any differences concerning the learning power of the two approaches in this work we will study the relationship between the case base the measure of distance and the target concept of the learning process to do so we transform a simple symbolic learning algorithm the version space algorithm into an equivalent case based variant the achieved results strengthen the conjecture of the equivalence of the learning power of symbolic and casebased methods and show the interdependency between the measure web search your way we describe a metasearch engine architecture in use at nec research institute that allows users to provide preferences in the form of an information need category this extra information is used to direct the search process providing more valuable results than by considering only the query using our architecture identical keyword queries may be sent to different search engines and results may be scored differently for different users improving category specific web search by learning query modifications a user searching for documents within a specific category using a general purpose search engine might have a difficult time finding valuable documents to improve category specific search we show that a trained classifier can recognize pages of a specified category with high precision by using textual content text location and html structure we show that query modifications to web search engines increase the probability that the documents returned are of the specific category we evaluate the effectiveness of several query modifications on real search engines showing that the approach is highly effective for locating personal homepages and calls for papers 1 introduction typical web search engines index millions of pages across a variety of categories and return results ranked by expected topical relevance only a small percentage of these pages may be of a specific category for example personal homepages or calls for papers a user may examine large numbers of pages abou using web structure for classifying and describing web pages the structure of the web is increasingly being used to improve organization search and analysis of information on the web for example google uses the text in citing documents documents that link to the target document for search we analyze the relative utility of document text and the text in citing documents near the citation for classification and description results show that the text in citing documents when available often has greater discriminative and descriptive power than the text in the target document itself the combination of evidence from a document and citing documents can improve on either information source alone moreover by ranking words and phrases in the citing documents according to expected entropy loss we are able to accurately name clusters of web pages even with very few positive examples our results confirm quantify and extend previous research using web structure in these areas introducing new methods for classification and description of pages architecture of a metasearch engine that supports user information needs when a query is submitted to a metasearch engine decisions are made with respect to the underlying search engines to be used what modifications will be made to the query and how to score the results these decisions are typically made by considering only the user s keyword query neglecting the larger information need users with specific needs such as research papers or homepages are not able to express these needs in a way that affects the decisions made by the metasearch engine in this paper we describe a metasearch engine architecture that considers the user s information need for each decision users with different needs but the same keyword query may search different sub search engines have different modifications made to their query and have results ordered differently our architecture combines several powerful approaches together in a single general purpose metasearch engine 1 introduction current metasearch engines make several decisions on behalf of the use recommending web documents based on user preferences making recommendations requires treating users as individuals in this paper we describe a metasearch engine available at nec research institute that allows individual search strategies to be used each search strategy consists of a different set of sources different query modification rules and a personalized ordering policy we combine these three features with a dynamic interface that allows users to see the current best recommendations displayed at all times and allows results to be displayed immediately upon retrieval we present several examples where a single query produces different results ordered based on different factors accomplished without the use of training or a local database rewriting conjunctive queries using views in description logics with existential restrictions this paper extending the work of 8 we study the problem of rewriting conjunctive queries over dl expressions into conjunctive queries using a set of views that are a set of distinguished dl expressions for three dls allowing existential restrictions fle ale and alne thus our rewriting problem is given a conjunctive query over expressions from a dl l 2 ffle ale alneg and a set of views v over expressions from l we want to compute a representative set of all the rewritings of the query that are conjunctive queries over v by representative set we mean that this set contains at least the rewritings that are maximally contained in the query document classification as an internet service choosing the best classifier this project investigates some of the issues involved in a new proposal for expanding the scope of the field of data mining by providing mining models as services on the internet this idea can widely increase the reach and accessibility of data mining to common people because one of the primary stumbling blocks in the adoption of mining is the extremely high level of expertise and data resources needed in building a robust mining model we feel this task should be left to the specialists with access to data and resources who can provide their most up to date model as a service on the internet for public use minimization in cooperative response to failing database queries when a query fails it is more cooperative to identify the cause of failure rather than just to report the empty answer set if there is not a cause for the query s failure it is worthwhile to report the part of the query which failed to identify a minimal failing subquery mfs of the query is the best way to do this this mfs is not unique there may be many of them likewise to identify a maximal succeeding subquery mss can help a user to recast a new query that leads to a non empty answer set database systems do not provide the functionality of these types of cooperative responses this may be in part because algorithmic approaches to finding the mfss and the msss to a failing query are not obvious the search space of subqueries is large despite work on mfss in the past the algorithmic complexity of these identification problems had remained uncharted this paper shows the complexity profile of mfs and mss identification it is shown that there exists a simple algorit integrity constraints semantics and applications this paper finally by having ics expressed in logic one can use deduction and logic as one s basic and natural tools for handling and reasoning over database specifications facts rules queries and ics this applies for relational databases with or without rules and can be applied to object oriented and object relational databases as well there is a broad body of work on logic and relational databases and a general consensus on what databases facts and rules and queries mean however there is less work on the meaning of integrity constraints and certainly no consensus what is meant by an ic can differ widely for instance one may define that ics must be consistent with the database or define that they must be provable statements deducible from the database this distinction is discussed in more detail in section 3 2 another view is that ics really represent meta knowledge knowledge about the database itself and should perhaps be written in a nonclassical logic the general situation becomes more complex when we permit databases to contain indefinite or disjunctive information or to use negation as with ic 2 above subtle but profound differences in meaning can arise due to different interpretations of ics in many systems the semantics for ics is never made clear at times one interpretation seems intended while at other times another interpretation is evident this ambiguity is dangerous and could allow a database to become corrupt in unanticipated ways in this chapter we introduce integrity constraints in a logical framework and overview the various work that has been done on ics in this context section 2 presents the basic definitions for databases and integrity constraints for the chapter we consider various semantics of ics automatically labeling web pages based on normal user actions for agents attempting to learn a user s interests the cost of obtaining labeled training instances is prohibitive because the user must directly label each training instance and few users are willing to do so we present an approach that circumvents the need for human labeled pages instead we learn surrogate tasks where the desired output is easily measured such as the number of hyperlinks clicked on a page or the amount of scrolling performed our assumption is that these outputs will highly correlate with the user s interests in other words by unobtrusively observing the user s behavior we are able to learn functions of value for example an agent could silently observe the user s browser behavior during the day then use these training examples to learn such functions and gather during the middle of the night pages that are likely to be of interest to the user previous work has focused on learning a user profile by passively observing the hyperlinks clicked on and tho an overview of the tatami project this paper describes the tatami project at ucsd which is developing a system to support distributed cooperative software development over the web and in particular the validation of concurrent distributed software the main components of our current prototype are a proof assistant a generator for documentation websites a database an equational proof engine and a communication protocol to support distributed cooperative work we believe behavioral specification and verification are important for software development and for this purpose we use first order hidden logic with equational atoms the paper also briefly describes some novel user interface design methods that have been developed and applied in the project social and semiotic analyses for theorem prover user interface design we describe an approach to user interface design based on ideas from cognitive science social science especially the theory of stories and a new area tentatively called algebraic semiotics social analysis helps to identify coherent classes of users and their requirements and suggests some ways to make proofs more understandable while algebraic semiotics which combines semiotics with algebraic specification provides a rigorous theory of interface functionality and quality we apply these techniques to designing user interfaces for a distributed cooperative theorem proving system whose main component is a website generation and proof assistance tool called kumo this interface integrates formal proving proof browsing animation informal explanation and online background tutorials experience with using the interface is reported and some conclusions are drawn 1 introduction recent large advances in performance have made it arguable that the most pressing open problems in eigentaste a constant time collaborative filtering algorithm abstract eigentaste is a collaborative filtering algorithm that uses universal queries to elicit real valued user ratings on a common set of items and applies principal component analysis pca to the resulting dense subset of the ratings matrix pca facilitates dimensionality reduction for offline clustering of users and rapid computation of recommendations for a database of n users standard nearest neighbor techniques require o n processing time to compute recommendations whereas eigentaste requires o 1 constant time we compare eigentaste to alternative algorithms using data from jester an online joke recommending system jester has collected approximately 2 500 000 ratings from 57 000 users we use the normalized mean absolute error nmae measure to compare performance of different algorithms in the appendix we use uniform and normal distribution models to derive analytic estimates of nmae when predictions are random on the jester dataset eigentaste computes recommendations two orders of magnitude faster with no loss of accuracy jester is coordinating mobile robot group behavior using a model of interaction dynamics in this paper we show how various levels of coordinated behavior may be achieved in a group of mobile robots by using a model of the interaction dynamics between a robot and its environment we present augmented markov models amms as a tool for capturing such interaction dynamics on line and in real time with little computational and storage overhead we begin by describing the structure of amms and the algorithm for generating them then verify the approach utilizing data from physical mobile robots performing elements of a foraging task finally we demonstrate the application of the model for resolving group coordination issues arising from three sources individual performance group affiliation and group performance corresponding respectively to these are the three experimental examples we present fault detection group membership based on ability and experience and dynamic leader selection 1 introduction learning models of the environment other robots and interact genetic and evolutionary algorithms in the real world introduction since 1992 i have made regular trips to japan to give talks about genetic algorithms gas search procedures based on the mechanics of natural selection and genetics back during my first visit the use of genetic and evolutionary algorithms geas was restricted to a relatively small cadre of devoted specialists today japanese researchers and practitioners are ably advancing the state of gea art and application across a broad front around the globe from traditional and cutting edge optimization in engineering and operations research to such non traditional areas as drug design financial prediction data mining and the composition of poetry and music geas are grabbing attention and solving problems across a broad spectrum of human endeavor of course science and technology go through fads and fashions much like those of apparel food and toys and many practitioners in japan and elsewhere are wondering whether geas like so many methods that have c optimizing global local search hybrids this paper develops a framework for optimizing global local hybrids of search or optimization procedures the paper starts by idealizing the search problem as a search by a global algorithm g for either 1 acceptable targets solutions that meet a specified criterion or for 2 basins of attraction that then lead to acceptable targets under a specified local search algorithm l the paper continues by abstracting two sets of parameters probabilities of successfully hitting targets and basins and time to criterion coefficients with these parameters equations may be written to account for the total time of search and for the probabilistic success reliability in reaching an acceptable solution thereafter optimization problems are formulated in which the division of local versus global search time is optimized so that solution time to acceptable reliability is minimized or reliability under specified solution time is maximized a two basin optimality criterion is derived and appl enhancing supervised learning with unlabeled data in many practical learning scenarios there is a small amount of labeled data along with a large pool of unlabeled data many supervised learning algorithms have been developed and extensively studied we present a new co training strategy for using unlabeled data to improve the performance of standard supervised learning algorithms unlike much of the prior work such as the co training procedure of blum and mitchell 1998 we do not assume there are two redundant views both of which are sufficient for perfect classification the only requirement our co training strategy places on each supervised learning algorithm is that its hypothesis partitions the example space into a set of equivalence classes e g for a decision tree each leaf defines an equivalence class we evaluate our co training strategy via experiments using data from the uci repository 1 introduction in many practical learning scenarios there is a small amount of labeled data along with a lar interactive query and search in semistructured databases semistructured graph based databases have been proposed as well suited stores for world wide web data yet so far languages for querying such data are too complex for casual web users further proposed query approaches do not take advantage of the interactivity of typical web sessions users are proficient at iteratively refining their web explorations in this paper we propose a new model for interactively querying and searching semistructured databases users can begin with a simple keyword search dynamically browse the structure of the result and then submit further refining queries enabling this model exposes new requirements of a semistructured database that are not apparent under traditional database uses we demonstrate the importance of efficient keyword search structural summaries of query results and support for inverse pointers we also describe some preliminary solutions to these technical issues 1 introduction querying the web has understandably gathered much att creating and evaluating multi document sentence extract summaries this paper discusses passage extraction approaches to multidocument summarization that use available information about the document set as a whole and the relationships between the documents to build on single document summarization methodology multi document summarization differs from single in that the issues of compression speed redundancy and passage selection are critical in the formation of useful summaries as well as the user s goals in creating the summary our approach addresses these issues by using domain independent techniques based mainly on fast statistical processing a metric for reducing redundancy and maximizing diversity in the selected passages and a modular framework to allow easy parameterization for different genres corpora characteristics and user requirements we examined howhumans create multi document summaries as well as the characteristics of such summaries and use these summaries to evaluate the performance of various multidocument summarization algorithms 1 automatic text summarization of multiple documents scientists have retrieved what appear to be normal human eggs from human ovarian tissue that was grafted onto research mice this is the first research group to obtain mature potentially fertilizable eggs results of the research are being presented today at the conference of the european society of human reproduction and embryology a report published last year demonstrated that ovarian tissue which was frozen and then replaced into a woman s body resulted in ovulation and menstruation such methods are being considered for women being treated for cancer with methods that would severely diminish or destroy their reproductive chances however there is concern that the retransplanted tissue might contain cancer cells the current study proposes to reduce that risk this is yet another step toward enabling women to freeze ovarian tissue in their early 20 s when it is generally most productive to delay reproduction until their later years a connectionist approach for learning search control heuristics for automated deduction systems the central problem in automated deduction is the explosive growth of search spaces when proof length increases in this paper a connectionist approach for learning search control heuristics for automated deduction systems is presented in particular we show how folding architecture networks a new type of neural networks capable of solving supervised learning tasks on structured data can be used for learning heuristics evaluation functions for algebraic logical expressions and how these evaluation functions can then be used to control the search process for new proof problems experimental results with the automated deduction system setheo in an algebraic domain show a considerable performance improvement controlled by heuristics which had been learned from simple problems in this domain the system is able to solve several problems from the same domain which had been out of reach for the original system 1 introduction the goal in automated deduction ad is to automatically feature extraction and learning vector quantization for data structures during the last years folding architecture networks and the closely related concept of recursive neural networks have been developed for solving supervised learning tasks on data structures in this paper we address the fundamental problem of finding fixedlength vector representations for structures in an unsupervised way a solution based on ideas from feature extraction and folding architecture networks is proposed furthermore a new method for supervised learning for data structures which combines ideas from learning vector quantization and folding architecture networks is suggested 1 introduction in almost all fields of scientific and technical reasoning people and systems assisting them have to deal with structured objects examples are chemical structures algebraic mathematical expressions and formulas software source code and conceptual and taxonomic graphs with structured objects we mean objects which are composed of smaller objects which may be structured too t versus a web repository this paper we consider a web application or simply an application as a versus client with the ability of executing a task through parallel data processing therefore each application should be composed by a group of independent threads information agents on the move a survey on load balancing with mobile agents information agents process and integrate heterogeneous distributed information to achieve this task efficiently some researchers promote the idea of mobile information agents 13 53 44 20 10 which migrate between a user s host and other hosts in the network we outline the concepts behind mobile information agents and give a survey on load balancing which aims to optimise distributed information processing combining collaborative filtering with personal agents for better recommendations information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile information filtering if focuses on the analysis of item content and the development of a personal user interest profile collaborative filtering cf focuses on identification of other users with similar tastes and the use of their opinions to recommend items each technique has advantages and limitations that suggest that the two could be beneficially combined this paper shows that a cf framework can be used to combine personal if agents and the opinions of a community of users to produce better recommendations than either agents or users can produce alone it also shows that using cf to create a personal combination of a set of agents produces better results than either individual agents or other combination mechanisms one key implication of these results is that users can avoid having to select among ag on representation and approximation of operations in boolean algebras several universal approximation and universal representation results are known for non boolean multi valued logics such as fuzzy logics in this paper we show that similar results can be proven for multi valued boolean logics as well introduction the study of boolean algebras started when g boole 1 considered the simplest boolean algebra the two valued algebra b 2 f0 1g ffalse trueg in this algebra the operation and negation a 0 a have the direct logical meaning of or and and not it is known that in this boolean algebra an arbitrary operation i e an arbitrary function b theta theta b b can be represented as a superposition of these three basic logical operations e g the implication a b can be represented as b a etc logic is still one of the area of application of boolean algebras but starting from the classical kolmogorov s monograph 4 boolean algebras namely algebras of events became an important tool i language model adaptation 15 attempt to exploit longer distance dependencies infer some notion of topic from text compute topic dependent probability 8th elsnet summer school 2 language model adaptation 26 july 2000 adaptive language modelling stage 1 automatic derivation of topic information from text ffl loose definition of document a unit of spoken or written data of a certain length that contains some topic s or content s ffl topic of a document long distance or document wide statistics ffl information retrieval ir bag of words model based on a histogram of weighted unigram frequencies stage 2 combination of global and topic dependent text statistics ffl mixture ffl maximum entropy modelling ref jelinek 1997 mixtur topic based mixture language modelling this paper describes an approach for constructing a mixture of language models based on simple statistical notions of semantics using probabilistic models developed for information retrieval the approach encapsulates corpus derived semantic information and is able to model varying styles of text using such information the corpus texts are clustered in an unsupervised manner and a mixture of topic specific language models is automatically created the principal contribution of this work is to characterise the document space resulting from information retrieval techniques and to demonstrate the approach for mixture language modelling a comparison is made between manual and automatic clustering in order to elucidate how the global content information is expressed in the space we also compare in terms of association with manual clustering and language modelling accuracy alternative term weighting schemes and the effect of singular value decomposition dimension reduction monadic queries over tree structured data monadic query languages over trees currently receive considerable interest in the database community as the problem of selecting nodes from a tree is the most basic and widespread database query problem in the context of xml partly a survey of recent work done by the authors and their group on logical query languages for this problem and their expressiveness this paper provides a number of new results related to the complexity of such languages over so called axis relations such as child or descendant which are motivated by their presence in the xpath standard or by their utility for data extraction wrapping end of first year report displays of information graphs plots etc are a recent invention at around 1750 1800 15 andrews defines information visualisation as the visual presentation of information spaces and structures to facilitate their rapid assimilation and understanding 149 in the same document the authors give a collection of information visualisation pointers references also available at http www iicm edu hci ivis a more complete on line document for information visualisation from andrews is available at http www iicm edu hci ivis node2 htm a report on three dimensional information visualisation is given by young also available on line at http www dur ac uk dcs3py pages work documents litsurvey iv survey 150 this report gives a visualisation techniques enumeration and a survey of research visualisation systems two other web resources for information visualisation are olive online of information visualisation environments http otal umd edu olive and the cs348 course a technological related discussion on the potential of change in education learning and training this is a poster prepared based on the presentation material for a paper with the same name presented at the conference the goal is to discuss the supporting role of information communication technology ict in education activities and puts in context the impact that cscw systems can have both in open and distance learning and in general education learning and training the netlab concept is presented and used to support the paper positions and serves as the base to propose a roadmap to a virtual university setting 1 presentation context at the end of the century education is on change in particular the high levels of students that miss presence classes and display a lack of interest to attend most of the subjects in their higher education is already a common feasibility discussion of a collaborative virtual environment finding alternative ways for university members interaction this paper discusses the potential impact and roadmap for the creation of a collaborative virtual environment where all university members can interact in novel ways some actual netlab figures are presented to justify this evolution as feasible a related project that uses the potential created by the laptops for all action is a virtual incubator to simulate entrepreneurship bias is presented construction of adaptive web applications from reusable components the web has become a ubiquitous environment for application towards a distributed environment centered agent framework this paper will discuss the internal architecture for an agent framework called decaf distributed environment centered agent framework decaf is a software toolkit for the rapid design development and execution of intelligent agents to achieve solutions in complex software systems from a research community perspective decaf provides a modular platform for evaluating and disseminating results in agent architectures including communication planning scheduling execution monitoring coordination diagnosis and learning from a user programmer perspective decaf distinguishes itself by removing the focus from the underlying components of agent building such as socket creation message formatting and agent communication instead users may quickly prototype agent systems by focusing on the domain specific parts of the problem via a graphical plan editor reusable generic behaviors 9 and various supporting middle agents 10 this paper will briefly describe the algebraic rewritings for optimizing regular path queries rewriting queries using views is a powerful technique that has applications in query optimization data integration data warehousing etc query rewriting in relational databases is by now rather well investigated however in the framework of semistructured data the problem of rewriting has received much less attention in this paper we focus on extracting as much information as possible from algebraic rewritings for the purpose of optimizing regular path queries the cases when we can find a complete exact rewriting of a query using a set a views are very ideal however there is always information available in the views even if this information is only partial we introduce lower and possibility partial rewritings and provide algorithms for computing them these rewritings are algebraic in their nature i e we use only the algebraic view definitions for computing the rewritings this fact makes them a main memory product which can be used for reducing secondary memory and remote access we give two algorithms for utilizing the partial lower and partial possibility rewritings in the context of query optimization a generalized modeling framework for schema versioning support advanced object oriented applications require the management of schema versions in order to cope with changes in the structure of the stored data two types of versioning have been separately considered so far branching and temporal the former arose in application domains like cad cam and software engineering where different solutions have been proposed to support design schema versions consolidated versions the latter concerns temporal databases where some works considered temporal schema versioning to fulfil advanced needs of other typical objectoriented applications like gis and the multimedia ones in this work we propose a general model which integrates the two approaches by supporting both design and temporal schema versions the model is provided with a complete set of schema change primitives for full fledged version manipulation whose semantics is described in the paper keywords schema versioning schema evolution oodbms temporal databases 1 introduction in th multi scale em icp a fast and robust approach for surface registration we investigate in this article the rigid registration of large sets of points generally sampled from surfaces we formulate this problem as a general maximum likelihood ml estimation of the transformation and the matches we show that in the specific case of a gaussian noise it corresponds to the iterative closest point algorithm icp with the mahalanobis distance a pattern supported approach to the user interface design process patterns describe generic solutions to common problems in context originating from the world of architecture patterns have been used mostly in object oriented programming and data analysis the goal of hci patterns is to create an inventory of solutions to help designers and usability engineers to resolve ui development problems that are common difficult and frequently encountered in this paper we present our pattern supported approach to user interface design in the context of information visualization using a concrete example from the telecommunications domain we will focus on a task subtask pattern to illustrate how knowledge about a task and an appropriate interaction design solution can be captured and communicated 1 augmenting recommender systems by embedding interfaces into office practices automated collaborative filtering systems collect evaluations from users of the quality and relevance of stored information items such as scientific papers books and movies a number of users need to give evaluations for the systems to be able to produce statistically high quality predictions of an item s interest promoting the creation of a rich meta layer of evaluations is essential for these systems but several important issues remain to be resolved the work presented here first analyses the issues around the collection of recommendations then proposes a set of design principles for improving and automating the collection of recommendations and finally presents how these principles have been implemented in a real usage setting 1 systems to alleviate the information overload information overload may be an abused term but it is an increasingly apt description of our current experience in dealing with information the increase in communication channels and publishing me emile marshalling passions in training and education emotional reasoning can be an important contribution to auto mated tutoring and training systems this paper describes mile a model of emotional reasoning that builds upon existing approaches and significantly generalizes and extends their capabilities the main contribution is to show how an explicit planning model allows a more general treatment of several stages of the reasoning process the model supports educational applications by allowing agents to appraise the emotional significance of events as they relate to students or their own plans and goals model and predict the emotional state of others and alter behavior accordingly 1 introduction emotional computers may seem an oxymoron but recent years have seen a flurry of computation accounts of emotion in a variety of applications this paper describes mile a model of emotional reasoning that extends and significantly generalizes prior work mile illustrates how an explicit planning model supports a more general treatme d agents applications and performance of a mobile agent system d agents is a general purpose mobile agent system that has been used in several informationretrieval applications in this paper we rst examine one such application operational support for military eld personnel where d agents greatly simpli es the task of providing ecient application speci c access to remote information resources after describing the application we discuss the key dierences between d agents and most other mobile agent systems notably its support for strong mobility and multiple agent languages finally we derive a small simple application that is representative of many information retrieval tasks including those in the example application and use this application to compare the scalability of mobile agents and traditional client server approaches the results con rm and quantify the usefulness of mobile code and perhaps more importantly con rm that intuition about when to use mobile code is usually correct although signi cant additional experiments are needed to fully characterize the complex mobile agent performance space the results here help answer the basic question of when mobile agents should be considered at all particularly for information retrieval applications finding and moving constraints in cyberspace agent based architectures are an effective method for constructing open dynamic distributed information systems the kraft system exploits such an architecture focusing on the exchange of information in the form of constraints and data among participating agents the kraft approach is particularly wellsuited to solving design and configuration problems in which constraints and data are retrieved from agents representing customers and vendors on an extranet network transformed to a common ontology and processed by mediator agents this paper describes the kraft system discusses the issues involved in joining a kraft network from the point of view of information providers in cyberspace and examines the role of autonomous and mobile agents in kraft introduction traditional distributed database systems provide uniform and transparent access to data objects across the network these systems however are focused on the utilisation of data instead of other semantic knowledg sdlip starts sdarts a protocol and toolkit for metasearching in this paper we describe how we combined sdlip and starts two complementary protocols for searching over distributed document collections the resulting protocol which we call sdarts is simple yet expressible enough to enable building sophisticated metasearch engines sdarts can be viewed as an instantiation of sdlip with metasearchspecific elements from starts we also report on our experience building three sdarts compliant wrappers for locally available plain text document collections for locally available xml document collections and for external webaccessible collections these wrappers were developed to be easily customizable for new collections our work was developed as part of columbia university s digital libraries initiative phase 2 dli2 project which involves the departments of computer science medical informatics and electrical engineering the columbia university libraries and a large number of industrial partners the main goal of the project is to provide personalized access to a distributed patient care digital library phidgets easy development of physical interfaces through physical widgets physical widgets or phidgets are to physical user interfaces what widgets are to graphical user interfaces similar to widgets phidgets abstract and package input and output devices they hide implementation and construction details they expose functionality through a well defined api and they have an optional on screen interactive interface for displaying and controlling device state unlike widgets phidgets also require a connection manager to track how devices appear on line a way to link a software phidget with its physical counterpart and a simulation mode to allow the programmer to develop debug and test a physical interface even when no physical device is present our evaluation shows that everyday programmers using phidgets can rapidly develop physical interfaces using a room metaphor to ease transitions in groupware many groupware systems contain gaps that hinder or block natural social interaction or that do not let people easily move between different styles of work we believe that the adoption of a room metaphor can ease people s transitions across these gaps allowing them to work together more naturally using the teamwave workplace system as an example we show how particular gaps are removed first we ease a person s transition between single user and groupware applications by making rooms suitable for both individual and group activity second people can move fluidly between asynchronous and synchronous work because room artifacts persist people can leave messages documents and annotations for others or work on them together when occupying the room at the same time third we ease the difficulty of initiating real time work by providing people with awareness of others who may be available for real time interactions and by automatically establishing connections as users enter a commo adapting the locales framework for heuristic evaluation of groupware heuristic evaluation is a rapid cheap and effective way for identifying usability problems in single user systems however current heuristics do not provide guidance for discovering problems specific to groupware usability in this paper we take the locales framework and restate it as heuristics appropriate for evaluating groupware these are 1 provide locales 2 provide awareness within locales 3 allow individual views 4 allow people to manage and stay aware of their evolving interactions and 5 provide a way to organize and relate locales to one another to see if these new heuristics are useful in practice we used them to inspect the interface of teamwave workplace a commercial groupware product we were successful in identifying the strengths of teamwave as well as both major and minor interface problems key words groupware evaluation heuristic evaluation inspection methods locales framework 1 introduction hci researchers and practitioners now have a good rep autonomous bidding agents in the trading agent competition designing agents that can bid in online simultaneous auctions is a complex task the authors describe task specific details and strategies of agents in a trading agent competition anatural offshoot of the growing prevalence of online auctions is the creation of autonomous bidding agents that monitor and participate in these auctions it is straightforward to write a bidding agent to participate in an online auction for a single good particularly when the value of that good is fixed ahead of time the agent can bid slightly over the ask price until the auction closes or the price exceeds the value in simultaneous auctions offering complementary and substitutable goods however agent deployment is a much more complex endeavor the first trading agent competition tac held in boston massachusetts on 8 july 2000 challenged participants to design a trading agent capable of bidding in online simultaneous auctions for complimentary and substitutable goods tac was organized by a group of researchers and developers led by michael wellman of a theory of term weighting based on exploratory data analysis techniques of exploratory data analysis are used to study the weight of evidence that the occurrence of a query term provides in support of the hypothesis that a document is relevant to an information need in particular the relationship between the document frequency and the weight of evidence is investigated a correlation between document frequency normalized by collection size and the mutual information between relevance and term occurrence is uncovered this correlation is found to be robust across a variety of query sets and document collections based on this relationship a theoretical explanation of the efficacy of inverse document frequency for term weighting is developed which differs in both style and content from theories previously put forth the theory predicts that a flattening of idf at both low and high frequency should result in improved retrieval performance this altered idf formulation is tested on all trec query sets retrieval results corroborate the predicti a unifying approach to html wrapper representation and learning the number the size and the dynamics of internet information sources bears abundant evidence of the need for automation in information extraction this calls for representation formalisms that match the world wide web reality and for learning approaches and learnability results that apply to these formalisms the concept of elementary formal systems is appropriately generalized to allow for the representation of wrapper classes which are relevant to the description of internet sources in html format related learning results prove that those wrappers are automatically learnable from examples this is setting the stage for information extraction from the internet by exploitation of inductive learning techniques 1 motivation today s online access to millions or even billions of documents in the world wide web is a great challenge to research areas related to knowledge discovery and information extraction ie the general task of ie is to locate specific pieces of text i a query calculus for spatio temporal object databases the development of any comprehensive proposal for spatio temporal databases involves significant extensions to many aspects of a non spatio temporal architecture one aspect that has received less attention than most is the development of a query calculus that can be used to provide a semantics for spatio temporal queries and underpin an effective query optimization and evaluation framework in this paper we show how a query calculus for spatiotemporal object databases that builds upon the monoid calculus proposed by fegaras and maier for odmg compliant database systems can be developed the paper shows how an extension of the odmg type system with spatial and temporal types can be accommodated into the monoid approach it uses several queries over historical possibly spatial data to illustrate how by mapping them into monoid comprehensions the way is open for the application of a logical optimizer based on the normalization algorithm proposed by fegaras and maier on concept space and hypothesis space in case based learning algorithms in order to learn more about the behaviour of case based reasoners as learning systems we formalise a simple case based learner as a pac learning algorithm we show that the case based representation hcb oei is rich enough to express any boolean function we define a family of simple case based learning algorithms which use a single fixed similarity measure and we give necessary and sufficient conditions for the consistency of these learning algorithms in terms of the chosen similarity measure finally we consider the way in which these simple algorithms when trained on target concepts from a restricted concept space often output hypotheses which are outside the chosen concept space a case study investigates this relationship between concept space and hypothesis space and concludes that the case based algorithm studied is a less than optimal learning algorithm for the chosen small concept space 1 introduction the performance of a case based reasoning system 13 will chan cooperative plan selection through trust cooperation plays a fundamental role in multi agent systems in which individual agents must interact for the overall system to function effectively revisiting structured storage a transactional record store an increasing number of applications such as electronic mail servers web servers and personal information managers handle large amounts of homogeneous data this data can be effectively represented as records and manipulated through simple operations e g record reading writing and searching unfortunately modern storage systems are inappropriate for the needs of these applications on one side file systems store only unstructured data byte strings with very limited reliability guarantees on the other side relational databases store structured data and provide both concurrency control and transactions but relational databases are often too slow complex and difficult to manage for many applications this paper presents a transactional record store that directly addresses the needs of modern applications the store combines the simplicity and manageability of the file system interface with a select few features for managing record oriented data we describe the principles guiding the design of our transactional record store as well as its design we also present a prototype implementation and its performance evaluation 1 systems directions for pervasive computing pervasive computing with its focus on users and their tasks rather than on computing devices and technology provides an attractive vision for the future of computing but while hardware and networking infrastructure to realize this vision are becoming a reality precious few applications run in this infrastructure we believe that this lack of applications stems largely from the fact that it is currently too hard to design build and deploy applications in the pervasive computing space in this paper we argue that existing approaches to distributed computing are flawed along three axes when applied to pervasive computing we sketch out alternatives that are better suited for this space first application data and functionality need to be kept separate so that they can evolve gracefully in a global computing infrastructure second applications need to be able to acquire any resource they need at any time so that they can continuously provide their services in a highly dynamic environment third pervasive computing requires a common system platform allowing applications to be run across the range of devices and to be automatically distributed and installed 1 adaptive information extraction and sublanguage analysis introduction 1 information extraction ie has made significant progress in the last decade we have developed practical efficient approaches to ie which have yielded modest levels of performance on general texts and quite good performance on restricted semi structured texts more notably over the last few years there has been a blossoming of work in adaptive ie the topic of this and other recent workshops ie systems which can be rapidly and automatically or semi automatically moved to new extraction tasks to date these developments have been relatively little influenced by linguistic studies of the texts in fact the trend has been towards less linguistic analysis some early ie systems used full parsing and in a few cases relatively deep semantic analysis because of limitations of full parsing methods particularly a decade ago this gave way to a common methodology based on limited parsing and simple pattern matching adaptive ie systems have in automated knowledge and information fusion from multiple text based sources using formal concept analysis this report explores the space of this problem and develops some steps towards its solutions our experience with knowledge representation languages and dynamic hyperlinking of html documents using conceptual graphs seems complementary to the multiple source knowledge fusion task 40 the reasons are that conceptual graphs are based on lexical hierarchically structured ontologies of semantically related terms and these structures lend themselves to traditional information retrival engines at least so far as term extraction from text is concerned furthermore conceptual graphs provide for typical and necessary conditions logical inference and term signatures that aid with disambiguiation of semantic intent conceptual graphs also permit knowledge fusion by way of the maximal join operation conceptual graphs are not without their research challenges however who or what can automatically generate the cgs and how the approach scales to real world multiple source fusion are open questions to address the first of these issues there are two schools of thought on who or what generates the cgs needed for knowledge fusion from multiple sources the first of these is that index expressions generated by a meta level information retrival engine called the hyperindex browser the hib 5 6 7 4 can be used to automatically generate knowledge annotations as input to an inference engine such as the one used by our research group s webkb software 38 39 40 the idea here is that we can efficiently extract and fuse knowledge structures from example documents to construct a knowledge exemplar and that these exemplars are subsequently used to compute the closure of the knowledge base and generate pattern recognition classifiers see figure 1 1 domain background knowled integrating case based reasoning and tabu search for solving optimisation problems tabu search is an established heuristic optimisation technique for problems where exact algorithms are not available it belongs to the same family as simulated annealing or genetic algorithms it extends the basic iterative improvement scheme by adding control learning a technique of this kind intensification captures experience established on a frequency based analysis of past search experience is reused while the same optimisation process is going on in order to guide search to better solutions in this paper we introduce a case based reasoning approach for control learning in tabu search search experience concerns operator selection and is represented by cases the aim of case reuse is to improve conflict resolution while the proposed method is domain independent we present its application to the nphard uncapacitated facility location problem experimental results show that adding our approach to a basic tabu search optimisation significantly improves solution quality on t manipulating interpolated data is easier than you thought data defined by interpolation is frequently found in new applications involving geographical entities moving objects or spatiotemporal data these data lead to potentially infinite collections of items e g the elevation of any point in a map whose definitions are based on the association of a collection of samples with an interpolation function the naive manipulation of the data through direct access to both the samples and the interpolation functions leads to cumbersome or inaccurate queries it is desirable to hide the samples and the interpolation functions from the logical level while their manipulation is performed automatically we propose to model such data using infinite relations e g the map with elevation yields an infinite ternary relation which can be manipulated through standard relational query languages e g sql with no mention of the interpolated definition the clear separation between logical and physical levels ensures the accu developing adaptable user interfaces for component based systems developing software components with user interfaces that can be adapted to diverse reuse situations is challenging examples of such adaptations include extending composing and reconfiguring multiple component user interfaces and adapting component user interfaces to particular user preferences roles and subtasks we describe our recent work in facilitating such adaptation via the concept of user interface aspects which facilitate effective component user interface design and realisation using an extended component based software architecture 1 introduction component based software applications are composed from diverse software components to form an application 1 14 16 17 typically many of these components have been developed separately with no knowledge of the user interfaces of other components they may be composed with this can result in component based applications with inappropriate inconsistent interfaces for example two components with user interfaces that software tools software is growing ever more complex and new software processes methods and products put greater demands on software engineers than ever before the support of appropriate software tools is essential for developers to maximise their ability to effectively and efficiently deliver quality software products this article surveys current practice in the software tools area along with recent and expected near future trends in software tools development we provide a summary of tool applications during the software lifecycle but focus on particular aspects of software tools that have changed in recent years and are likely to change in the near future as tools continue to evolve these include the internal structure of tools provision of multiple view interfaces tool integration techniques collaborative work support and the increasing use of automated assistance within tools we hope this article will both inform software engineering practitioners of current research trends and tool researchers of the relevant state of the art in commercial tools and various likely future research trends in tools development from active objects to autonomous agents this paper studies how to extend the concept of active objects into a structure of agents it first discusses the requirements for autonomous agents that are not covered by simple active objects we propose then the extension of the single behavior of an active object into a set of behaviors with a meta behavior scheduling their activities to make a concrete proposal based on these ideas we describe how we extended a framework of active objects named actalk into a generic multi agent platform named dima we discuss how this extension has been implemented we finally report on one application of dima to simulate economic models keywords active object agent implementation meta behavior modularity re usability simulation 1 introduction object oriented concurrent programming oocp is the most appropriate and promising technology to implement agents the concept of active object may be considered as the basic structure for building agents furthermore the combinat algorithm directed exploration for model based reinforcement learning in factored mdps one of the central challenges in reinforcement learning is to balance the exploration exploitation tradeoff while scaling up to large problems although model based reinforcement learning has been less prominent than value based methods in addressing these challenges recent progress has generated renewed interest in pursuing modelbased approaches theoretical work on the exploration exploitation tradeoff has yielded provably sound model based algorithms such as e rmax while work on factored mdp representations has yielded model based algorithms that can scale up to large problems recently the benefits of both achievements have been combined in the algorithm of kearns and koller in this paper we address a significant shortcoming of factored e namely that it requires an oracle planner that cannot be feasibly implemented we propose an alternative approach that uses a practical approximate planner approximate linear programming that maintains desirable properties further we develop an exploration strategy that is targeted toward improving the performance of the linear programming algorithm rather than an oracle planner this leads to a simple exploration strategy that visits states relevant to tightening the lp solution and achieves sample efficiency logarithmic in the size of the problem description our experimental results show that the targeted approach performs better than using approximate planning for implementing either factored e or factored rmax the web graph an overview this paper a study is made on a 200 millions vertices graph obtained from a crawl of the web and it appears that is is composed of four parts of equivalent sizes see figure 3 the first part is the largest strongly connected component of the graph the second largest is much smaller which composes the core of the well connected pages the second part called in is composed of those pages from which the core is reachable but which are not reachable from the core conversly the third part called out is the set of pages reachable from the core but from which the core is unreachable finally the dendrites are the pages reachable from one of the three first parts or from which one of the three first parts is reachable but which belong to none of the previous parts only ten percent of the whole graph do not belong to one of these four parts which compose the bow tie using experience to guide web server selection we examine the use of the anycasting communication paradigm to improve client performance when accessing replicated multimedia objects anycasting supports dynamic selection of a server amongst a group of servers that provide equivalent content if the selection is done well the client will experience improved performance a key issue in anycasting is the method used to maintain performance information used in server selection we explore using past performance or experience to predict future performance we conduct our work in the context of a customized web prefetching application called websnatcher we examine a variety of algorithms for selecting a server using past performance and find that the overall average and weighted average algorithms are closest to optimal performance in addition to the websnatcher application this work has implications for responsible network behavior by other applications that generate network traffic automatically by using the techniques we present support vector machines for classification and regression the problem of empirical data modelling is germane to many engineering applications in empirical data modelling a process of induction is used to build up a model of the system from which it is hoped to deduce responses of the system that have yet to be observed ultimately the quantity and quality of the observations govern the performance of this empirical model by its observational nature data obtained is finite and sampled typically this sampling is non uniform and due to the high dimensional nature of the problem the data will form only a sparse distribution in the input space consequently the problem is nearly always ill posed poggio et al 1985 in the sense of hadamard hadamard 1923 traditional neural network approaches have suffered difficulties with generalisation producing models that can overfit the data this is a consequence of the optimisation algorithms used for parameter selection and the statistical measures used to select the best model the foundations of support vector machines svm have been developed by vapnik 1995 and are gaining popularity due to many attractive features and promising empirical performance the formulation embodies the structural risk minimisation srm principle which has been shown to be superior gunn et al 1997 to traditional empirical risk minimisation erm principle employed by conventional neural networks srm minimises an upper bound on the expected risk as opposed to erm that minimises the error on the training data it is this difference which equips svm with a greater ability to generalise which is the goal in statistical learning svms were developed to solve the classification problem but recently they have been extended to the domain of regression problems vapnik et al 1997 in the literature the terminology for svms can be slightly confusing the term svm is typically used to describe classification with support vector methods and support vector regression is used to describe regression with support vector methods in this report the term svm will refer to both classification and regression methods and the terms support vector classification svc and support vector regression svr will be used for specification this section continues with a brief introduction to the structural risk learning similarity for texture image retrieval a novel algorithm is proposed to learn pattern similarities for texture image retrieval similar patterns in different texture classes are grouped into a cluster in the feature space each cluster is isolated from others by an enclosed boundary which is represented by several support vectors and their weights obtained from a statistical learning algorithm called support vector machine svm the signed distance of a pattern to the boundary is used to measure its similarity furthermore the patterns of different classes within each cluster are separated by several sub boundaries which are also learned by the svms the signed distances of the similar patterns to a particular sub boundary associated with the query image are used for ranking these patterns experimental results on the brodatz texture database indicate that the new method performs significantly better than the traditional euclidean distance based approach the basis system a benchmarking approach for spatial index structures this paper describes the design of the basis prototype system which is currently under implementation basis stands for benchmarking approach for spatial index structures it is a prototype system aiming at performance evaluation of spatial access methods and query processing strategies under different data sets various query types and different workloads basis is based on a modular architecture composed of a simple storage manager a query processor and a set of algorithmic techniques to facilitate benchmarking the main objective of basis is twofold i to provide a benchmarking environment for spatial access methods and related query evaluation techniques and ii to allow comparative studies of spatial access methods in different cases but under a common framework we currently extend it to support the fundamental features of spatiotemporal data management and access methods adapt a multimodal conversational dialogue system in an apartment domain a general overview of the adapt project and the research that is performed within the project is presented in this project various aspects of human computer interaction in a multimodal conversational dialogue systems are investigated the project will also include studies on the integration of user system dialogue dependent speech recognition and multimodal speech synthesis a domain in which multimodal interaction is highly useful has been chosen namely finding available apartments in stockholm a wizard of oz data collection within this domain is also described 1 geometric foundations for interval based probabilities the need to reason with imprecise probabilities arises in a wealth of situations ranging from pooling of knowledge from multiple experts to abstraction based probabilistic planning researchers have typically represented imprecise probabilities using intervals and have developed a wide array of different techniques to suit their particular requirements in this paper we provide an analysis of some of the central issues in representing and reasoning with interval probabilities at the focus of our analysis is the probability cross product operator and its interval generalization the cc operator we perform an extensive study of these operators relative to manipulation of sets of probability distributtions this study provides insight into the sources of the strengths and weaknesses of various approaches to handling probability intervals we demonstrate the application of our results to the problems of inference in interval bayesian networks and projection and evaluation of abstract pro a description logic with concrete domains and a role forming predicate operator description logics dls are a family of logic based knowledge representation formalisms designed to represent and reason about conceptual knowledge due to a nice compromise between expressivity and the complexity of reasoning dls have found applications in many areas such as e g modelling database schemas and the semantic web however description logics represent knowledge in an abstract way and lack the power to describe more concrete quantitative qualities like size duration or amounts the standard solution is to equip dls with concrete domains e g natural numbers with predicates or strings with a string concatenation predicate moreover recently it has been suggested that the expressive power of dls with concrete domains can be further enhanced by providing them with database like key constraints key constraints can be a source of additional inconsistencies in database schemas and dls applied in reasoning about database schemas are thus wanted to be able to capture such constraints up to now only the integration of uniqueness key constraints into dls with concrete an overview of some recent developments in bayesian problem solving techniques the last five years have seen a surge in interest in the use of techniques from bayesian decision theory to address problems in ai decision theory provides a normative framework for representing and reasoning about decision problems under uncertainty within the context of this framework researchers in uncertainty in the ai community have been developing computational techniques for building rational agents and representations suited to engineering their knowledge bases this special issue reviews recent research in bayesian problem solving techniques the articles cover the topics of inference in bayesian networks decision theoretic planning and qualitative decision theory here i provide a brief introduction to bayesian networks and then cover applications of bayesian problem solving techniques knowledge based model construction and structured representations and the learning of graphical probability models the past five years or so have seen increased interest and tremendous integrating sounds and motions in virtual environments sounds are often the result of motions of virtual objects in a virtual environment therefore sounds and the motions that caused them should be treated in an integrated way when sounds and motions do not have the proper correspondence the resultant confusion can lessen the effects of each in this paper we present an integrated system for modeling synchronizing and rendering sounds for virtual environments the key idea of the system is the use of a functional representation of sounds called timbre trees this representation is used to model sounds that are parameterizable these parameters can then be mapped to the parameters associated with the motions of objects in the environment this mapping allows the correspondence of motions and sounds in the environment representing arbitrary sounds using timbre trees is a difficult process that we do not address in this paper we describe approaches for creating some timbre trees including the use of genetic algorithms rendering the theory of answering queries using views the problem of answering queries using views is to nd ecient methods of answering a query using a set of previously materialized views over the database rather than accessing the database relations the problem has recently received signicant attention because of its relevance to a wide variety of data management problems such as query optimization the maintenance of physical data independence data integration and data warehousing this article surveys the theoretical issues concerning the problem of answering queries using views 1 the information integration wizard iwiz project data to illustrate the need for integration of heterogeneous data sources suppose we have a scenario where consumers want to purchase computer related products from one of the many e commerce sites however before making the purchase they would like to gather all the relevant available information in order to help them in their decision making process for example consumers may want to access product information on available desktops laptops software and other accessories and check availability and pricing information in addition users may also want to access other online sources for related background information such as consumer reports press releases etc this situation is depicted in figure 1 typically each source uses different tools and data modeling techniques to create and manage their data this means the same concept for example the entity software may be described by a different term and different set of attributes in different sources e g automated robot behavior recognition applied to robotic soccer automated recognition of the behavior of robots is increasingly needed in a variety of tasks as we develop more autonomous robots and general information processing agents for example in environments with multiple autonomous robots a robot may need to make decisions based on the behavior of the other robots as another interesting example an intelligent narrator agent observing a robot will need to automatically identify the robot s behaviors in this paper we introduce a novel framework for using hidden markov models hmms to represent and recognize strategic behaviors of robotic agents we first introduce and characterize the perceived signal in terms of behavioral relevant state features we then show how several hmms capture different defined robot behaviors finally we present the hmm based recognition algorithm which orchestrates and selects the appropriate hmms in real time we use the multi robot robotic soccer domain as the substrate of our empirical validation both in efficient mining of partial periodic patterns in time series database partial periodicity search i e search for partial periodic patterns in time series databases is an interesting data mining problem previous studies on periodicity search mainly consider finding full periodic patterns where every point in time contributes precisely or approximately to the periodicity however partial periodicity is very common in practice since it is more likely that only some of the time episodes may exhibit periodic patterns we present several algorithms for efficient mining of partial periodic patterns by exploring some interesting properties related to partial periodicity such as the apriori property and the max subpattern hit set property and by shared mining of multiple periods the max subpattern hit set property is a vital new property which allows us to derive the counts of all frequent patterns from a relatively small subset of patterns existing in the time series we show that mining partial periodicity needs only two scans over the time series database even for mining multiple periods the performance study shows our proposed methods are very efficient in mining long periodic patterns authoring and annotation of web pages in cream richly interlinked machine understandable data constitute the basis for the semantic web we provide a framework cream that allows for creation of metadata while the annotation mode of cream allows to create metadata for existing web pages the authoring mode lets authors create metadata almost for free while putting together the content of a page as a particularity of our framework cream allows to create relational metadata i e metadata that instantiate interrelated definitions of classes in a domain ontology rather than a comparatively rigid template like schema as dublin core we discuss some of the requirements one has to meet when developing such an ontology based framework e g the integration of a metadata crawler inference services document management and a meta ontology and describe its implementation viz ont o mat a component based ontology driven web page authoring and annotation tool composable agents for patient flow control preliminary concepts in this article we describe our research efforts in coping with a trade off that can be often found in the control and optimization of todays business processes though centralized control may achieve better results in controlling the system behavior there are usually social technical and security constraints on applying centralized control distributed control on the other hand may cope with these constraints but also entails suboptimal results and communicational overhead our concept of composable agents tries to allow a dynamic and fluent transition between globalization and localization in business process control by adapting to the current real world system structure we are currently evaluating this concept in the framework of a patient flow control project at charit e berlin todays applications of information technology face at least two major aspects of business settings the first aspect is the partially or fully automated execution of complex business processes this enfo dynamic reconfiguration in collaborative problem solving in this article we will describe our research efforts in coping with a trade off that can be often found in the control and optimization of todays business processes though centralized control may achieve nearto optimum results in optimizing the system behavior there are usually social technical and security restrictions on applying centralized control distributed control on the other hand may cope with these restrictions but also entails sub optimality and communicational overhead our concept of composable agents tries to allow a dynamic and fluent transition between globalization and localization in business process control by adapting to the current real world system structure we are currently evaluating this concept in the framework of patient flow control at charit e berlin introduction research in distributed artificial intelligence dai bond gasser 1988 has been traditionally divided into distributed problem solving dps and multi agent systems mas however r rapid concurrent software engineering in competitive situations this article is an experience report on the evolutionary development process of at humboldt a multi agent system which has become world champion 1997 and vice world champion 1998 of robocup simulator league it details why the artifical soccer initiative robocup is a tempting domain for rapid concurrent software engineering both the development processes in 1997 and 1998 are described compared and evaluated lessons learned for development projects in distributed control conclude this report 1 introduction in this article the project managers describe the evolutionary development process of the software project at agentteam humboldt which has become world champion 1997 and vice world champion 1998 in the simulator league of the artifical soccer contest robocup 10 the robocup initiative recently gets more and more popular among scientists in robotics distributed systems and distributed artificial intelligence because of its strong competitive character and tight resource b scalable trigger processing current database trigger systems have extremely limited scalability this paper proposes a way to develop a truly scalable trigger system scalability to large numbers of triggers is achieved with a trigger cache to use main memory effectively and a memory conserving selection predicate index based on the use of unique expression formats called expression signatures a key observation is that if a very large number of triggers are created many will have the same structure except for the appearance of different constant values when a trigger is created tuples are added to special relations created for expression signatures to hold the trigger s constants these tables can be augmented with a database index or main memory index structure to serve as a predicate index the design presented also uses a number of types of concurrency to achieve scalability including token tuple level condition level rule action level and datalevel concurrency 1 introduction trigger feature characterizing a media enhanced classroom server media enhanced classrooms are changing the way we teach and learn servers for such applications differ from a typical web server in the nature and geographic spread of the user community and the type of information accessed from the server this paper compares the workload of the classroom 2000 server a media enhanced classroom server with that of a typical web server we find several similarities and differences between the two workloads one of the most striking differences is that the inter reference times for files accessed more than once from the classroom 2000 server are not independent as opposed to those of a web server we show that the user interface presented by the classroom 2000 server affects server workload we find that client side caching improves over time finally in addition to accessing lecture material for their current courses students also access archieved material from previous quarters 1 introduction recent improvements in computational power workload of a media enhanced classroom server we charaterize a workload of media enhanced classrooms such classrooms include equipment for presenting multimedia streams and for capturing streams of information audio video and notes during a lecture we present detailed quantitative performance measurements of one media enhanced classroom system classroom 2000 we characterize the workload from the point of view of a server that supports multiple classrooms the workload includes server bandwidth network bandwidth and server storage requirements we identify patterns in user behavior and demonstrate how the number of simultaneous study sessions varies with time of day and with the proximity of a specific date to exams 1 workload of a media enhanced classroom server 1 introduction the ways we teach and learn will be dramatically affected by current unprecedented rates of improvement in computational power and network bandwidth as well as the development of innovative user interfaces and virtual environments alre linkage learning via probabilistic modeling in the ecga the goal of linkage learning or building block identification is the creation of a more effective genetic algorithm ga this paper explores the relationship between the linkage learning problem and that of learning probability distributions over multi variate spaces herein it is argued that these problems are equivalent using a simple but effective approach to learning distributions and by implication linkage this paper reveals the existence of ga like algorithms that are potentially orders of magnitude faster and more accurate than the simple ga i introduction linkage learning in genetic algorithms gas is the identification of building blocks to be conserved under crossover theoretical studies have shown that if an effective linkage learning ga were developed it would hold significant advantages over the simple ga 2 therefore the task of developing such an algorithm has drawn significant attention past approaches to developing such an algorithm have focused on ev a parameter less genetic algorithm from the user s point of view setting the parameters of a genetic algorithm ga is far from a trivial task moreover the user is typically not interested in population sizes crossover probabilities selection rates and other ga technicalities he is just interested in solving a problem and what he would really like to do is to hand in the problem to a blackbox algorithm and simply press a start button this paper explores the development of a ga that fulfills this requirement it has no parameters whatsoever the development of the algorithm takes into account several aspects of the theory of gas including previous research works on population sizing the schema theorem building block mixing and genetic drift real time index concurrency control real time database systems are expected to rely heavily on indexes to speed up data access and thereby help more transactions meet their deadlines accordingly highperformance index concurrency control icc protocols are required to prevent contention for the index from becoming a bottleneck in this paper we develop real time variants of a representative set of classical b tree icc protocols and using a detailed simulation model compare their performance for real time transactions with firm deadlines we also present and evaluate a new real time icc protocol called guard link that augments the classical b link protocol with a feedback based admission control mechanism both point and range queries as well as the undos of the index actions of aborted transactions are included in the scope of our study the performance metrics used in evaluating the icc protocols are the percentage of transactions that miss their deadlines and the fairness with respect to transaction type and size comparing evolutionary programs and evolutionary pattern search algorithms a drug docking application evolutionary programs eps and evolutionary pattern search algorithms epsas are two general classes of evolutionary methods for optimizing on continuous domains the relative performance of these methods has been evaluated on standard global optimization test functions and these results suggest that epsas more robustly converge to nearoptimal solutions than eps in this paper we evaluate the relative performance of epsas and eps on a real world application flexible ligand binding in the autodock docking software we compare the performance of these methods on a suite of docking test problems our results confirm that epsas and eps have comparable performance and they suggest that epsas may be more robust on larger more complex problems 1 introduction evolutionary programs eps and evolutionary pattern search algorithms epsas are two classes of evolutionary algorithms eas that have been specifically developed for solving problems of the form min x2r n f x in particula socialware multiagent systems for supporting network communities ing with credit is permitted to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee request permissions from publications dept acm inc fax 1 212 869 0481 or permissions acm org community b community a community c community agent a1 community agent b1 community agent b2 personal unit 1 personal unit 2 personal unit 5 personal unit 6 personal unit 3 personal unit 7 personal unit 4 user 5 to join user 4 to leave figure 1 a general architecture of socialware as a multiagent system socialware as multiagent systems there are several characteristics specific to network communities which make a multiagent architecture attractive to use the first characteristic is that the participants of a network community are widely distributed and the number of potential participants is large hence no solid centralized or monolithic system would be adequate a distributed system would be required in which perso neurules improving the performance of symbolic rules in this paper we present a method for improving the performance of classical symbolic rules hymes a hybrid modular expert system with efficient inference and explanation a hybrid modular expert system called hymes is presented hymes provides a dual representation scheme a symbolic one based on conventional symbolic rules and a hybrid one based on neumles a kind of rules that combine a symbolic and a connectionist representation symbolic rules are internally converted into neumles for efficiency reasons in this way hybrid modular knowledge bases can be constructed multi inference with multi neurules neurules are a type of hybrid rules combining a symbolic and a connectionist representation there are two disadvantages of neurules the first is that the created neurule bases usually contain multiple representations of the same piece of knowledge also the inference mechanism is rather connectionism oriented than symbolism oriented thus reducing naturalness to remedy these deficiencies we introduce an extension to neurules called multineurules and an alternative inference process which is rather symbolism oriented experimental results comparing the two inference processes are also presented evaluating strategies for similarity search on the web finding pages on the web that are similar to a query page related pages is an important component of modern search engines a variety of strategies have been proposed for answering related pages queries but comparative evaluation by user studies is expensive especially when large strategy spaces must be searched e g when tuning parameters we present a technique for automatically evaluating strategies using web hierarchies such as open directory in place of user feedback we apply this evaluation methodology to a mix of document representation strategies including the use of text anchor text and links we discuss the relative advantages and disadvantages of the various approaches examined finally we describe how to efficiently construct a similarity index out of our chosen strategies and provide sample results from our index measuring search engine quality the effectiveness of twenty public search engines is evaluated using trec inspired methods and a set of 54 queries taken from real web search logs the world wide web is taken as the test collection and a combination of crawler and text retrieval system is evaluated the engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned statistical testing reveals a significant difference between engines and high inter correlations between measures surprisingly given the dynamic nature of the web and the time elapsed there is also a high correlation between results of this study and a previous study by gordon and pathak for nearly all engines there is a gradual decline in precision at increasing cutoff after some initial fluctuation performance of the engines as a group is found to be inferior to the group of participants in the trec 8 large web task although the best engines approach the median of those systems shortcomings of current web search evaluation methodology are identified and recommendations are made for future improvements in particular the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic by contrast real web search reflects a range of other information need types which require different judging and different measures the authors wish to acknowledge that this work was carried out partly within the cooperative research centre for advanced computational systems established under the australian government s cooperative research centres program 1 1 which search engine is best at finding online services we report results for an independent blind evaluation of the performance of 11 commercial search engines on 106 online service queries and on 54 topic relevance queries we found a strong correlation between performance on the two types of query and significant differences between engines a comparison between two neural network rule extraction techniques for the diagnosis of hepatobiliary disorders neural networks have been widely used as tools for prediction in medicine we expect to see even more applications of neural networks for medical diagnosis as recently developed neural network rule extraction algorithms make it possible for the decision process of a trained network to be expressed as classification rules these rules are more comprehensible to a human user than the classification process of the networks which involves complex nonlinear mapping of the input data this paper reports the results from two neural network rule extraction techniques neurolinear and neurorule applied to the diagnosis of hepatobiliary disorders the data set consists of nine measurements collected from patients in a japanese hospital and these measurements have continuous values neurolinear generates piece wise linear discriminant functions for this data set the continuous measurements have previously been discretized by domain experts neurorule is applied to the discretized data architectural design patterns for multiagent coordination this paper presents our first step towards agent oriented software engineering focusing on the area of coordinated multi agent systems in multi agent systems the interactions between the agents are crucial in determining the effectiveness of the system hence the adoption of an appropriate coordination mechanism is pivotal in the design of multi agent system architectures this paper does not focus on agent theory rather on the development of an agent oriented software engineering methodology collaboration architectures and design patterns for collaboration a catalog of coordination patterns inherent in multi agent architectures is presented such patterns may be utilized in the architectural design for multiagent systems allowing researchers and practitioners to improve the integrability and reusability properties of their systems learning cases to resolve conflicts and improve group behavior groups of agents following fixed behavioral rules can be limited in performance and efficiency adaptability and flexibility are key components of intelligent behavior which allow agent groups to improve performance in a given domain using prior problem solving experience we motivate the usefulness of individual learning by group members in the context of overall group behavior in particular we propose a framework in which individual group members learn cases to improve their model of other group members we use a testbed problem from the distributed ai literature to show that simultaneous learning by group members can lead to significant improvement in group performance and efficiency over agent groups following static behavioral rules introduction an agent is rational if when faced with a choice from a set of actions it chooses the one that maximizes the expected utilities of those actions implicit in this definition is the assumption that the preference of the agent for diffe agent technology in communications systems an overview telecommunications infrastructures are a natural application domain for the distributed software agent paradigm the authors clarify the potential application of software agent technology in legacy and future communications systems and provide an overview of publicly available research on software agents as used for network management the authors focus on the so called stationary intelligent agent type of software agent although the paper also reviews the reasons why mobile agents have made an impact in this domain the authors objective is to describe some of the intricacies of using the software agent approach in the management of communications systems the paper is in four main sections the first section provides a brief introduction to software agent technology the second section considers general problems of network management and the reasons why software agents may provide a suitable solution the third section reviews some selected research on agents in a telec binary decision diagram representations of firewall and router access lists network firewalls and routers can use a rule database to decide which packets will be allowed from one network onto another by filtering packets the firewalls and routers can improve security and performance by excluding packets which may pose a security risk to a network or are not relevant to it however as the size of the rule list increases it becomes difficult to maintain and validate the rules and the cost of rule lookup may add significantly to latency ordered binary decision diagrams bdds a compact method of representing and manipulating boolean expressions are a potential method of representing the rules this paper explores how bdds can be used to develop methods that help analysis of rules to validate them and changes to them to improve performance and facilitate hardware support 1 introduction the growth of network and internet communication creates several challenges for network design two important issues are security and performance as the volume o a comparative study on chinese text categorization methods this paper reports our comparative evaluation of three machine learning methods on chinese text categorization whereas a wide range of methods have been applied to english text categorization relatively few studies have been done on chinese text categorization based on a re constructed people s daily corpus a series of controlled experiments evaluate three machine learning methods namely k nearest neighbor knn algorithm support vector machines svm and adaptive resonance associative map aram in terms of their capabilities in mining categorization knowledge from high dimensional sparse and relatively noisy document feature vectors experiments reveal that all three methods produce satisfactory performance on the test corpus while aram exhibits a marginally better generalization capability especially from relatively small and noisy training sets keywords chinese text categorization supervised learning 2 1 introduction text categorization refers to the task of auto building efficient wireless sensor networks with low level naming in most distributed systems naming of nodes for low level communication leverages topological location such as node addresses and is independent of any application in this paper we investigate an emerging class of distributed systems where low level communication does not rely on network topological location rather low level communication is based on attributes that are external to the network topology and relevant to the application when combined with dense deployment of nodes this kind of named data enables in network processing for data aggregation collaborative signal processing and similar problems these approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited this paper is the first description of the software architecture that supports named data and in network processing in an operational multi application sensor network we show that approaches such as in network aggregation and nested queries can significantly affect network traffic in one experiment aggregation reduces traffic by up to 42 and nested queries reduce loss rates by 30 although aggregation has been previously studied in simulation this paper demonstrates nested queries as another form of in network processing and it presents the first evaluation of these approaches over an operational testbed www robots and search engines the web robots are programs that automatically traverse through networks currently their most visible and familiar application is to provide indices for search engines such as lycos and alta vista and semiautomatically maintained topic references or subject directories in this article we survey the state of art of the web robots and the search engines that utilize the results of robot searches we also present notions about robot ethics and distributed web robots robosoc a system for developing robocup agents for educational use this report describes robosoc a system for developing robocup agents designed especially but not only for educational use robosoc is designed to be as general open and easy to use as possible and to encourage and simplify the modification extension and sharing of robocup agents and parts of them to do this i assumed four requirements from the user she wants the best possible data use as much time as possible for the decision making rather act on incomplete information than not act at all and she wants to manipulate the objects found in the soccer environment robosoc consists of three parts a library of basic objects and utilities used by the rest of the system a basic system handling the interactions with the soccer server and the timing of the agent and a framework for world modeling and decision support the framework defines three concepts views predicates and skills the views are specialized information processing units responsible for a specific part of the wo plan recognition in military simulation incorporating machine learning with intelligent agents a view of plan recognition shaped by both operational and computational requirements is presented operational requirements governing the level of fidelity and nature of the reasoning process combine with computational requirements including performance speed and software engineering effort to constrain the types of solutions available to the software developer by adopting machine learning to provide spatio temporal recognition of environmental events and relationships an agent can be provided with a mechanism for mental state recognition qualitatively different from previous research an architecture for integrating machine learning into a bdi agent is suggested and the results from the development of a prototype provide proof of concept 1 introduction this paper proposes machine learning as a tool to assist in the construction of agents capable of plan recognition this paper focuses on the beliefs desires intentions bdi class of agents these agents have been a visual interface for human robot interaction this paper describes the architecture of a human robot interaction system and recent work on the vision based human robot interface the aim of the project is to develop a robotic system that is safely able to work with a human operator this means in particular that the operator should be allowed into the work space of the robot and even to interact with the robot by direct contact with the manipulator safety issues are of major importance here so open loop force control and backdrivability of the manipulator are mandatory features of the system the human robot interface is visionbased to achieve a natural interaction between the operator and the robot important aspects of this interaction are the ability of the vision system to find and track the operators face to recognise facial gestures and to determine the users gaze point these functions are implemented in a robust way so tracking failures can be compensated for and temporary occlusions of the face are tolerated 1 introdu dogma a ga based relational learner we describe a ga based concept learning theory revision system dogma and discuss how it can be applied to relational learning the search for better theories in dogma is guided by a novel fitness function that combines the minimal description length and information gain measures to show the efficacy of the system we compare it to other learners in three relational domains keywords relational learning genetic algorithms minimal description length 1 introduction genetic algorithms gas are stochastic general purpose search algorithms that have been applied to a wide range of machine learning problems they work by evolving a population of chromosomes each of which encodes a potential solution to the problem at hand the task of a ga is to find a highly fit chromosome through the application of different selection and perturbation operators in this paper we consider the use of gas in relational concept learning i e in the process of learning and extracting relational classif recognizing end user transactions in performance management providing good quality of service e g low response times in distributed computer systems requires measuring end user perceptions of performance unfortunately in practice such measures are often expensive or impossible to obtain herein we propose a machine learning approach to recognizing end user transactions consisting of sequences of remote procedure calls rpcs received at a server two problems are addressed the first is labeling previously segmented transaction instances with the correct transaction type this is akin to work done in document classification the second problem is segmenting rpc sequences into transaction instances this is a more difficult problem but it is similar to segmenting sounds into words as in speech understanding using naive bayes we tackle the labeling problem with four combinations of feature vectors and probability distributions rpc occurrences with the bernoulli distribution and rpc counts with the multinomial geometric and shifted ge optimization techniques for queries with expensive methods object relational database management systems allow knowledgeable users to de ne new data types as well as new methods operators for the types this exibility produces an attendant complexity which must be handled in new ways for an object relational database management system to be e cient in this paper we study techniques for optimizing queries that contain time consuming methods the focus of traditional query optimizers has been on the choice of join methods and orders selections have been handled by pushdown rules these rules apply selections in an arbitrary order before as many joins as possible using the assumption that selection takes no time however users of object relational systems can embed complex methods in selections thus selections may take signi cant amounts of time and the query optimization model must be enhanced in this paper we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections we develop an algorithm called predicate migration and prove that it produces optimal plans for queries with expensive methods we then describe our implementation of predicate migration in the commercial object relational database management system illustra and discuss practical issues that a ect our earlier assumptions we compare predicate migration to a variety of simpler optimization techniques and demonstrate that predicate migration is the best general solution to date the alternative techniques we presentmaybe useful for constrained workloads automated discovery of concise predictive rules for intrusion detection this paper details an essential component of a multi agent distributed knowledge network system for intrusion detection we describe a distributed intrusion detection architecture complete with a data warehouse and mobile and stationary agents for distributed problem solving to facilitate building monitoring and analyzing global spatio temporal views of intrusions on large distributed systems an agent for the intrusion detection system which uses a machine learning approach to automated discovery of concise rules from system call traces is described we use a feature vector representation to describe the system calls executed by privileged processes the feature vectors are labeled as good or bad depending on whether or not they were executed during an observed attack a rule learning algorithm is then used to induce rules that can be used to monitor the system and detect potential intrusions we study the performance of the rule learning algorithm on this task with an software fault tree and colored petri net based specification design and implementation of agent based intrusion detection systems the integration of software fault tree sft which describes intrusions and colored petri nets cpns which specifies design is examined for an intrusion detection system ids the ids under development is a collection of mobile agents that detect classify and correlate system and network activities software fault trees sfts augmented with nodes that describe trust temporal and contextual relationships are used to describe intrusions cpns for intrusion detection are built using cpn templates created from the augmented sfts hierarchical cpns are created to detect critical stages of intrusions the agent based implementation of the ids is then constructed from the cpns examples of intrusions and descriptions of the prototype implementation are used to demonstrate how the cpn approach has been used in development of the ids the main contribution of this paper is an approach to systematic specification design and implementation of an ids innovations include 1 using stages of intrusions to structure the specification and design of the ids 2 augmentation of sft with trust temporal and contextual nodes to model intrusions 3 algorithmic construction of cpns from augmented sft and 4 generation of mobile agents from cpns 1 using the cross entropy method to guide govern mobile agent s path finding in networks the problem of finding paths in networks is general and many faceted with a wide range of engineering applications in communication networks finding the optimal path or combination of paths usually leads to np hard combinatorial optimization problems a recent and promising method the cross entropy method proposed by rubinstein manages to produce optimal solutions to such problems in polynomial time however this algorithm is centralized and batch oriented in this paper we show how the cross entropy method can be reformulated to govern the behaviour of multiple mobile agents which act independently and asynchronously of each other the new algorithm is evaluate on a set of well known travelling salesman problems a simulator based on the network simulator package has been implemented which provide realistic simulation environments results show good performance and stable convergence towards near optimal solution of the problems tested regression models for ordinal data a machine learning approach in contrast to the standard machine learning tasks of classification and metric regression we investigate the problem of predicting variables of ordinal scale a setting referred to as ordinal regression the task of ordinal regression arises frequently in the social sciences and in information retrieval where human preferences play a major role also many multi class problems are really problems of ordinal regression due to an ordering of the classes although the problem is rather novel to the machine learning community it has been widely considered in statistics before all the statistical methods rely on a probability model of a latent unobserved variable and on the condition of stochastic ordering in this paper we develop a distribution independent formulation of the problem and give uniform bounds for our risk functional the main difference to classification is the restriction that the mapping of objects to ranks must be transitive and asymmetric combining our theoretical framework with results from measurement theory we present an approach that is based on a mapping from objects to scalar utility values and thus guarantees transitivity and asymmetry applying the principle of structural risk minimization as employed in support vector machines we derive a new learning algorithm based on large margin rank boundaries for the task of ordinal regression our method is easily extended to nonlinear utility functions we give experimental results for an information retrieval task of learning the order of documents with respect to an initial query moreover we show that our algorithm outperforms more naive approaches to ordinal regression such as support vector classification and support vector regression in the case of more than two ranks support vector learning for ordinal regression we investigate the problem of predicting variables of ordinal scale this task is referred to as ordinal regression and is complementary to the standard machine learning tasks of classification and metric regression in contrast to statistical models we present a distribution independent formulation of the problem together with uniform bounds of the risk functional the approach presented is based on a mapping from objects to scalar utility values similar to support vector methods we derive a new learning algorithm for the task of ordinal regression based on large margin rank boundaries we give experimental results for an information retrieval task learning the order of documents w r t an initial query experimental results indicate that the presented algorithm outperforms more naive approaches to ordinal regression such as support vector classification and support vector regression in the case of more than two ranks 1 introduction problems of ordinal regression arise in many fi secure meeting scheduling with agenta when people want to schedule a meeting the agendas of the participants must be compared to nd a time suitable for all of them however at the same time participants want to keep their agendas private this paper presents a negotiation protocol which tries to solve this contradiction the protocol is implemented in the agenta system using mobile software agents hereby alleviating communication overhead and allowing disconnected operation keywords mobile agents secure distributed computation meeting scheduling 1 explaining collaborative filtering recommendations xwrpdwhg frooderudwlyh ilowhulqj v vwhpv suhglfw d shuvrqv diilqlw iru lwhpv ru lqirupdwlrq e frqqhfwlqj wkdw shuvrqv uhfrughg lqwhuhvwv zlwk wkh uhfrughg lqwhuhvwv ri d frppxqlw ri shrsoh dqg vkdulqj udwlqjv ehwzhhq olnh plqghg shuvrqv rzhyhu fxuuhqw uhfrpphqghu v vwhpv duh eodfn er hv surylglqj qr wudqvsduhqf lqwr wkh zrunlqj ri wkh uhfrpphqgdwlrq sodqdwlrqv surylgh wkdw wudqvsduhqf h srvlqj wkh uhdvrqlqj dqg gdwd ehklqg d uhfrpphqgdwlrq q wklv sdshu zh dgguhvv h sodqdwlrq lqwhuidfhv iru v vwhpv krz wkh vkrxog eh lpsohphqwhg dqg zk wkh vkrxog eh lpsohphqwhg 7r h soruh krz zh suhvhqw d prgho iru h sodqdwlrqv edvhg rq wkh xvhuv frqfhswxdo prgho ri wkh uhfrpphqgdwlrq surfhvv h wkhq suhvhqw h shulphqwdo uhvxowv ghprqvwudwlqj zkdw frpsrqhqwv ri dq h sodqdwlrq duh wkh prvw frpshoolqj 7r dgguhvv zk zh suhvhqw h shulphqwdo hylghqfh wkdw vkrzv wkdw surylglqj h sodqdwlrqv fdq lpsuryh wkh dffhswdqfh ri v vwhpv h dovr ghvfuleh vrph lql traps classifiers of temporal patterns the work proposes a radically different set of features for asr where temporal patterns of spectral energies are used in place of the conventional spectral patterns the approach has several inherent advantages among them robustness to stationary or slowly varying disturbances 1 introduction 1 1 spectral features in 1665 isaac newton made the following observation the filling of a very deepe flaggon with a constant streame of beere or water sounds yer vowells in this order w u o a e i y 8 what young newton observed was the spectral resonance peak which enhanced the spectrum of the beer pouring sound and moved up in frequency as the deepe flaggon was filling up since then attempts to find acoustic correlates of phonetic categories mostly followed newton s lead and studied the spectrum of speech spectrum based techniques form the basis of most feature extraction methods in current asr a problem with the spectrum of sound is that it can easily be modified by v temporal patterns traps in asr of noisy speech in this paper we study a new approach to processing temporal information for automatic speech recognition asr specifically we study the use of rather longtime temporal patterns traps of spectral energies in place of the conventional spectral patterns for asr the proposed neural traps are found to yield significant amount of complementary information to that of the conventional spectral feature based asr system a combination of these two asr systems is shown to result in improved robustness to several types of additive and convolutive environmental degradations 1 introduction 1 1 spectral features spectrum based techniques form the basis of most feature extraction methods in current asr a drawback of the spectral features is that they are quite sensitive to changes in the communication environment e g characteristics of different communication channels or environmental noise subsequently recognizers based on spectral features exhibit rapid degradation in performance in webadapter a prototype of a www browser with new special needs adaptations this paper presents a prototypical www world wide web browser called webadapter which provides new special needs adaptations for physically handicapped blind and visually impaired end users these adaptations include near miss tolerances implementation of sophisticated htmlguidelines and advanced speech output for evaluation purposes a usability test was conducted proving the suitability of the implemented special needs adaptations the future goal of this work is a user interface for all ui4all for a standard webbrowser with regard to this perspective the webadapter is still an reactive approach in that it only reacts to shortcomings of common webbrowsers instead of proactively integrating a standardized software layer between the user front end and underlying applications by which the i o interface can easily and universally be adapted to a variety of different personal needs of handicapped as well as able bodied end users thus the webadapter only illustrates robust entropy estimation strategies based on edge weighted random graphs with corrections in this paper we treat the problem of robust entropy estimation given a multidimensional random sample from an unknown distribution in particular we consider estimation of the renyi entropy of fractional order which is insensitive to outliers e g high variance contaminating distributions using the k point minimal spanning tree kmst a greedy algorithm for approximating the np hard problem of computing the k minimal spanning tree is given which is a generalization of the potential function partitioning method of ravi etal 1 the basis for our approach is an asymptotic theorem establishing that the log of the overall length or weight of the greedy approximation is a strongly consistent estimator of the renyi entropy quantitative robustness of the estimator to outliers is established using hampel s method of inuence functions 2 the structure of the inuence function indicates that the k mst is a natural extension of the one dimensional trimmed mean for multi dimensional estimation of r nyi information divergence via pruned minimal spanning trees in this paper we develop robust estimators of the r enyi information divergence i divergence given a reference distribution and a random sample from an unknown distribution estimation is performed by constructing a minimal spanning tree mst passing through the random sample points and applying a change of measure which flattens the reference distribution in a mixture model where the reference distribution is contaminated by an unknown noise distribution one can use these results to reject noise samples by implementing a greedy algorithm for pruning the k longest branches of the mst resulting in a tree called the k mst we illustrate this procedure in the context of density discrimination and robust clustering for a planar mixture model 1 introduction let xn fx 1 x 2 xn g denote a sample of i i d data points in r d having unknown lebesgue multivariate density f x i supported on 0 1 d define the order renyi entropy of f 7 h f 1 1 ln z solving a huge number of similar tasks a combination of multi task learning and a hierarchical bayesian approach in this paper we propose a machine learning solution to problems consisting of many similar prediction tasks each of the individual tasks has a high risk of overfitting we combine two types of knowledge transfer between tasks to reduce this risk multi task learning and hierarchical bayesian modeling multitask learning is based on the assumption that there exist features typical to the task at hand to find these features we train a huge two layered neural network each task has its own output but shares the weights from the input to the hidden units with all other tasks in this way a relatively large set of possible explanatory variables the network inputs is reduced to a smaller and easier to handle set of features the hidden units given this set of features and after an appropriate scale transformation we assume that the tasks are exchangeable this assumption allows for a hierarchical bayesian analysis in which the hyperparameters can be estimated from the data effect multiple agent probabilistic pursuit evasion games in this paper we develop a probabilistic framework for pursuit evasion games we propose a greedy policy to control a swarm of autonomous agents in the pursuit of one or several evaders at each instant of time this policy directs the pursuers to the locations that maximize the probability of finding an evader at that particular time instant it is shown that under mild assumptions this policy guarantees that an evader is found in finite time and that the expected time needed to find the evader is also finite simulations are included to illustrate the results 1 introduction this paper addresses the problem of controlling a swarm of autonomous agents in the pursuit of one or several evaders to this effect we develop a probabilistic framework for pursuit evasion games involving multiple agents the problem is nondeterministic because the motions of the pursuers evaders and the devices they use to sense their surroundings require probabilistic models it is also assumed that when multi faceted insight through interoperable visual information analysis paradigms to gain insight and understanding of complex information collections users must be able to visualize and explore many facets of the information this paper presents several novel visual methods from an information analyst s perspective we present a sample scenario using the various methods to gain a variety of insights from a large information collection we conclude that no single paradigm or visual method is sufficient for many analytical tasks often a suite of integrated methods offers a better analytic environment in today s emerging culture of information overload and rapidly changing issues we also conclude that the interactions among these visual paradigms are equally as important as if not more important than the paradigms themselves keywords information visualization user scenario information analysis document analysis 1 introduction information analysts face significant challenges dealing with the growing amount of information available and how to gain needed i the vesta approach to software configuration management vesta is a system for software configuration management it stores collections of source files keeps track of which versions of which files go together and automates the process of building a complete software artifact from its component pieces vesta s novel approach gives it three important properties first every build is repeatable because its component sources and build tools are stored immutably and immortally and its configuration description completely specifies what components and tools are used and how they are put together second every build is incremental because results of previous builds are cached and reused third every build is consistent because all build dependencies are automatically captured and recorded so that a cached result from a previous build is reused only when doing so is certain to be correct in addition vesta s flexible language for writing configuration descriptions makes it easy to describe large software configurations in a modular fashion and to create variant configurations by customizing build parameters this paper gives a brief overview of vesta outlining vesta s advantages over traditional tools how those benefits are achieved and the system s overall performance an integrated vision sensor for the computation of optical flow singular points a robust integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation the singular point in optical flow fields such as those generated by self motion measurements are shown of a fully parallel cmos analog vlsi motion sensor array which computes the direction of local motion sign of optical flow at each pixel and can directly implement this algorithm the flow field singular point is computed in real time with a power consumption of less than 2 mw computation of the singular point for more general flow fields requires measures of field expansion and rotation which it is shown can also be computed in real time hardware again using only the sign of the optical flow field these measures along with the location of the singular point provide robust real time self motion information for the visual guidance of a moving platform such as a robot 1 introduction visually guided navigation of autonomous vehicles requires robust measures real time error in location modeling for ubiquitous computing no matter which technologies or techniques a ubiquitous location system uses its measurements will have some amount of quantifiable error a survey and taxonomy of location systems for ubiquitous computing emerging mobile computing applications often need to know where things are physically located to meet this need many di erent location systems and technologies have been developed in this paper we present a the basic techniques used for location sensing describe a taxonomy of location system properties present a survey of research and commercial location systems that define the field show how the taxonomy can be used to evaluate location sensing systems and o er suggestions for future research it is our hope that this paper is a useful reference for researchers and location aware application builders alike for understanding and evaluating the many options in this domain 1 time series segmentation for context recognition in mobile devices recognizing the context of use is important in making mobile devices as simple to use as possible finding out what the user s situation is can help the device and underlying service in providing an adaptive and personalized user interface the device can infer parts of the context of the user from sensor data the mobile device can include sensors for acceleration noise level luminosity humidity etc in this paper we consider context recognition by unsupervised segmentation of time series produced by sensors dynamic programming can be used to find segments that minimize the intra segment variances while this method produces optimal solutions it is too slow for long sequences of data we present and analyze randomized variations of the algorithm one of them global iterative replacement or gir gives approximately optimal results in a fraction of the time required by dynamic programming we demonstrate the use of time series segmentation in context recognition for mobile phone applications 1 architecture for agent programming languages as the field of agent based systems continues to expand rapidly one of the most significant problems lies in being able to compare and evaluate the relative benefits and disadvantages of different systems in part this is due to the various different ways in which these systems are presented one solution is to develop a set of architectural building blocks that can be used as a basis for further construction to avoid re inventing wheels and to ensure a strong and effective yet simple and accessible means of presentation that allows for comparison and analysis of agent systems in this paper we address this issue in providing just such an architectural framework by using the 3apl agent programming language as a starting point for identification and specification of more general individual agent components this provides three additional benefits it moves the work further down the road of implementation contributes to a growing library of agent techniques and features and allows a detailed comparison of different agent based systems specified in similar ways 1 how does the observation strategy influence the correctness of alerting services application elds of alerting services range from digital libraries webbase a repository of web pages in this paper we study the problem of constructing and maintaining a large shared repository of web pages we discuss the unique characteristics of such a repository propose an architecture and identify its functional modules we focus on the storage manager module and illustrate how traditional techniques for storage and indexing can be tailored to meet the requirements of a web repository to evaluate design alternatives we also present experimental results from a prototype repository called webbase that is currently being developed at stanford university keywords repository webbase architecture storage management 1 introduction a number of important applications require local access to substantial portions of the web examples include traditional text search engines google avista related page services google alexa and topic based search and categorization services yahoo such applications typically access mine or index a local cache or repository of web economic value of ewa lite a functional theory of learning in games this paper describes a theory of learning in decisions and games called ewa lite with only one parameter ewa lite predicts the time path of individual behavior in any normal form game given initial conditions including new games in which behavior has never been observed between information and communication middle spaces in computer media for learning in this paper we identify two categories of media that are common in computer supported collaborative learning and software in general communication media and information media these two types of media map easily on to two types of social activities in which learning is grounded dialogue and monologue drawing on literature in learning theory we suggest the need for interfaces that helpstudents transition from dialogue to monologue and back again this middle space between communication and information interfaces is illustrated with several examples from cscl we advocate filling in this middle space with software and activities that transcend some of the traditional design tradeoffs associated with information and communication interfaces keywords collaboration interaction design tradeoffs introduction computers communication learning computer mediated acts of communication are becoming more commonplace in today s classroom like all media particular computer techn decidable fragments of first order temporal logics in this paper we introduce a new fragment of the first order temporal language called the monodic fragment in which all formulas beginning with a temporal operator since or until have at most one free variable we show that the satisfiability problem for monodic formulas in various linear time structures can be reduced to the satisfiability problem for a certain fragment of classical first order logic this reduction is then used to single out a number of decidable fragments of first order temporal logics and of two sorted first order logics in which one sort is intended for temporal reasoning besides standard first order time structures we consider also those that have only finite first order domains and extend the results mentioned above to temporal logics of finite domains we prove decidability in three different ways using decidability of monadic second order logic over the intended flows of time by an explicit analysis of structures with natural numbers time and by a composition method that builds a model from pieces in finitely many steps 1 probabilistic latent semantic analysis probabilistic latent semantic analysis is a novel statistical technique for the analysis of two mode and co occurrence data which has applications in information retrieval and filtering natural language processing machine learning from text and in related areas compared to standard latent semantic analysis which stems from linear algebra and performs a singular value decomposition of co occurrence tables the proposed method is based on a mixture decomposition derived from a latent class model this results in a more principled approach which has a solid foundation in statistics in order to avoid overfitting we propose a widely applicable generalization of maximum likelihood model fitting by tempered em our approach yields substantial and consistent improvements over latent semantic analysis in a number of experiments nexus an open global infrastructure for spatial aware applications due to the lack of a generic platform for location and spatial aware systems many basic services have to be reimplemented in each application that uses spatial awareness a cooperation among different applications is also difficult to achieve without a common platform in this paper we present a platform that solves these problems it provides an infrastructure that is based on computer models of regions of the physical world which are augmented by virtual objects we show how virtual objects make the integration of existing information systems and services in spatial aware systems easier furthermore our platform supports interactions between the computer models and the real world and integrates single models in a global augmented world contents 1 introduction 3 2 general idea 4 2 1 augmented areas 4 2 2 augmented world 6 3 example scenario 6 4 require visual recognition of hand motion hand gesture recognition is an active area of research in recent years being used in various applications from deaf sign recognition systems to humanmachine interaction applications the gesture recognition process in general may be divided into two stages the motion sensing which extracts useful data from hand motion and the classification process which classifies the motion sensing data as gestures the existing vision based gesture recognition systems extract 2 d shape and trajectory descriptors from the visual input and classify them using various classification techniques from maximum likelihood estimation to neural networks finite state machines fuzzy associative memory fam or hidden markov models hmms this thesis presents the framework of the vision based hand motion understanding hmu system that recognises static and dynamic australian sign language auslan signs by extracting and classifying 3 d hand configuration data from the visual input the hmu system is adaptive fuzzy expert system for sign recognition the hand motion understanding hmu system is a vision based australian sign language recognition system that recognises static and dynamic hand signs it uses a visual hand tracker to extract 3d hand configuration data from a visual motion sequence and a classifier that recognises the changes of these 3d kinematic data as a sign this paper presents the hmu classifier that uses an adaptive fuzzy inference engine for sign recognition fuzzy set theory allows the system to express the sign knowledge in natural and imprecise descriptions the hmu classifier has an adaptive engine that trains the system to be adaptive to the errors caused by the tracker or the motion variations exhibited amongst the signers the hmu system is evaluated with 22 static and dynamic auslan signs and recognised 20 signs before training and 21 signs after training of the hmu classifier keywords sign language sign recognition fuzzy logic adaptive fuzzy system expert system 1 introduction exploring mars developing indoor and outdoor user interfaces to a mobile augmented reality system we describe an experimental mobile augmented reality system mars testbed that employs different user interfaces to allow outdoor and indoor users to access and manage information that is spatially registered with the real world outdoor users can experience spatialized multimedia presentations that are presented on a head tracked see through head worn display used in conjunction with a hand held pen based computer indoor users can get an overview of the outdoor scene and communicate with outdoor users through a desktop user interface or a head and hand tracked immersive augmented reality user interface key words augmented reality wearable computing mobile computing hypermedia gps 1 introduction as computers increase in power and decrease in size new mobile and wearable computing applications are rapidly becoming feasible promising users access to online resources always and everywhere this new flexibility makes possible a new kind of application one that exploits situated documentaries embedding multimedia presentations in the real world we describe an experimental wearable augmented reality system that enables users to experience hypermedia presentations that are integrated with the actual outdoor locations to which they are are relevant our mobile prototype uses a tracked see through head worn display to overlay 3d graphics imagery and sound on top of the real world and presents additional coordinated material on a hand held pen computer we have used these facilities to create several situated documentaries that tell the stories of events that took place on our campus we describe the software and hardware that underly our prototype system and explain the user interface that we have developed for it database replication using epidemic update due to severe performance penalties associated with synchronous replication there is an increasing interest in asynchronous replica management protocols in which database transactions are executed locally and the effects of these transactions are incorporated asynchronously on remote database copies however the asynchronous protocols currently in use either do not guarantee consistency and serializability as needed by transactional semantics or they impose restrictions on placement of data and on which data objects can be updated in this paper we investigate an epidemic update protocol that guarantees consistency and serializability in spite of a write anywhere capability we conducted experiments on a detailed simulation of a distributed replicated database to evaluate this protocol our results establish that this epidemic approach is indeed a viable alternative to traditional eager update protocols for a distributed database environment where consistency and full seri exploiting planned disconnections in mobile environments we present the notion of a distributed database made up entirely of mobile components since disconnections will be frequent in such an environment we develop a disconnection and reconnection procedure to allow normal processing on the connected components we briefly discuss a protocol based on epidemic communication to support such a system while ensuring one copy serializability 1 introduction mobile computers and wireless networks are now being integrated into a variety of enterprises for different applications the prevailing mode of operation with occasional disconnection by a single user will rapidly evolve into a situation where many if not all users are disconnecting and reconnecting in networks that are created in an ad hoc manner e g a wireless network in a meeting room this will result in mobile computers being integrated as first class entities in distributed information systems such mobile computers will inevitably contain data and information that will need to b the play research group entertainment and innovation in sweden in a short time the research group play has established an unorthodox but effective work style where a creative approach to research in information technology is combined with a strong focus on achieving high quality results being a young research group both regarding the time it has existed and the average age of its members has presented play with both challenges and opportunities we face the challenge of building a credible basis for research in the academic community but also think that we have the opportunity to contribute innovative results to the research community and our industrial partners keywords hci research groups future hci european hci it design introduction how can one perform exciting and unorthodox research in information technology while still assuring that results are useful and of good quality how can a small group consisting mostly of relatively inexperienced students in a small country with very little traditions in groundbreaking it research supporting group collaboration with inter personal awareness devices an inter personal awareness device or ipad is a hand held or wearable device designed to support awareness and collaboration between people who are in the physical vicinity of each other an ipad is designed to supply constant awareness information to users in any location without relying on an underlying infrastructure we have constructed one such device the hummingbird which gives members of a group continuous aural and visual indication when other group members are close we have used the hummingbirds in several different situations to explore how they affect group awareness these experiences indicated that the hummingbird increased awareness between group members and that it could complement other forms of communication such as phone and e mail in particular we found the hummingbird to be useful when a group of people were in an unfamiliar location for instance during a trip where no other communication support was available we argue that ipads such as th automatic reclustering of objects in very large databases for high energy physics in the very large object database systems planned for some future particle physics experiments typical physics analysis jobs will traverse millions of read only objects many more objects than fit in the database cache thus a good clustering of objects on disk is highly critical to database performance we present the implementation and performance measurements of a prototype reclustering mechanism which was developed to optimise i o performance under the changing access patterns in a high energy physics database reclustering is done automatically and on line the methods used by our prototype differ greatly from those commonly found in proposed general purpose reclustering systems by exploiting some special characteristics of the access patterns of physics analysis jobs the prototype manages to keep database i o throughput close to the optimum throughput of raw sequential disk access keywords object oriented databases object clustering object reclustering automatic reclust distributed knowledge networks distributed knowledge networks dkn provide some of the key enabling technologies for translating recent advances in automated data acquisition digital storage computers and communications into fundamental advances in organizational decision support data analysis and related applications dkn include computational tools for accessing organizing transforming and analyzing the contents of heterogeneous distributed data and knowledge sources and for distributed problem solving and decision making under tight time resource and performance constraints this paper presents an overview of the dkn project in the iowa state university artificial intelligence laboratory i introduction advanced scientific research e g the genome project military applications e g intelligence data handling situation assessment command and control law enforcement e g terrorism prevention crisis management design and manufacturing systems and medical information infrastructure pow browsing information spaces this document contains the generic background and targets of the advanced information space browser which is planned to be included in the decomate ii library information system at tilburg university it gives an overview of the current state of the art in information retrieval focusing especially on topic browsers thesauri and semantic networks some preliminary ideas for actual implementations are included as well keywords semantic network conceptual modeling topic browsing document retrieval decomate ii 1 introduction the decomate ii library system currently under development at tilburg university in cooperation with several european partners aims at a web based single point user interface to a multitude of possibly distributed databases a single user query usually a set of keywords is mapped to all connected databases each with its own query language data schema and contents the individual query results are merged together by the system post processed to eli an information search cost perspective for designing interfaces for electronic commerce this research helps web developers apply knowledge on information search costs to the design of a web site for selling consumer products or services the goal of the research is to predict how subtle changes in the user interface design influence information search costs an empirical study compared 1 411 choices subjects made regarding a business to patronize from paper and electronic telephone directories the choices were contingent upon information search costs imposed by the media by providing a theoretical basis for predicting differences in information search costs this research helps designers create more effective web sites for achieving their marketing objectives 1 an information search cost perspective for designing interfaces for electronic commerce introduction now more than ever the promise of electronic commerce and on line shopping will depend to a great extent upon the user interface and how people interact with the computer in particular success will depend using self diagnosis to adapt organizational structures the specific organization used by a multi agent system is crucial for its effectiveness and efficiency in dynamic environments or when the objectives of the system shift the organization must therefore be able to change as well in this abstract we propose using a general diagnosis engine to drive this process of adaptation using the tms modeling language as the primary representation of organizational information a complete version of this paper is at 1 as the sizes of multi agent systems grow in the number of their participants the organization of those agents will be increasingly important in such an environment an organization is used to limit the range of control decisions agents must make which is a necessary component of scalable systems are agent agents arranged in clusters a hierarchy a graph or some other type of organization are the agents activities or behaviors driven solely by local concerns or do external peers or managers have direct influence as wel diagnosis as an integral part of multi agent adaptability agents working under real world conditions may face an environment capable of changing rapidly from one moment to the next either through perceived faults unexpected interactions or adversarial intrusions to gracefully and efficiently handle such situations the members of a multi agent system must be able to adapt either by evolving internal structures and behavior or repairing or isolating those external influenced believed to be malfunctioning the first step in achieving adaptability is diagnosis being able to accurately detect and determine the cause of a fault based on its symptoms in this paper we examine how domain independent diagnosis plays a role in multi agent systems including the information required to support and produce diagnoses particular attention is paid to coordination based diagnosis directed by a causal model several examples are described in the context of an intelligent home environment and the issue of diagnostic sensitivity versus efficiency is ad using diagnosis to learn contextual coordination rules knowing when and how to communicate and coordinate with other agents in a multi agent system is an important efficiency and reliability question contextual rules governing this communication must be provided to the agent or generated at runtime through environmental analysis in this paper we describe how the taems task modeling language is used to encode such contextual coordination rules and how runtime diagnosis can be used to dynamically update them overview communication and coordination is an essential component of most complex multi agent systems contention over shared resources the desire to employ remote information and the need to coordinate interrelated activities may each require some sort of information transfer between agents to be resolved to this end individual actors in a multi agent system must be able to explicitly or implicitly communicate requests and results desires and beliefs to make the system an efficient and cohesive unit thus a set of situation ai in medicine on its way from knowledge intensive to data intensive systems the last 20 years of research and development in the field of artificial intelligence in medicine show a path from knowledge intensive systems which try to capture the essential knowledge of experts in a knowledge based system to data intensive systems available today nowadays enormous amounts of information is accessible electronically large data sets are collected continuously monitoring physiological parameters of patients knowledgebased systems are needed to make use of all these data available and to help us to cope with the information explosion in addition temporal data analysis and intelligent information visualization can help us to get a summarized view of the change over time of clinical parameters integrating aim modules into the daily routine software environment of our care providers gives us a great chance for maintaining and improving quality of care 1 aim a partial view of its scope and potential this paper gives a personalized view of research and develop using an expressive description logic fact or fiction description logics form a family of formalisms closely related to semantic networks but with the distinguishing characteristic that the semantics of the concept description language is formally defined so that the subsumption relationship between two concept descriptions can be computed by a suitable algorithm description logics have proved useful in a range of applications but their wider acceptance has been hindered by their limited expressiveness and the intractability of their subsumption algorithms this paper addresses both these issues by describing a sound and complete tableaux subsumption testing algorithm for a relatively expressive description logic which in spite of the logic s worst case complexity has been shown to perform well in realistic applications 1 introduction description logics dls form a family of formalisms which have grown out of knowledge representation techniques using frames and semantic networks principles of mixed initiative user interfaces recent debate has centered on the relative promise of focusing user interface research on developing new metaphors and tools that enhance users abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation in this paper we review principles that show promise for allowing engineers to enhance human computer interaction through an elegant coupling of automated services with direct manipulation key ideas will be highlighted in terms of the lookout system for scheduling and meeting management keywords intelligent agents direct manipulation user modeling probability decision theory ui design introduction there has been debate among researchers about where great opportunities lay for innovating in the realm of human computer interaction 10 one group of researchers has expressed enthusiasm for the development and application of new kinds of automated services often referred to as interface agents the effo a rule induction approach to modeling regional pronunciation variation this 1 q er descril es the use of rule indue tion techniques fi r the mli omatic exl ra l ion of l honemic knowledge mm rules fl om pairs of l romm intion lexi a this xtra ted knowl edge allows the ndat tntion of sl ee h pro ess ing systelns to regional vm iants of a language as a case sl u ty we apply the approach to northern dutch and flemish the wtriant of dutch spoken in flan lers a t art of bel gium based oll c lex and l bnilex promm clarion lexi a tbr norttmrn l utch mm fhm ish r sl e tively in our study we omt ar l wo rule ilmu tion techniques franslbrmation b tsed error l riven learning i i e i brill 1995 mm c5 0 quinl m 1993 and valu ate the extr tct xl knowh dge quanl it l ively a ura cy mm qualitatively linguistic r levanc e of the rules we onchme that whereas classificntion 1 ased rule induct ion with c5 0 is 11101 0 a lr l e th rallst rnl l ion l quot ules le rne t with tbe1 i can 1 e more easily ini ert reted 1 architecture and implementation of a java package for multiple input devices mid a major difficulty in writing single display groupware co present collaborative applications is getting input from multiple devices we introduce mid a java package that addresses this problem and offers an architecture to access advanced events through java in this paper we describe the features architecture and limitations of mid we also briefly describe an application that uses mid to get input from multiple mice kidpad keywords single display groupware sdg computer supported cooperative work cscw multiple input devices mid multi modal input java directinput windows 98 universal serial bus usb kidpad jazz pad introduction communication collaboration and coordination are brought to many people s desktops thanks to groupware applications such as lotus notes and microsoft exchange some of the leading commercial products in the field of computer supported cooperative work cscw they help people collaborate when they are not in the same place at th examining locally varying weights for nearest neighbor algorithms previous work on feature weighting for case based learning algorithms has tended to use either global weights or weights that vary over extremely local regions of the case space this paper examines the use of coarsely local weighting schemes where feature weights are allowed to vary but are identical for groups or clusters of cases we present a new technique called class distribution weighting cdw that allows weights to vary at the class level we further extend cdw into a family of related techniques that exhibit varying degrees of locality from global to local the class distribution techniques are then applied to a set of eleven concept learning tasks we find that one or more of the cdw variants significantly improves classification accuracy for nine of the eleven tasks in addition we find that the relative importance of classes features and feature values in a particular domain determines which variant is most successful 1 introduction the k nearest neighbor k nn savvysearch a meta search engine that learns which search engines to query search engines are among the most successful applications on the web today so many search engines have been created that it is difficult for users to know where they are how to use them and what topics they best address meta search engines reduce the user burden by dispatching queries to multiple search engines in parallel the savvysearch meta search engine is designed to efficiently query other search engines by carefully selecting those search engines likely to return useful results and by responding to fluctuating load demands on the web savvysearch learns to identify which search engines are most appropriate for particular queries reasons about resource demands and represents an iterative parallel search strategy as a simple plan 1 the application meta search on the web companies institutions and individuals must have a presence on the web each are vying for the attention of millions of people not too surprisingly then the most successful applications on the web to dat learning gestures for visually mediated interaction this paper reports initial research on supporting visually mediated interaction vmi by developing person specific and generic gesture models for the control of active cameras we describe a time delay variant of the radial basis function tdrbf network and evaluate its performance on recognising simple pointing and waving hand gestures in image sequences towards visually mediated interaction using appearance based models this paper reports initial research on supporting visually mediated interaction vmi by developing generic expression models and person specific and generic gesture models for the control of active cameras we investigate the recognition of both head pose and expression using simple generalisation of trained generic models using radial basis function rbf networks then we go on to describe a time delay variant tdrbf of the network and evaluate its performance on recognising simple pointing and waving hand gestures in image sequences experimental results are presented that show that high levels of performance in gesture recognition can be obtained using these techniques both for particular individuals and across a set of individuals characteristic visual evidence can be automatically selected and used even to recognise individuals from their gestures depending on the task demands 1 introduction this paper reports initial research on supporting visually mediated gesture recognition for visually mediated interaction this paper reports initial research on supporting visually mediated interaction vmi by developing person specific and generic gesture models for the control of active cameras we describe a time delay variant of the radial basis function tdrbf network and evaluate its performance on recognising simple pointing and waving hand gestures in image sequences experimental results are presented that show that high levels of performance can be obtained for this type of gesture recognition using such techniques both for particular individuals and across a set of individuals characteristic visual evidence can be automatically selected depending on the task demands 1 introduction in general robust tracking of non rigid objects such as human bodies is difficult due to rapid motion occlusion and ambiguities in segmentation and model matching ongoing research at the mit media lab has shown progress in the modelling and interpretation of human body activity 24 30 31 compu genetic programming and multi agent layered learning by reinforcements we present an adaptation of the standard genetic program gp to hierarchically decomposable multi agent learning problems to break down a problem that requires cooperation of multiple agents we use the team objective function to derive a simpler intermediate objective function for pairs of cooperating agents we apply gp to optimize first for the intermediate then for the team objective function using the final population from the earlier gp as the initial seed population for the next this layered learning approach facilitates the discovery of primitive behaviors that can be reused and adapted towards complex objectives based on a shared team goal a mobile agent based active network architecture active networks enable customization of network functionality without the lengthy standard mediated committee processes most of the works in the literature utilize the capsules or active packets as the means to transfer code information across active networks in this paper we propose an active network infrastructure based on mobile agents technologies in our prototype implementation mobile agents are the building blocks of carrying functional customizations and the active nodes offer software application layers the agent servers to process mobile agent specific customizations to facilitate network functionality both integrated and discrete operational models of network customizations are supported in addition for the application specific protocol development and deployment an abstract protocol structure and a protocol loading mechanism are presented furthermore we provide an agent management control mechanism and devise a protocol management control mechanism as a result improved network functionality can be achieved 1 an overview of world wide web search technologies with over 800 million pages covering most areas of human endeavor the world wide web is fertile ground for information retrieval numerous search technologies have been applied to web searches and the dominant search method has yet to be identified this chapter provides an overview of existing web search technologies and classifies them into six categories i hyperlink exploration ii information retrieval iii metasearches iv sql approaches v content based multimedia searches and vi others a comparative study of some major commercial and experimental search services is presented and some future research directions for web searches are suggested keywords survey world wide web searches search engines and information retrieval 1 dialogue management for multimodal user registration user registration refers to associating certain personal information with a user it is widely used in hospitals hotels and conferences in this paper we propose an approach to interactive user registration by combining face recognition speech recognition and speech synthesis technologies together through an efficient dialogue manager in order to minimize a user s effort we employ a new dialogue management model based on a finite state automaton fsa which uses a baysian network to fuse the user s information from multiple channels e g face image speech records stored in a pre constructed database to reliably estimate the confidence about user identity instead of fixing weights the fsa adjusts its weights dynamically by integrating partial information from multiple information sources this is achieved by maximizing an objective function to determine an optimal action at each succeeding state according to current confidence and information cues thus the transition between states can be done along the shortest path from the initial state to the goal state we have developed a multimodal user registration system to demonstrate the feasibility of the proposed approach running the web backwards appliance data services appliance digital devices such as handheld cameras scanners and microphones generate data that people want to put on web pages unfortunately numerous complex steps are required contrast this with web output handheld web browsers enjoy increasing infrastructural support such as user transparent transformation proxies allowing unmodified web pages to be conveniently viewed on devices not originally designed for the task we hypothesize that the utility of input appliances will be greatly increased if they too were infrastructure enabled appliance data services attempts to systematically describe the task domain of providing seamless and graceful interoperability between input appliances and the web we offer an application architecture and a validating prototype that we hope will open up the playing field and motivate further work our initial efforts have identified two main design challenges dealing with device heterogeneity and providing a no futz out of the box user experience for novices without sacrificing expressive power for advanced users we address heterogeneity by isolating device and protocol heterogeneity considerations into a single extensible architectural component allowing most of the application logic to deal exclusively with web friendly protocols and formats we address the user interface issue in two ways first by specifying how to tag input with commands that specify how data is to be manipulated once injected into the infrastructure second by describing a late binding mechanism for these command tags which allows natural extensions of the device s ui for application selection and minimizes the amount of configuration required before end users benefit from appliance data services finally we describe how to leverage existi appliance data services making steps towards an appliance computing world although digital appliances are designed to be easy to use their users often cannot even perform simple tasks because the devices lack infrastructural support the appliance data services project seeks to explore the attributes of an appliance computing world and develop the infrastructure required to support users with digital appliances 1 an architecture for web agents in this paper we propose an extended bdi architecture for web agents the architecture is general it covers 2d web agent with text based interfaces for retrieval services as well as 3d web agents like avatar embodied guides that help visitors to navigate in virtual environments furthermore we define the primitives of sensor effector of web agents and show how those different types of web agents can be implemented based on the general architecture programmability of intelligent agent avatars in this paper we propose an approach to the programmability of intelligent agent avatars supported by the distributed logic programming language dlp intelligent agent avatars can be considered as one of the applications of web agents as one of the testbeds of 3d web agents we are developing and implementing soccer playing avatars we discuss how the language dlp can be used to support soccer playing avatars using rules to guide their behaviors in networked virtual environments categories and subject descriptors i 2 computing methodologies artificial intelligence h 4 m information systems miscellaneous general terms intelligent agent keywords avatar intelligent agent distributed logic programming networked virtual environment 1 instructable autonomous agents instructable autonomous agents by scott bradley huffman chair john e laird in contrast to current intelligent systems which must be laboriously programmed for each task they are meant to perform instructable agents can be taught new tasks and associated knowledge this thesis presents a general theory of learning from tutorial instruction and its use to produce an instructable agent tutorial instruction is a particularly powerful form of instruction because it allows the instructor to communicate whatever kind of knowledge a student needs at whatever point it is needed to exploit this broad flexibility however a tutorable agent must support a full range of interaction with its instructor to learn a full range of knowledge thus unlike most machine learning tasks which target deep learning of a single kind of knowledge from a single kind of input tutorability requires a breadth of learning from a broad range of instructional interactions the theory of learning from tutorial flexibly instructable agents this paper presents an approach to learning from situated interactive tutorial instruction within an ongoing agent tutorial instruction is a exible and thus powerful paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise to support this exibility however the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions our approach called situated explanation achieves such learning through a combination of analytic and inductive techniques it combines a form of explanation based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations the approach is implemented in an agent called instructo soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions instructo soar meets three key requirements of exible instructability that distinguish it from previous systems 1 it can take known or unknown commands at any instruction point 2 it can handle instructions that apply to either its current situation or to a hypothetical situation speci ed in language as in for instance conditional instructions and 3 it can learn from instructions each class of knowledge it uses to perform tasks 1 agent uml class diagrams revisited multiagent system designers already use agent uml in order to represent the interaction protocols 8 2 agent uml is a graphical modeling language based on uml as uml agent uml provides several types of representation covering the description of the system of the components the dynamics of the system and the deployment since agents and objects are not exactly the same one can guess that the uml class diagram has to be changed for describing agents the aim of this paper is to present how to extend agent uml class diagrams in order to represent agents we then compare our approach to bauer s approach 1 an application of agent uml to supply chain management agent uml is certainly the most well known graphical modeling language for describing multiagent systems but until now it is not applied to real world applications the aim of our project is to apply agent uml to the supply chain management this project has several objectives 1 it allows to prove that agent uml can be applied to real world applications 2 it allows to discover what is missing in agent uml and what is wrong and finally 3 it allows to define a methodology based on agent uml and several tools the aim of this paper is to sum up our first results on appying agent uml to the supply chain management and especially the paper sketches what diagrams are interesting and what could be done after in our project desiderata for agent oriented programming languages multiagent system designers need programming languages in order to develop agents and multiagent systems current approaches consist to use classical programming languages like c or c and above all java which is the most preferred language by agent community thanks to its rich library of functions the aim of java is not to design multiagent systems so it does not encompass multiagent features the aim of this paper is to present a set of characteristics which could be present in an agent oriented programming language this paper also describes what kind of multiagent systems could be developed with this set of characteristics extending agent uml protocol diagrams this paper is to present some new features that we propose several features deal with reliability such as triggering actions and exception handling these new features are in fact proposed due to our work in electronic commerce and in supply chain management 16 as much as possible we apply these new features to needs in the supply chain management example a language for exchanging agent uml protocol diagrams for several years interaction protocol designers have a new formalism which takes into account multiagent system features autonomy cooperation etc this formalism is called agent uml 6 for the moment designers can describe protocols with the agent uml protocol diagrams but they do not have a textual language in order to exchange protocols or to check properties on them the aim of this paper is to provide such a language this language is called axf agent uml exchange format and is structured as an xml file this paper presents the syntax of this language and applies axf to the example of the english auction protocol this paper is published as the technical report ulcs 02 009 from the department of computer science university of liverpool model checking agent uml protocol diagrams agents in multiagent systems use protocols in order to exchange messages and to coordinate together since agents and objects are not exactly the same designers do not use directly communication protocols used in distributed systems but a new type called interaction protocols encompassing agent features such as richer messages and the ability to cooperate and to coordinate obviously designers consider formal description techniques used for communication protocols new graphical modeling languages based on uml appeared several years ago agent uml is certainly the best known until now no validation is given for agent uml the aim of this paper is to present how to model check agent uml protocol diagrams merging potentially inconsistent items of structured text structured text is a general concept that is implicit in a variety of approaches to handling information syntactically an item of structured text is a number of grammatically simple phrases together with a semantic label for each phrase items of structured text may be nested within larger items of structured text the semantic labels in a structured text are meant to parameterize a stereotypical situation and so a particular item of structured text is an instance of that stereotypical situation much information is potentially available as structured text including tagged text in xml text in relational and object oriented databases and the output from information extraction systems in the form of instantiated templates in this paper we formalize the concept of structured text and then focus on how we can identify inconsistency in the logical representation of items of structured text we then present a new framework for merging logical theories that can be employed t ramification analysis using causal mapping to operate in the real world intelligent agents constantly need to absorb new information and to consider the ramifications of it this raises interesting questions for knowledge representation and reasoning here we consider ramification analysis in which we wish to determine both the likely outcomes from events occuring and the less likely but very significant outcomes from events occuring to formalize ramification analysis we introduce the notion of causal maps for modelling causal relationships between events in particular we consider existential event classes for example presidential election with instances being true false or unknown and directional events classes for example inflation with instances being increasing decreasing or unchanging using causal maps we can propagate new information to determine possible ramifications these ramifications are also described in terms of events whilst causal maps offer a lucid view on ramifications we also want to su bismarc a biologically inspired system for map based autonomous rover control as the complexity of the missions to planetary surfaces increases so too does the need for autonomous rover systems this need is complicated by the power mass and computer storage restrictions on such systems miller 1992 to address these problems we have recently developed a system called bismarc biologically inspired system for map based autonomous rover control for planetary missions involving multiple small lightweight surface rovers huntsberger 1997 bismarc is capable of cooperative planetary surface retrieval operations such as a multiple cache recovery mission to mars the system employs autonomous navigation techniques behavior based control for surface retrieval operations and an action selection mechanism based on a modified form of free flow hierarchy rosenblatt and payton 1989 this paper primarily describes the navigation and map mapping subsystems of bismarc they are inspired by some recent studies of london taxi drivers indicating that the right hippo spatially aware local communication in the raum system in this paper we propose a new paradigm for local communication between devices in ubiquitous computing environments assuming a multitude of computerized everyday appliances communicating with each other to solve tasks this paradigm is based on the concept that the location of devices is central for the communication in such a scenario devices define their communication scope by spatial criteria in our paradigm no explicit addressing or identification of communication partners is used in comparison to traditional communication methods the approach eases routing and discovery problems and can be deployed in a highly dynamic environment without centralized services we use the term local communication as inter device communication in a physically restricted local area this is well distinguish from the terms telecommunication as communication over distance where location information is explicitly hidden the communication model raum introduced is based on the observ attribute grammars for genetic representations of neural networks and syntactic constraints of genetic programming context free grammar augmented by the assignment of semantic attributes to the symbols of the grammar a production rule specifies not only the replacement of symbols but also the evaluation of the symbol s attributes in our research an attribute grammar is used to specify classes of neural network structures with explicit representation of their functional organization these representations provide useful constraints upon a genetic optimization that guarantee the preservation of syntactically correct genetic trees with semantically meaningful sub trees in this paper we give a broad overview of our research into attribute grammar representations from the basic and known capabilities to the current ideas being addressed to the future directions of our research attribute grammars and neural networks normal forms and proofs in combined modal and temporal logics in this paper we present a framework for the combination of modal and temporal logic this framework allows us to combine different normal forms in particular a separated normal form for temporal logic and a first order clausal form for modal logics the calculus of the framework consists of temporal resolution rules and standard first order resolution rules we show that the calculus provides a sound complete and terminating inference systems for arbitrary combinations of subsystems of multimodal s5 with linear temporal logic 1 verification within the karo agent theory abstract this paper discusses automated reasoning in the karo framework the karo framework accommodates a range of expressive modal logics for describing the behaviour of intelligent agents we concentrate on a core logic within this framework in particular we describe two new methods for providing proof methods for this core logic discuss some of the problems we have encountered in their design and present an extended example of the use of the karo framework and the two proof methods 1 kea practical automatic keyphrase extraction keyphrases provide semantic metadata that summarize and characterize documents this paper describes kea an algorithm for automatically extracting keyphrases from text kea identifies candidate keyphrases using lexical methods calculates feature values for each candidate and uses a machine learning algorithm to predict which candidates are good keyphrases the machine learning scheme first builds a prediction model using training documents with known keyphrases and then uses the model to find keyphrases in new documents we use a large test corpus to evaluate kea s effectiveness in terms of how many author assigned keyphrases are correctly identified the system is simple robust and publicly available introduction keyphrases provide a brief summary of a document s contents as large document collections such as digital libraries become widespread the value of such summary information increases keywords and keyphrases 1 are particularly useful because they can be interpret intelligent anticipated exploration of web sites in this paper we describe a web search agent called global search agent hereafter gsa for short gsa integrates and enhances several search techniques in order to achieve significant improvements in the user perceived quality of delivered information as compared to usual web search engines gsa features intelligent merging of relevant documents from different search engines anticipated selective exploration and evaluation of links from the current resuk set automated derivation of refined queries based on user relevance feedback system architecture as well as experimental accounts are also illustrated a survey of agent oriented methodologies this article introduces the current agent oriented methodologies it discusseswhat approacheshave been followed mainly extending existing objectoriented and knowledge engineering methodologies the suitability of these approaches for agent modelling and some conclusions drawn from the survey 1 introduction agent technology has received a great deal of attention in the last few years and as a result the industry is beginning to get interested in using this technology to develop its own products in spite of the different developed agent theories languages architectures and the successful agent based applications very little work for specifying and applying techniques to develop applications using agent technology has been done the role of agent oriented methodologies is to assist in all the phases of the life cycle of an agent based application including its management this article reviews the current approaches to the development of an agent oriented ao methodology towards the use of case properties for maintaining case based reasoning systems because of the importance of maintenance in the realm of case based reasoning systems methods of maintaining case bases using case properties will be presented the necessary notation is given along with definitions of the properties themselves which are correctness consistency incoherence minimality and uniqueness use of these properties in five experiments is explained and the results of these experiments on three real world case bases is given while the prediction accuracy remains constant the case base size is reduced up to 69 1 1 introduction the maintaining of case based reasoning systems has become increasingly an important research topic during the last few years for example a workshop entitled flexible strategies for maintaining knowledge containers 7 held at the 14th european conference on artificial intelligence gave researchers the opportunity to present and discuss new progress in this field the development of useful and accurate quality measur evolving swimming controllers for a simulated lamprey with inspiration from neurobiology this paper presents how neural swimming controllers for a simulated lamprey can be developed using evolutionary algorithms a genetic algorithm is used for evolving the architecture of a connectionist model which determines the muscular activity of a simulated body this work is inspired by the biological model developed by ekeberg which reproduces the central pattern generator observed in the real lamprey ekeberg 93 in evolving artificial controllers we demonstrate that a genetic algorithm can be an interesting design technique for neural controllers and that there exist alternative solutions to the biological connectivity a variety of neural controllers are evolved which can produce the pattern of oscillations necessary for swimming these patterns can be modulated through the external excitation applied to the network in order to vary the speed and the direction of swimming the best evolved controllers cover larger ranges of frequencies phase lags and speeds of swimming than evolution and development of a central pattern generator for the swimming of a lamprey this paper describes the design of neural control architectures for locomotion using an evolutionary approach inspired by the central pattern generators found in animals we develop neural controllers which can produce the patterns of oscillations necessary for the swimming of a simulated lamprey this work is inspired by ekeberg s neuronal and mechanical model of a lamprey 11 and follows experiments in which swimming controllers were evolved using a simple encoding scheme 26 25 here controllers are developed using an evolutionary algorithm based on the sgoce encoding 31 32 in which a genetic programming approach is used to evolve developmental programs which encode the growing of a dynamical neural network the developmental programs determine how neurons located on a 2d substrate produce new cells through cellular division and how they form efferent or afferent interconnections swimming controllers are generated when the growing networks eventually create connections to revisiting and versioning in virtual special reports adaptation personalization is one of the main issues for web applications and require large repositories creating adaptive web applications from these repositories requires to have methods to facilitate web application creation and management and to ensure reuse sharing and exchange of data through the internet intranet virtual documents deal with these issues in our framework we are interested in adaptive virtual documents for author oriented web applications providing several reading strategies to readers these applications have the following characteristics authors have know how which enables them to choose document contents and to organize them in one or more consistent ways a reading strategy and the corresponding content are semantically coherent and convey a particular meaning to the readers such author s know how can be represented at knowledge level and then be used for generating web documents dynamically for ensuring reader comprehension and for sharing and reuse then an adaptive virtual document can be computed on the fly by means of a semantic composition engine using i an overall document structure for instance a narrative structure representing a reading strategy for which node contents are linked at run time according to user s needs for adaptation ii an intelligent search engine and semantic metadata relying on semantic web initiative and iv a user model in this paper we focus on a semantic composition engine enabling us to compute on the fly adaptive personalized web documents in the iccars project its main goal is to assist the journalist in building adaptive special reports in such a framework adaptation personalization and reusability are central issues for delivering adaptive special reports controlling speculative computation in multi agent environments in this paper we propose a multi agent system which performs speculative computation under incomplete communication environments in a master slave style multi agent system with speculative computation a master agent asks queries to slave agents in problem solving and proceeds computation with default answers when answers from slave agents are delayed we rst provide a semantics for speculative computation using default logic then in the proposed system we use the consequence nding procedure sol written in the java language to perform data driven deductive reasoning the use of a consequence nding procedure is convenient for updating agents beliefs according to situation changes in the world in our system slave agents can change their answers frequently yet a master agent can avoid duplicate computation as long as actual answers from slave agents do not con ict with any previously encountered situation the obtained conclusions are never recomputed we applied the proposed system to the meeting room reservation problem to see the usefulness of the framework 1 computing extended abduction through transaction programs this paper we propose a computational mechanism for extended abduction when a background theory is written in a normal logic program we introduce its transaction program for computing extended abduction a transaction program is a set of non deterministic production rules that declaratively specify addition and deletion of abductive hypotheses abductive explanations are then computed by the fixpoint of a transaction program using a bottom up model generation procedure the correctness of the proposed procedure is shown for the class of acyclic covered abductive logic programs in the context of deductive databases a transaction program provides a declarative specification of database update keywords abduction nonmonotonic reasoning database update 1 introduction 1 1 motivation and background abduction is inference to best explanations and has recently been recognized as a very important form of reasoning in both ai 2 feature subset selection by bayesian networks based optimization in this paper we perform a comparison among fss ebna a randomized populationbased and evolutionary algorithm and two genetic and other two sequential search approaches in the well known feature subset selection fss problem in fss ebna the fss problem stated as a search problem uses the ebna estimation of bayesian network algorithm search engine an algorithm within the eda estimation of distribution algorithm approach the eda paradigm is born from the roots of the ga community in order to explicitly discover the relationships among the features of the problem and not disrupt them by genetic recombination operators the eda paradigm avoids the use of recombination operators and it guarantees the evolution of the population of solutions and the discovery of these relationships by the factorization of the probability distribution of best individuals in each generation of the search in ebna this factorization is carried out by a bayesian network induced by a chea probe count and classify categorizing hidden web databases the contents of many valuable web accessible databases are only accessible through search interfaces and are hence invisible to traditional web crawlers recent studies have estimated the size of this hidden web to be 500 billion pages while the size of the crawlable web is only an estimated two billion pages recently commercial web sites have started to manually organize web accessible databases into yahoo like hierarchical classification schemes in this paper we introduce a method for automating this classification process by using a small number of query probes to classify a database our algorithm does not retrieve or inspect any documents or pages from the database but rather just exploits the number of matches that each query probe generates at the database in question we have conducted an extensive experimental evaluation of our technique over collections of real documents including over one hundred web accessible databases our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases 1 digital city kyoto towards a social information infrastructure this paper proposes the concept of digital cities as a social information infrastructure for urban life including shopping business transportation education welfare and so on we propose the three layer architecture for digital cities a the information layer integrates both www archives and real time sensory information related to the city b the interface layer provides 2d and 3d views of the city and c the interaction layer assists social interaction among people who are living visiting in at the city we started a three year project to develop a digital city for kyoto the old capital and cultural center of japan based on the newest technologies including gis 3d animation agents and mobile computing this paper introduces the system architecture and the current status of digital city kyoto 1 introduction as the number of internet users is continuing to increase various community networks are being tested 1 the internet is used not only for research model for unistroke writing time unistrokes are a viable form of text input in pen based user interfaces however they are a very heterogeneous group of gestures the only common feature being that all are drawn with a single stroke several unistroke alphabets have been proposed including the original unistrokes graffiti allegro t cube and mditim comparing these methods usually requires a lengthy study with many writers and even then the results are biased by the earlier handwriting experience that the writers have therefore a simple descriptive model can make these comparisons easier in this paper we propose a model for predicting the writing time for an expert user on any given unistroke alphabet thus enabling sounder argumentation on the properties of different writing methods keywords modeling of motor performance handwriting pen input introduction unistrokes were introduced as a text input method for penbased user interfaces by goldberg and richardson in their 1993 paper 8 unistrokes are an alte automatic hierarchical e mail classification using association rules the explosive growth of on line communication in particular e mail communication makes it necessary to organize the information for faster and easier processing and searching storing e mail messages into hierarchically organized folders where each folder corresponds to a separate topic has proven to be very useful previous approaches to this problem use nave bayes or tf idf style classifiers that are based on the unrealistic term independence assumption these methods are also context insensitive in that the meaning of words is independent of presence absence of other words in the same message it was shown that text classification methods that deviate from the independence assumption and capture context achieve higher accuracy in this thesis we address the problem of term dependence by building an associative classifier called classification using cohesion and multiple association rules or comar in short the problem of context capturing is addressed by looking for phrases in message corpora both rules and phrases are generated using an efficient fp growth like approach since the amount of rules and phrases produced can be very large we propose two new measures rule cohesion and phrase cohesion that possess the anti monotone property which allows the push of rule and phrase pruning deeply into the process of their generation this approach to pattern pruning proves to be much more efficient than generate and prune methods both unstructured text attributes and semi structured non text attributes such as senders and recipients are used for the classification comar classification algorithm uses multiple rules to predict several highest probability topics for each message different feature selection and rule ranking methods are compared our studies show a model of saliency based visual attention for rapid scene analysis a visual attention system inspired by the behavior and the neuronal architecture of the early primate visual system is presented multiscale image features are combined into a single topographical saliency map a dynamical neural network then selects attended locations in order of decreasing saliency the system breaks down the complex problem of scene understanding by rapidly selecting in a computationally efficient manner conspicuous locations to be analyzed in detail index terms visual attention scene analysis feature extraction target detection visual search pi i introduction primates have a remarkable ability to interpret complex scenes in real time despite the limited speed of the neuronal hardware available for such tasks intermediate and higher visual processes appear to select a subset of the available sensory information before further processing 1 most likely to reduce the complexity of scene analysis 2 this selection appears to be implemented in the a comparison of feature combination strategies for saliency based visual attention systems bottom up or saliency based visual attention allows primates to detect non specific conspicuous targets in cluttered scenes a classical metaphor derived from electrophysiological and psychophysical studies describes attention as a rapidly shiftable spotlight the model described here reproduces the attentional scanpaths of this spotlight simple multi scale feature maps detect local spatial discontinuities in intensity color orientation or optical flow and are combined into a unique master or saliency map the saliency map is sequentially scanned in order of decreasing saliency by the focus of attention we study the problem of combining feature maps from different visual modalities and with unrelated dynamic ranges such as color and motion into a unique saliency map four combination strategies are compared using three databases of natural color images 1 simple normalized summation 2 linear combination with learned weights 3 global non linear normalization learning to detect salient objects in natural scenes using visual attention in the primate s visual system selective attention rapidly selects conspicuous image locations to be analyzed in more details such selection is guided by several low level feature extraction mechanisms which detect candidate salient locations based on their local properties for a given feature type e g intensity color orientation or motion one difficulty which arises is how the information from different modalities should be combined into a single saliency map controlling where visual attention should be focused here we quantitatively compare three feature combination strategies simple summation learned linear summation and contents based non linear normalization using three databases of natural color images 1 introduction bottom up or saliency based visual attention allows primates to detect in real time nonspecific conspicuous targets from cluttered visual environments reproducing such nonspecific target detection capability in artificial systems has important adaptive query processing for internet applications as the area of data management for the internet has gained in popularity recent work has focused on effectively dealing with unpredictable dynamic data volumes and transfer rates using adaptive query processing techniques important requirements of the internet domain include 1 the ability to process xml data as it streams in from the network in addition to working on locally stored data 2 dynamic scheduling of operators to adjust to i o delays and flow rates 3 sharing and re use of data across multiple queries where possible 4 the ability to output results and later update them an equally important consideration is the high degree of variability in performance needs for different query processing domains perhaps an ad hoc query application should optimize for display of incomplete and partial incremental results whereas a corporate data integration application may need the best time to completion and may have very strict data freshness guarantees the goal of boosting for document routing rankboost is a recently proposed algorithm for learning ranking functions it is simple to implement and has strong justifications from computational learning theory we describe the algorithm and present initial experimental results on applying it to the document routing problem the first set of results applies rankboost to a text representation produced using modern term weighting methods performance of rankboost is somewhat inferior to that of a state of the art routing algorithm which is however more complex and less theoretically justified than rankboost rankboost achieves comparable performance to the state of the art algorithm when combined with feature or example selection heuristics our second set of results examines the behavior of rankboost when it has to learn not only a ranking function but also all aspects of term weighting from raw data performance is usually though not always less good here but the term weighting functions implicit in the resulting ranking fun a software model and specification language for non wimp user interfaces we present a software model and language for describing and programming the fine grained aspects of interaction in a non wimp user interface such as a virtual environment our approach is based on our view that the essence of a non wimp dialogue is a set of continuous relationships most of which are temporary the model combines a data flow or constraint like component for the continuous relationships with an event based component for discrete interactions which can enable or disable individual continuous relationships to demonstrate our approach we present the pmiw user interface management system for non wimp interactions a set of examples running under it a visual editor for our user interface description language and a discussion of our implementation and our restricted use of constraints for a performance driven interactive situation our goal is to provide a model and language that captures the formal structure of non wimp interactions in the way that various previous te querying network directories hierarchically structured directories have recently proliferated with the growth of the internet and are being used to store not only address books and contact information for people but also personal pro les network resource information and network and service policies these systems provide a means for managing scale and heterogeneity while allowing for conceptual unity and autonomy across multiple directory servers in the network in a way far superior to what conventional relational or object oriented databases o er yet in deployed systems today much of the data is modeled in an ad hoc manner and many of the more sophisticated queries quot involve navigational access in this paper we develop the core of a formal data model for network directories and propose a sequence of e ciently computable query languages with increasing expressive power the directory data model can naturally represent rich forms of heterogeneity exhibited in the real world answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity we present external memory algorithms for the evaluation of queries posed in our directory query languages and prove the e ciency of each algorithm in terms of its i o complexity our data model and query languages share the exibility and utility of the recent proposals for semi structured data models while at the same time e ectively addressing the speci c needs of network directory applications which we demonstrate by means of a representative real life example this work was done when the authors were at at t labs integrating multiple classifiers in visual object detectors learned from user input there have been many recent efforts in contentbased retrieval to perform automatic classification of images visual objects most approaches however have focused on using individual classifiers in this paper we study the way in which in a dynamic framework multiple classifiers can be combined when applying visual object detectors we propose a hybrid classifier combination approach in which decisions of individual classifiers are combined in the following three ways 1 classifier fusion 2 classifier cooperation and 3 hierarchical combination in earlier work we presented the visual apprentice framework in which a user defines visual object models via a multiple level object definition hierarchy region perceptual area object part and object as the user provides examples from images or videos visual features are extracted and multiple classifiers are learned for each node of the hierarchy in this paper we discuss the benefits of hybrid classifier combination in the visual apprentice framework and show some experimental results in classifier fusion these results suggest possible improvements in classification accuracy particularly of detectors reported earlier for baseball video images with skies and images with handshakes a comparison of mobile agent and client server paradigms for information retrieval tasks in virtual enterprises in next generation enterprises it will become increasingly important to retrieve information efficiently and rapidly from widely dispersed sites in a virtual enterprise and the number of users who wish to do using wireless and portable devices will increase significantly this paper considers the use of mobile agent technology rather than traditional clientserver computing for information retrieval by mobile and wireless users in a virtual enterprise we argue that to be successful mobile agent platforms must coexist with and be presented to the applications programmer sideby side with traditional client server middleware like corba and dcom and we sketch a middleware architecture for doing so we then develop an analytical model that examines the claimed performance benefits of mobile agents over client server computing for a mobile information retrieval scenario our evaluation of the model shows that mobile agents are not always better than client server calls in terms of average response times they are only beneficial if the space overhead of the mobile agent code is not too large or if the wireless link connecting the mobile user to the fixed servers of the virtual enterprise is error prone statistical pattern recognition a review abstract the primary goal of pattern recognition is supervised or unsupervised classification among the various frameworks in which pattern recognition has been traditionally formulated the statistical approach has been most intensively studied and used in practice more recently neural network techniques and methods imported from statistical learning theory have been receiving increasing attention the design of a recognition system requires careful attention to the following issues definition of pattern classes sensing environment pattern representation feature extraction and selection cluster analysis classifier design and learning selection of training and test samples and performance evaluation in spite of almost 50 years of research and development in this field the general problem of recognizing complex patterns with arbitrary orientation location and scale remains unsolved new and emerging applications such as data mining web searching retrieval of multimedia data face recognition and cursive handwriting recognition require robust and efficient pattern recognition techniques the objective of this review paper is to summarize and compare some of the well known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field minimal simulations for evolutionary robotics this paper the line is drawn between controller and environment 3 1 1 drawing the line between controller and environment running across the reality gap octopod locomotion evolved in a minimal simulation this paper describes experiments in which neural network control architectures were evolved in minimal simulation for an octopod robot the robot is around 30cm long and has 4 infra red sensors that point ahead and to the side various bumpers and whiskers and ten ambient light sensors positioned strategically around the body each of the robot s eight legs is controlled by two servo motors one for movement in the horizontal plane and one for movement in the vertical plane which means that the robots motors have a total of sixteen degrees of freedom the aim of the experiments was to evolve neural network control architectures that would allow the robot to wander around its environment avoiding objects using its infra red sensors and backing away from objects that it hits with its bumpers this is a hard behaviour to evolve when one considers that in order to achieve any sort of coherent movement the controller has to control not just one or two motors in a coordinated fashion bu an integrated secure web architecture for protected mobile code distribution ipr intellectual property rights protection is one of the key elements to be considered in the development of mobile code technologies applets agents etc due to the mobile nature of this kind of software and the power of servers the absence of protection would increase the risk of piracy to such a level that the economy of this sector would be weakened perhaps even destroyed filigrane flexible ipr for software agent reliance framework complementary to the legal provisions anti piracy laws ipr protection is one of the absolute elements in the development of these new markets in the course of esprit project filigrane flexible ipr for software agent reliance we developed an integrated web architecture and associated security framework and protocol for the trading of mobile code in internet the term mobile code includes all kinds of mobile java software applets and agents and java beans cardlets etc 1 a multimodal shopping assistant for home e commerce electronic commerce has rapidly grown with the expansion of the internet e commerce has also become a promising field for applying agent and artificial intelligence technologies software agents help to automate a variety of tasks including those involved in buying and selling products over the internet in this paper we describe a multimodal intelligent shopping assistant developed in the embassi project 1 embassi is a project involving more than twenty big german companies and sponsored by bmbf 3 its goal is not to focus on the unlimited possibilities of this technology but rather on the individual prerequisites of the human in contact with it therefore the user interfaces of a big class of appliances and systems including shopping and ecommerce need to be easily and efficiently accessible for everyone taking into account psychological and ergonomic aspects and using innovative interaction techniques by realization of intelligent anthropomorphous assistants keywords intelligent agents assistant systems e commerce multimodality home shopping mobile agents 1 gql a reasonable complex sql for genomic databases validating hypotheses and reasoning about objects is becoming commonplace in biotechnology research the capability to reason strengthens comparative genomics research by providing the much needed tool to pose intelligent queries in a more convenient and declarative fashion to be able to reason using genomic query language gql we propose the idea of parameterized views as an extension of sql s create view construct with an optional with parameter clause parameterizing enables traditional sql views to accept input values and delay the computation of the view until invoked with a call statement this extension empowers users with the capability of modifying the behavior of predened procedures views by sending arguments and evaluating the procedure on demand we demonstrate that the extension is soundly based with a parallel in datalog we also show that the idea of relational unication proposed here empowers sql to reason and infer in exactly the same way as an object oriented a case for parameterized views and relational unification in this paper we address the issue of remedying the scepter of impedance mismatch in object relational sql our approach makes it possible to remain within the current connes of relational models yet oers the capability of dening methods by sql s declarative means thereby preserving all opportunities of query optimization to the fullest extent we propose the idea of parameterized views and an extension of sql s create view construct with an optional with parameter clause parameterizing enables traditional sql views to accept input values and delay the computation of the view until invoked with a call statement this extension empowers users with the capability of modifying the behavior of predened procedures views by sending arguments and evaluating the procedure on demand keywords parameterized views declarative methods objectrelational databases inheritance and overriding reasoning unication 1 introduction an outstanding issue in object relational databases dem belief reasoning in mls deductive databases it is envisaged that the application of the multilevel security mls scheme will enhance exibility and e ectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view de nitions on a per user basis however as advances in this area are being made and ideas crystallized the concomitantweaknesses of the mls databases are also surfacing we insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported critics also argue that it is imperative for mls database users to theorize about the belief of others perhaps at di erent security levels an apparatus that is currently missing and the absence of which is seriously felt the impetus for our current research is this need to provide an adequate framework for belief reasoning in mls databases we demonstrate that a prudent application of the concept of inheritance in a deductive database setting will help capture the notion of declarative belief and belief reasoning in mls databases in an elegantway to this end we develop a function to compute belief in multiple modes which can be used to reason about the beliefs of other users we strive to develop a poised and practical logical characterization of mls databases for the rst time based on the inherently di cult concept of non monotonic inheritance we present an extension of the acclaimed datalog language called the multilog and show that datalog is a special case of our language we also suggest an implementation scheme for multilog as a front end for coral key words mls databases belief assertion reasoning information extraction via heuristics for a movie showtime query system semantic interpretation for limited domain spoken dialogue systems often amounts to extracting information from utterances for a system that provides movie showtime information queries are classified along four dimensions question type and movie titles towns and theaters that were mentioned simple heuristics suffice for constructing highly accurate classifiers for the latter three attributes classifiers for the question type attribute are induced from data using features tailored to spoken language phenomena since separate classifiers are used for the four attributes which are not independent certain errors can be detected and corrected thus increasing robustness 1 mobile agents in intrusion detection and response effective intrusion detection capability is an elusive goal not solved easily or with a single mechanism however mobile software agents go a long way toward realizing the ideal behavior desired in an intrusion detection system ids this paper is an initial look at the relatively unexplored terrain of using mobile agents for intrusion detection and response it looks not only at the benefits derived from mobility but also those associated with software agent technology we explore these benefits in some detail and propose a number of innovative ways to apply agent mobility to address the shortcomings of current ids designs and implementations we also look at new approaches for automating response to an intrusion once detected 1 appeared in the proceedings of the 12th annual canadian information technology security symposium ottawa canada june 2000 mobile agents in intrusion detection and response 1 mobile agents in intrusion detection and response background intrusion case based representation and learning of pattern languages pattern languages seem to suit case based reasoning particularly well therefore the problem of inductively learning pattern languages is paraphrased in a case based manner a careful investigation requires a formal semantics for case bases together with similarity measures in terms of formal languages two basic semantics are introduced and investigated it turns out that representability problems are major obstacles for case based learnability restricting the attention to so called proper patterns avoids these representability problems a couple of learnability results for proper pattern languages are derived both for case based learning from only positive data and for case based learning from positive and negative data under the so called competing semantics we show that the learnability result for positive and negative data can be lifted to the general case of arbitrary patterns learning under the standard semantics from positive data is closely related to monotonic language l types of incremental learning this paper is intended to introduce a closer look at incremental learning by developing the two concepts of informationally incremental learning and operationally incremental learning these concept are applied to the problem of learning containment decision lists for demonstrating its relevance 1 introduction the intention of the present paper is to introduce two new notions in incremental learning which allow a classification of phenomena finer than known so far in the area these concepts are denoted by the phrases informationally incremental learning and operationally incremental learning respectively roughly spoken informationally incremental algorithms are required to work incrementally as usual i e they have no permission to look back at the whole history of information presented during the learning process operationally incremental learning algorithms may have permission to look back but they are not allowed to use information of the past in some effective way nonstandard concepts of similarity in case based reasoning introduction the present paper is aimed at propagating new concepts of similarity more flexible and expressive than those underlying most case based reasoning approaches today so it mainly deals with criticizing approaches in use with motivating and introducing new notions and notations and with first steps towards future applications the investigations at hand originate from the author s work in learning theory in exploring the relationship between inductive learning and case based learning within a quite formal setting cf jan92b it turned out that both areas almost coincide if sufficiently flexible similarity concepts are taken into acount this provides some formal arguments for the necessity of non symmetric similarity measures encouraged by these first results the author tried to investigate more structured learning problems from the view point of case based reasoning it turned out that an appropriate handling requires formalisms allowing similarity concep developing and investigating two level grammar concepts for design logical case memory systems foundations and learning issues the focus of this paper is on the introduction of a quite general type of case based reasoning systems called logical case memory systems the development of the underlying concepts has been driven by investigations in certain problems of case based learning therefore the present development of the target concepts is accompanied by an in depth discussion of related learning problems logical case memory systems provide some formal framework for the investigation and for the application of structural similarity concepts those concepts have some crucial advantage over traditional numerical similarity concepts the result of determining a new case s similarity to some formerly experienced case can be directly taken as a basis for performing case adaptation essentially every logical case memory system consists of two constituents some partially ordered case base and some partially ordered set of predicates cases are terms in a logical sense given some problem case every predicat the necessity of user guidance in case based knowledge acquisition the intention of the present paper is to justify both theoretically and experimentally that user guidance is inevitable in case based knowledge acquisition the methodology of our approach is simple we choose some paradigmatic idea of case based learning which can be very briefly expressed as follows given any cbr system apply it whenever it works sucessfully do not change it whenever it fails on some input case add this experience to the case base don t do anything else then we perform a number of knowledge acquisition experiments they clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases as a consequence we develop scenarios of user guidance based on these theoretical concepts we prove a few theoretical results characterizing the power of our approach next we perform a new series of more constrained results which support our theoretical investigations this paper is based on more than 1 000 000 runs of case based knowledge acquisi theoretical investigations and experimental explorations of the necessity of user guidance in case based knowledge acquisition the intention of the present paper is to justify both theoretically and experimentally that user guidance is inevitable in case based knowledge acquisition the methodology of our approach is quite simple we choose a well understood area which is tailored to case based knowledge acquisition furthermore we choose a prototypical case based learning algorithm which is obviously suitable for the problem domain under consideration then we perform a number of knowledge acquisition experiments they clearly exhibit essential limitations of knowledge acquisition from randomly chosen cases as a consequence we develop scenarios of user guidance based on these theoretical concepts we prove a few theoretical results characterizing the power of our approach next we perform a new series of more constrained results which support our theoretical investigations the present report aims at presenting a large amount of experimental data exceeding the space available in conference proceedings learning to perceive the world as articulated an approach for hierarchical learning in sensory motor systems this paper describes how agents can learn an internal model of the world structurally by focusing on the problem of behavior based articulation we develop an on line learning scheme the so called mixture of recurrent neural net rnn experts in which a set of rnn modules becomes self organized as experts on multiple levels in order to account for the different categories of sensory motor flow which the robot experiences autonomous switching of activated modules in the lower level actually represents the articulation of the sensory motor flow in the meanwhile a set of rnns in the higher level competes to learn the sequences of module switching in the lower level by which articulation at a further more abstract level can be achieved the proposed scheme was examined through simulation experiments involving the navigation learning problem our dynamical systems analysis clarified the mechanism of the articulation the possible correspondence between the articulation concept based design of data warehouses the dwq demonstrators the esprit project dwq foundations of data warehouse quality aimed at improving the quality of dw design and operation through systematic enrichment of the semantic foundations of data warehousing logic based knowledge representation and reasoning techniques were developed to control accuracy consistency and completeness via advanced conceptual modeling techniques for source integration data reconciliation and multi dimensional aggregation this is complemented by quantitative optimization techniques for view materialization optimizing timeliness and responsiveness without losing the semantic advantages from the conceptual approach at the operational level query rewriting and materialization refreshment algorithms exploit the knowledge developed at design time the demonstration shows the interplay of these tools under a shared metadata repository based on an example extracted from an application at telecom italia 1 overview of the demonstration the demonstration follows architecture and quality in data warehouses an extended repository approach this paper makes two scaling personalized web search recent web search techniques augment traditional text matching with a global notion of importance based on the linkage structure of the web such as in google s pagerank algorithm for more refined searches this global notion of importance can be specialized to create personalized views of importance for example importance scores can be biased according to a user specified set of initially interesting pages computing and storing all possible personalized views in advance is impractical as is computing personalized views at query time since the computation of each view requires an iterative computation over the web graph we present new graph theoretical results and a new technique based on these results that encode personalized views as partial vectors partial vectors are shared across multiple personalized views and their computation and storage costs scale well with the number of views our approach enables incremental computation so that the construction of personalized views from partial vectors is practical at query time we present efficient dynamic programming algorithms for computing partial vectors an algorithm for constructing personalized views from partial vectors and experimental results demonstrating the effectiveness and scalability of our techniques 1 simrank a measure of structural context similarity the problem of measuring similarity of objects arises in many applications and many domain specific measures have been developed e g matching text across documents or computing overlap among item sets we propose a complementary approach applicable in any domain with object to object relationships that measures similarity of the structural context in which objects occur based on their relationships with other objects effectively we compute a measure that says two objects are similar if they are related to similar objects this general similarity measure called simrank is based on a simple and intuitive graph theoretic model for a given domain simrank can be combined with other domain specific similarity measures we suggest techniques for efficient computation of simrank scores and provide experimental results on two application domains showing the computational feasibility and effectiveness of our approach primitive based movement classification for humanoid imitation motor control is a complex problem and imitation is a powerful mechanism for acquiring new motor skills in this paper we describe perceptuo motor primitives a biologically inspired notion for a basis set of perceptual and motor routines primitives serve as a vocabulary for classifying and imitating observed human movements and are derived from the imitator s motor repertoire we describe a model of imitation based on such primitives and demonstrate the feasibility of the model in a constrained implementation we present approximate motion reconstruction generated from visually captured data of typically imitated tasks taken from aerobics dancing and athletics 1 introduction imitation is a powerful mechanism for acquiring new skills it involves an intricate interaction between perceptual and motor mechanisms both of which are complex in themselves research into vision and motor control has explored the role of subroutines schemas 1 and other variants based on autonomous agents for business process management traditional approaches to managing business processes are often inadequate for large scale organisation wide dynamic settings however since internet and intranet technologies have become widespread an increasing number of business processes exhibit these properties therefore a new approach is needed to this end we describe the motivation conceptualisation design and implementation of a novel agent based business process management system the key advance of our system is that responsibility for enacting various components of the business process is delegated to a number of autonomous problem solving agents to enact their role these agents typically interact and negotiate with other agents in order to coordinate their actions and to buy in the services they require this approach leads to a system that is significantly more agile and robust than its traditional counterparts to help demonstrate these benefits a companion paper describes the application of our system to a implementing a business process management system using adept a real world case study this paper describes how adept s agent based design and implementation philosophy was used to prototype a business process management system for a real world application the application illustrated is based on the british telecom bt business process of providing a quote to a customer for installing a network to deliver a specified type of telecommunications service particular emphasis is placed upon the techniques developed for specifying services for allowing agents with heterogeneous information models to interoperate for allowing rich and flexible inter agent negotiation to occur and on the issues related to interfacing agent based systems and humans this paper builds upon the companion paper that provides details of the rationale and design of the adept technology deployed in this application 1 introduction many advances have been made in recent years within organisations in preparing a culture of dynamic improvement from the globalisation of trade total quality mana aspects of network edge intelligence is it the case that the migration of intelligence from the core of networks to the periphery is simply a function of the ip protocols or are there more fundamental forces at work this report addresses this issue from the perspective of core network protocols mobile networking and the emerging embedded internet it identifies the forces at work and concludes that the long term trends are driven by more than simply the end to end argument if the logical progression of this trend is that intelligence will migrate to the embedded internet then a new type of peripheral intelligence may form the basis for further progress the report concludes by identifying the challenges for the embedded internet in constructed environments keywords ip networking intelligence architecture smart spaces aspects of network edge intelligence i controlling cooperative problem solving in industrial multi agent systems using joint intentions one reason why distributed ai dai technology has been deployed in relatively few real size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex dynamic and unpredictable environments as a consequence of the experience gained whilst building a number of dai systems for industrial applications a new principled model of cooperation has been developed this model called joint responsibility has the notion of joint intentions at its core it specifies pre conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty the theoretical model has been used to guide the implementation of a general purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ robust finger tracking with multiple cameras this paper gives an overview of a system for robustly tracking the 3d position and orientation of a finger using a few closely spaced cameras accurate results are obtained by combining features of stereo range images and color images this work also provides a design framework for combining multiple sources of information including stereo range images color segmentations shape information and various constraints this information is used in robust model fitting techniques to track highly over constrained models of deformable objects fingers learning quantitative knowledge for multiagent coordination a central challenge of multiagent coordination is reasoning about how the actions of one agent affect the actions of another knowledge of these interrelationships can help coordinate agents preventing conflicts and exploiting beneficial relationships among actions we explore three interlocking methods that learn quantitative knowledge of such non local effects in taems a well developed framework for multiagent coordination the surprising simplicity and effectiveness of these methods demonstrates how agents can learn domain specific knowledge quickly extending the utility of coordination frameworks that explicitly represent coordination knowledge introduction a major challenge of designing effective multiagent systems is managing non local effects situations where the actions of one agent impact the performance of other agents actions for example one agent s action can enable disable facilitate or hinder the actions of other agents poor accounting for path materialization revisited an efficient storage model for xml data xml is emerging as a new major standard for representing data on the world wide web several xml storage models have been proposed to store xml data in di erent database management systems the unique feature of model mappingbased approaches is that no dtd information is required for xml data storage in this paper we present a new modelmapping based storage model called xparent unlike the existing work on model mapping based approaches that emphasized on converting xml documents to from database schema and translation of xml queries into sql queries in this paper we focus ourselves on the e ectiveness of storage models in terms of query processing we study the key issues that a ect query performance namely storage schema design storing xml data across multiple tables and path materialization storing path information in databases we show that similar but di erent storage models significantly a ect query performance a performance study is conducted using three data sets and query sets the experimental results are presented keywords semistructured data xml database 1 a statistical learning model of text classification for support vector machines this paper develops a theoretical learning model of text classification for support vector machines svms it connects the statistical properties of text classification tasks with the generalization performance of a svm in a quantitative way unlike conventional approaches to learning text classifiers which rely primarily on empirical evidence this model explains why and when svms perform well for text classification in particular it addresses the following questions why can support vector machines handle the large feature spaces in text classification effectively how is this related to the statistical properties of text what are sufficient conditions for applying svms to text classification problems successfully unbiased evaluation of retrieval quality using clickthrough data this paper proposes a new method for evaluating the quality of retrieval functions unlike traditional methods that require relevance judgements by experts or explicit user feedback it is based entirely on clickthrough data this is a key advantage since clickthrough data can be collected at very low cost and without overhead for the user taking an approach from experiment design the paper proposes an experiment setup that generates unbiased feedback about the relative quality of two search results without explicit user feedback a theoretical analysis shows that the method gives the same results as evaluation with traditional relevance judgements under mild statistical assumptions an empirical analysis verifies that the assumptions are indeed justified and that the new method leads to conclusive results in a www retrieval study transductive inference for text classification using support vector machines this paper introduces transductive support vector machines tsvms for text classification while regular support vector machines svms try to induce a general decision function for a learning task transductive support vector machines take into account a particular test set and try to minimize misclassifications of just those particular examples the paper presents an analysis of why tsvms are well suited for text classification these theoretical findings are supported by experiments on three test collections the experiments show substantial improvements over inductive methods especially for small training sets cutting the number of labeled training examples down to a twentieth on some tasks this work also proposes an algorithm for training tsvms efficiently handling 10 000 examples and more how emotions and personality effect the utility of alternative decisions a terrorist target selection case study the role of emotion modeling in the development of computerized agents has long been unclear this is partially due to instability in the philosophical issues of the problem as psychologists struggle to build models for their own purposes and partially due to the often wide gap between these theories and that which can be implemented by an agent author this paper describes an effort to use emotion models in part as a deep model of utility for use in decision theoretic agents this allows for the creation of simulated forces capable of balancing a great deal of competing goals and in doing so they behave for better or for worse in a more realistic manner affectively tunable environments for the virtual stage abstract affective computing 8 9 is an emerging area of human computer interaction which studies the ways in which computer systems can recognize and work with human emotions the word affective is an adjective used in psychology to mean relating to emotions examples of affective computing are the creation of systems which respond in different ways according to the system s perception of the user s emotional state 1 2 3 and systems which facilitate the communication of emotion through a virtual environment 7 one aspect of aective computing which is particularly apposite to the theatre is the ability to create environments in which the capacity for communication of emotion what we might call the affective bandwidth of the environment is variable 6 consider the follo the cambridge university spoken document retrieval system this paper describes the spoken document retrieval system that we have been developing and assesses its performance using automatic transcriptions of about 50 hours of broadcast news data the recognition engine is based on the htk broadcast news transcription system and the retrieval engine is based on the techniques developed at city university the retrieval performance over a wide range of speech transcription error rates is presented and a number of recognition error metrics that more accurately reflect the impact of transcription errors on retrieval accuracy are defined and computed the results demonstrate the importance of high accuracy automatic transcription the final system is currently being evaluated on the 1998 trec 7 spoken document retrieval task 1 hierarchical presentation of expansion terms different presentations of candidate expansion terms have not been fully explored in interactive query expansion iqe most existing systems that offer an iqe facility use a list form of presentation this paper examines an hierarchical presentation of the expansion terms which are automatically generated from a set of retrieved documents organised in a general to specific manner and visualised by cascade menus to evaluate the effectiveness of the presentation a user test was carried out to compare the hierarchical form with the conventional list form this shows that users of the hierarchy can complete the expansion task in less time and with fewer terms over those using the lists relations between initial query terms and selected expansion terms were also investigated keywords information retrieval interactive query expansion concept hierarchies 1 temporal requirements for anticipatory reasoning about intentional dynamics in social contexts abstract in this paper a temporal trace language is defined in which formulae can be expressed that provide an external temporal grounding of intentional notions justifying conditions are presented that formalise criteria that a candidate formula must satisfy in order to qualify as an external representation of a belief desire or intention using these conditions external represenation formulae for intentional notions can be identified using these external representations anticipatory reasoning about intentional dynamics can be performed 1 a multi agent architecture for an intelligent website in insurance in this paper a multi agent architecture for intelligent websites is presented and applied in insurance the architecture has been designed and implemented using the compositional development method for multi agent systems desire the agents within this architecture are based on a generic broker agent model it is shown how it can be exploited to design an intelligent website for insurance developed in co operation with the software company ordina utopics and an insurance company 1 introduction an analysis of most current business websites from the perspectives of marketing and customer relations suggests that websites should become more active and personalised just as in the nonvirtual case where contacts are based on human servants intelligent agents provide the possibility to reflect at least a number of aspects of the nonvirtual situation in a simulated form and in addition enables to use new opportunities for one to one marketing integrated in the website the generic a planning in interplanetary space theory and practice on may 17th 1999 nasa activated for the first time an ai based planner scheduler running on the flight processor of a spacecraft this was part of the remote agent experiment rax a demonstration of closedloop planning and execution and model based state inference and failure recovery this paper describes the rax planner scheduler rax ps both in terms of the underlying planning framework and in terms of the fielded planner rax ps plans are networks of constraints built incrementally by consulting a model of the dynamics of the spacecraft the rax ps planning procedure is formally well defined and can be proved to be complete rax ps generates plans that are temporally flexible allowing the execution system to adjust to actual plan execution conditions without breaking the plan the practical aspect developing a mission critical application required paying attention to important engineering issues such as the design of methods for programmable search contr automated state abstraction for options using the u tree algorithm learning a complex task can be significantly facilitated by defining a hierarchy of subtasks an agent can learn to choose between various temporally abstract actions each solving an assigned subtask to accomplish the overall task in this paper we study hierarchical learning using the framework of options we argue that to take full advantage of hierarchical structure one should perform option specific state abstraction and that if this is to scale to larger tasks state abstraction should be automated we adapt mccallum s u tree algorithm to automatically build option specific representations of the state feature space and we illustrate the resulting algorithm using a simple hierarchical task results suggest that automated option specific state abstraction is an attractive approach to making hierarchical learning systems more effective 1 introduction researchers in the field of reinforcement learning have recently focused considerable attention on temporally abst an introduction to variational methods for graphical methods this paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models bayesian networks and markov random fields we present a number of examples of graphical models including the qmr dt database the sigmoid belief network the boltzmann machine and several variants of hidden markov models in which it is infeasible to run exact inference algorithms we then introduce variational methods which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient inference in the simpified model provides bounds on probabilities of interest in the original model we describe a general framework for generating variational transformations based on convex duality finally we return to the examples and demonstrate how variational algorithms can be formulated in each case an anthropomorphic agent for the use of spatial language in this paper we describe the communication with a responsive virtual environment with the main emphasis on the processing of spatial expressions in natural language instructions this work is part of the viena project in whichwechose interior design as an example domain a multiagent system acts as an intelligent mediator between the user and a graphics system to make the communication about spatial relations more intuitive we developed an anthropomorphic agent which is graphically visualized in the scene considering the human like gure we explain the use of qualitative spatial expressions like right of and there 1 introduction interactive 3 dimensional graphics systems are more useful e g in design when users can concentrate on their imaginations and be free from technical considerations therefore it is important to improveinteraction with the virtual environmentbyway of natural intuitive communication forms in our work we consider a virtual interface why autonomy makes the agent this paper works on the premise that the position stated by jennings et al 17 is correct specifically that amongst other things the agent metaphor is a useful extension of the object oriented metaphor object oriented oo programming 29 is programming where data abstraction is achieved by users defining their own data structures see figure 1 or objects these objects encapsulate data and methods for operating on that data and the oo framework allows new objects to be created that inherit the properties both data and methods of existing objects this allows archetypeal objects to be defined and then extended by different programmers who needn t have complete understanding of exactly how the underlying objects are implemented levels of control and closure in complex semiotic systems it is natural to advance closures as atomic processes of universal evolution and to analyze this concept specifically while real complex systems like organisms and complex mechanisms cannot exist at either extreme of complete closure or lack of closure nevertheless we should consider the properties of closures in general the introduction of boundaries a corresponding stability the establishment of system autonomy and identity and thereby the introduction of emergent new systems of potentially new types our focus should move from simple physical closure of common objects and classical self organizing systems to semiotically closed systems which maintain cyclic relations of perception interpretation decision and action with their environments thus issues arise concerning the use and interpretation of symbols representations and or internal models whether explicit or implicit by the system and the syntactic semantic and pragmatic relations among the sign tok general query expansion techniques for spoken document retrieval this paper presents some developments in query expansion and document representation of our spoken document retrieval sdr system since the 1998 text retrieval conference trec 7 we have shown that a modification of the document representation combining several techniques for query expansion can improve average precision by 17 relative to a system similar to that which we presented at trec 7 1 these new experiments have also confirmed that the degradation of average precision due to a word error rate wer of 25 is relatively small around 2 relative we hope to repeat these experiments when larger document collections become available to evaluate the scalability of these techniques 1 identifying and handling structural incompleteness for validation of probabilistic knowledge bases the peski probabilities expert systems knowledge and inference system attempts to address some of the problems in expert system design through the use of the bayesian knowledge base bkb representation learning complex patterns for document categorization knowledge based approaches to document categorization make use of well elaborated and powerful pattern languages for manual writing of classification rules although such classification patterns have proven useful in many practical applications algorithms for learning classifiers from examples mostly rely on much simpler representations of classification knowledge in this paper we describe a learning algorithm which employs a pattern language similar to languages used for manual rule editing we focus on the learning of three specific constructs of this pattern language namely phrases tolerance matches of words and substring matches of words introduction manually writing document categorization rules is labor intensive and requires much expertise this caused a growing research interest in learning systems for document categorization most of these systems transform pre classified example documents into a propositional attribute value representation simple attributes indicate w inductive learning and case based reasoning this paper describes an application of an inductive learning techniques to case based reasoning we introduce two main forms of induction define case based reasoning and present a combination of both the evaluation of the proposed system called ta3 is carried out on a classification task namely character recognition we show how inductive knowledge improves knowledge representation and in turn flexibility of the system its performance in terms of classification accuracy and its scalability 1 introduction inductive learning is a process of generalizing specific facts or observations mcm86 it is a basic strategy by which one can acquire knowledge there are two main forms associated with inductive learning 1 instance to class induction where the learning system is presented with independent instances representing class and the task is to induce a general description of the class 2 clustering problem arises when several objects or situations are presented to a learner learning and decision making in the framework of fuzzy lattices a novel theoretical framework is delineated for supervised and unsupervised learning it is called framework of fuzzy lattices or flframework for short and it suggests mathematically sound tools for dealing separately and or jointly with disparate types of data including vectors of numbers fuzzy sets symbols etc specific schemes are proposed for clustering and classification having the capacity to deal with both missing and don t care data values the schemes in question can be implemented as neural networks the proposed learning schemes are employed here for pattern recognition on seven data sets including benchmark data sets and the results are compared with those ones by various learning techniques from the literature finally aiming at a mutual cross fertilization the fl framework is associated with established theories for learning and or decision making including probability theory fuzzy set theory bayesian decision making theory of evidence and adaptive resonance t seamless guidance by personal agent in virtual space based on user interaction in real world this paper describes a personal agent that guides a visitor through a virtual space by using context i e visitor interaction in the real world context aware guidance in the real world has become an increasingly important research demand since the advent of small and powerful palmtop computers or pdas personal digital assistants that can obtain useful information about objects in the real world based on context e g current location and visit history in the real world we have tried to extend this context aware guidance into a virtual environment by using context not from the virtual environment but from the real world such seamless guidance allows visitors to concentrate their attention on the understanding of actual elements this paper reports our experiment on seamless guidance by a personal agent in a virtual ancient village using real world context this was presented at an open house exhibition in our research laboratories 1 introduction we have proposed a notion a highly adaptable infrastructure for service discovery and management in ubiquitous computing in an age where wirelessly networked appliances and devices are becoming commonplace there is a necessity for providing a standard interface to them that is easily accessible by any mobile user the design outlined in this paper provides an infrastructure and communication protocol for presenting services to heterogeneous mobile clients in a physical space via some short range wireless links this system uses a communication manager to communicate with the client devices the communication manager can be modified easily to work with any type of communication medium including tcp ip infrared cdpd and bluetooth all the components in our model use a language based on extensible markup language xml giving it a uniform and easily adaptable interface we explain our trade offs in implementation and through experiments we show that the design is feasible and that it indeed provides a flexible structure for providing services centaurus defines a uniform infrastructure for heterogeneous services both hardware and software to be made available to diverse mobile users within a confined space 1 abduction in logic programming this paper is a survey and critical overview of recent work on the extension of logic programming to perform abductive reasoning abductive logic programming we outline the general framework of abduction and its applications to knowledge assimilation and default reasoning and we introduce an argumentation theoretic approach to the use of abduction as an interpretation for negation as failure we also analyse the links between abduction and the extension of logic programming obtained by adding a form of explicit negation finally we discuss the relation between abduction and truth maintenance 1 introduction this paper is a survey and analysis of work on the extension of logic programming to perform abductive reasoning the purpose of the paper is to provide a critical overview of some of the main research results in order to develop a common framework for evaluating these results to identify the main unresolved problems and to indicate directions for future work the emphasis i on the integration of technologies for capturing and navigating knowledge with ontology driven services nowadays many distinct communities are researching on technologies for knowledge capturing modelling and navigation moreover advances in internet technology makes it possible to perform most of these tasks on heterogeneous and distributed environments such as the web these advances though have raise the need for knowledge services to accommodate the ever increasing number of web users to provide such a service one needs to combine key technologies for different aspects of knowledge management capturing modelling navigating this should be tightly integrated with the intended service we describe such an integration effort in this paper our domain is a web based news repository and we aimed to provide personalised ontology driven services on the top of it we used knowledge capturing technologies to populate the underlying ontologies knowledge modelling techniques to provide reasoning capabilities for the ontology driven service and navigating technologies to overlay web pages with the ontology driven service keywords knowledge capture ontology driven services knowledge navigation 1 evolving more efficient digital circuits by allowing circuit layout evolution and multi objective fitness we use evolutionary search to design combinational logic circuits the technique is based on evolving the functionality and connectivity of a rectangular array of logic cells whose dimension is defined by the circuit layout the main idea of this approach is to improve quality of the circuits evolved by the genetic algorithm ga by reducing the number of active gates used we accomplish this by combining two ideas 1 using multiobjective fitness function 2 evolving circuit layout it will be shown that using these two approaches allows us to increase the quality of evolved circuits the circuits are evolved in two phases initially the genome fitness in given by the percentage of output bits that are correct once 100 functional circuits have been evolved the number of gates actually used in the circuit is taken into account in the fitness function this allows us to evolve circuits with 100 functionality and minimise the number of active gates in circuit structure the populati efficient web form entry on pdas we propose a design for displaying and manipulating html forms on small pda screens the form input widgets are not shown until the user is ready to fill them in at that point only one widget is shown at a time the form is summarized on the screen by displaying just the text labels that prompt the user for each widget s information the challenge of this design is to automatically find the match between each text label in a form and the input widget for which it is the prompt we developed eight algorithms for performing such label widget matches some of the algorithms are based on n gram comparisons while others are based on common form layout conventions we applied a combination of these algorithms to 100 simple html forms with an average of four input fields per form these experiments achieved a 95 matching accuracy we developed a scheme that combines all algorithms into a matching system this system did well even on complex forms achieving 80 accuracy in our experiments involving 330 input fields spread over 48 complex forms a behavioral interface to simulate agent object interactions in real time this paper shows a new approach to model and control interactive objects for simulations with virtual human agents when real time interactivity is essential a general conceptualization is made to model objects with behaviors that can provide information about their functionality changes in appearance from parameterized deformations and a complete plan for each possible interaction with a virtual human such behaviors are described with simple primitive commands following the actual trend of many standard scene graph file formats that connects language with movements and events to create interactive animations in our case special attention is given to correctly interpret object behaviors in parallel situation that arrives when many human agents interact at the same time with one same object keywords virtual humans virtual environments object modeling object interaction script languages parameterized deformations 1 introduction the necessity to have interactive obje domain specific informative and indicative summarization for information retrieval in this paper we propose the use of multidocument summarization as a post processing step in document retrieval we examine the use of the summary as a replacement to the standard ranked list the form of the summary is novel because it has both informative and indicate elements designed to help di erent users perform their tasks better our summary uses the documents topical structure as a backbone for its own structure as it was deemed the most useful document feature in our study of a corpus of summaries 1 visualization methods for personal photo collections browsing and searching in the photofinder software tools for personal photo collection management are proliferating but they usually have limited searching and browsing functions we implemented the photofinder prototype to enable non technical users of personal photo collections to search and browse easily photofinder provides a set of visual boolean query interfaces coupled with dynamic query and query preview features it gives users powerful search capabilities using a scatter plot thumbnail display and dragand drop interface photofinder is designed to be easy to use for searching and browsing photos keywords photofinder user interface dynamic query query preview search browsing boolean query digital photo library 1 introduction digital cameras scanners and personal computers are now common but as collections grow in size the need to organize search and browse digital photos increases 1 there are many personal photo collection management tools available either commercially or non commercially qrtdb qos sensitive real time database introduction recently the demand for real time database services is exploding the applications requiring such services include sensor data fusion decision support web information service e commerce online trading and dataintensive smart space applications furthermore the information system is being globalized due to the fast growth of the internet despite the importance and wide applicability the performance and predictability of a database system gamma the core component of global information systems gamma are relatively limited compared to the other real time systems such as real time operating systems it can not be easily replicated due to the consistency problem in addition the database system has relatively low predictability compared to other real time systems due to data dependence of the transaction execution data and resource conflicts dynamic paging and i o and transaction aborts and the resulting rollbacks and restarts 36 because of the limited perfo privacy preserving distributed mining of association rules on horizontally partitioned data abstract data mining can extract important knowledge from large data collections but sometimes these collections are split among various parties privacy concerns may prevent the parties from directly sharing the data and some types of information about the data this paper addresses secure mining of association rules over horizontally partitioned data the methods incorporate cryptographic techniques to minimize the information shared while adding little overhead to the mining task index terms data mining security privacy 1 omnipage vs sakhr paired model evaluation of two arabic ocr products characterizing the performance of optical character recognition ocr systems is crucial for monitoring technical progress predicting ocr performance providing scientific explanations for the system behavior and identifying open problems while research has been done in the past to compare performances of two or more ocr systems all assume that the accuracies achieved on individual documents in a dataset are independent when in fact they are not in this paper we show that accuracies reported on any dataset are correlated and invoke the appropriate statistical technique the paired model to compare the accuracies of two recognition systems theoretically we show that this method provides tighter confidence intervals than methods used in ocr and computer vision literature we also propose a new visualization method which we call the accuracy scatter plot for providing a visual summary of performance results this method summarizes the accuracy comparisons on the entire cor evolution of the walden s paths authoring tools changing user skills available infrastructure and work practices have caused many differences in the authoring support provided by the walden s paths project since its conception in this paper we trace these changes and the transition from the earlier authoring tools that supported an integrated authoring process to the more recent tools designed to work with the web applications that teachers have become accustomed to 1 introduction hypertext has come a long way from being found only in research systems to being a part of our everyday lives in the form of the world wide web www or the web we use the web for browsing academic information for furthering business interests for entertainment and a variety of other purposes there is an immense amount of information on the web that can be used for a variety of reasons web based information can be harnessed to supplement classroom teaching for k 12 students k 12 teachers can use web based information in the curriculum t mobimine monitoring the stock market from a pda this paper describes an experimental mobile data mining system that allows intelligent monitoring of time critical financial data from a hand held pda it presents the overall system architecture and the philosophy behind the design it explores one particular aspect of the system automated construction of personalized focus area that calls for user s attention this module works using data mining techniques the paper describes the data mining component of the system that employs a novel fourier analysis based approach to efficiently represent visualize and communicate decision trees over limited bandwidth wireless networks the paper also discusses a quadratic programming based personalization module that runs on the pdas and the multi media based user interfaces it reports experimental results using an ad hoc peer to peer ieee 802 11 wireless network image recognition and neuronal networks intelligent systems for the improvement of imaging information this paper we have concentrated on describing issues related to the development and use of artificial neural network based intelligent systems for medical image interpretation research in intelligent systems to date remains centred on technological issues and is mostly application driven however previous research and experience suggests that the successful implementation of computerised systems e g 34 35 and decision support systems in particular e g 36 in the area of healthcare relies on the successful integration of the technology with the organisational and social context within which it is applied therefore the successful implementation of intelligent medical image interpretation systems 9 should not only rely on their technical feasibility and effectiveness but also on organisational and social aspects that may rise from their applications as clinical information is acquired processed used and exchanged between professionals all these issues are critical in healthcare applications because they ultimately reflect on the quality of care provided iterative information retrieval using fast clustering and usage specific genres this paper describes how collection specific empirically defined stylistics based genre prediction can be brought together together with rapid topical clustering to build an interactive information retrieval interface with multi dimensional presentation of search results the prototype presented addresses two specific problems of information retrieval how to enrich the information seeking dialog by encouraging and supporting iterative refinement of queries and how to enrich the document representation past the shallow semantics allowed by term frequencies searching for more than words today s tools for searching information in a document database are based on term occurrence in texts the searcher enters a number of terms and a number of documents where those terms or closely related terms appear comparatively frequently are retrieved and presented by the system in list form this method works well up to a point it is intuitively understandable and for competent users and well e stylistic experiments for information retrieval a discussion on various experiments to utilize stylistic variation among texts for information retrieval purposes 1 stylistics texts vary in many ways authors make choices when they write a text they decide how to organize the material they have planned to introduce they make choices between synonyms and syntactic constructions they choose an intended audience for the text authors will make these choices in various ways and for various reasons based on personal preferences on their view of the reader and on what they know and like about other similar texts a style is a consistent and distinguishable tendency to make some of these linguistic choices style is on a surface level very obviously detectable as the choice between items in a vocabulary between types of syntactical constructions between the various ways a text can be woven from the material it is made of it is the information carried in a text when compared to other texts or in a sense compared to language security in the ajanta mobile agent system this paper describes the security architecture of ajanta concept indexing a fast dimensionality reduction algorithm with applications to document retrieval categorization in recent years we have seen a tremendous growth in the volume of text documents available on the internet digital libraries news sources and company wide intranets this has led to an increased interest in developing meth ods that can efficiently categorize and retrieve relevant information retrieval techniques based on dimensionality reduction such as latent semantic indexing lsi have been shown to improve the quality of the information being retrieved by capturing the latent meaning of the words present in the documents unfortunately the high computa tional requirements of lsi and its inability to compute an effective dimensionality reduction in a supervised setting limits its applicability in this paper we present a fast dimensionality reduction algorithm called concept indexing ci that is equally effective for unsupervised and supervised dimensionality reduction ci computes a k dimensional representation of a collection of documents by first clustering the documents into k groups and then using the cen troid vectors of the clusters to derive the axes of the reduced k dimensional space experimental results show that the dimensionality reduction computed by ci achieves comparable retrieval performance to that obtained using lsi while requiring an order of magnitude less time moreover when ci is used to compute the dimensionality reduction in a supervised setting it greatly improves the performance of traditional classification algorithms such as c4 5 and knn 1 a probabilistic framework for memory based reasoning in this paper we propose a probabilistic framework for memory based reasoning mbr the framework allows us to clarify the technical merits and limitations of several recently published mbr methods and to design new variants the proposed computational framework consists of three components a specification language to define an adaptive notion of relevant context for a query mechanisms for retrieving this context and local learning procedures that are used to induce the desired action from this context based on the framework we derive several analytical and empirical results that shed light on mbr algorithms we introduce the notion of an mbr transform and discuss its utility for learning algorithms we also provide several perspectives on memory based reasoning from a multi disciplinary point of view 1 introduction reasoning can be broadly defined as the task of deciding what action to perform in a particular state or in response to a given query actions can range from admit dimensionality reduction by random mapping fast similarity computation for clustering when the data vectors are high dimensional it is computationally infeasible to use data analysis or pattern recognition algorithms which repeatedly compute similarities or distances in the original data space it is therefore necessary to reduce the dimensionality before for example clustering the data if the dimensionality is very high like in the websom method which organizes textual document collections on a self organizing map then even the commonly used dimensionality reduction methods like the principal component analysis may be too costly it will be demonstrated that the document classi cation accuracy obtained after the dimensionality has been reduced using a random mapping method will be almost as good as the original accuracy if the nal dimensionality is sufficiently large about 100 out of 6000 in fact it can be shown that the inner product similarity between the mapped vectors follows closely the inner product of the original vectors improving interaction with virtual environments introduction virtual environments ves provide a computer based interface to a real life or abstract space using 3d graphics and 3d interaction techniques ves represent a novel interface style which offers new possibilities and challenges to human computer interface design however studies of the design of ves kaur et al 1996 show that designers lack a coherent approach to design especially interaction design designers appear to be pre occupied with difficult technical issues and think little about supporting user interaction however major interaction problems have been found with current ves such as disorientation perceptual misjudgements and difficulty finding and understanding available interactions mcgovern 1993 coven 1997 these common problems have been known to result in user frustration and a low usability and acceptability for the ve kaur et al 1996 miller 1994 guidance is needed on interaction design for ves to avoid such usability problems an evaluation of real time transaction management issues in mobile database systems a critical issue in mobile data management is to respond to real time data access requirements of the supported application however it is difficult to handle real time constraints in a mobile computing environment due to the physical constraints imposed by the mobile computer hardware and the wireless network technology in this paper we present a mobile database system model that takes into account the timing requirements of applications supported by mobile computing systems we provide a transaction execution model with two alternative execution strategies for mobile transactions and evaluate the performance of the system considering various mobile system characteristics such as the number of mobile hosts in the system the handoff process disconnection coordinator site relocation and wireless link failure performance results are provided in terms of the fraction of real time requirements that are satisfied 1 introduction a mobile computing system is a dynamic type of trad proverb the probabilistic cruciverbalist we attacked the problem of solving crossword puzzles by computer given a set of clues and a crossword grid try to maximize the number of words correctly filled in after an analysis of a large collection of puzzles we decided to use an open architecture in which independent programs specialize in solving specific types of clues drawing on ideas from information retrieval database search and machine learning each expert module generates a possibly empty candidate list for each clue and the lists are merged together and placed into the grid by a centralized solver we used a probabilistic representation throughout the system as a common interchange language between subsystems and to drive the search for an optimal solution proverb the complete system averages 95 3 words correct and 98 1 letters correct in under 15 minutes per puzzle on a sample of 370 puzzles taken from the new york times and several other puzzle sources this corresponds to missing roughly 3 words or 4 l a new approach to developing and implementing eager database replication protocols database replication is traditionally seen as a way to increase the availability and performance of distributed databases although a large number of protocols providing data consistency and fault tolerance have been proposed few of these ideas have ever been used in commercial products due to their complexity and performance implications instead current products allow inconsistencies and often resort to centralized approaches which eliminates some of the advantages of replication as an alternative we propose a suite of replication protocols that addresses the main problems related to database replication on the one hand our protocols maintain data consistency and the same transactional semantics found in centralized systems on the other hand they provide flexibility and reasonable performance to do so our protocols take advantage of the rich semantics of group communication primitives and the relaxed isolation guarantees provided by most databases this allows us to eliminate the possibility of deadlocks reduce the message overhead and increase performance a detailed simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented a suite of database replication protocols based on group communication primitives this paper proposes a family of replication protocols based on group communication in order to address some of the concerns expressed by database designers regarding existing replication solutions due to these concerns current database systems allow inconsistencies and often resort to centralized approaches thereby reducing some of the key advantages provided by replication the protocols presented in this paper take advantage of the semantics of group communication and use relaxed isolation guarantees to eliminate the possibility of deadlocks reduce the message overhead and increase performance a simulation study shows the feasibility of the approach and the flexibility with which different types of bottlenecks can be circumvented 1 introduction replication is often seen as a mechanism to increase availability and performance in distributed databases most of the work done in this area which we will refer to as traditional replication protocols is on synchronous and update a java application framework for agent based systems agents are the next significant software abstraction especially for distributed systems agent based systems have been developed in response to the following requirements personalized and customized user interfaces that are pro active in assisting the user adaptable fault tolerant distributed systems that solve complex problems open systems where components come and go and new components are continually added migration and load balancing across platforms throughout a network new metaphors such as negotiation for solving distributed multi disciplinary problems agents appear in a wide range of applications but all agent development to date has been done independently by each development team this has led to several problems including i duplication of effort ii inability to satisfy industrial strength requirements for security and scalability and iii incompatibility of agent systems the application framework described in this chapter captures and clarifies th the application of object oriented analysis to agent based systems agents are important software abstractions for distributed problem solving and autonomous pro active behavior they have been used in many applications including manufacturing enterprise integration network management and advanced user interfaces for worldwide web applications as agent systems become more prevalent the need arises for software engineering methodologies the relationships between agents and objects and the role of object oriented analysis in multiagent system development are discussed here the approach is illustrated with a case study from discrete parts manufacturing 1 0 introduction numerous examples can be found of applications of agent based systems to enterprise integration 31 concurrent engineering 5 and manufacturing 26 many agent based systems have also been developed for network management 31 scheduling 9 and advanced user interfaces 28 these systems exhibit significant advances in distributed problem solving and pro active anomaly driven concept acquisition in this paper we identify some principles of maturana and varela s autopoiesis theory and piaget s theory of child development both of which fall into the constructivist category of epistemology we then apply them to the problem of autonomous concept acquisition for artificial agents one consequence of constructivist philosophy is that concept acquistion should be possible in situations anomalies which were completely unforeseen by a human designer of the system another major consequence is that concepts should not merely be defined on the formal logical level but should be rooted in sensorimotor interaction with the environment this requires the existence of an intermediate level in the architecture which allows the construction of original response patterns we also consider the computational implications of the piagetian concept of integrating environment driven and model driven tendencies when searching for new concepts known as accommodation and assimilation respectively distributed reflective architectures the autonomy of a system can be defined as its capability to recover from unforeseen difficulties without any user intervention this thesis proposal addresses a small part of this problem namely the detection of anomalies within a system s own operation by the system itself it is a response to a challenge presented by immune systems which can distinguish between self and nonself i e they can recognise a foreign pattern due to a virus or bacterium as different from those associated with the organism itself even if the pattern was not previously encountered the aim is to apply this requirement to an artificial system where nonself may be any form of deliberate intrusion or random anomalous behaviour due to a fault when designing reflective architectures or self diagnostic systems it is simpler to rely on a single coordination mechanism to make the system work as intended however such a coordination mechanism cannot be inspected or repaired by the system i level lines as global minimizers of energy functionals in image segmentation we propose a variational framework for determining global minimizers of rough energy functionals used in image segmentation segmentation is achieved by minimizing an energy model which is comprised of two parts the first part is the interaction between the observed data and the model the second is a regularity term the optimal boundaries are the set of curves that globally minimize the energy functional on computing functions with uncertainty we study the problem of computing a function f x1 xn giv en that the actual values of the variables xi s are kno wn only with some uncertainty f or each variable xi aninterval ii is kno wn such that the value of xi is guaranteed to fall within this interval any such interval can be probed to obtain the actual value of the underlying variable how ever there is a cost associated with each suc h probe the goal is to adaptively iden tify a minimum cost sequence of probes suc h that regardless of the actual values tak en b y the unprobed xi s the v alue of the functionf can be computed to within a speci ed precision we design online algorithms for this problem when f is either the selection function or an aggregation function such as sum or average we consider three natural models of precision and give algorithms for each model we analyze our algorithms in the framework of competitive analysis and sho w that our algorithms are asymptotically optimal finally we also study online algorithms for functions that are obtained by composing together selection and aggregation functions 1 a maximum entropy language model integrating n grams and topic dependencies for conversational speech recognition a compact language model which incorporates local dependencies in the form of n grams and long distance dependencies through dynamic topic conditional constraints is presented these constraints are integrated using the maximum entropy principle issues in assigning a topic to a test utterance are investigated recognition results on the switchboard corpus are presented showing that with a very small increase in the number of model parameters reduction in word error rate and language model perplexity are achieved over trigram models some analysis follows demonstrating that the gains are even larger on content bearing words the results are compared with those obtained by interpolating topicindependent and topic specific n gram models the framework presented here extends easily to incorporate other forms of statistical dependencies such as syntactic word pair relationships or hierarchical topic constraints 1 introduction language modeling is a crucial component of systems that c using dynamic mediation to integrate cots entities in a ubiquitous computing environment the original vision of ubiquitous computing 14 is about enabling people to more easily accomplish tasks through the seamless interworking of the physical environment and a computing infrastructure a major challenge to the practical realization of this vision involves the integration of commercial o the shelf cots hardware and software components consider the awkwardness of such a mundane task as exporting a textual memo written on a palm pilot to a microsoft word document it is not enough to overcome the protocol and data format mismatches that currently impede the interoperation of these entities for the user experience to be truly seamless we must provide a framework for the dynamic connection of such endpoints on demand to support the ad hoc interactions that are an integral part of ubiquitous computing to this end we oer a dynamic mediation framework called paths a path consists of dynamically instantiated automatically composable operators that brid towards group communication for mobile participants extended abstract group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements moreover location awareness is clearly central to mobile applications such as traffic management and smart spaces in this paper we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group membership management service suitable for proximity groups we describe a novel approach to efficient coverage estimation giving applications feedback on the proportion of the area of interest covered by a proximity group and also discuss our approach to partition anticipation heterogeneous multimedia database selection on the web multimedia databases on the web have two distinct characteristics autonomy and heterogeneity they are established and maintained independently and queries are processed depending on their own schemes in this paper we investigate the problem of the database selection from a number of multimedia databases dispersed on the web in the multimedia objects retrieval from distributed sites it is crucial that the metaserver has the capability to find objects globally similar to a given query object from different multimedia databases with different local similarity measures we propose a novel selection algorithm to determine candidate databases that contain more objects relevant to the query than other databases the selection of databases is based on the number of relevant objects at each local database that is estimated by using a few sample objects and the histogram information an extensive experiment on a large number of real data sets demonstrates that our proposed method perform secret computation with secrets for mobile agent using one time proxy signature as an application for electronic commerce a mobile agent is now used to search for special products or services and is executed for a specific job designated by a customer in the server s environment on behalf of a customer on the way of performing its role a mobile agent can be vulnerable to several cryptographic attacks these attacks can be more serious when done by malicious servers among schemes to resolve this problem the concept of encrypted function for secret computation was proposed in st97 kbc00 however schemes that employ such encrypted functions enforce the server host to execute the functions of customer before verifying the mobile codes even in the case that the codes are maliciously modified in this paper we apply proxy signature scheme to the mobile agent system to enhance security and efficiency also we suggest one time proxy signature scheme to limit the signing power of the server compensatory negotiation for agent based project schedule coordination practitioners have tried to solve a project schedule coordination problem involving many subcontractors with a centralized approach but failed to provide a cohesive solution they have overlooked a principle that general contractors cannot coordinate subcontractors like they do their own forces the project schedule coordination problem could be solved better by subcontractors in a distributed manner this paper presents a formalized negotiation methodology for distributed project schedule coordination a framework wherein a project can be rescheduled dynamically by all of the concerned project participants the compensatory negotiation methodology is developed to allow agents to transfer utility to other agents for compensation of disadvantageous agreements through a multi linked negotiation process by employing software agents that are capable of compensatory negotiation practitioners now can solve the problem and explore and exploit new opportunities an agent based framework offers 1 effective temporal aggregation using point based trees temporal databases introduce the concept of time into underlying data and provide built in facilities that allow users to store and retrieve time varying data the aggregation in temporal databases that is temporal aggregation is an extension of conventional aggregation on the domain and range of aggregates to include time concept temporal aggregation is important for various applications but is very expensive in this paper we propose a new tree structure for temporal aggregation called pa tree and aggregate processing method based on the pa tree we show that the time complexity of the proposed method is better than those of the existing methods the time complexity of the proposed method is shown to be indeed the lower bound of the problem we perform comparative experiments and show the performance advantage of our proposed method in practice 1 introduction while conventional database systems store the most recent snapshots of the real world temporal datab practical dependency analysis through a share quotient def the domain of definite boolean functions expresses sure dependencies between the program variables of say a constraint program the domain share on the other hand captures the possible variable sharing between the variables of a logic program the connection between these domains has been explored in the domain comparison quotienting and decomposition literature we develop this link further and show how the meet as well as the join of def can be modelled with efficient quadratic operations on share we show how the connection leads to an attractive way of constructing a dependency analysis and how widening can make the approach practical a methodology and modelling technique for systems of bdi agents the construction of large scale embedded software systems demands the use of design methodologies and modelling techniques that support abstraction inheritance modularity and other mechanisms for reducing complexity and preventing error if multi agent systems are to become widely accepted as a basis for large scale applications adequate agentoriented methodologies and modelling techniques will be essential this is not just to ensure that systems are reliable maintainable and conformant but to allow their design implementation and maintenance to be carried out by software analysts and engineers rather than researchers in this paper we describe an agent oriented methodology and modelling technique for systems of agents based upon the belief desire intention bdi paradigm our models extend existing object oriented oo models by building upon and adapting existing well understood techniques we take advantage of their maturity to produce an approach that can be easily lear modelling and design of multi agent systems abstract agent technologies are now being applied to the development of large scale commercial and industrial software systems such systems are complex involving hundreds perhaps thousands of agents and there is a pressing need for system modelling techniques that permit their complexity to be e ectively managed and principled methodologies to guide the process of system design without adequate techniques to support the design process such systems will not be su ciently reliable maintainable or extensible will be di cult to comprehend and their elements will not be re usable in this paper we present techniques for modelling agents and multi agent systems which adapt and extend existing object oriented representation techniques and a methodology which provides a clear conceptual framework to guide system design and speci cation we have developed these techniques for systems of agents based upon a particular belief desire intention architecture but have soughttoprovide a framework for the description of agent systems that is su ciently general to be applicable to other agent architectures and which may be extended in various ways 1 a reference model for situation aware assistance as computers are becoming more and more ubiquitous moving from the desktop into the infrastructure of our everyday life they begin to influence the way we interact with this environment the physical entities that we operate upon in order to achieve our daily goals the most important aspect of future human computer interaction therefore is the way computers support us in efficiently managing our personal environment conventionally human computer interaction looks at a process where only two partners are involved the human and the computer however looking at the computer as a mediator between the user and his environment we have to acknowledge a more complex communication process this paper proposes a reference model that identifies the fundamental components which are involved in human computer environment interaction the robocup synthetic agent challenge 97 robocup challenge offers a set of challenges for intelligent agent researchers using a friendly competition in a dynamic real time multiagent domain while robocup in general envisions longer range challenges over the next few decades robocup challenge presents three specific challenges for the next two years i learning of individual agents and teams ii multi agent team planning and plan execution in service of teamwork and iii opponent modeling robocup challenge provides a novel opportunity for machine learning planning and multi agent researchers it not only supplies a concrete domain to evalute their techniques but also challenges researchers to evolve these techniques to face key constraints fundamental to this domain real time uncertainty and teamwork 1 introduction robocup the world cup robot soccer is an attempt to promote ai and robotics research by providing a common task soccer for evaluation of various theories algorithms and agent architectur robocup rescue search and rescue in large scale disasters as a domain for autonomous agents research disaster rescue is one of the most serious social issue which involves very large numbers of hetergenious agents in the hostile environment robocup rescue intends to promote research and development in this socially significant domain by creating a standard simulator and forum for researchers and practitioners while the rescue domain intuitively appealing as large scale multi agent domains it has not yet given through analysis on its domain characteristics in this paper we present detailed analysis on the task domain and elucidate characteristics necessary for multi agent systems for this domain 1 introduction in this paper we propose robocup rescue as a secondary domain for robocup activities kitano et al 1997 the aim of robocup rescue are 1 to ensure smooth transfer of technologies invented through robocup activity to a socially significant real world domain 2 to establish a domain which complements features that are missing in soccer and 3 to examine funderm supporting conflict resolution in cooperative design systems complex modern day artifacts are designed cooperatively by groups of experts each with their own areas of expertise the interaction of such experts inevitably involves conflict this paper presents an implemented computational model based on studies of human cooperative design for supporting the resolution of such conflicts this model is based centrally on the insights that general conflict resolution expertise exists separately from domain level design expertise and that this expertise can be instantiated in the context of particular conflicts into specific advice for resolving those conflicts conflict resolution expertise consists of a taxonomy of design conflict classes in addition to associated general advice suitable for resolving conflicts in these classes the abstract nature of conflict resolution expertise makes it applicable to a wide variety of design domains this paper describes this conflict resolution model and provides examples of its operation from an implemente computer response to user frustration use of computer technology often has unpleasant side effects some of which are strong negative emotional states that arise in humans during interaction with computers frustration confusion anger anxiety and similar emotional states can affect not only the interaction itself but also productivity learning social relationships and overall well being this thesis presents the idea of designing human computer interaction systems to actively support human users in their ability to regulate manage and recover from their own negative emotional states particularly frustration this document describes traditional theoretical strategies for emotion regulation the design of a human computer interaction agent built by the author to actively help relieve frustration and an evaluation that shows the effectiveness of the agent a study designed to test this agent was conducted a system was built that elicits frustration in human subjects the interaction agent then initiated several social emotional content feedback strategies with some of the subjects in an effort to help relieve their emotional state these strategies were designed to provide many of the same cues that skilled human listeners employ when helping relieve strong negative emotions in others two control groups were exposed to the same frustrating stimuli one of which was given no emotional support at all the other enabled subjects to report problems and vent at the computer subsequent behavior was then observed and self report data was collected behavioral results showed the agent was significantly more effective than the two controls in helping relieve frustration levels in subjects these results demonstrate that strategic social emotional content interaction with a computer by users who a microeconomic view of data mining we present a rigorous framework based on optimization for evaluating data mining operations such as associations and clustering in terms of their utility in decisionmaking this framework leads quickly to some interesting computational problems related to sensitivity analysis segmentation and the theory of games department of computer science cornell university ithaca ny 14853 email kleinber cs cornell edu supported in part by an alfred p sloan research fellowship and by nsf faculty early career development award ccr 9701399 y computer science division soda hall uc berkeley ca 94720 christos cs berkeley edu z ibm almaden research center 650 harry road san jose ca 95120 pragh almaden ibm com 1 introduction data mining is about extracting interesting patterns from raw data there is some agreement in the literature on what qualifies as a pattern association rules and correlations 1 2 3 5 6 12 20 21 as well as clustering of the data points 9 are exploiting models of personality and emotions to control the behavior of animated interactive agents the german research centre for artificial intelligence dfki recently started three new projects1 to advance our understanding of the fundamental technology required to drive the social behaviour of interactive animated agents this initiative has been timed to catch the current wave of research and commercial interest in the field of lifelike characters 1 and detecting concept drift with support vector machines for many learning tasks where data is collected over an extended period of time its underlying distribution is likely to change a typical example is information filtering i e the adaptive classification of documents with respect to a particular user interest both the interest of the user and the document content change over time a filtering system should be able to adapt to such concept changes this paper proposes a new method to recognize and handle concept changes with support vector machines the method maintains a window on the training data the key idea is to automatically adjust the window size so that the estimated generalization error is minimized the new approach is both theoretically well founded as well as effective and efficient in practice since it does not require complicated parameterization it is simpler to use and more robust than comparable heuristics experiments with simulated concept drift scenarios based on real world text data com system and software visualisation if the entire of what many consider to be the software visualisation field is reviewed then this article would be much larger in size and also consist mainly of variations on the nodes and arcs theme because the use of the third dimension for system and software visualisation is emerging as a viable alternative for the representation of complex artefacts then it was considered much better to focus on this form of software visualisation the previous techniques had various identified shortcomings and the space and freedom afforded by the extra dimension has the potential to be usefully employed to overcome some of these problems software visualisation can be seen as a specialised subset of information visualisation this is because information visualisation is the process of creating a graphical representation of abstract generally non numerical data this is exactly what is required when trying to visualise software the term software visualisation has many visualisation effectiveness providing evaluations of visualisations is one way to demonstrate that they support a purpose and are adequate for the role claimed for them the problem in doing so is that there is no central source of evaluation issues that one can use a subset of for this purpose there is also very little in the way of agreement over what constitutes a good visualisation hence the evaluation criteria differ there are the human computer interaction ideals the slightly differing ones from usability engineering those from the visualisation community and also the need to be able to support the variable abilities of the users graphics as the medium behind visualisation may support greater bandwidth but is also prone to more likes and dislikes than other forms of interface the concept of visualisation effectiveness and therefore ways of evaluating visualisations provide the focus for this paper keywords visualisation evaluation usability understanding 1 accurately and reliably extracting data from the web a machine learning approach a critical problem in developing information agents for the web is accessing data that is formatted for human use we have developed a set of tools for extracting data from web sites and transforming it into a structured data format such as xml the resulting data can then be used to build new applications without having to deal with unstructured data the advantages of our wrapping technology over previous work are the the ability to learn highly accurate extraction rules to verify the wrapper to ensure that the correct data continues to be extracted and to automatically adapt to changes in the sites from which the data is being extracted 1 introduction there is a tremendous amount of information available on the web but much of this information is not in a form that can be easily used by other applications there are hopes that xml will solve this problem but xml is not yet in widespread use and even in the best case it will only address the problem within application domains evolutionary approaches to off line routing in backbone communications networks off line routing in backbone communications networks is an important combinatorial optimisation problem it has three main uses first off line routing provides reference benchmark results for dynamic on line routing strategies second and more interestingly off line routing is becoming more and more investigated and employed in its own right as a way of quickly finding significantly improved routings for live networks which can then be imposed on the network to offer a net improvement in quality of service third it can be used in networks where bandwidth may be booked in advance in this paper we introduce and investigate a number of heuristic techniques applicable to the routing problem for use in stochastic iterative search results are presented which indicate that these heuristics significantly improve the search for solutions particularly when on line performance is considered we also investigate how computation time can be further reduced by the use of delta evaluation the pareto archived evolution strategy a new baseline algorithm for pareto multiobjective optimisation most popular evolutionary algorithms for multiobjective optimisation maintain a population of solutions from which individuals are selected for reproduction in this paper we introduce a simpler evolution scheme for multiobjective problems called the pareto archived evolution strategy paes we argue that paes may represent the simplest possible non trivial algorithm capable of generating diverse solutions in the pareto optimal set the algorithm is identified as being a 1 1 evolution strategy using local search from a population of one but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors paes is intended as a good baseline approach against which more involved methods may be compared and may also serve well in some real world applications when local search seems superior to or competitive with population based methods the performance of the new algorithm is compared w towards a multi agent system for pro active information management in anesthesia decision making in anesthesia requires the integration of information from various clinical data sources such as the patient the laboratory the surgery and the clinical personnel however information management and processing is aggravated by the highly heterogeneous and distributed nature of the current clinical data and information repositories furthermore critical situations such as the arrival of emergency patients demand the context sensitive provision of information in a timely fashion we argue that multi agent architectures can help to overcome these problems in the clinical information space autonomous agents can pro actively collect integrate and analyze the required data and condense and communicate the most relevant information in this paper we presentanapproach for an anesthesia information management system based on intelligent agents 1 introduction the anesthesiological department is an essential service provider in modern clinical health care extreme programming of multi agent systems the complexity of communication scenarios between agents make multi agent systems difficult to build most of the existing agent oriented software engineering methodologies face this complexity by guiding the developers through a rather waterfall based process with a series of intermediate modeling artifacts while these methodologies lead to executable prototypes relatively late and are expensive when requirements change we explore a rather evolutionary approach with explicit support for change and rapid feedback in particular we apply extreme programming a modern light weight methodology from object oriented software technology for the design and implementation of multiagent systems the only modeling artifacts that are being maintained in our approach are a process model with which domain experts and developers design and communicate agent application scenarios and the executable agent source code including automated test cases our methodology has been successfully applied for the development of a prototypical multi agent system for clinical information logistics 1 information retrieval on the web in this paper we review studies on the growth of the internet and technologies which are useful for information search and retrieval on the web we present data on the internet from several dierent sources e g current as well as projected number of users hosts and web sites although numerical gures vary overall trends cited by the sources are consistent and point to exponential growth in the past and in the coming decade as such it is not surprising that about 85 of internet users surveyed claim to be using search engines and search services to nd speci c information of interest the same surveys show however that users are not satis ed with the performance of the current generation of search engines the slow speed of retrieval communication delays and poor quality of retrieved results e g noise and broken links are commonly cited problems we discuss the development of new techniques which are targeted to resolve some of the problems associated with web information retrieval on the web selected topics in this paper we review studies on the growth of the internet and technologies which are useful for information search and retrieval on the web in the rst section we present data on the internet from several dierent sources e g current as well as projected number of users hosts and web sites although the numerical gures vary the overall trends cited by the sources are consistent and point to exponential growth during the coming decade and internet users are increasingly using search engines and search services to nd speci c information of interest however users are not satis ed with the performance of the current generation of search engines the slow speed of retrieval communication delays and poor quality of retrieved results e g noise and broken links are commonly cited problems the main body of our paper focuses on linear algebraic models and techniques for solving these problems keywords clustering indexing information retrieval internet late adapting web information to disabled and elderly users substantial research and standardization efforts already exist to make it easier for people with physical impairments to perceive and interact with web pages this paper describes work aimed at catering the content of web pages to the needs of different users including elderly people and users with vision and motor impairments the avanti system and related efforts in the avanti project will be discussed and experiences reported 1 introduction the world wide web is currently the most frequently visited electronic resource and is likely to become the access ramp to the electronic information highway of the next millennium web access should therefore ideally be available to everyone in order not to create yet another informational and hence economical and social disparity in society special efforts must be put into making the access to the web available to those who so far have been at a disadvantage including people with disabilities and elderly people who until recently integrating community services a common infrastructure proposal computer based community systems can provide powerful support in knowledge transfer both in the direct exchange of information and in finding people to exchange information with several such information exchange and expert finding services have already been implemented the problem of current approaches is that they are not able to exchange data with each other user profile information and contributed information have to be entered separately for every community based information system so what one system has learned is not available for other systems in this paper we present our ideas for a community services infrastructure as an enabling technology for further integration of community related services a simple query facility for the objectivity db persistent object manager this document discusses the reasons that lead to the development of a query faciliy within the cristal project its design criteria syntax semantics use and restrictions it is furthermore intended to serve as a preliminary manual the query facility is discussed in its immediately next development stage which should be finished within the next few weeks comparing machine learning and knowledge discovery in databases an application to knowledge discovery in texts introduction kdd is better known by the oversimplified name of data mining dm actually most academics are rather interested by dm which develops methods for extracting knowledge from a given set of data industrialists and experts should be more interested in kdd which comprises the whole process of data selection data cleaning transfer to a dm technique applying the dm technique validating the results of the dm technique and finally interpreting them for the user in general this process is a cycle that improves under the criticism of the expert machine learning ml and kdd have in common a very strong link they both acknowledge the importance of induction as a normal way of thinking while other scientific fields are reluctant to accept it to say the least we shall first explore this common point we believe that this reluctance relies on a misuse of apparent contradictions inside the theory of confirmation that is we shall revisit hempel paradox in order t improving collaborative filtering with multimedia indexing techniques to create user adapting web sites the internet is evolving from a static collection of hypertext to a rich assortment of dynamic services and products targeted at millions of internet users for most sites it is a crucial matter to keep a close tie between the users and the site half pipe anchoring an efficient technique for multiple connection handoff this paper presents half pipe anchoring a novel technique to build a multiple connection handoff mechanism that enables efficient use of resources in a server cluster improves the scalability of the cluster and supports construction of heterogeneous cluster architectures where nodes are specialized to perform specific tasks of client requests efficiently the key idea behind our approach is to decouple the two unidirectional half pipes that make up a tcp connection between a client and a server and anchor the unidirectional half pipe from the client to the cluster at a designated server while allowing the half pipe from the cluster to the client to migrate on a per request basis to an optimal server where the request is best serviced we describe the design and implementation of a prototype multiple connection handoff mechanism in the linux kernel and demonstrate the benefits of our technique probabilistic frame based systems two of the most important threads of work in knowledge representation today are frame based representation systems frs s and bayesian networks bns frs s provide an excellent representation for the organizational structure of large complex domains but their applicability is limited because of their inability to deal with uncertainty and noise bns provide an intuitive and coherent probabilistic representation of our uncertainty but are very limited in their ability to handle complex structured domains in this paper we provide a language that cleanly integrates these approaches preserving the advantages of both our approach allows us to provide natural and compact definitions of probability models for a class in a way that is local to the class frame these models can be instantiated for any set of interconnected instances resulting in a coherent probability distribution over the instance properties our language also allows us to represent important types of uncertainty tha nearest neighbor queries in a mobile environment nearest neighbor queries have received much interest in recent years due to their increased importance in advanced database applications however past work a flexible interoperable framework for active spaces this paper we describe the requirements faced by such a system and propose an integrated architecture meeting these requirements the paper focuses on a representation of active spaces using standard naming and trading mechanisms and on an object oriented framework for managing heterogeneous devices secure dynamic reconfiguration of scalable corba systems with mobile agents as various internet services electronic commerce and information and communication systems permeate our lives their continual availability becomes a dominant issue but continuing software evolution requires system recon guration running systems must upgrade their components or change their con guration parameters in addition internet services often need to serve thousands or millions of users this scenario raises three conicting issues that need to be resolved availability con gurability and scalability we propose the use of mobile recon guration agents for the ecient secure and scalable dynamic recon guration of internet systems in our system the agents are deployed within the object oriented architecture of a corba compliant orb that supports safe recon guration both of its middleware engine and of user applications using a graphical front end administrators build recon guration agents and specify the topology of the recon guration network the agents traverse to recon gure the distributed system on the y the agents collect group and return the results of inspection and recon guration operations in the distributed system to the administrator 1 using dynamic configuration to manage a scalable multimedia distribution system multimedia applications and interfaces will change radically the way computer systems will look like in the coming years radio and tv broadcasting will assume a digital format and their distribution networks will be integrated to the internet existing hardware and software infrastructures however are unable to provide all the scalability flexibility and quality of service that these applications require we present a framework for building scalable and flexible multimedia distribution systems that greatly improves the possibilities for the provision of quality of service in large scale networks we show how to use architectural awareness mobile agents and a corba based framework to support dynamic re configuration efficient code distribution and fault tolerance this approach can be applied not only for multimedia distribution but also for any qos sensitive distributed application actor critic algorithms we propose and analyze a class of actor critic algorithms for simulation based optimization of a markov decision process over a parameterized family of randomized stationary policies these are two time scale algorithms in which the critic uses td learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic we show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor we conclude by discussing convergence properties and some open problems 1 introduction the vast majority of reinforcement learning rl 9 and neuro dynamic programming ndp 1 methods fall into one of the following two categories a actor only methods work with a parameterized family of policies the gradient of the performance with respect to the actor parameters is directly estimated by simulation and the parameters are updated in a direction o induction of decision trees using relieff in the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies between them greedy search prevents current inductive machine learning algorithms to detect significant dependencies between the attributes recently kira and rendell developed the relief algorithm for estimating the quality of attributes that is able to detect dependencies between attributes we show strong relation between relief s estimates and impurity functions that are usually used for heuristic guidance of inductive learning algorithms we propose to use relieff an extended version of relief instead of myopic impurity functions we have reimplemented assistant a system for top down induction of decision trees using relieff as an estimator of attributes at each selection step the algorithm is tested on several artificial and several real world problems results show the advantage of the presented approach to inductive lea model generation without normal forms and applications in natural language semantics i present a new tableaux based model generation method for first order formulas without function symbols unlike comparable approaches the relational models rm tableaux calculus does not require clausal input theories i propose some applications of the rm calculus in natural language semantics and discuss its usefulness as an inference procedure in natural language processing 1 introduction refutational methods in automated deduction prove the unsatisfiability of logical theories for many applications the interpretations of a theory that show its satisfiability are at least as interesting as proofs model generation refers to the automatic construction of such interpretations from first order theories in the recent years there has been a growing interest in the automated deduction community in developing model generation methods for various application areas such as finite mathematics 25 22 deductive databases 7 diagnosis 13 1 and planning 19 as a result mode model generation for natural language semantic analysis semantic analysis refers to the analysis of semantic representations by inference on the basis of semantic information and world knowledge i present some potential applications of model generators and model generation theorem provers in the construction and analysis of natural language semantics where both the process of model generation and the computed models are valuable sources of information i discuss bry and torge s hyper resolution tableaux calculus ep as an approach to model generation for natural language semantic analysis 1 introduction one goal of modern natural language semantics has been to capture the conditions under which a sentence can be uttered truthfully in logic based semantic formalisms such as discourse representation theory 17 or montague grammar 22 the representations constructed for natural language sentences are logical formulas whose truth conditions are described by their models the construction of semantic representations requires amongst oth mining knowledge in geographical data this article a short overview is provided to summarize recent studies on spatial data mining including spatial data mining techniques their strengths and weaknesses how and when to apply them and what are the challenges yet to be faced a knowledge based approach for lifelike gesture animation the inclusion of additional modalities into the communicative behavior of virtual agents besides speech has moved into focus of human computer interface researchers as humans are more likely to consider computer generated figures lifelike when appropriate nonverbal behaviors are displayed in addition to speech in this paper we propose a knowledge based approach for the automatic generation of gesture animations for an articulated figure it combines a formalism for the representation of spatiotemporal gesture features methods for planning individual gestural animations w r t to form and timing and formation of arm trajectories finally enhanced methods for rendering animations from motor programs are incorporated in the execution of planned gestures the approach is targetted to achieve a great variety of gestures as well as a higher degree of lifelikeness in synthetic agents 1 introduction the communicative behaviors of virtual anthropomorphic agents widely used in human planning and motion control in lifelike gesture a refined approach in this paper an operational model for the automatic generation of lifelike gestures of an anthropomorphic virtual agent is described the biologically motivated approach to controlling the movements of a highly articulated figure provides a transformation of spatiotemporal gesture specifications into an analog representation of the movement from which the animations are directly rendered to this end knowledge based computer animation techniques are combined with appropriate methods for trajectory formation and articulated figure animation 1 introduction the inclusion of nonverbal modalities into the communicative behaviors of virtual agents has moved into focus of human computer interface researchers humans are more likely to consider computer generated figures lifelike when appropriate nonverbal behaviors are displayed in addition to speech this enables the evocation of social communicative attributions to the artificial agent which are supposed to be advantageous for a natu close encounters supporting mobile collaboration through interchange of user profiles this paper introduces the notion of profile based cooperation as a way to support awareness and informal communication between mobile users during chance encounters we describe the design of proem a wearable system for profile based cooperation that enables users to publish and exchange personal profile information during physical encounters the proem system is used to initiate contact between individuals by identifying mutual interests or common friends in contrast to most previous research that concentrates on collaboration in well defined and closed user groups proem supports informal communication between individuals who have never met before and who don t know each other we illustrate the benefits of profile based cooperation by describing several usage scenarios for the proem system 1 introduction during the course of a day we encounter and meet a large number of people some of whom we know personally and some of whom we never met before in everyday languag web mining research a survey with the huge amount of information available online the world wide web is a fertile area for data mining research the web mining research is at the cross road of research from several research communities such as database information retrieval and within ai especially the sub areas of machine learning and natural language processing however there is a lot of confusions when comparing research efforts from different point of views in this paper we survey the research in the area of web mining point out some confusions regarded the usage of the term web mining and suggest three web mining categories then we situate some of the research with respect to these three categories we also explore the connection between the web mining categories and the related agent paradigm for the survey we focus on representation issues on the process on the learning algorithm and on the application of the recent works as the criteria we conclude the paper with some research issues managing the operator ordering problem in parallel databases this paper focuses on parallel query optimization we consider the operator problem and introduce a new class of execution strategies called linear oriented bushy trees lbt compared to the related approach of the general bushy trees gbt a significant complexity reduction of the operator ordering problem can be derived theoretically and demonstrated experimentally e g compared with gbts lbts authorize optimization time improvement that can reach up to 49 without loosing quality finally we demonstrate that existing commercial parallel query optimizers need little extension modifications in order to handle lbts key words parallel databases parallel query optimization linear oriented bushy trees extending existing optimizers 1 introduction modern database applications such as data mining and decision support pose several new challenges to query optimization and processing 1 one of the main issues concerns the processing of complex queries e g recent teradata rela the picsom retrieval system description and evaluations we have developed an experimental system called picsom for retrieving images similar to a given set of reference images in large unannotated image databases the technique is based on a hierarchical variant of the self organizing map som called the tree structured self organizing map ts som given a set of reference images picsom is able to retrieve another set of images which are most similar to the given ones each ts som is formed using a different image feature representation like color texture or shape a new technique introduced in picsom facilitates automatic combination of the responses from multiple ts soms and their hierarchical levels this mechanism adapts to the user s preferences in selecting which images resemble each other in this paper a brief description of the system and a set of methods applicable to evaluating retrieval performance of image retrieval applications are presented 1 introduction content based image retrieval cbir has been a subject normalization and matching in the doro system this paper is concerned with the use of linguistically motivated phrases as indexing terms in information retrieval applications apart from the conventional noun phrases we propose to use verb phrases as index terms for text classification techniques for phrase matching through syntactic normalization and semantical matching are described in particular we show how to perform syntactic normalization of phrases in order to enhance recall semantical normalization is based on lexico semantical relations taking into account certain properties of the classification algorithms used the ideas described here are being implemented in the document routing system doro in which statistical learning algorithms are applied to document profiles consisting of phrases this paper describes the rationale behind work in progress rather than presenting final results a multi threaded approach to simulated soccer agents for the robocup competition to meet the timing requirements set by the robocup soccer server simulator this paper proposes a multi threaded approach to simulated soccer agents for the robocup competition at its higher level each agent works at three distinct phases sensing thinking and acting instead of the traditional single threaded approaches posix threads have been used here to break down these phases and implement them concurrently the details of how this parallel implementation can significantly improve the agent s responsiveness and its overall performance are described implementation results show that a multi threaded approach clearly outperforms a singlethreaded one in terms of efficiency responsiveness and scalability the proposed approach will be very efficient in multi processor systems 1 introduction the creation of the robotic soccer the robot world cup initiative robocup is an attempt to foster ai and intelligent robotics research by providing a standard problem where wide range o transaction oriented computational models for multi agent systems bdi belief desire intention is a mature and commonly adopted architecture for intelligent agents however the current computational model adopted by bdi has a number of problems with concurrency control recoverability and predictability this has hindered the construction of agents having robust and predictable behaviour indeed the conceptual and practical tools needed for building dependable agent systems resilient to faults and other unexpected situations are still at an early stage of research mobile agents and the future of the internet use of the internet has exploded in recent years with the appearance of the world wide web in this paper we show how current technological trends may lead to a system based substantially on mobile code and in many cases mobile agents we discuss several technical and non technical hurdles along the path to that eventuality it seems likely that within a few years nearly all major internet sites will be capable of hosting and willing to host some form of mobile code or mobile agents 1 introduction rapidly evolving network and computer technology coupled with the exponential growth of the services and information available on the internet will soon bring us to the point where hundreds of millions of people will have fast pervasive access to a phenomenal amount of information through desktop machines at work school and home through televisions phones pagers and car dashboards from anywhere and everywhere mobile code and in particular mobile agents will be an essential querying temporal constraint networks in ptime we start with the assumption that temporal knowledge usually captured by constraint networks can be represented and queried more effectively by using the scheme of indefinite constraint databases proposed by koubarakis although query evaluation in this scheme is in general a hard computational problem we demonstrate that there are several interesting cases where query evaluation can be done in ptime these tractability results are original and subsume previous results by van beek brusoni console and terenziani introduction when temporal constraint networks are used in applications their nodes represent the times when certain facts are true or when certain events take place or when events start or end by labeling nodes with appropriate natural language expressions e g breakfast or walk and arcs by temporal relations temporal constraint networks can be queried in useful ways for example the query is it possible or certain that event walk happened after event breakfast tractable query answering in indefinite constraint databases basic results and applications to querying spatiotemporal information we consider the scheme of indefinite constraint databases proposed by koubarakis this scheme can be used to represent indefinite information arising in temporal spatial and truly spatiotemporal applications the main technical problem that we address in this paper is the discovery of tractable classes of databases and queries in this scheme we start with the assumption that we have a class of constraints c with satisfiability and variable elimination problems that can be solved in ptime under this assumption we show that there are several general classes of databases and queries for which query evaluation can be done with ptime data complexity we then search for tractable instances of c in the area of temporal and spatial constraints classes of constraints with tractable satisfiability problems can be easily found in the literature the largest class that we consider is the class of horn disjunctive linear constraints over the rationals because variable eliminati executing suspended logic programs we present an extension of logic programming lp which in addition to ordinary lp clauses also includes integrity constraints explicit representation of disjunction in the bodies of clauses and in goals and suspension of atoms as in concurrent logic languages the resulting framework aims to unify constraint logic programming clp abductive logic programming alp and semantic query optimisation sqo in deductive databases we present a proof procedure for the new framework simplifying and generalising previously proposed proof procedures for alp we discuss applications of the framework formulating traditional problems from lp alp clp and sqo keywords logic programming lp constraint logic programming clp abductive logic programming alp semantic query optimisation sqo in deductive databases the second author is supported by the epsrc project logic based multi agent systems the third author is supported by onr grant n00014 96 1 1057 the authors are grat a memetic algorithm with self adaptive local search tsp as a case study in this paper we introduce a promising hybridization scheme for a memetic algorithm ma our ma is composed of two optimization processes a genetic algorithm and a monte carlo method mc in contrast with other ga monte carlo hybridized memetic algorithms in our work the mc stage serves two purposes when the population is diverse it acts like a local search procedure and when the population converges its goal is to diversify the search to achieve this the mc is self adaptive based on observations from the underlying ga behavior the ga controls the long term optimization process we present preliminary yet statistically significant results on the application of this approach to the tsp problem we also comment it successful application to a molecular conformational problem protein folding application of machine learning to robotics an analysis robotics is one of the most challenging applications of machine learning ml techniques it is characterized by direct interaction with a real world sensory feedback and an enormous complexity of the control system in recent years several approaches to apply ml to specific robotics tasks have been published nevertheless we are still far from a complete autonomous robot control system with learning components this paper aims at pointing out the problems and possible applications for integrating learning capabilities into a robot control system and then describes a new integrated system architecture which shows up a set of components necessary for partially autonomous systems with learning facilities 1 introduction in recent years there has been an increasing interest in applying machine learning techniques to robotics the applications are manipulator as well as mobile system tasks the learning techniques used range from rote learning 25 22 1 8 27 and inductive learning managing intervals efficiently in object relational databases modern database applications show a growing demand for efficient and dynamic management of intervals particularly for temporal and spatial data or for constraint handling common approaches require the augmentation of index structures which however is not supported by existing relational database systems by design the new relational interval tree 1 ri tree employs built in indexes on an as they are basis and is easy to implement whereas the functionality and efficiency of the ri tree is supported by any off the shelf relational dbms it is perfectly encapsulated by the object relational data model the ri tree requires o n b disk blocks of size b to store n intervals o log b n i o operations for insertion or deletion and o h log b n r b i os for an intersection query producing r results the height h of the virtual backbone tree corresponds to the current expansion and granularity of the data space but does not depend on n as demonstrated by our ex parameter control using the agent based patchwork model the setting of parameters in evolutionary algorithms ea has crucial influence on their performance typically the best choice depends on the optimization task some parameters yield better results when they are varied during the run recently the so called terrainbased genetic algorithm tbga was introduced which is a self tuning version of the traditional cellular genetic algorithm cga in a tbga the individuals of the population are placed in a two dimensional grid where only neighbored individuals can mate with each other the position of an individual in this grid is interpreted as its offspring s specific mutation rate and number of crossover points this approach allows to apply ga parameters that are optimal for i the type of optimization task and ii the current state of the optimization process however only a few individuals can apply the optimal parameters simultaneously due to their fixed position in the grid lattice in this paper we substituted the fixed s a dynamic replica selection algorithm for tolerating timing faults server replication is commonly used to improve the fault tolerance and response time of distributed services an important problem when executing time critical applications in a replicated environment is that of preventing timing failures by dynamically selecting the replicas that can satisfy a client s timing requirement even when the quality of service is degraded due to replica failures and excess load on the server in this paper we describe the approach we have used to solve this problem in aqua a corba based middleware that transparently replicates objects across a local area network the approach we use estimates a replica s response time distribution based on performance measurements regularly broadcast by the replica an online model uses these measurements to predict the probability with which a replica can prevent a timing failure for a client a selection algorithm then uses this prediction to choose a subset of replicas that can together meet the client s timing constraints with at least the probability requested by the client we conclude with experimental results based on our implementation speech and gesture multimodal control of a whole earth 3d visualization environment a growing body of research shows several advantages to multimodal interfaces including increased expressiveness flexibility and user freedom this paper investigates the design of such an interface that integrates speech and hand gestures the interface has the additional property of operating relative to the user and can be used while the user is in motion or stands at a distance from the computer display the paper then describes an implementation of the multimodal interface for a whole earth 3d visualization environment which presents navigation interface challenges due to the large magnitude of scale and extended spaces that is available the characteristics of the multimodal interface are examined such as speed recognizability of gestures ease and accuracy of use and learnability under likely conditions of use this implementation shows that such a multimodal interface can be e ective in a real environment and sets some parameters for the design and use of such interfaces extracting semistructured data lessons learnt the yellow pages assistant ypa is a natural language dialogue system which guides a user through a dialogue in order to retrieve addresses from the yellow pages part of the work in this project is concerned with the construction of a backend i e the database extracted from the raw input text that is needed for the online access of the addresses here we discuss some aspects involved in this task as well as report on experiences which might be interesting for other projects as well exploiting structure for intelligent web search together with the rapidly growing amount of online data we register an immense need for intelligent search engines that access a restricted amount of data as found in intranets or other limited domains this sort of search engines must go beyond simple keyword indexing matching but they also have to be easily adaptable to new domains without huge costs this paper presents a mechanism that addresses both of these points first of all the internal document structure is being used to extract concepts which impose a directorylike structure on the documents similar to those found in classified directories furthermore this is done in an efficient way which is largely language independent and does not make assumptions about the document structure oceanstore an architecture for global scale persistent storage oceanstore is a utility infrastructure designed to span the globe and provide continuous access to persistent information since this infrastructure is comprised of untrusted servers data is protected through redundancy and cryptographic techniques to improve performance data is allowed to be cached anywhere anytime additionally monitoring of usage patterns allows adaptation to regional outages and denial of service attacks monitoring also enhances performance through pro active movement of data a prototype implementation is currently under development 1 a holistic process performance analysis through a performance data warehouse this paper describes how a performance data warehouse can be used to facilitate business process improvement that is based on holistic performance measurement the feasibility study shows how management and analysis of performance data can be facilitated by a data warehouse approach it is argued that many of the shortcomings of traditional measurement systems can be overcome with this performance data warehouse approach querying as an enabling technology in software reengineering in this paper it is argued that different kinds of reengineering technologies can be based on querying several reengineering technologies are presented as being integrated into a technically oriented reengineering taxonomy the usefulness of querying is pointed out with respect to these reengineering technologies to impose querying as a base technology in reengineering examples are given with respect to the eer gral approach to conceptual modeling and implementation this approach is presented together with greql as its query part the different reengineering technologies are finally reviewed in the context of the greql query facility 1 introduction reengineering may be viewed as any activity that either improves the understanding of a software or else improves the software itself 2 according to this view software reengineering can be partitioned into two kinds of activities the first kind of activities is concerned with understanding such as source code retrieval browsin the adaptive agent architecture achieving fault tolerance using persistent broker teams brokers are used in many multi agent systems for locating agents for routing and sharing information for managing the system and for legal purposes as independent third parties however these multi agent systems can be incapacitated and rendered non functional when the brokers become inaccessible due to failures such as machine crashes network breakdowns and process failures that can occur in any distributed software system we propose that the theory of teamwork can be used to create robust brokered architectures that can recover from broker failures and we present the adaptive agent architecture aaa to show the feasibility of this approach the aaa brokers form a team with a joint commitment to serve any agent that registers with the broker team as long as the agent remains registered with the team this commitment enables the brokers to substitute for each other when needed a multiagent system based on the aaa can continue to work despite broker failures as long a behavior based intelligent control architecture with application to coordination of multiple underwater vehicles the paper presents a behavior based intelligent control architecture for designing controllers which based on their observation of sensor signals compute the discrete control actions these control actions then serve as the set points for the lower level controllers the behavior based approach yields an intelligent controller which is a cascade of a perceptor and a response controller the perceptor extracts the relevant symbolic information from the incoming continuous sensor signals which enables the execution of one of the behaviors the response controller is a discrete event system that computes the discrete control actions by executing one of the enabled behaviors the behavioral approach additionally yields a hierarchical two layered response controller which provides better complexity management the inputs from the perceptor are used to first compute the higher level activities called behaviors and next to compute the corresponding lower level activities called actio beyond best effort router architectures for the differentiated services of tomorrow s internet with the transformation of the internet to a commercial infrastructure the ability to provide differentiated services to users with widely varying requirements is rapidly becoming as important as meeting the massive increases in bandwidth demand hence while deploying routers switches and transmission systems of ever increasing capacity internet service providers would also like toprovide customer specific differentiated services using the same shared network infrastructure in this paper we describe router architectures that can support the two trends of rising bandwidth demand and rising demand for differentiated services we focus on router mechanisms that can support differentiated services at a level not being contemplated in proposals currently under consideration due to concern regarding their implementability at high speeds we consider the types of differentiated services that service providers may want to offer and then discuss the mechanisms needed in routers to support them we describe plausible implementations of these mechanisms the scalability and performance of which have been demonstrated by implementation in a prototype system and argue that it is technologically possible to raise considerably the level of differentiated services that service providers can offer their customers and that it is not necessary to restrict differentiated services to rudimentary offerings even in very high speed networks flexible distributed database management with agentteam ideally distributed management of relational data should enable the sharing of consistent data and provide system transparency in contrast to information retrieval users can access compact data and utilise it for further relational processing however distributed database management is still a challenging research area that involves bridging syntactic and semantic heterogeneity of data as well as of functionality since existing distributed database management systems were usually built with a focus on the implementation of some dedicated protocols for distributed database management they are inflexible for major modifications or exchange of the protocols in addition their software architecture usually does not comply with well known design paradigms which could facilitate the maintenance of the software system we present an agent based approach for flexible distributed database management where independent distributed database management protocols and methods are web document classification based on hyperlinks and document semantics besides the basic content a web document also contains a set of hyperlinks pointing to other related documents hyperlinks in a document provide much information about its relation with other web documents by analyzing hyperlinks in documents inter relationship among documents can be identified in this paper we will propose an algorithm to classify web documents into subsets based on hyperlinks in documents and their content representativedocuments will also be identified in each subset based on the proposed similarity definition with the representative document searching for related documents can be achieved 1 introduction the www is growing through a decentralized process and documents in the web are lack of logical organization besides the enormous number of web documents make the manipulation and further operation on web documents difficult although the size of web document set is large we need not analyze all the web documents as a whole web documents can latent semantic indexing by self organizing map an important problem for the information retrieval from spoken documents is how to extract those relevant documents which are poorly decoded by the speech recognizer in this paper we propose a stochastic index for the documents based on the latent semantic analysis lsa of the decoded document contents the original lsa approach uses singular value decomposition to reduce the dimensionality of the documents as an alternative we propose a computationally more feasible solution using random mapping rm and self organizing maps som the motivation for clustering the documents by som is to reduce the effect of recognition errors and to extract new characteristic index terms experimental indexing results are presented using relevance judgments for the retrieval results of test queries and using a document perplexity defined in this paper to measure the power of the index models 1 introduction in this paper we present methods for indexing speech data which has been automatically t agents in delivering personalized content based on semantic metadata in the smartpush project professional editors add semantic metadata to information flow when the content is created this metadata is used to filter the information flow to provide the end users with a personalized news service personalization and delivery process is modeled as software agents to whom the user delegates the task of sifting through incoming information the key components of the smartpush architecture have been implemented and the focus in the project is shifting towards a pilot implementation and testing the ideas in practice introduction internet and online distribution are changing the rules of professional publishing physical constraints of media products are not any more the key factors in the publishing process in newspaper publishing the bottleneck was what could fit in the paper today publishers are capable of creating more content but they need new methods for personalized distribution incremental price for publishing on the internet is neg wrapper induction efficiency and expressiveness the internet presents numerous sources of useful information telephone directories product catalogs stock quotes event listings etc recently many systems have been built that automatically gather and manipulate such information on a user s behalf however these resources are usually formatted for use by people e g the relevant content is embedded in html pages so extracting their content is difficult most systems use customized wrapper procedures to perform this extraction task unfortunately writing wrappers is tedious and error prone as an alternative we advocate wrapper induction a technique for automatically constructing wrappers in this article we describe six wrapper classes and use a combination of empirical and analytical techniques to evaluate the computational tradeoffs among them we first consider expressiveness how well the classes can handle actual internet resources and the extent to which wrappers in one class can mimic those in another we then adaptive information extraction core technologies for information agents introduction for the purposes of this chapter an information agent can be described as a distributed system that receives a goal through its user interface gathers information relevant to this goal from a variety of sources processes this content as appropriate and delivers the results to the users we focus on the second stage in this generic architecture we survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources for example consider an agent that mediates package delivery requests to satisfy such requests the agent might need to retrieve address information from geographic services ask an advertising service for freight forwarders that serve the destination request quotes from the relevant freight forwarders retrieve duties and legal constraints from government sites get weather information to estimate transportation delays etc information extraction ie is a form of sh finite state approaches to web information extraction introduction an information agent is a distributed system that receives a goal through its user interface gathers information relevant to this goal from a variety of sources processes this content as appropriate and delivers the results to the users we focus on the second stage in this generic architecture we survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources for example consider an agent that mediates package delivery requests to satisfy such requests the agent might need to retrieve address information from geographic services ask an advertising service for freight forwarders that serve the destination request quotes from the relevant freight forwarders retrieve duties and legal constraints from government sites get weather information to estimate transportation delays etc information extraction ie is a form of shallow document processing that involves populating decomposition in data mining an industrial case study data mining offers tools for discovery of relationships patterns and knowledge in large databases the knowledge extraction process is computationally complex and therefore a subset of all data is normally considered for mining in this paper numerous methods for decomposition of data sets are discussed decomposition enhances the quality of knowledge extracted from large databases by simplification of the data mining task the ideas presented are illustrated with examples and an industrial case study in the case study reported in this paper a data mining approach is applied to extract knowledge from a data set the extracted knowledge is used for the prediction and prevention of manufacturing faults in wafers g algorithm for extraction of robust decision rules children s postoperative intra atrial arrhythmia case study clinical medicine is facing a challenge of knowledge discovery from the growing volume of data in this paper a data mining algorithm g algorithm is proposed for extraction of robust rules that can be used in clinical practice for better understanding and prevention of unwanted medical events the g algorithm is applied to the data set obtained for children born with a malformation of the heart univentricular heart as the result of the fontan surgical procedure designed to palliate the children 10 35 of patients postoperatively develop an arrhythmia known as the intra atrial reentrant tachycardia there is an obvious need to identify the children that may develop the tachycardia before the surgery is performed prior attempts to identify such children with statistical techniques have been unrewarding the g algorithm discussed in this paper shows that there exists an unambiguous relationship between measurable features and the tachycardia the data set used in this study shows that for 78 08 of infants the occurrence of tachycardia can be accurately predicted the authors prior computational experience with diverse medical data sets indicates that the percentage of accurate predictions may become even higher if data on additional features is collected for a larger data set affine object representations for calibration free augmented reality we describe the design and implementation of a videobased augmented reality system capable of overlaying threedimensional graphical objects on live video of dynamic environments the key feature of the system is that it is completely uncalibrated it does not use any metric information about the calibration parameters of the camera or the 3d locations and dimensions of the environment s objects the only requirement is the ability to track across frames at least four feature points that are specified by the user at system initialization time and whose world coordinates are unknown our approach is based on the following observation given a set of four or more non coplanar 3d points the projection of all points in the set can be computed as a linear combination of the projections of just four of the points we exploit this observation by 1 tracking lines and fiducial points at frame rate and 2 representing virtual objects in a non euclidean affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points 1 an evolutionary algorithm using multivariate discretization for decision rule induction we describe edrl md an evolutionary algorithm based system for learning decision rules from databases the main novelty of our approach lies in dealing with continuous valued attributes most of decision rule learners use univariate discretization methods which search for threshold values for one attribute at the same time in contrast to them edrl md simultaneously searches for threshold values for all continuous valued attributes when inducing decision rules we call this approach multivariate discretization since multivariate discretization is able to capture interdependencies between attributes it may improve the accuracy of obtained rules the evolutionary algorithm uses problem specific operators and variable length chromosomes which allows it to search for complete rulesets rather than single rules the preliminary results of the experiments on some real life datasets are presented scaling question answering to the web the wealth of information on the web makes it an attractive resource for seeking quick answers to simple factual questions such as who was the first american in space or what is the second tallest mountain in the world yet today s most advanced web search services e g google and askjeeves make it surprisingly tedious to locate answers to such questions in this paper we extend question answering techniques first studied in the information retrieval literature to the web and experimentally evaluate their performance first we introduce mulder which we believe to be the first general purpose fully automated question answering system available on the web second we describe mulder s architecture which relies on multiple search engine queries natural language parsing and a novel voting procedure to yield reliable answers coupled with high recall finally we compare mulder s performance to that of google and askjeeves on questions drawn from the trec 8 question answering track we find that mulder s recall is more than a factor of three higher than that of askjeeves in addition we find that google requires 6 6 times as much user effort to achieve the same level of recall as mulder automated text categorization using support vector machine in this paper we study the use of support vector machine in text categorization unlike other machine learning techniques it allows easy incorporation of new documents into an existing trained system moreover dimension reduction which is usually imperative now becomes optional thus svm adapts efficiently in dynamic environments that require frequent additions to the document collection empirical results on the reuters 22173 collection are also discussed 1 introduction the increasingly widespread use of information services made possible by the internet and world wide web www has led to the so called information overloading problem today millions of online documents on every topic are easily accessible via the internet as the available information increases the inability of people to assimilate and profitably utilize such large amounts of information becomes more and more apparent developing user friendly automatic tools for efficient as well as effective retrieval picsom a framework for content based image database retrieval using self organizing maps we have developed an image retrieval system which uses tree structured self organizing maps ts soms as the method for retrieving images similar to a given set of reference images in a database it also provides a framework for the research on algorithms and methods for contentbased retrieval of images a novel technique introduced in this paper facilitates automatic combination of the responses from multiple ts soms and their hierarchical levels the system tries to adapt to the user s preferences in selecting which images resemble each other in the particular sense the user is interested of this mechanism implements a relevance feedback technique on content based image retrieval the image queries are performed through the world wide web and the queries are iteratively re ned as the system exposes more images to the user 1 introduction content based image retrieval from unannotated image databases has been an object for ongoing research for a long period 12 digital image and v standardizing agent communication an agent communication language acl is a collection of speech act like message types with agreed upon semantics which facilitate the knowledge and information exchange between software agents agent communication languages the current landscape this article suggest a paradigm for software development that emphasizes autonomy both at design time and runtime adaptivity and cooperation this approach seems appealing in a world of distributed heterogeneous systems languages for communicating agents promise to play the role that natural languages played for their human counterparts an agent communication language that allows agents to interact while hiding the details of their internal workings will result in agent communities that tackle problems no individual agent could the interoperability problem bringing together mobile agents and agent communication languages interoperability is a central issue for both the mobile agents community and the wider agents community unfortunately the interoperability concerns are different between the two communities as a result inter agent communication is an issue that has been addressed in a limited manner by the mobile agents community agent communication languages acls have been developed as tools with the capacity to integrate disparate sources of information and support interoperability but have achieved limited use by mobile agents we investigate the origins of the differences of perspective on agent to agent communication examine the reasons for the relative lack of interest in acls by mobile agents researchers and explore the integration of acls into mobile agents frameworks copyright 1999 ieee published in the proceedings of the hawaii international conference on system sciences january 5 8 1999 maui hawaii 1 introduction there are two kinds of discussions that have plagued agent re facilitating the exchange of explicit knowledge through ontology mappings in this paper we give an overview of a system caiman that can facilitate the exchange of relevant documents between geographically dispersed people in communities of interest the nature of communities of interest prevents the creation and enforcement of a common organizational scheme for documents to which all community members adhere each community member organizes her documents according to her own categorization scheme ontology caiman exploits this personal ontology which is essentially the perspective of a user on a domain for information retrieval related documents are retrieved on a concept granularity level from a central community document repository to find the related concepts in the queried ontology caiman performs an ontology mapping the ontology mapping in caiman is based on a novel approach which considers the concepts in an ontology implicitly represented by the documents assigned to each concept using machine learning techniques for text classification a concept in a personal ontology is mapped to a concept in a community ontology the caiman system uses this mapping to provide document publishing and retrieval services both for the community and the user first results of the prototype system showed that this approach can be a valid alternative to existing techniques for information retrieval an agent based framework for mobile users user mobility together with an easy access to distributed resources is one of the greatest challenge to be faced in the future years at the same time agent technology is seen as a very promising approach to deal with distributed computing and user mobility in this paper an agent based strategy for support of mobile users is presented it is based on a mobile agent platform developed at the university of catania which has been enhanced in order to allow the user to access network services in a mobile environment main functionalities and architecure of the above platform are described 1 introduction the quick expansion of wireless communication technologies and of portable computing devices has made mobile computing more and more important the user wishes to access the information he she needs at any moment independently of the place where he she is the ever increasing computing power available in notebooks makes them a valid working tool for the user who needs to move from the open archives initiative building a low barrier interoperability framework the open archives initiative oai develops and promotes interoperability solutions that aim to facilitate the efficient dissemination of content the roots of the oai lie in the e print community over the last year its focus has been extended to include all content providers this paper describes the recent history of the oai its origins in promoting e prints the broadening of its focus the details of its technical standard for metadata harvesting the applications of this standard and future plans categories and subject descriptors the evolution of the soar cognitive architecture the origins of the soar architecture can be traced back to the seminal research of allen newell and herbert simon on symbol systems heuristic search goals problem spaces and production systems since its official inception in 1982 soar has evolved through six major releases as both an ai architecture and as the basis for a unified theory of cognition this paper traces this evolutionary path starting with soar s intellectual roots and then proceeding through the stages defined by the six major system releases each stage is characterized with respect to a hierarchy of four levels of analysis the knowledge level the problem space level the symbolic architecture level and the implementation level broadcasting consistent data to mobile clients with local cache although data broadcast has been shown to be an efficient method for disseminating data items in a mobile computing system with large number of clients the issue on how to ensure currency and consistency of the data items has not been examined adequately broadcasting consistent data to read only transactions from mobile clients in this paper we study the data inconsistency problem in data broadcast to mobile transactions while data items in a mobile computing system are being broadcast update transactions may install new values for the data items if the executions of update transactions and broadcast of data items are interleaved without any control the transactions generated by mobile clients called mobile transactions may observe inconsistent data values in this paper we propose a new protocol called update first with order ufo for concurrency control between read only mobile transactions and update transactions we show that although the protocol is simple all the schedules are serializable when the ufo protocol is applied furthermore the new protocol possesses many desirable properties for mobile computing systems such as the mobile transactions do not need to set any lock before they read a data item from the air and the protocol can be applied to different broadcast algorithms its performance has been investigated with extensive simulation experiment the results show that the protocol can maximize the freshness of the data items provided to mobile transactions and the broadcast overhead is not heavy especially when the arrival rate of the update transactions is not very high the overview of web search engines the world wide web allows people to share information globally the amount of information grows without bound in order to extract information that we are interested in we need a tool to search the web the tool is called a search engine this survey covers different components of the search engine and how the search engine really works it provides a background understanding of information retrieval it discusses different search engines that are commercially available it investigates how the search engines find information in the web and how they rank its pages to the give n query also the paper provides guidelines for users on how to use search engines broadcast of consistent data to read only transactions from mobile clients in this paper we study the data inconsistency problem in data broadcast to mobile transactions while data items in a mobile computing system are being broadcast update transactions may install new values for the data items if the executions of update transactions and broadcast of data items are interleaved without any control the transactions generated by mobile clients called mobile transactions may observe inconsistent data values in this paper we propose a new protocol called update first with order ufo for concurrency control between read only mobile transactions and update transactions we show that although the protocol is simple all the schedules are serializable when the ufo protocol is applied furthermore the new protocol possesses many desirable properties for mobile computing systems such as the mobile transactions do not need to set any lock before they read a data item from the air and the protocol can be applied to different broadcast algorithms its performance has been investigated with extensive simulation experiment the results show that the protocol can maximize the freshness of the data items provided to mobile transactions and the broadcast overhead is not heavy especially when the arrival rate of the update transactions is not very high feature reduction for neural network based text categorization in a text categorization model using an artificial neural network as the text classifier scalability is poor if the neural network is trained using the raw feature space since textural data has a very high dimension feature space we proposed and compared four dimensionality reduction techniques to reduce the feature space into an input space of much lower dimension for the neural network classifier to test the effectiveness of the proposed model experiments were conducted using a subset of the reuters 22173 test collection for text categorization the results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall among the four dimensionality reduction techniques proposed principal component analysis was found to be the most effective in reducing the dimensionality of the feature space 1 introduction text categorization is the classification of text documents into a set of one or more categories in this paper strategies in combined learning via logic programs abstract we discuss the adoption of a three valued setting for inductive concept learning distinguishing between what is true what is false and what is unknown can be useful in situations where decisions have tobetaken on the basis of scarce ambiguous or downright contradictory information in a three valued setting we learn a de nition for both the target concept and its opposite considering positive and negative examples as instances of two disjoint classes to this purpose we adopt extended logic programs elp under a well founded semantics with explicit negation wfsx as the representation formalism for learning and show how elps can be used to specify combinations of strategies in a declarative way also coping with contradiction and exceptions explicit negation is used to represent the opposite concept while default negation is used to ensure consistency and to handle exceptions to general rules exceptions are represented by examples covered by the de nition for a concept that belong to the training set for the opposite concept standard inductive logic programming techniques are employed to learn the concept and its opposite depending on the adopted technique we can learn the most general or the least general atnosferes a model for evolutive agent behaviors this paper introduces atnosfeers a model aimed at designing evolutive and adaptive behaviors for agents or multi agent systems we first discuss briefly the main problems raised by classical evolutionary models which are not intended to produce agents or behaviors but to solve problems then we provide detailed explanations about the model we propose and its components we also show through a simple example how the system works and give some experimental results finally we discuss the features of our model and propose extensions a comparison between atnosferes and xcsm in this paper we present atnosferes a new framework based on an indirect encoding genetic algorithm which builds finite state automata controllers able to deal with perceptual aliasing we compare it with xcsm a memory based extension of the most studied learning classifier system xcs through a benchmark experiment we then discuss the assets and drawbacks of atnosferes in the context of that comparison first steps towards an event based infrastructure for smart things in this paper we examine requirements for an infrastructure that supports implementation and deployment of smart things in the real world we describe a case study rfid chef where kitchen items and ingredients equipped with remotely accessible electronic tags drive an interactive context aware recipe finder through the use of an event based infrastructure oblivious decision trees and abstract cases in this paper we address the problem of case based learning in the presence of irrelevant features we review previous work on attribute selection and present a new algorithm oblivion that carries out greedy pruning of oblivious decision trees which effectively store a set of abstract cases in memory we hypothesize that this approach will efficiently identify relevant features even when they interact as in parity concepts we report experimental results on artificial domains that support this hypothesis and experiments with natural domains that show improvement in some cases but not others in closing we discuss the implications of our experiments consider additional work on irrelevant features and outline some directions for future research acquisition of place knowledge through case based learning in this paper we define the task of place learning and describe one approach to this problem the framework represents distinct places as evidence grids a probabilistic description of occupancy place recognition relies on case based classification augmented by a registration process to correct for translations the learning mechanism is also similar to that in case based systems involving the simple storage of inferred evidence grids experimental studies with physical and simulated robots suggest that this approach improves place recognition with experience that it can handle significant sensor noise that it benefits from improved quality in stored cases and that it scales well to environments with many distinct places previous researchers have studied evidence grids and place learning but they have not combined these two powerful concepts nor have they used the experimental methods of machine learning to evaluate their methods abilities keywords place acquisition case b applications of machine learning and rule induction an important area of application for machine learning is in automating the acquisition of knowledge bases required for expert systems in this paper we review the major paradigms for machine learning including neural networks instance based methods genetic learning rule induction and analytic approaches we consider rule induction in greater detail and review some of its recent applications in each case stating the problem how rule induction was used and the status of the resulting expert system in closing we identify the main stages in fielding an applied learning system and draw some lessons from successful applications introduction machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience expert performance requires much domainspecific knowledge and knowledge engineering has produced hundreds of ai expert systems that are now used regularly in industry machine learning aims to provide probabilistic hierarchical clustering with labeled and unlabeled data this paper presents hierarchical probabilistic clustering methods for unsupervised and supervised learning in datamining applications where supervised learning is performed using both labeled and unlabeled examples the probabilistic clustering is based on the previously suggested generalizable gaussian mixture model and is extended using a modified expectation maximization procedure for learning with both unlabeled and labeled examples the proposed hierarchical scheme is agglomerative and based on probabilistic similarity measures here we compare a l 2 dissimilarity measure error confusion similarity and accumulated posterior cluster probability measure the unsupervised and supervised schemes are successfully tested on artificially data and for e mails segmentation 1 version space algebra and its application to programming by demonstration machine learning research has been very successful at producing powerful broadlyapplicable classification learners however many practical learning problems do not fit the classification framework well and as a result the initial phase of suitably formulating the problem and incorporating the relevant domain knowledge can be very di cult and typically consumes the majority of the project time here we propose a framework to systematize and speed this process based on the notion of version space algebra we extend the notion of version spaces beyond concept learning and propose that carefullytailored version spaces for complex applications can be built by composing simpler restricted version spaces we illustrate our approach with smartedit a programming by demonstration application for repetitive text editing that uses version space algebra to guide a search over text editing action sequences we demonstrate the system on a suite of repetitive text editing privacy interfaces for information management this article we propose a set of guidelines for designing privacy interfaces that facilitate the creation inspection modification and monitoring of privacy policies these guidelines are based on our experience with collabclio a system that supports automated sharing of web browsing histories collab clio stores a person s browsing history and makes it searchable by content keyword and other attributes a typical collabclio query might be show me all the pages tessa has visited in the edu domain that contain the phrase direct manipulation since a collabclio user can make queries regarding the browsing history of other users there are obvious privacy concerns programming by demonstration an inductive learning formulation although programming by demonstration pbd has the potential to improve the productivity of unsophisticated users previous pbd systems have used brittle heuristic domain specific approaches to execution trace generalization in this paper we define two applicationindependent methods for performing generalization that are based on well understood machine learning technology tgen vs uses version space generalization and tgen foil is based on the foil inductive logic programming algorithm we analyze each method both theoretically and empirically arguing that tgen vs has lower sample complexity but tgen foil can learn a much more interesting class of programs keywords programming by demonstration machine learning inductive logic programming version spaces introduction computer users are largely unable to customize massproduced applications to fit their individual tasks this problem of end user customization has been addressed in several ways ffl adjustable preference the view holder approach utilizing customized materialized views to create database services suitable for mobile database applications among mobile devices i e a laptop vs a pager and the amount of information available from today s database environments and the internet to this end this dissertation presents the development of customizable view maintenance services called the view holder approach whose middleware mechanism within the fixed network dynamically maintains versions of the views so that to meet the data consistency and currency requirements of a particular mobile client in a general form a view holder can support a community of mobile clients with common interests the motivation for maintaining versions is to compensate for the data changes that occurred to the materialized views that were used during disconnection as well as to reduce the cost of wireless communication in order to maintain these views customized view maintenance is performed at the data sources by translating the mobile machine s request into a materialization program containing a triggering flex and pinch a case study of whole hand input design for virtual environment interaction we present a discussion of design issues involving whole hand input in virtual environments in many cases whole hand input devices limit the types of interaction that the user can perform in the virtual world due to the nature of the device one possible approach to alleviate these limitations is to provide hybrid input devices which enable the user to combine information generated from two different whole hand input devices in this paper we describe our pinch glove like input device which is used as a tool to augment bend sensing gloves for object manipulation and menu selection as well as a method to test and evaluate different hand postures and gestures that could not be developed with a single whole hand device keywords human computer interaction virtual environments 3d graphics applications conductive cloth flex and pinch input introduction there have been a number of different approaches for interacting in virtual environments in general these approaches have attem msvt a virtual reality based multimodal scientific visualization tool recent approaches to providing users with more natural methods of interacting with virtual environment applications have shown that more than one mode of input can be both beneficial and intuitive as a communication medium between humans and computer applications although there are many different modes that could be used in these applications hand gestures and speech appear to be two of the most logical since users will typically be in environments that will have them immersed in a virtual world with limited access to traditional input devices such as the keyboard or mouse in this paper we describe a prototype application msvt multimodal scientific visualization tool for visualizing fluid flow around a dataset msvt uses a multimodal interface which combines whole hand and voice input to allow users to visualize and interact with the dataset in a natural manner a discussion of the various interaction techniques and the results of an informal user evaluation are presented ke whole hand and speech input in virtual environments recent approaches to providing users with a more natural method of interacting with computer applications have shown that more than one mode of input can be both beneficial and intuitive as a communication medium between humans and computers two modalities in particular whole hand and speech input represent a natural form of communication that has been ingrained in our physical and mental makeup since birth in this thesis we investigate the use of whole hand and speech input in virtual environments in the context of two applications domains scientific visualization and interior design by examining the two modalities individually and in combination and through the creation of two application prototypes multimodal scientific visualization tool and room designer we present anumber of contributions including a set of interface guidelines and interaction techniques for whole hand and speech input intelligent data analysis in medicine extensive amounts of knowledge and data stored in medical databases require the development of specialized tools for storing and accessing of data data analysis and effective use of stored knowledge and data this paper focuses on methods and tools for intelligent data analysis aimed at narrowing the increasing gap between data gathering and data comprehension the paper sketches the history of research that led to the development of current intelligent data analysis techniques discusses the need for intelligent data analysis in medicine and proposes a classification of intelligent data analysis methods the scope of the paper covers temporal data abstraction methods and data mining methods a selection of methods is presented and illustrated in medical problem domains presently data abstraction and data mining are attracting considerable research interest however the two technologies in spite of the fact that they share their central objective namely the intelligen context in web search web search engines generally treat search requests in isolation the results for a given query are identical independent of the user or the context in which the user made the request nextgeneration search engines will make increasing use of context information either by using explicit or implicit context information from users or by implementing additional functionality within restricted contexts greater use of context in web search may help increase competition and diversity on the web context and page analysis for improved web search nec research institute has developed a metasearch engine that improves the efficiency of web searches by downloading and analyzing each document and then displaying results that show the query terms in context several popular and useful search engines such as altavista excite hotbot infoseek lycos and northern light attempt to maintain full text indexes of the world wide web however relying on a single standard search engine has limitations the standard search engines have limited coverage 1 2 outdated databases and are sometimes unavailable due to problems with the network or the engine itself the precision of standard engine results can also vary because they generally focus on handling queries quickly and use relatively simple ranking schemes 3 rankings can be further muddled by keyword spamming to increase a page s rank order often the relevance of a particular page is obvious only after loading it and finding the query terms metasearch engines such as metacrawler and savvysearch attempt to contend with the problem of limited coverage by submitting queries to several standard search engines at once 4 5 the primary advantages of metasearch engines are that they combine the results of several search engines and present a consistent user interface 5 however most metasearch engines rely on the documents and summaries returned by standard search engines and so inherit their limited precision and vulnerability to keyword spamming we developed the nec research institute neci metasearch engine now called inquirus to improve the efficiency and precision of web search by downloading and analyzing each document and then displaying results that show the query terms in searching the world wide web the coverage and recency of the major world wide web search engines was analyzed yielding some surprising results the coverage of any one engine is significantly limited no single engine indexes more than about one third of the indexable web the coverage of the six engines investigated varies by an order of magnitude and combining the results of the six engines yields about 3 5 times as many documents on average as compared with the results from only one engine analysis of the overlap between pairs of engines gives an estimated lower bound on the size of the indexable web of 320 million pages the internet has grown rapidly since its inception in december 1969 1 and is anticipated to expand 1000 over the next few years 2 the amount of scientific information and the number of electronic journals on the internet continue to increase about 1000 journals as of 1996 2 3 the internet and the world wide web the web represent significant advancements for the retrieval and dissemination of scientific and other literature and for the advancement of education 2 4 with the introduction of full text search engines such as altavista www searching the web general and scientific information access he world wide web is revolutionizing the way people access information and has opened up new possibilities in areas such as digital libraries general and scientific information dissemination and retrieval education commerce entertainment government and health care the amount of publicly available information on the web is increasing rapidly 1 the web is a gigantic digital library a searchable 15 billion word encyclopedia 2 it has stimulated research and development in information retrieval and dissemination and fostered search engines such as altavista these new developments are not limited to the web and can enhance access to virtually all forms of digital libraries the revolution the web has brought to information access is not so much due to the availability of information huge amounts of information has long been available in libraries and elsewhere but rather the increased efficiency of accessing information which can make previously impractical tasks practical there are many avenues for improvement in the efficiency of accessing information on the web for example in the areas of locating and organizing information this article discusses general and scientific information access on the web and many of our comments are applicable to digital libraries in general the effectiveness of web search engines is discussed including results that show that the major search engines cover only a fraction of the publicly indexable web the part of the web which is considered for indexing by the major engines which excludes pages hidden behind search forms pages with authorization requirements etc current research into improved searching of the web is discussed including new techniques for ranking the relevance of results and new techniques in metasearch that can improve the efficiency and effectiveness of web search the amount of scientific information and the number of electronic journals on the internet continues to increase researchers are increasingly making their work available online this article also discusses the creation of digital libraries of the scientific literature incorporating autonomous citation indexing the autonomous creation of citation indices text and image metasearch on the web as the web continues to increase in size the relative coverage of web search engines is decreasing and search tools that combine the results of multiple search engines are becoming more valuable this paper provides details of the text and image metasearch functions of the inquirus search engine developed at the nec research institute for text metasearch we describe features including the use of link information in metasearch and provide statistics on the usage and performance of inquirus and the web search engines for image metasearch inquirus queries multiple image search engines on the web downloads the actual images and creates image thumbnails for display to the user inquirus handles image search engines that return direct links to images and engines that return links to html pages for the engines that return html pages inquirus analyzes the text on the pages in order to predict which images are most likely to correspond to the query the individual image search engin a new research tool for intrinsic hardware evolution the study of intrinsic hardware evolution relies heavily on commercial fpga devices which can be configured in real time to produce physical electronic circuits use of these devices presents certain drawbacks to the researcher desirous of studying fundamental principles underlying hardware evolution since he has no control over the architecture or type of basic configurable element furthermore analysis of evolved circuits is difficult as only external pins of fpgas are accessible to test equipment after discussing current issues arising in intrinsic hardware evolution this paper presents a new test platform designed specifically to tackle them together with experimental results exemplifying its use the results include the first circuits to be evolved intrinsically at the transistor level 1 introduction in recent years evolutionary algorithms eas have been applied to the design of electronic circuitry with significant results being attained using both computer simulations the spectral independent components of natural scenes abstract we apply independent component analysis ica for learning an efficient color image representation of natural scenes in the spectra of single pixels the algorithm was able to find basis functions that had a broadband spectrum similar to natural daylight as well as basis functions that coincided with the human cone sensitivity response functions when applied to small image patches the algorithm found homogeneous basis functions achromatic basis functions and basis functions with overall chromatic variation along lines in color space our findings suggest that icamay be used to reveal the structure of color information in natural images 1 learning codes for color images the efficient encoding of visual sensory information is an important task for image processing systems as well as for the understanding of coding principles in the visual cortex barlow 1 proposed that the goal of sensory information processing is to transform the input signals such that it reduces the redundancy collaborative learning for recommender systems recommender systems use ratings from users on items such as movies and music for the purpose of predicting the user preferences on items that have not been rated predictions are normally done by using the ratings of other users of the system by learning the user preference as a function of the features of the items or by a combination of both these methods in this paper we pose the problem as one of collaboratively learning of preference functions by multiple users of the recommender system we study several mixture models for this task we show via theoretical analyses and experiments on a movie rating database how the models can be designed to overcome common problems in recommender systems including the new user problem the recurring startup problem the sparse rating problem and the scaling problem 1 a unifying information theoretic framework for independent component analysis we show that different theories recently proposed for independent component analysis ica lead to the same iterative learning algorithm for blind separation of mixed independent sources we review those theories and suggest that information theory can be used to unify several lines of research pearlmutter and parra 1996 and cardoso 1997 showed that the infomax approach of bell and sejnowski 1995 and the maximum likelihood estimation approach are equivalent we show that negentropy maximization also has equivalent properties and therefore all three approaches yield the same learning rule for a fixed nonlinearity girolami and fyfe 1997a have shown that the nonlinear principal component analysis pca algorithm of karhunen and joutsensalo 1994 and oja 1997 can also be viewed from information theoretic principles since it minimizes the sum of squares of the fourth order marginal cumulants and therefore approximately minimizes the mutual information comon 1994 lambert 19 picashow pictorial authority search by hyperlinks on the web we describe picashow a fully automated www image retrieval system that is based on several link structure analyzing algorithms our basic premise is that a page displays or links to an image when the author of considers the image to be of value to the viewers of the page wethus extend some well known link based www schemes to the context of image retrieval picashow s analysis of the link structure enables it to retrieve relevant images even when those are stored in les with meaningless names the same analysis also allows it to identify and we dene these as web pages that are rich in relevant images or from which many images are readily accessible picashow requires no image analysis whatsoever and no creation of taxonomies for pre classication of the web s images it can be implemented by standard www search engines with reasonable overhead in terms of both computations and storage and with no change to user query formats it can thus be used to easily add image retrieving capabilities to standard search engines our results demonstrate that picashow while relying almost exclusively on link analysis compares well with dedicated www image retrieval systems we conclude that link analysis a bona de eective technique for web page search can improve the performance of web image retrieval as well as extend its denition to include the retrieval of image hubs and containers keywords image retrieval link structure analysis hubs and authorities image hubs 1 contract net based learning in a user adaptive interface agency this paper describes a multi agent learning approach to adaptation to users preferences realized by an interface agency using a contract net based negotiation technique agents as contractors as well as managers negotiate with each other to pursue the overall goal of dynamic user adaptation by learning from indirect user feedback the adjustment of internal credit vectors and the assignment of contractors that gained maximal credit with respect to the user s current preferences the preceding session and current situational circumstances can be realized in this way user adaptation is achieved without accumulating explicit user models but by the use of implicit distributed user models 1 introduction interface agents are computer programs that enhance the human computer interaction by mediating a relationship between technical systems and users lau90 on the one hand they provide assistance to users by acting on his her behalf and automating his her actions nor94 virtual information towers a metaphor for intuitive location aware information access in a mobile environment this paper introduces virtual information towers vits as a concept for presenting and accessing location aware information with mobile clients a vit is a means of structuring location aware information which is assigned to a certain geographical position while having a certain area of visibility a user equipped with a mobile wearable computer has access to the vits which are visible from his her current location the architecture and protocols of a system are described which allows its users to create vits and to access the information on them using internet mechanisms we have implemented a prototype of this system and a vit client for a wearable computer and will present some aspects of this implementation 1 introduction mobile information access is an important field of application for wearable computers in a mobile environment much of the accessed information is location dependent i e the content of the information or the user s interest in the information depends on coalition formation for large scale electronic markets coalition formation is a desirable behavior in a multiagent system when a group of agents can perform a task more efficiently than any single agent can computational and communications complexity of traditional approaches to coalition formation e g through negotiation make them impractical for large systems we propose an alternative physics motivated mechanism for coalition formation that treats agents as randomly moving locally interacting entities a new coalition may form when two agents encounter one another and it may grow when a single agent encounters it such agent level behavior leads to a macroscopic model that describes how the number and distribution of coalitions change with time we increase the generality and complexity of the model by letting the agents leave coalitions with some probability the model is expressed mathematically as a series of differential equations these equations have steady state solutions that describe the equilibrium distribution of coa a comparative study of neural network based feature extraction paradigms the projection maps and derived classification accuracies of a neural network nn implementation of sammon s mapping an auto associative nn aann and a multilayer perceptron mlp feature extractor are compared with those of the conventional principal component analysis pca tested on five real world databases the mlp provides the highest classification accuracy at the cost of deforming the data structure whereas the linear models preserve the structure but usually with inferior accuracy using plan recognition in human computer collaboration human computer collaboration provides a practical and useful application for plan recognition techniques we describe a plan recognition algorithm which is tractable by virtue of exploiting properties of the collaborative setting namely the focus of attention the use of partially elaborated hierarchical plans and the possibility of asking for clarification we demonstrate how the addition of our plan recognition algorithm to an implemented collaborative system reduces the amount of communication required from the user 1 introduction an important trend in recent work on human computer interaction and user modeling has been to view human computer interaction as a kind of collaboration e g ferguson and allen 1998 guinn 1996 rich and sidner 1998 rickel and johnson 1998 in this approach the human user and the computer often personified as an agent coordinate their actions toward achieving shared goals a common setting for collaboration illustrated in figure 1 cosmo a life like animated pedagogical agent with deictic believability life like animated interface agents for knowledgebased learning environments can provide timely customized advice to support students problem solving because of their strong visual presence they hold significant promise for substantially increasing students enjoyment of their learning experiences a key problem posed by life like agents that inhabit artificial worlds is deictic believability in the same manner that humans refer to objects in their environment through judicious combinations of speech locomotion and gesture animated agents should be able to move through their environment and point to and refer to objects appropriately as they provide problemsolving advice in this paper we describe a framework for achieving deictic believability in animated agents a deictic behavior planner exploits a world model and the evolving explanation plan as it selects and coordinates locomotive gestural and speech behaviors the resulting behaviors and utterances are believable and towards web scale web archeology web scale web research is difficult information on the web is vast in quantity unorganized and uncatalogued and available only over a network with varying reliability thus web data is difficult to collect to store and to manipulate efficiently despite these difficulties we believe performing web research at web scale is important we have built a suite of tools that allow us to experiment on collections that are an order of magnitude or more larger than are typically cited in the literature two key components of our current tool suite are a fast extensible web crawler and a highly tuned in memory database of connectivity information a web page repository that supports easy access to and storage for billions of documents would allow us to study larger data sets and to study how the web evolves over time probabilistic affine invariants for recognition under a weak perspective camera model the image plane coordinates in different views of a planar object are related by an affine transformation because of this property researchers have attempted to use affine invariants for recognition however there are two problems with this approach 1 objects or object classes with inherent variability cannot be adequately treated using invariants and 2 in practice the calculated affine invariants can be quite sensitive to errors in the image plane measurements in this paper we use probability distributions to address both of these difficulties under the assumption that the feature positions of a planar object can be modeled using a jointly gaussian density we have derived the joint density over the corresponding set of affine coordinates even when the assumptions of a planar object and a weak perspective camera model do not strictly hold the results are useful because deviations from the ideal can be treated as deformability in the creating specialised integrity checks through partial evaluation of meta interpreters interpretation danny de schreye is senior research associate of the belgian national fund for scientific research we would like to thank bern martens for proof reading several versions of this paper and for his helpful insights and comments on the topic of this paper we would also like to thank him for his huge pile of references on integrity checking and for introducing the first author to the subject we thank bart demoen for sharing his expertise on writing efficient prolog programs our thanks also go to john gallagher for pointing out several errors in an earlier version of the paper and for the fruitful discussions on partial evaluation and integrity checking we are also grateful for discussions and extremely helpful comments by hendrik decker other interesting discussions about the topic of this paper were held with stefan decker the members of the compulog ii project as well as with the participants of the 1996 dagstuhl seminar on logic and the meaning of change fina computing the entropy of user navigation in the web navigation through the web colloquially known as surfing is one of the main activities of users during web interaction when users follow a navigation trail they often tend to get disoriented in terms of the goals of their original query and thus the discovery of typical user trails could be useful in providing navigation assistance herein we give a theoretical underpinning of user navigation in terms of the entropy of an underlying markov chain modelling the web topology we present a novel method for online incremental computation of the entropy and a large deviation result regarding the length of a trail to realise the said entropy we provide an error analysis for our estimation of the entropy in terms of the divergence between the empirical and actual probabilities we then indicate applications of our algorithm in the area of web data mining finally we present an extension of our technique to higher order markov chains by a suitable reduction of a higher order markov chain model to a first order one key words web user navigation web data mining navigation problem markov chain entropy 1 web interaction and the navigation problem in hypertext written for encyclopedia of microcomputers the web has become a ubiquitous tool used in day to day work to find information and conduct business and it is revolutionising the role and availability of information one of the problems encountered in web interaction which is still unsolved is the navigation problem whereby users can get lost in hyperspace meaning that when following a sequence of links i e a trail of information users tend to become disoriented in terms of the goal of their original query and the relevance to the query of the information they are currently browsing herein we build statistical foundations for tackling the navigation problem based on a formal model of the web in terms of a probabilistic automaton which can also be viewed as a finite ergodic markov chain in our model of the web the probabilities attached to state transitions have two interpretations namely they can denote the proportion of times a user followed a link and alternatively they can denote the expected utility of following a link using this approach we have developed two techniques for constructing a web view based on the two interpretations of the probabilities of links where a web view is a collection of relevant trails the first method we describe is concerned with finding frequent user behaviour patterns a collection of trails is taken as input and an ergodic markov chain is produced as output with the probabilities of transitions corresponding to the frequency the user traversed the associated links the second method we describe is a reinforcement learning algorithm that attaches higher probabilities to links whose expected trail relevance is higher the user s home page and a query are taken as input and an ergodic markov chain is produced as output with the probabilities of justification for inclusion dependency normal form functional dependencies fds and inclusion dependencies inds are the most fundamental integrity constraints that arise in practice in relational databases in this paper we address the issue of normalisation in the presence of fds and inds and in particular the semantic justification for inclusion dependency normal form idnf a normal form which combines boyce codd normal form with the restriction on the inds that they be noncircular and keybased we motivate and formalise three goals of database design in the presence of fds and inds non interaction between fds and inds elimination of redundancy and update anomalies and preservation of entity integrity we show that as for fds in the presence of inds being free of redundancy is equivalent to being free of update anomalies then for each of these properties we derive equivalent syntactic conditions on the database design individually each of these syntactic conditions is weaker than idnf and the restriction that an fd not a probabilistic approach to navigation in hypertext one of the main unsolved problems confronting hypertext is the navigation problem namely the problem of having to know where you are in the database graph representing the structure of a hypertext database and knowing how to get to some other place you are searching for in the database graph previously we formalised a hypertext database in terms of a directed graph whose nodes represent pages of information the notion of a trail which is a path in the database graph describing some logical association amongst the pages in the trail is central to our model we defined a hypertext query language hql over hypertext databases and showed that in general the navigation problem i e the problem of finding a trail that satisfies a hql query technically known as the model checking problem is npcomplete herein we present a preliminary investigation of using a probabilistic approach in order to enhance the efficiency of model checking the flavour of our investigation is that if we h connecting planning and acting via object specific reasoning instructions from a high level planner are in general too abstract for a behavioral simulator to execute in this dissertation i describe an intermediate reasoning system the object specific reasoner which bridges the gap between high level task actions and action directives of a behavioral system it decomposes task actions and derives parameter values for each action directive thus enabling existing high level planners to instruct synthetic agents with the same task action commands that they currently produce the object specific reasoner s architecture follows directly from the hypothesis that action representations are underspecified descriptions and that objects in the same functional category are manipulated in similar ways the action representation and the object representation are combined to complete the action interpretation thereby grounding plans in action the object specific reasoner provides evidence that a small number of object functional categories organize automatic text detection and tracking in digital video text which appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video s structure and for classification in this paper we present algorithms for detecting and tracking text in digital video our system implements a scalespace feature extractor that feeds an artificial neural processor to detect text blocks our text tracking scheme consists of two modules an ssd sum of squared difference based module to find the initial position and a contour based module to refine the position experiments conducted with a variety of video sources show that our scheme can detect and track text robustly keywords text detection text tracking video indexing digital libraries neural network i introduction the continued proliferation of large amounts of digital video has increased demand for true content based indexing and retrieval systems traditionally content has been indexed primaril characterizing operating system activity in specjvm98 benchmarks complete system simulation to understand the influence of architecture and operating systems on application execution has been identified to be crucial for systems design this problem is particularly interesting in the context of java since it is not only the application that can invoke kernel services but so does the underlying java virtual machine jvm implementation which runs these programs further the jvm style jit compiler or interpreter and the manner in which the different jvm components such as the garbage collector and class loader are exercised can have a significant impact on the kernel activities to investigate these issues this chapter uses complete system simulation of the specjvm98 benchmarks on the simos simulation platform the execution of these benchmarks on both jit compilers and interpreters is profiled in detail the kernel activity of specjvm98 applications constitutes up to 17 of the execution time in the large dataset and up to 31 i indexing and querying xml data for regular path expressions with the advent of xml as a standard for data representation and exchange on the internet storing and querying xml data becomes more and more important several xml query languages have been proposed and the common feature of the languages is the use of regular path expressions to query xml data this poses a new challenge concerning indexing and searching xml data because conventional approaches based on tree traversals may not meet the processing requirements under heavy access requests in this paper we propose a new system for indexing and storing xml data based on a numbering scheme for elements this numbering scheme quickly determines the ancestor descendant relationship between elements in the hierarchy of xml data we also propose several algorithms for processing regular path expressions namely 1 join for searching paths from an element to another 2 join for scanning sorted elements and attributes to find element attribute pairs and 3 join for finding kleene closure on repeated paths or elements the join algorithm is highly effective particularly for searching paths that are very long or whose lengths are unknown experimental results from our prototype system implementation show that the proposed algorithms can process xml queries with regular path expressions by up to an or this work was sponsored in part by national science foundation career award iis 9876037 and research infrastructure program eia 0080123 the authors assume all responsibility for the contents of the paper permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage the vldb copyright notice and the title of the publication and its minimizing view sets without losing query answering power the problem of answering queries using views has been studied extensively due to its relevance in a wide variety of data management applications in these applications we often need to select a subset of views to maintain due to limited resources in this paper we show that traditional query containment is not a good basis for deciding whether or not a view should be selected instead we should minimize the view set without losing query answering power to formalize this notion we rst introduce the concept of p containment that is a view set v is p contained in another view set w if w can answer all the queries that can be answered by v we show that p containment and the traditional query containment are not related i e one does not imply the other we then discuss how to minimize a view set while retaining its query answering power we develop the idea further by considering p containment of two view sets with respect to a given set of queries and consider their relationship in terms of maximally contained rewritings of queries using the views web document prediction and presending using association rule sequential classifiers an important data source for data mining is the web log data that traces the user s web browsing actions from the web logs one can build prediction models that predict with high accuracy the user s next requests based on past behavior to do this with the traditional classification and association rule methods will cause a number of serious problems due to the extremely large data size and the rich domain knowledge that must be applied most web log data are sequential in nature and exhibit the most recent most important behavior to overcome this difficulty we examine two important dimensions of building prediction models namely the type of antecedents of rules and the criterion for selecting prediction rules this thesis proposes a better overall method for prediction model representation and refinement we show empirically on realistic web log data that the proposed model dramatically outperforms previous ones how to apply the learned prediction model to the task of presending web documents is also demonstrated iv dedication to my mother only v acknowledgments i would like to thank my senior supervisor dr qiang yang for his guidance encouragement and invaluable advice during my research his enthusiasm and devotion to research have given me great inspiration to accomplish this thesis it would be hard to imagine finishing this thesis without his continuous support my gratitude also goes to my supervisor dr ke wang and the external examiner dr jiawei han who share with me their boundless knowledge in the area of data mining and provide me valuable feedback on my thesis i also gratefully acknowledge the unselfish help given to me by my old friend henry zhang i have always learnt a lot from his amazing wisdom and creative thought over the crit adome an advanced object modelling environment adome advanced object modeling environment an approach to integrating data and knowledge management based on object oriented technology is presented next generation information systems will require more flexible data modelling capabilities than those provided by current object oriented dbmss in particular integration of data and knowledge management capabilities will become increasingly important in this context adome provides versatile role facilities that serve as dynamic binders between data objects and production rules thereby facilitating flexible data and knowledge management integration a prototype that implements this mechanism and the associated operators has been constructed on top of a commercial object oriented dbms and a rule base system index terms object modeling knowledge semantics dynamic roles object oriented databases nextgeneration information systems 1 introduction increasingly organizations require more intelligent information management in o automatic identification of text in digital video key frames scene and graphic text can provide important supplemental index information in video sequences in this paper we address the problem automatically identifying text regions in digital video key frames the text contained in video frames is typically very noisy because it is aliased and or digitized at a much lower resolution than typical document images making identification extraction and recognition difficult the proposed method is based on the use of a hybrid wavelet neural network segmenter on a series of overlapping small windows to classify regions which contain text to detect text over a wide range of font sizes the method is applied to a pyramid of images and the regions identified at each level are integrated 1 introduction the increasing availability of online digital imagery and video has rekindled interest in the problems of how to index multimedia information sources automatically and how to browse and manipulate them efficiently traditionally images and video seq multifaceted object modeling with roles a comprehensive approach in conventional object oriented o o database systems it is assumed silently that fundamental object types and inter object relationships can be classfied statically prescribing basic structural and behavioral properties for all the objects in the database such a classification based approach falls short of supporting emerging data intensive applications requiring more advanced dynamic capabilities one of such advanced capabilities is the support of modeling subjectivity the ability to among others provide multiple perspectives of application objects or to model socalled multifaceted objects which on different stages can exhibit different forms and behavior this article describes an extended o o approach that we have been investigating for this purpose advanced features embodied by dynamic and versatile role facilities have been introduced into a conventional object oriented model which facilitates specifying and modeling such applications in a natural incremental and summary this paper the main questions addressed in this setting deal with conditions under which it is possible to evaluate queries incrementally event composition in time dependent distributed systems many interesting application systems ranging from workflow management and cscw to air traffic control are eventdriven and time dependent and must interact with heterogeneous components in the real world event services are used to glue together distributed components they assume a virtual global time base to trigger actions and to order events the notion of a global time that is provided by synchronized local clocks in distributed systems has a fundamental impact on the semantics of event driven systems especially the composition of events the well studied 2g precedence model which assumes that the granularity of global time base g can be derived from a priori known and bounded precision of local clocks may not be suitable for the internet where the accuracy and external synchronization of local clocks is best effort and cannot be guaranteed because of large transmission delay variations and phases of disconnection in this paper we introduce a mechanism based on a digital photography framework enabling affective awareness in home communication by transforming the personal computer into a communication appliance the internet has initiated the true home computing revolution as a result computer mediated communication cmc technologies are increasingly used in domestic settings and are changing the way people keep in touch with their relatives and friends this article first looks at how cmc tools are currently used in the home and points at some of their benefits and limitations most of these tools support explicit interpersonal communication by providing a new medium for sustaining conversations the need for tools supporting implicit interaction between users in more natural and effortless ways is then argued for the idea of affective awareness is introduced as a general sense of being in touch with one s family and friends finally the kan g framework which enables affective awareness through the exchange of digital photographs is described various components which make the capture distribution observation a non obtrusive user interface for increasing social awareness on the world wide web arguing for the need of increasing social awareness on the world wide web we describe a user interface based on the metaphor of windows bridging electronic and physical spaces we present a system that with the aim of making on line activity perceptible in the physical world makes it possible to hear people visiting one s web site the system takes advantage of the seamless and continuous network connection offered by handheld web appliances such as pda s automated facial expression recognition based on facs action units automated recognition of facial expression is an important addition to computer vision research because of its relevance to the study of psychological phenomena and the development of human computer interaction hci we developed a computer vision system that automatically recognizes individual action units or action unit combinations in the upper face using hidden markov models hmms our approach to facial expression recognition is based on the facial action coding system facs which separates expressions into upper and lower face action in this paper we use three approaches to extract facial expression information 1 facial feature point tracking 2 dense flow tracking with principal component analysis pca and 3 high gradient component detection i e furrow detection the recognition results of the upper face expressions using feature point tracking dense flow tracking and high gradient component detection are 85 93 and 85 respectively 1 introduction fa localizing and segmenting text in images and videos many images especially those used for page design on web pages as well as videos contain visible text if these text occurrences could be detected segmented and recognized automatically they would be a valuable source of high level semantics for indexing and retrieval in this paper we propose a novel method for localizing and segmenting text in complex images and videos text lines are identified by using a complex valued multilayer feed forward network trained to detect text at a fixed scale and position the network s output at all scales and positions is integrated into a single text saliency map serving as a starting point for candidate text lines in the case of video these candidate text lines are refined by exploiting the temporal redundancy of text in video localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background for videos temporal redundancy is exploited to improve segmentation performance input images and videos can be of any size due to a true multiresolution approach moreover the system is not only able to locate and segment text occurrences into large binary images but is also able to track each text line with sub pixel accuracy over the entire occurrence in a video so that one text bitmap is created for all instances of that text line therefore our text segmentation results can also be used for object based video encoding such as that enabled by mpeg 4 training reinforcement neurocontrollers using the polytope algorithm a new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized experimental results from the application of the method to the pole balancing problem indicate improved training performance compared with critic based and genetic reinforcement approaches keywords reinforcement learning neurocontrol optimization polytope algorithm pole balancing genetic reinforcement training reinforcement neurocontrollers using the polytope algorithm abstract a new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized exper a comparison of prediction accuracy complexity and training time of thirty three old and new classification algorithms twenty two decision tree nine statistical and two neural network algorithms are compared on thirty two datasets in terms of classification accuracy training time and in the case of trees number of leaves classification accuracy is measured by mean error rate and mean rank of error rate both criteria place a statistical spline based algorithm called polyclass at the top although it is not statistically significantly different from twenty other algorithms another statistical algorithm logistic regression is second and third with respect to the two accuracy criteria the most accurate decision tree algorithm is quest with linear splits which ranks fourth and fifth respectively although spline based statistical algorithms tend to have good accuracy they also require relatively long training times polyclass for example is third last in terms of median training time it often requires hours of training compared to seconds for other algorithms the quest and logistic re extracting structures of html documents using a high level stack machine information on the web which are conglomeration of heterogeneous data such as texts images and audio clips are often accessed through documents written according to the html specification 7 according to the html specification html documents are semistructured in nature we propose a high level stack machine hsm which accesses an html document through its url and constructs a semistructured data graph sdg of the document the sdg of an html document h precisely captures the structure of the semistructured data embedded in h based on the dependency relationship 11 among the data objects in h hsm is configurable to accommodate a user s interest with respect to the html elements in h to be considered during the construction process of the sdg of h 1 introduction during the early days of the world wide web www or web users heavily relied on the mouse button click navigation method through hyperlinks provided by web browsers to retrieve information of interest and soon efficiently computing weighted proximity relationships in spatial databases spatial data mining recently emerges from a number of real applications such as real estate marketing urban planning weather forecasting medical image analysis road traffic accident analysis etc it demands for efficient solutions for many new expensive and complicated problems in this paper we investigate the problem of evaluating the top k distinguished features for a cluster based on weighted proximity relationships between the cluster and features we measure proximity in an average fashion to address possible nonuniform data distribution in a cluster combining a standard multi step paradigm with new lower and upper proximity bounds we presented an efficient algorithm to solve the problem the algorithm is implemented in several different modes our experiment results not only give a comparison among them but also illustrate the efficiency of the algorithm indexing and retrieving natural language using ternary expressions traditional information retrieval systems based on the bag of words paradigm cannot completely capture the semantic content of documents yet it is impossible with current technology to build a practical information access system that fully analyzes and understands unrestricted natural language however if we avoid the most complex and processing intensive natural language understanding techniques we can construct a large scale information access system which is capable of processing unrestricted text largely understanding it and answering natural language queries with high precision we believe that ternary expressions are the most suitable representational structure for such a system they are expressive enough for information retrieval purposes yet amenable to rapid large scale indexing acird intelligent internet documents organization and retrieval this paper presents an intelligent internet information system automatic classifier for the internet resource discovery acird which uses machine learning techniques to organize and retrieve internet documents acird consists of a knowledge acquisition process document classifier and two phase search engine the knowledge acquisition process of acird automatically learns classification knowledge from classified internet documents the document classifier applies learned classification knowledge to classify newly collected internet documents into one or more classes experimental results indicate that acird performs as well or better than human experts in both knowledge acquisition and document classification by using the learned classification knowledge and the given class lattice the acird two phase search engine responds to user queries with hierarchically structured navigable results instead of a conventional flat ranked document list which greatly aids users in locating information from numerous diversified internet documents discovering informative content blocks from web documents in this paper we propose a new approach to discover informative contents from a set of tabular documents or web pages of a web site our system infodiscoverer first partitions a page into several content blocks according to html tag table in a web page based on the occurrence of the features terms in the set of pages it calculates entropy value of each feature according to the entropy value of each feature in a content block the entropy value of the block is defined by analyzing the information measure we propose a method to dynamically select the entropy threshold that partitions blocks into either informative or redundant informative content blocks are distinguished parts of the page whereas redundant content blocks are common parts based on the answer set generated from 13 manually tagged news web sites with a total of 26 518 web pages experiments show that both recall and precision rates are greater than 0 956 that is using the approach informative blocks news articles of these sites can be automatically separated from semantically redundant contents such as advertisements banners navigation panels news categories etc by adopting infodiscoverer as the preprocessor of information retrieval and extraction applications the retrieval and extracting precision will be increased and the indexing size and extracting complexity will also be reduced issues in agent oriented software engineering in this paper i will discuss the conceptual foundation of agent oriented software development by relating the fundamental elements of the agent oriented view to those of other well established programming paradigms especially the object oriented approach furthermore i will motivate the concept of autonomy as the basic property of the agent oriented school and discuss the development history of programming paradigms that lead to this perspective on software systems the paper will be concluded by an outlook on how the new paradigm can change the way we think about software systems automatic facial expression interpretation where human computer interaction artificial intelligence and cognitive science intersect this paper is to attempt to bring together people results and questions from these three different disciplines hci ai and cognitive science to explore the potential of building computer interfaces which understand and respond to the richness of the information conveyed in the human face until recently information has been conveyed from the computer to the user mainly via the visual channel whereas inputs from the user to the computer have been made from the keyboard and pointing devices via the user s motor channel the recent emergence of multimodal interfaces as our everyday tools might restore a better balance between our physiology and sensory motor skills and impact for the better we hope the richness of activities we will find ourselves involved in given recent progress in user interface primitives composed of gesture speech context and affect it seems feasible to design environments which do not impose themselves as computer environments but have a much more natural feeling associated with them providing persistent objects in distributed systems abstract thor is a persistent object store that provides a powerful programming model thor ensures that persistent objects are accessed only by calling their methods and it supports atomic transactions the result is a system that allows applications to share objects safely across both space and time the paper describes how the thor implementation is able to support this powerful model and yet achieve good performance even in a wide area large scale distributed environment it describes the techniques used in thor to meet the challenge of providing good performance in spite of the need to manage very large numbers of very small objects in addition the paper puts the performance of thor in perspective by showing that it substantially outperforms a system based on memory mapped files even though that system provides much less functionality than thor 1 empirically evaluating an adaptable spoken dialogue system recent technological advances have made it possible to build real time interactive spoken dialogue systems for a wide variety of applications however when users do not respect the limitations of such systems performance typically degrades although users differ with respect to their knowledge of system limitations and although different dialogue strategies make system limitations more apparent to users most current systems do not try to improve performance by adapting dialogue behavior to individual users this paper presents an empirical evaluation of toot an adaptable spoken dialogue system for retrieving train schedules on the web we conduct an experiment in which 20 users carry out 4 tasks with both adaptable and non adaptable versions of toot resulting in a corpus of 80 dialogues the values for a wide range of evaluation measures are then extracted from this corpus our results show that adaptable toot generally outperforms non adaptable toot and that the utility of adaptation depends on toot s initial dialogue strategies design and implementation of the rol system rol is a deductive object oriented database system developed at the university of regina it eectively integrates important features of deductive databases and object oriented databases in a uniform framework and provides a uniform rule based declarative language for dening manipulating and querying a database this paper describes the latest implementation of rol 1 introduction in the past decade a lot of interests arose in integrating deductive and object oriented databases to gain the best of the two approaches such as recursion declarative querying and rm logical foundations from deductive approaches and object identity complex objects classes class hierarchy property inheritance with overriding and schema from object oriented approach a number of deductive object oriented database languages have been proposed such as o logic 17 revised o logic 11 c logic 8 iql 2 iql2 1 flogic 10 logres 7 llo 16 lol 6 coral 19 datalog method 3 dlt an extended genetic rule induction algorithm this paper describes an extension of a gabased separate and conquer propositional rule induction algorithm called sia 24 while the original algorithm is computationally attractive and is also able to handle both nominal and continuous attributes efficiently our algorithm further improves it by taking into account of the recent advances in the rule induction and evolutionary computation communities the refined system has been compared to other ga based and non ga based rule learning algorithms on a number of benchmark datasets from the uci machine learning repository results show that the proposed system can achieve higher performance while still produces a smaller number of rules 1 introduction the increasingly widespread use of information system technologies and the internet has resulted in an explosive growth of many business government and scientific databases as these terabyte size databases become prevalent the traditional approach of using human experts to sift thro implementation and performance analysis of incremental equations for nested relations view materialization is an important way of improving the performance of query processing when an update occurs to the source data from which a materialized view is derived the materialized view has to be updated so that it is consistent with the source data this update process is called view maintenance the incremental method of view maintenance which computes the new view using the old view and the update to the source data is widely preferred to full view recomputation when the update is small in size the small update size becomes an important concept for measuring the cheap performance of the incremental methods in this paper we investigate what is the limit of the small update size which we call size limit for the incremental maintenance when the size of an update exceeds the limit the incremental maintenance is no longer cheaper than the view recomputation the investigation is based on incremental equations for operators in the nested relational model learning the face space representation and recognition this paper advances an integrated learning and evolutionary computation methodology for approaching the task of learning the face space the methodology is geared to provide a framework whereby enhanced and robust face coding and classification schemes can be derived and evaluated using both machine and human benchmark studies in particular we take an interdisciplinary approach drawing from the accumulated and vast knowledge of both the computer vision and psychology communities and describe how evolutionary computation and statistical learning can engage in mutually beneficial relationships in order to define an exemplar absolute based coding abc multidimensional face space representation for successfully coping with changing population face types and to leverage past experience for incremental face space definition 1 introduction among the most challenging tasks for visual form shape analysis and object recognition are understanding how people process and recognize three dimensional pc toward novel forms of human computer interaction the ongoing integration of it systems is offering computer users a wide range of new networked services and the access to an explosively growing host of multimedia information consequently apart from the currently established computer tasks the access and exchange of information is generally considered as the driving computer application of the next decade major advances are required in the development of the human computer interface in order to enable end users with the most varied backgrounds to master the added functionalities of their computers with ease and to make the use of computers in general an enjoyable experience research efforts in computer science are concentrated on user interfaces that support the highly evolved human perception and interaction capabilities better than today s 2d graphic user interfaces with a mouse and keyboard thanks to the boosting computing power of general purpose computers highly interactive interfaces are becoming feasible supporting a drawcad using deductive object relational databases in cad computer aided design cad involves the use of computers in the various stages of engineering design it has large volumes of data with complex structures that needs to be stored and managed efficiently and properly in cad graphical objects with complex structures can be created by reusing previously created objects the data of these objects have the references to the other objects they contain deductive object relational databases can be used to compute the complete data of graphical objects that reuse other objects this is the idea behind the development of the drawcad system drawcad is a cad system built on top of the relationlog object relational deductive database system it facilitates the creation of graphical objects by reusing previously created objects the drawcad system illustrates how cad systems can be developed using deductive object relational databases to store and manage data and also perform the computations that are normally performed by the application program a rule based query language for html with the recent popularity of the web enormous amount of information is now available on line most web documents available over the web are in html format and are hierarchically structured in nature how to query such web documents based on their internal hierarchical structure becomes more and more important in this paper we present a rule based language called webql to support effective and flexible web queries unlike other web query languages webql is a high level declarative query language with a logical semantics it allows us to query web documents based on their internal hierarchical structures it supports not only negation and recursion but also query result restructuring in a natural way we also describe the implementation of the system that supports the webql query language face recognition using evolutionary pursuit this paper describes a novel and adaptive dictionary method for face recognition using genetic algorithms gas in determining the optimal basis for encoding human faces in analogy to pursuit methods our novel method is called evolutionary pursuit ep and it allows for different types of non orthogonal bases ep processes face images in a lower dimensional whitened pca subspace directed but random rotations of the basis vectors in this subspace are searched by gas where evolution is driven by a fitness function defined in terms of performance accuracy and class separation scatter index accuracy indicates the extent to which learning has been successful so far while the scatter index gives an indication of the expected fitness on future trials as a result our approach improves the face recognition performance compared to pca and shows better generalization abilities than the fisher linear discriminant fld based methods 1 introduction a successful face recognition met logical semantics and language for databases with partial and complete tuples and sets extended abstract mengchi liu department of computer science university of regina regina saskatchewan canada s4s 0a2 email mliu cs uregina ca abstract we discuss the semantics of complex object databases with both partial and complete tuples and sets we redefine the notion of database to reflect the existence of partial and complete tuples and sets and study how to integrate partial information about tuples and sets spread in the database and check consistency in the meantime we also present a deductive language rlog ii for complex objects with null unknown and inconsistent values based on relationlog the main novel feature of the language is that it is the only one that supports the null extended nested relational algebra operations directly and more importantly recursively this work provides a firm logical foundation for nested relational and complex object databases that have both partial and complete tuples and sets and solves an open problem of supporting recursion with generic null relationlog a typed extension to datalog with sets and tuples this paper presents a novel logic programming based language for nested relational and complex value models called relationlog it stands in the same relationship to the nested relational and complex value models as datalog stands to the relational model the main novelty of the language is the introduction of powerful mechanisms namely partial and complete set terms for representing and manipulating both partial and complete information on nested sets tuples and relations they generalize the set grouping and set enumeration mechanisms of ldl and allow the user to directly encode the open and closed world assumptions on nested sets tuples and relations they allow direct inference and access to deeply embedded values in a complex value relation as if the relation is normalized which greatly increases the ease of use of the language as a result the extended relational algebra operations can be represented in relationlog directly and more importantly recursively in a way similar to datalog like datalog relationlog has a well defined herbrand model theoretic semantics which captures the intended semantics of nested sets tuples and relations and also a well defined proof theoretic semantics which coincides with its model theoretic semantics a statistical method for estimating the usefulness of text databases searching desired data in the internet is one of the most common ways the internet is used no single search engine is capable of searching all data in the internet the approach that provides an interface for invoking multiple search engines for each user query has the potential to satisfy more users when the number of search engines under the interface is large invoking all search engines for each query is often not cost effective because it creates unnecessary network traffic by sending the query to a large number of useless search engines and searching these useless search engines wastes local resources the problem can be overcome if the usefulness of every search engine with respect to each query can be predicted in this paper we present a statistical method to estimate the usefulness of a search engine for any given query for a given query the usefulness of a search engine in this paper is defined to be a combination of the number of documents in the search engine that are sufficiently similar to the query and the average similarity of these documents experimental results indicate that our estimation method is much more accurate than existing methods deductive database languages problems and solutions this paper we discuss these problems from four different aspects complex values object orientation higher orderness and updates in each case we examine four typical languages that address the corresponding issues face recognition using shape and texture we introduce in this paper a new face coding and recognition method which employs the enhanced fld fisher linear discrimimant model efm on integrated shape vector and texture shape free image information shape encodes the feature geometry of a face while texture provides a normalized shape free image by warping the original face image to the mean shape i e the average of aligned shapes the dimensionalities of the shape and the texture spaces are first reduced using principal component analysis pca the corresponding but reduced shape and texture features are then integrated through a normalization procedure to form augmented features the dimensionality reduction procedure constrained by efm for enhanced generalization maintains a proper balance between the spectral energy needs of pca for adequate representation and the fld discrimination requirements that the eigenvalues of the within class scatter matrix should not include small trailing values after the dimensionality reduction procedure as they appear in the denominator olog a deductive object database language extended abstract mengchi liu department of computer science university of regina regina saskatchewan canada s4s 0a2 mliu cs uregina ca http www cs uregina ca mliu abstract deductive object oriented databases are intended to combine the best of the deductive and object oriented approaches however some important object oriented features are not properly supported in the existing proposals this paper proposes a novel deductive language that supports important structurally object oriented features such as object identity complex objects typing classes class hierarchies multiple property inheritance with overriding conict handling and blocking and schema denitions in a uniform framework the language eectively integrates useful features in deductive and object oriented database languages the main novel feature is the logical semantics that cleanly accounts for those structurally object oriented features that are missing in object oriented database languages therefor overview of the rol2 deductive object oriented database system this paper presents an overview of rol2 a novel deductive object oriented database system developed at the university of regina rol2 supports in a rule based framework nearly all important object oriented features such as object identity complex objects typing information hiding rule based methods encapsulation of such methods overloading late binding polymorphism class hierarchies multiple structural and behavioral inheritance with overriding blocking and conflict handling it is so far the only deductive system that supports all these features in a pure rule based framework partial and complete tuples and sets in deductive databases in a nested relational or complex object database nested tuples and sets are used to represent real world objects for various reasons such tuples and sets can be partial or complete in this paper we discuss how to support them in deductive databases in particular we present a deductive database language rlog ii that supports partial and complete tuples and sets based on relationlog this work provides a firm logical foundation for nested relational and complex object databases that have both partial and complete tuples and sets 1 introduction in a nested relational database or complex object database nested tuples and sets are used to represent real world objects a relation is just a set of tuples and a database is a tuple of relations for various reasons the information about real world objects in a database may be incomplete hence both tuples and sets can be partial or complete in the past several years a sub problem that is a database which contains partial an rol2 towards a real deductive object oriented database language rol is a strongly typed deductive object oriented database language it integrates many important features of deductive databases and object oriented databases however it is only a structurally object oriented language in this paper we present our extension of rol called rol2 most importantly rol2 supports behaviorally objectoriented features such as rule based methods and encapsulation so that it is a now real deductive object oriented database language it supports in a rule based framework nearly all important object oriented features such as object identity complex objects typing information hiding rule based methods encapsulation of such methods overloading late binding polymorphism class hierarchies multiple structural and behavioral inheritance with overriding blocking and conict handling it is so far the only deductive system that supports all these features in a pure rule based framework keywords object oriented databases deductive databases closing the loop heuristics for autonomous discovery autonomous discovery systems will be able to peruse very large databases more thoroughly than people can in a companion paper 1 we describe a general framework for autonomous systems we present and evaluate heuristics for use in this framework although these heuristics were designed for a prototype system we believe they provide good initial solutions to problems encountered when implementing fully autonomous discovery systems as such these heuristics may be used as the starting point for future research into fully autonomous discovery systems 1 an analysis of webwho how does awareness of presence affect written messages we present preliminary results from a study of how awareness of presence affects instant messaging in a computer lab the easily accessible web based awareness tool webwho visualizes a large university computer lab allowing students to virtually locate one another and communicate via an instant messaging system messages can be sent anonymously by a conscious act of ticking a box cross analyses of sender location collocated distributed and distant sender status anonymous vs identified and message content were made results show that awareness of both physical presence and virtual presence affect the messages and that these factors affect the text differently the students use the messaging system to support collaborative work and coordinate social activities as well as allow for playful behavior keywords instant messaging computer mediated communication awareness of presence web visualization social coordination introduction webwho 15 is a lightweight web bas the wearboy a platform for low cost public wearable devices we introduce the wearboy a wearable modified nintendo gameboy as a platform for exploring public wearable devices we have minimized a color gameboy to enable users to comfortably wear it making the device not much larger than the actual screen technical properties of the wearboy are discussed along with two applications using the platform 1 introduction currently many wearable computing prototypes are rather clumsy and heavy to wear and often rely on several different electronic devices connected together by cables hidden in the user s clothing this might be necessary for computationally demanding applications but in many cases the application does not need much computational power especially not if wireless access to more powerful resources is available several such low end wearable platforms have been built and tested e g the thinking tags 1 these prototypes are usually custom designed around a small microcontroller with some additional features but commonl webwho support for student awareness and coordination this paper webwho is a lightweight value adding service that relies on readily available server status information which is refined and visualized in a way that is easily accessible to individuals from any workstation with a web browser the deep web surfacing hidden value this document is preliminary brightplanet plans future revisions as better information and documentation is obtained we welcome submission of improved information and statistics from others involved with the deep web mata hari is a registered trademark and brightplanet tm completeplanet tm lexibot tm search filter tm and a better way to search tm are pending trademarks of brightplanet com llc all other trademarks are the respective property of their registered owners 2000 brightplanet com llc all rights reserved the deep web surfacing hidden value iii filtering algorithms and implementation for very fast publish subscribe systems publish subscribe is the paradigm in which users express long term interests subscriptions and some agent publishes events e g oers the job of publish subscribe software is to send events to the owners of subscriptions satis ed by those events for example a user subscription may consist of an interest in an airplane of a certain type not to exceed a certain price a published event may consist of an oer of an airplane with certain properties including price each subscription consists of a conjunction of attribute comparison operator value predicates a subscription closely resembles a trigger in that it is a longlived conditional query associated with an action usually informing the subscriber however it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable high performance publish subscribe systems this paper describes an attempt at the construction of such algorithms and its implementation using a combination of data structures application speci c caching policies and application speci c query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions extended compact genetic algorithm in c this report tells you how to download compile and run the extended compact genetic algorithm ecga described in harik s paper harik 1999 it also explains how to modify the objective function that comes with the distribution of the code the source is written in c but a knowledge of the c programming language is sufficient to modify the objective function so that you can try the ecga on your own problems 2 how to download the code simulated annealing algorithms for continuous global optimization introduction in this paper we consider simulated annealing algorithms sa in what follows applied to continuous global optimization problems i e problems with the following form f min x2x f x 1 1 where x n is a continuous domain often assumed to be compact which combined with the continuity or lower semicontinuity of f guarantees the existence of the minimum value f sa algorithms are based on an analogy with a physical phenomenon while at high temperatures the molecules in a liquid move freely if the temperature is slowly decreased the thermal mobility of the molecules is lost and they form a pure crystal which also corresponds to a state of minimum energy if the temperature is decreased too quickly the so called quenching a liquid metal rather ends up in a polycrystalline or amorphous state with the distributed simulation of multi agent systems agent based systems are increasingly being applied in a wide range of areas including telecommunications business process modelling computer games control of mobile robots and military simulations such systems are typically extremely complex and it is often useful to be able to simulate an agent based system to learn more about its behaviour or investigate the implications of alternative architectures in this paper we discuss the application of distributed discrete event simulation techniques to the simulation of multi agent systems we identify the efficient distribution of the agents environment as a key problem in the simulation of agent based systems and present an approach to the decomposition of the environment which facilitates load balancing consumer eye movement patterns on yellow pages advertising process tracing data help understand how yellow pages advertisement characteristics influence consumer information processing behavior a laboratory experiment collected eye movement data while consumers chose businesses from phone directories consumers scan listings in alphabetic order their scan is not exhaustive as a result some ads are never seen consumers noticed over 93 of the quarter page display ads but only 26 of the plain listings consumers perceived color ads before ads without color noticed more color ads than non color ads and viewed color ads 21 longer than equivalent ads without color users viewed 42 more bold listings than plain listings consumers spent 54 more time viewing ads they end up choosing which demonstrates the importance of attention on subsequent choice behavior 1 introduction in 1992 yellow pages directories were a 9 4 billion dollar information services business that reached 98 of american households mangel 1992 it is the fourth larg a classification scheme for negotiation in electronic commerce in the last few years we have witnessed a surge of business to consumer and business to business commerce operated on the internet however many of these systems are often nothing more than electronic catalogues on which the user can choose a product which is made available for a fixed price this modus operandi is clearly failing to exploit the full potential of electronic commerce against this background we argue here that in the next few years we will see a new generation of systems emerge based on automatic negotiation in this paper we identify the main parameters on which any automatic negotiation depends this classification schema is then used to categorise the subsequent papers in this book that focus on automatic negotiation qlb a quantified logic for belief this paper describes qlb a quantified logic of belief that is a possible extension of the modal system kd45n to predicate level the main features of qlb are that i it is allowed to quantify over the agents of belief ii the belief operator can be indexed by any term of the formal language iii terms are not rigid designators but are interpreted contextually iv automatic theorem proving is possible in qlb but it is not presented in this paper qlb is constructed as a partial logic with a monotonic semantics on ordered sets and its semantic theorems are defined as the formulae that are sometimes true and never false 1 introduction agents are complex objects they can be modelled in terms of mental states like knowledge beliefs intentions goals plans etc and they perform actions sometimes cooperatively in a society of other agents in this book the reader can find contributions from different schools of thought related to agents agent theories for specificati a framework for designing and implementing the user interface of a geographic digital library geographic data are useful for a large set of applications such as urban planning and environmental control these data are however very expensive to acquire and maintain moreover their use is often restricted due to a lack of dissemination mechanisms digital libraries are a good approach for increasing data availability and therefore reducing costs since they provide e cient storage and access to large volumes of data one major drawback to this approach is that it creates the necessity of providing facilities for a large and heterogeneous community of users to search and interact with these geographic libraries we present a solution to this problem based on a framework that allows the design and construction of customizable user interfaces for applications based on geographic digital libraries gdl this framework relies on two main concepts a geographic user interface architecture and a geographic digital library model key words digital libraries user interfaces a framework for norm based inter agent dependence a significant class of agent architectures designed for operation in a multi agent world choose their next actions or plans based on a limited analysis feature selection using expected attainable discrimination we propose expected attainable discrimination ead as a measure to select discrete valued features for reliable discrimination between two classes of data ead is an average of the area under the roc curves obtained when a simple histogram probability density model is trained and tested on many random partitions of a data set ead can be incorporated into various stepwise search methods to determine promising subsets of features particularly when misclassification costs are difficult or impossible to specify experimental application to the problem of risk prediction in pregnancy is described keywords receiver operating characteristic roc area under the roc curve feature selection risk prediction in pregnancy failure to progress 1 introduction feature selection is a key step towards solving any practical classification problem kittler 1975 even though a bayes classifier can never be improved by omission of features van campenhout 1982 there are compelling practical simulated 3d painting this technical report looks at the motivation for simulating painting directly on 3d objects and investigates the main issues faced by such systems these issues include the provision of natural user interfaces the reproduction of realistic brush effects and the surface parameterization for texture mapping so that the results of the painting can be stored on texture maps the technical report further investigates the issues involved in using a haptic interface for simulating 3d painting and the issues in surface parameterization for texture mapping with application to 3d painting a survey of some work related to 3d painting haptic rendering and surface parameterization for texture mapping is presented 1 computing and comparing semantics of programs in four valued logics the different semantics that can be assigned to a logic program correspond to different assumptions made concerning the atoms whose logical values cannot be inferred from the rules thus the well founded semantics corresponds to the assumption that every such atom is false while the kripke kleene semantics corresponds to the assumption that every such atom is unknown in this paper we propose to unify and extend this assumption based approach by introducing parameterized semantics for logic programs the parameter holds the value that one assumes for all atoms whose logical values cannot be inferred from the rules we work within belnap s four valued logic and we consider the class of logic programs defined by fitting following fitting s approach we define a simple operator that allows us to compute the parameterized semantics and to compare and combine semantics obtained for different values of the parameter the semantics proposed by fitting corresponds to the value false we also show that our approach captures and extends the usual semantics of conventional logic programs thereby unifying their computation a conceptual framework for agent definition and development the use of agents of many different kinds in a variety of fields of computer science and artificial intelligence is increasing rapidly and is due in part to their wide applicability the richness of the agent metaphor that leads to many different uses of the term is however both a strength and a weakness its strength lies in the fact that it can be applied in very many different ways in many situations for different purposes the weakness is that the term agent is now used so frequently that there is no commonly accepted notion of what it is that constitutes an agent this paper addresses this issue by applying formal methods to provide a defining framework for agent systems the z specification language is used to provide an accessible and unified formal account of agent systems allowing us to escape from the terminological chaos that surrounds agents in particular the framework precisely and unambiguously provides meanings for common concepts and terms enables alternative models of particular classes of system to be described within it and provides a foundation for subsequent development of increasingly more refined concepts information seeking as socially situated activity this paper we discuss implications of situatedness in its social and cultural meaning in the context of information seeking support we proceed as follows first we discuss some of the varying meanings of the term situated then we outline how we interpret accounting for situatedness in the context of information seeking support finally we discuss tools that implement aspects of what we consider important in this context supporting situated actions in high volume conversational data situations the global conferencing system usenet news offers an amount of articles per day that exceeds human cognitive capabilities by far although the articles are already organized in hierarchically structured discussion groups covering distinct topics we report here on a situated information filtering system that significantly reduces the burden by supporting the user in acting situated interpreting the user s actions as situated actions the approach complements current filtering and recommender approaches by completely avoiding the modeling of user interests the user is the only instance for assigning un interestingness to usenet discussions keywords situated cognition situated actions usenet news information filtering introduction the huge and increasing amount of information available in the information age suggests to investigate new ways to support humans in gathering information that might be interesting helpful or necessary for them since the overall amount of informat probabilistic logic programming with conditional constraints we introduce a new approach to probabilistic logic programming in which probabilities are defined over a set of possible worlds more precisely classical program clauses are extended by a subinterval of 0 1 that describes a range for the conditional probability of the head of a clause given its body we then analyze the complexity of selected probabilistic logic programming tasks it turns out that probabilistic logic programming is computationally more complex than classical logic programming more precisely the tractability of special cases of classical logic programming generally does not carry over to the corresponding special cases of probabilistic logic programming moreover we also draw a precise picture of the complexity of deciding and computing tight logical consequences in probabilistic reasoning with conditional constraints in general we then present linear optimization techniques for deciding satisfiability and computing tight logical consequences of probabilistic a hybrid model for sharing information between fuzzy uncertain and default reasoning models in multi agent systems this paper develops a hybrid model which provides a unified framework for the fol lowing four kinds of reasoning 1 zadeh s fuzzy approximate reasoning 2 truthqualification uncertain reasoning with respect to fuzzy propositions 3 fuzzy default reasoning proposed in this paper as an extension of reiter s default reasoning and 4 truth qualification uncertain default reasoning associated with fuzzy statements developed in this paper to enrich fuzzy default reasoning with uncertain information our hybrid model has the following characteristics 1 basic uncertainty is estimated in terms of words or phrases in natural language and basic propositions are fuzzy 2 uncertainty linguistically expressed can be handled in default reasoning and 3 the four kinds of rea soning models mentioned above and their combination models will be the special cases of our hybrid model moreover our model allows the reasoning to be performed in the case in which the information is fuzzy uncertain and partial more importantly the problems of sharing the information among heterogeneous fuzzy uncertain and default reasoning models can be solved efficiently by using our model given this our framework can be used as a basis for information sharing and exchange in knowledge based multi agent systems for practical applications such as automated group negotiations actually to build such a foundation is the motivation of this paper on the complexity of terminological reasoning tboxes are an important component of knowledge representation systems based on description logics dls since they allow for a natural representation of terminological knowledge largely due to a classical result given by nebel 20 complexity analyses for dls have until now mostly focused on reasoning without acyclic tboxes in this paper we concentrate on dls for which reasoning without tboxes is pspace complete and show that there exist logics for which the complexity of reasoning remains in pspace if acyclic tboxes are added and also logics for which the complexity increases an example for a logic of the former type is alc while examples for logics of the latter kind include alc d and alcf this demonstrates that it is necessary to take tboxes into account for complexity analyses furthermore we show that reasoning with the description logic alcrp d is nexptime complete regardless of whether tboxes are admitted or not 1 introduction a core feature of description an xml based runtime user interface description language for mobile computing devices in a time where mobile computing devices and embedded systems gain importance too much time is spent to reinventing user interfaces for each new device to enhance future extensibility and reusability of systems and their user interfaces we propose a runtime user interface description language which can cope with constraints found in embedded systems and mobile computing devices xml seems to be a suitable tool to do this when combined with java following the evolution of java towards xml it is logical to introduce the concept applied to mobile computing devices and embedded systems 1 guided by voices an audio augmented reality system this paper presents an application of a low cost lightweight audio only augmented reality infrastructure the system uses a simple wearable computer and a rf based location system to play digital sounds corresponding to the user s location and current state using this infrastructure we implemented a game in the fantasy genre where players move around in the real world and trigger actions in the virtual game world we present some of the issues involved in creating audio only augmented reality games and show how our location infrastructure is generalizable to other audio augmented realities keywords audio augmented reality wearable computing context awareness introduction this paper presents a lightweight and inexpensive infrastructure for augmented realities that uses a simple wearable computer whereas most traditional augmented reality systems overlay graphics onto the user s environment this system employs only audio furthermore we have created a positioning infrastruct future multimedia user interfaces this article we examine some of the work that has been done in these two fields and explore where they are heading first we review their often confusing terminology and provide a brief historical overview since both fields rely largely on relatively unusual and largely immature hardware technologies we next provide a high level introduction to important hardware issues this is followed by a description of the key approaches to system architecture used by current researchers we then build on the background provided by these sections to lay out a set of current research issues and directions for future work throughout we attempt to emphasize the many ways in which virtual environments and ubiquitous computing can complement each other creating an exciting new form of multimedia computing that is far more powerful than either approach would make possible alone augmented reality linking real and virtual worlds a new paradigm for interacting with computers a revolution in computer interface design is changing the way we think about computers rather than typing on a keyboard and watching a television monitor augmented reality lets people use familiar everyday objects in ordinary ways the difference is that these objects also provide a link into a computer network doctors can examine patients while viewing superimposed medical images children can program their own lego constructions construction engineers can use ordinary paper engineering drawings to communicate with distant colleagues rather than immersing people in an artificiallycreated virtual world the goal is to augment objects in the physical world by enhancing them with a wealth of digital information and communication capabilities keywords augmented reality interactive paper design space exploration participatory design introduction computers are everywhere in the past several decades they have transformed our work and our lives but the conversion from traditi intelligent information triage in many applications large volumes of time sensitive textual information require triage rapid approximate prioritization for subsequent action in this paper we explore the use of prospective indications of the importance of a time sensitive document for the purpose of producing better document filtering or ranking by prospective we mean importance that could be assessed by actions that occur in the future for example a news story may be assessed retrospectively as being important based on events that occurred after the story appeared such as a stock price plummeting or the issuance of many follow up stories if a system could anticipate prospectively such occurrences it could provide a timely indication of importance clearly perfect prescience is impossible however sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently we describe a process for creating and evaluating approximate information triage procedures that are based on prospective indications unlike many informationretrieval applications for which document labeling is a laborious manual process for many prospective criteria it is possible to build very large labeled training corpora automatically such corpora can be used to train text classification procedures that will predict the prospective importance of each document this paper illustrates the process with two case studies demonstrating the ability to predict whether a news story will be followed by many very similar news stories and also whether the stock price of one or more companies associated with a news story will move significantly following the appearance of that story we conclude by discussing how the comprehensibility of the learned classifiers can be critical to success 1 fjording the stream an architecture for queries over streaming sensor data if industry visionaries are correct our lives will soon be full of sensors connected together in loose conglomerations via wireless networks each monitoring and collecting data about the environment at large these sensors behave very differently from traditional database sources they have intermittent connectivity are limited by severe power constraints and ty pically sample periodically and push immediately keeping no record of historical information these limitations make traditional database sy tems inappropriate for queries over sensors we present the fjords architecture for managing multiple queries over many sensors and show how it can be used to limit sensor resource demands while maintaining high query throughput we evaluate our architecture using traces from a network of traffic sensors deploym on interstate 80 near berkeley and present performance results that show how query throughput communication costs and power consumption are necessarily coupled in sensor environments generic schema matching with cupid schema matching is a critical step in many applications such as xml message mapping data warehouse loading and schema integration in this paper we investigate algorithms for generic schema matching outside of any particular data model or application we first present a taxonomy for past solutions showing that a rich range of techniques is available we then propose a new algorithm cupid that discovers mappings between schema elements based on their names data types constraints and schema structure using a broader set of techniques than past approaches some of our innovations are the integrated use of linguistic and structural matching context dependent matching of shared types and a bias toward leaf structure where much of the schema content resides after describing our algorithm we present experimental results that compare cupid to two other schema matching systems bootstrapping an ontology based information extraction system automatic intelligent web exploration will benefit from shallow information extraction techniques if the latter can be brought to work within many different domains the major bottleneck for this however lies in the so far difficult and expensive modeling of lexical knowledge extraction rules and an ontology that together define the information extraction system in this paper we present a bootstrapping approach that allows for the fast creation of an ontology based information extracting system relying on several basic components viz a core information extraction system an ontology engineering environment and an inference engine we make extensive use of machine learning techniques to support the semi automatic incremental bootstrapping of the domain specific target information extraction system keywords ontologies information extraction machine learning 1 hybrid methods using evolutionary algorithms for on line training a novel hybrid evolutionary approach is presented in this paper for improving the performance of neural network classifiers in slowly varying environments for this purpose we investigate a coupling of differential evolution strategy and stochastic gradient descent using both the global search capabilities of evolutionary strategies and the effectiveness of on line gradient descent the use of differential evolution strategy is related to the concept of evolution of a number of individuals from generation to generation and that of on line gradient descent to the concept of adaptation to the environment by learning the hybrid algorithm is tested in two real life image processing applications experimental results suggest that the hybrid strategy is capable to train on line effectively leading to networks with increased generalization capability an investigation of machine learning based prediction systems traditionally researchers have used either o the shelf models such as cocomo or developed local models using statistical techniques such as stepwise regression to obtain software e ort estimates more recently attention has turned to a variety of machine learning methods such as arti cial neural networks anns case based reasoning cbr and rule induction ri this paper outlines some comparative research into the use of these three machine learning methods to build software e ort prediction systems we brie y describe each method and then apply the techniques to a dataset of 81 software projects derived from a canadian software house in the late 1980s we compare the prediction systems in terms of three factors accuracy explanatory value and con gurability we show that ann methods have superior accuracy and that ri methods are least accurate however this view is somewhat counteracted by problems with explanatory value and con gurability for example we found that considerable e ort was required to con gure the ann and that this compared very unfavourably with the other techniques particularly cbr and least squares regression lsr we suggest that further work be carried out both to further explore interaction between the enduser hierarchical multi agent reinforcement learning in this paper we investigate the use of hierarchical reinforcement learning to speed up the acquisition of cooperative multi agent tasks we extend the maxq framework to the multi agent case each agent uses the same maxq hierarchy to decompose a task into sub tasks learning is decentralized with each agent learning three interrelated skills how to perform subtasks which order to do them in and how to coordinate with other agents coordination skills among agents are learned by using joint actions at the highest level s of the hierarchy the q nodes at the highest level s of the hierarchy are configured to represent the joint task action space among multiple agents in this approach each agent only knows what other agents are doing at the level of sub tasks and is unaware of lower level primitive actions this hierarchical approach allows agents to learn coordination faster by sharing information at the level of sub tasks rather than attempting to learn coordination taking into account primitive joint state action values we apply this hierarchical multi agent reinforcement learning algorithm to a complex agv scheduling task and compare its performance and speed with other learning approaches including at multi agent single agent using maxq selfish multiple agents using maxq where each agent acts independently without communicating with the other agents as well as several well known agv heuristics like first come first serve highest queue first and nearest station first we also compare the tradeoffs in learning speed vs performance of modeling joint action values at multiple levels in the maxq hierarchy implementing teams of agents playing simulated robot soccer this article is intended to present an overview of the issues related to implementing teams of cooperating agents playing simulated robot soccer first we introduce the concept of robot soccer and the simulated environment then we discuss why the simulated robot soccer is an interesting application area from the point of view of robotics and articial intelligence the main part of the paper contains a discussion of agent architectures both from theoretical and practical point of view later we discuss how to combine individual agents into teams having common strategies and goals we also discuss learning both on individual and team levels 1 introduction robot soccer is a growing area of interest for the robotics and articial intelligence communities there are many reasons for that the main one is the complexity of the domain together with a set of well dened rules governing the behaviour of the agents in this domain the domain is suited for experiments both in positioning a coarse calibrated camera with respect to an unknown object by 2d 1 2 visual servoing in this paper we propose a new vision based robot control approach halfway between the classical positionbased and image based visual servoings it allows to avoid their respective disadvantages the homography between some planar feature points extracted from two images corresponding to the current and desired camera poses is computed at each iteration then an approximate partial pose where the translational term is known only up to a scale factor is deduced from which can be designed a closed loop control law controlling the six camera d o f contrarily to the position based visual servoing our scheme does not need any geometric 3d model of the object furthermore and contrarily to the image based visual servoing our approach ensures the convergence of the control law in all the task space a machine learning researcher s foray into recidivism prediction we discuss an application of machine learning to recidivism prediction our initial results motivate the need for a methodology for technique selection for applications that involve unequal but unknown error costs a skewed data set or both evaluation methodologies traditionally used in machine learning are inadequate for analyzing performance in these situations although they arise frequently when addressing real world problems after discussing the problem of recidivism prediction and the particulars of our data set we present experimental results that motivate the need to evaluate learning algorithm over a range of error costs we then describe receiver operating characteristic roc analysis which has been used extensively in signal detection theory for decades but has only recently begun to filter into machine learning research with this new perspective we revisit the recidivism prediction task and present results that contradict those obtained using a traditional integration of spatial join algorithms for joining multiple inputs several techniques that compute the join between two spatial datasets have been proposed during the last decade among these methods some consider existing indices for the joined inputs while others treat datasets with no index thus providing solutions for the case where at least one input comes as an intermediate result of another database operator in this paper we analyze previous work on spatial joins and propose a novel algorithm called slot index spatial join sisj that efficiently computes the spatial join between two inputs only one of which is indexed by an r tree going one step further we show how sisj and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial inputs we study the differences between relational and spatial multi way joins and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries contact author dimitris papadias tel 852 23586971 http www topographic maps based on kohonen self organizing maps an empirical approach two dimensional maps are a valuable interface element for the visualization of information retrieval results or other large sets of objects various methods exist for the creation of these maps this article describes a comparative evaluation of topographic maps based on kohonen self organizing maps som these results show that the mapping method has to be chosen very carefully and different methods should be tested for an application keywords self organizing maps kohonen maps topic maps 2d maps evaluation information retrieval information visualization analysis of approximate nearest neighbor searching with clustered point sets this paper we study the performance of two other splitting methods and compare them against the kd tree splitting method the first called slidingmidpoint is a splitting method that was introduced by mount and arya in the ann library for approximate nearest neighbor searching 30 this method was introduced into the library in order to better handle highly clustered data sets we know of no analysis empirical or theoretical of this method this method was designed as a simple technique for addressing one of the most serious flaws in the standard kd tree splitting method the flaw is that when the data points are highly clustered in low dimensional subspaces then the standard kd tree splitting method may produce highly elongated cells and these can lead to slow query times this splitting method starts with a simple midpoint split of the longest side of the cell but if this split results in either subcell containing no data points it translates or slides the splitting plane in the direction of the points until hitting the first data point in section 3 1 we describe this splitting method and analyze some of its properties the second splitting method called minimum ambiguity is a query based technique the tree is given not only the data points but also a collection of sample query points called the training points the algorithm applies a greedy heuristic to build the tree in an attempt to minimize the expected query time on the training points we model query processing as the problem of eliminating data points from consideration as the possible candidates for the nearest neighbor given a collection of query points we can model any stage of the nearest neighbor algorithm as a bipartite graph called the candidate graph whose vertices correspond t providing integrated toolkit level support for ambiguity in recognition based interfaces recognition technologies are being used extensively in both the commercial and research worlds but recognizers are still error prone and this results in performance problems and brittle dialogues these problems are a barrier to acceptance and usefulness of recognition systems better interfaces to recognition systems which can help to reduce the burden of recognition errors are difficult to build because of lack of knowledge about the ambiguity inherent in recognition we have extended a user interface toolkit in order to model and to provide structured support for ambiguity at the input event level 7 this makes it possible to build re usable interface components for resolving ambiguity and dealing with recognition errors these interfaces can help to reduce the negative effects of recognition errors by providing these components at a toolkit level we make it easier for application writers to provide good support for error handling and we can explore new types of interfaces for resolving a more varied range of ambiguity bringing people and places together with dual augmentation this paper describes initial work on the domisilica project at georgia tech we are exploring the dual augmentation of physical and virtual worlds in domisilica and applying this novel concept to support home life beyond the boundaries of the actual house we will demonstrate applications of dual augmentation in supporting distributed communities through a home the regency and a specific appliance cyberfridge both of which serve as a communications link between physical and virtual worlds three specific types of communication are supported direct supporting real time user to user interaction indirect interaction mediated by devices such as games a refrigerator etc and peripheral supporting awareness of subtle information keywords augmented reality augmented virtuality home ubiquitous computing 1 introduction physical barriers such as time and distance have traditionally stood in the way of successfully building communities which are not located in a single geogr modeling score distributions for combining the outputs of search engines in this paper the score distributions of a number of text search engines are modeled it is shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non relevant documents and a normal distribution for the set of relevant documents experiments show that this model fits trec 3 and trec 4 data for not only probabilistic search engines like inquery but also vector space search engines like smart for english we have also used this model to fit the output of other search engines like lsi search engines and search engines indexing other languages like chinese it is then shown that given a query for which relevance information is not available a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution these distributions can be used to map the scores of a search engine to probabilities we also discuss how the shape of the score distributions arise given certain assumptions about word distributions in documents we hypothesize that all good text search engines operating on any language have similar characteristics this model has many possible applications for example the outputs of different search engines can be combined by averaging the probabilities optimal if the search engines are independent or by using the probabilities to select the best engine for each query results show that the technique performs as well as the best current combination techniques this material is based on work supported in part by the national science foundation library of congress and department of commerce under cooperative agreement number eec 9209623 in part by the national science foundation under grant numbers iri 9619117 and iis 9909073 in part by n partial replication in the vesta software repository the vesta repository is a special purpose replicated file system developed as part of the vesta software configuration management system one of the major goals of vesta is to make all software builds reproducible to this end the repository provides an append only name space new names can be inserted but once a name exists its meaning cannot change more concretely all files and some designated directories are immutable while the remaining directories are appendable allowing new names to be defined but not allowing existing names to be redefined the data stored validating access to external information sources in a mediator environment a mediator integrates existing information sources into a new application in order to answer complex queries the mediator splits them up into sub queries which it sends to the information sources afterwards it combines the replies to answer the original query since the information sources are usually external autonomous systems the access to them can sometimes be erroneous most notably when the information source is changed this results in an incorrect behaviour of the whole system the question that this paper addresses is how to check whether or not the access was correct the paper introduces a notational framework for the general information access validation problem describes the typical errors that can occur in a mediator environment and proposes several validation mechanisms it is also investigated how the validation functionality can be integrated into the mediator architecture and what the most important quality measures of a validation method are moreover the practical usability of the presented approaches is demonstrated on a real world application using web based information sources several measurements are performed to compare the presented methods with previous work in the field testing access to external information sources in a mediator environment this paper discusses the testing of communication in the increasingly important class of distributed information systems that are based on a mediator architecture a mediator integrates existing information sources into a new application in order to answer complex queries the mediator splits them up into subqueries which it sends to the information sources and it combines the replies to answer the original query since the information sources are usually remote autonomous systems the access to them can be erroneous most notably when the information source is subject to modifications such errors may result in incorrect behaviour of the whole system this paper addresses the problem of deciding whether an information source as part of a mediatory system was successfully queried or not the primary contribution is a formal framework for the general information access testing problem besides proposing several solutions it is investigated what the most important quality measures of such testing methods are moreover the practical usability of the presented approaches is demonstrated on a real world application using web based information sources several empirical experiments are conductedto compare the presented methods with previous work in the field prediction with local patterns using cross entropy sets of local patterns in the forms of rules and co occurrence counts are produced by many data mining methods such as association rule algorithms while such patterns can yield useful insights it is not obvious how to synthesize local sparse information into a coherent global predictive model we study the use of a cross entropy approach to combining local patterns each local pattern is viewed as a constraint on an appropriate high order joint distribution of interest typically a set of patterns returned by a data mining algorithm under constrains the high order model the cross entropy criterion is used to select a specific distribution in this constrained family relative to a prior we review the iterative scaling algorithm which is an iterative technique for finding a joint distribution given constraints we then illustrate the application of this method to two specific problems the first problem is combining information about frequent itemsets we show that the cross entropy a efficient data and program integration using binding patterns in this work we investigate data and program integration in a fully distributed peer to peer mediation architecture the challenge in making such a system succeed at a large scale is twofold first sharing a resource should be easy therefore we need a simple concept for modeling resources second we need an ecient architecture for distributed query execution capable of handling well costly computations and large data transfers to model heterogeneous resources we propose using the unied abstraction of table with binding patterns simple yet powerful enough to capture data and programs to exploit a resource with restricted binding patterns we propose an ecient bindjoin operator following the classical iterator model in which we build optimization techniques for minimizing large data transfers and costly computations and maximizing parallelism furthermore our bindjoin operator can be tuned to deliver most of its output in the early stages of the execution which is an important asset in a system meant for human interaction our preliminary experimental evaluation validates the proposed bindjoin algorithms and shows they can provide good performance in queries involving distributed data and expensive programs key words data and program integration distributed query processing binding patterns inria caravel project contact ioana manolescu inria fr y prism laboratory university of versailles and inria caravel project contact luc bouganim prism uvsq fr z inria caravel project contact francoise fabret inria fr x inria caravel project contact eric simon inria fr intgration ecace de donnes et de programmes utilisants des patterns d accs rsum dans ce rapport nous tudions l intgration de donnes et de programmes dans une archite micro workflow a workflow architecture supporting compositional object oriented software development this dissertation proposes micro workflow a new workflow architecture that bridges the gap between the type of functionality provided by current workflow systems and the type of workflow functionality required in object oriented applications micro workflow provides a better solution when the focus is on customizing the workflow features and integrating with other systems in this thesis i discuss how micro workflow leverages object technology to provide workflow functionality as an example i present the design of an object oriented framework which provides a reusable micro workflow architecture and enables developers to customize it through framework specific reuse techniques i show how through composition developers extend micro workflow to support history persistence monitoring manual intervention worklists and federated workflow i evaluate this approach with three case studies that implement processes with different requirements adaptive fp an efficient and effective method for multi level multi dimensional frequent pattern mining real life transaction databases usually contain both item information and dimension information moreover taxonomies about items likely exist knowledge about multilevel and multi dimensional frequent patterns is interesting and useful the classic frequent pattern mining algorithms based on a uniform minimum support such as apriori and fp growth either miss interesting patterns of low support or suffer from the bottleneck of itemset generation other frequent pattern mining algorithms such as adaptive apriori though taking various supports focus mining at a single abstraction level furthermore as an apriori based algorithm the efficiency of adaptive apriori suffers from the multiple database scans in this thesis we extend fp growth to attack the problem of multi level multidimensional frequent pattern mining we call our algorithm ada fp which stands for adaptive fp growth the efficiency of our ada fp is guaranteed by the high scalability of fp growth to increase the effectiveness our ada fp pushes various support constraints into the mining process first item taxonomy has been explored our ada fp can discover both inter level frequent patterns and intra level frequent patterns second in our ada fp dimension information has been taken into account we show that our ada fp is more flexible at capturing desired knowledge than previous studies interfaces and tools for the library of congress national digital library program this paper describes a collaborative effort to explore user needs in a digital library develop interface prototypes for a digital library and suggest and prototype tools for digital librarians and users at the library of congress lc interfaces were guided by an assessment of user needs and aimed to maximize interaction with primary resources and support both browsing and analytical search strategies tools to aid users and librarians in overviewing collections previewing objects and gathering results were created and serve as the beginnings of a digital librarian toolkit the design process and results are described and suggestions for future work are offered digital libraries dl offer new challenges to an emerging breed of digital librarians who must combine the principles and practices of information management with rapidly evolving technological developments to create new information products and services this paper describes a collaborative effort to explore user needs a simple heuristic based genetic algorithm for the maximum clique problem this paper proposes a novel heuristic based genetic algorithm hga for the maximum clique problem which consists of the combination of a simple genetic algorithm and a naive heuristic algorithm the heuristic based genetic algorithm is tested on the so called dimacs benchmark graphs with up to 4000 nodes and up to 5506380 edges consisting of randomly generated graphs with known maximum clique and of graphs derived from various practical applications the performance of hga on these graphs is very satisfactory both in terms of solution quality and running time despite its simplicity hga dramatically improves on all previous approaches based on genetic algorithms we are aware of and yields results comparable to those of more involved heuristic algorithms based on local search this provides empirical evidence of the effectiveness of heuristic based genetic algorithms as a search technique for solving the maximum clique problem which is competitive with respect to other variants a hybrid approach to profile creation and intrusion detection anomaly detection involves characterizing the behaviors of individuals or systems and recognizing behavior that is outside the norm this paper describes some preliminary results concerning the robustness and generalization capabilities of machine learning methods in creating user profiles based on the selection and subsequent classification of command line arguments we base our method on the belief that legitimate users can be classified into categories based on the percentage of commands they use in a specified period the hybrid approach we employ begins with the application of expert rules to reduce the dimensionality of the data followed by an initial clustering of the data and subsequent refinement of the cluster locations using a competitive network called learning vector quantization since learning vector quantization is a nearest neighbor classifier and new record presented to the network that lies outside a specified distance is classified as a masquerader thus this system does not require anomalous records to be included in the training set 1 hemasl a flexible language to specify heterogeneous agents in the realization of agent based applications the developer generally needs to use heterogeneous agent architectures so that each application component can optimally perform its task languages that easily model the heterogeneity of agents architectures are very useful in the early stages of the application development this paper presents hemasl a simple meta language used to specify heterogeneous agent architectures and sketches how hemasl should be implemented in an object oriented commercial programming language as java moreover the paper briefly discusses the benefits of adding hemasl to caselp a lp based specification and prototyping environment for multi agent systems in order to enhance its flexibility and usability 1 introduction intelligent agents and multi agent systems mass are more and more recognized as the new modeling techniques to be used to engineer complex and distributed software applications 12 agent based software engineering is concerned with specification of heterogeneous agent architectures agent based software applications need to incorporate agents having heterogeneous architectures in order for each agent to optimally perform its task hemasl is a simple meta language used to specify intelligent agents and multi agent systems when different and heterogeneous agent architectures must be used hemasl specifications are based on an agent model that abstracts several existing agent architectures the paper describes some of the features of the language presents examples of its use and outlines its operational semantics we argue that adding hemasl to caselp a specification and prototyping environment for mas can enhance its flexibility and usability 1 introduction intelligent agents and multi agent systems mas are increasingly being acknowledged as the new modelling techniques to be used to engineer complex and distributed software applications 17 9 agent based software development is concerned with the realization of software applications modelled multiagent systems engineering a methodology for analysis and design of multiagent systems ix i introduction 1 1 1 background 2 1 2 problem 3 1 3 goal 4 1 4 assumptions 4 1 5 areas of collaboration effective resource discovery on the world wide web this paper we present usewebnet a resource discovery tool built on top of traditional search engines usewebnet registers each user s interests and repeatedly queries several search engines for urls matching a user s registered interests usewebnet keeps track of which urls have been visited by each user thus when a user invokes usewebnet s he is presented only with new or unvisited urls we view usewebnet as a value added service on top of existing search engines and information providers which helps users effectively find what s new in the rapidly evolving web of our world introduction on caching search engine results in this paper we explore the problem of caching of search engine query results in order to reduce the computing and i o requirements needed to support the functionality of a search engine of the world wide web based on traces from search engines we show that there is significant locality in the queries asked that is 20 30 of the queries have been previously submitted by the same or a different user using trace driven simulation we show that medium sized caches can hold most of the frequently submitted queries finally we propose and evaluate a new cache replacement algorithm named lru 2s that takes into account both the frequency and the recency of access to a page when making a replacement decision location aware information delivery with commotion this paper appears in the huc 2000 proceedings pp 157 171 springer verlag commotion a context aware communication system how many times have you gone to the grocery store but left the grocery list on the refrigerator door wouldn t it be more efficient to have a reminder to buy groceries and the shopping list delivered to you when you were in the vicinity of the store we live in a world in which the information overload is part of our daily life many of us receive large quantities of email or voice mail messages yet many of these messages are relevant only in a particular context we can use a system of reminders to keep up with all we have to do but these reminders are often relevant only to a specific location if reminders to do lists messages and other information were delivered in the most timely and relevant context part of the overload would be reduced this paper describes commotion a context aware communication system for a mobile or wearable computing platform keywords mobile ubiquitous and wearable computing locationaware context aware applications going beyond mobile agent platforms component based development of mobile agent systems although mobile agents are a promising programming paradigm the actual deployment of this technology in real applications has been far away from what the researchers were expecting one important reason for this is the fact that in the current mobile agent frameworks it is quite difficult to develop applications without having to center them on the mobile agents and on the agent platforms in this paper we present a component based framework that enables ordinary applications to use mobile agents in an easy and flexible way by using this approach applications can be developed using current objectoriented approaches and become able of sending and receiving agents by the simple drag and drop of mobility components the framework was implemented using the javabeans component model and provides integration with activex which allows applications to be written in a wide variety of programming languages by using this framework the development of applications that can make use of mobile agents is greatly simplified which can contribute to a wider spreading of the mobile agent technology integrating mobile agents into off the shelf web servers the m m approach the mobile agent paradigm provides a new approach for developing distributed systems during the last two years we have been working on a project that tries to overcome some of the limitations found in terms of programmability and usability of the mobile agent paradigm in real applications in the m m framework there are no agent platforms instead applications become agent enabled by using simple javabeans components in our approach the agents arrive and departure directly from the applications interacting with them from the inside providing applications with mobile agent technology over the last couple of years we have been working on the development of mobile agents systems and its application to the areas of telecommunications and network management this work path produced positive results a competitive mobile agent platform was built the run time benefits of mobile agents were proved and our industrial partners have developed practical applications that are being integrated into commercial products however a component based approach for integrating mobile agents into the existing web infrastructure mobile agents provide a new abstraction for deploying functionality over the existing internet infrastructure during the last two years we have been working on a project that tries to overcome some of the limitations found in terms of programmability and usability of the mobile agent paradigm in real applications in the m m framework there are no agent platforms instead applications become agent enabled by using simple javabeans components in this paper we present an architecture that allows currently available web servers to become capable of sending and receiving agents in an easy way by using this approach existing web infrastructure can be maintained while gaining a whole new potential by being able to make use of agent technology our approach involves wrapping the components inside a java servlet that can be included in any web server supporting the servlet specification this servlet enables the servers to receive and send agents that can query local information and also enables the agents to behave as servlets themselves machine learning and natural language processing in this report some collaborative work between the fields of machine learning ml and natural language processing nlp is presented the document is structured in two parts the first part includes a superficial but comprehensive survey covering the state of the art of machine learning techniques applied to natural language learning tasks in the second part a particular problem namely word sense disambiguation wsd is studied in more detail in doing so four algorithms for supervised learning which belong to different families are compared in a benchmark corpus for the wsd task both qualitative and quantitative conclusions are drawn this document stands for the complementary documentation for the conference aprendizaje autom atico aplicado al procesamiento del lenguaje natural given by the author within the course curso de industrias de la lengua la ingenier ia lingu istica en la sociedad de la informaci on fundaci on duques de soria soria july 2000 1 con interactive pedagogical drama this paper describes an agent based approach to realizing interactive pedagogical drama characters choose their actions autonomously while director and cinematographer agents manage the action and its presentation in order to maintain story structure achieve pedagogical goals and present the dynamic story to as to achieve the best dramatic effect artistic standards must be maintained while permitting substantial variability in story scenario to achieve these objectives scripted dialog is deconstructed into elements that are portrayed by agents with emotion models learners influence how the drama unfolds by controlling the intentions of one or more characters who then behave in accordance with those intentions interactions between characters create opportunities to move the story in pedagogically useful directions which the automated director exploits this approach is realized in the multimedia title carmen s bright ideas an interactive health intervention designed to impro an instructor s assistant for team training in dynamic multi agent virtual worlds the training of teams in highly dynamic multi agent virtual worlds places a heavy demand on an instructor we address the instructor s problem with the puppetmaster the puppetmaster manages a network of monitors that report on the activities in the simulation in order to provide the instructor with an interpretation and situation speci c analysis of student behavior the approach used to model student teams is to structure the state space into an abstract situation based model of behavior that supports interpretation in the face of missing information about agent s actions and goals 1 introduction teams of people operating in highly dynamic multi agentenvironments must learn to deal with rapid and unpredictable turns of events simulation based training environments inhabited by synthetic agents can be e ective in providing realistic but safe settings in which to develop skills these environments require e g 14 to faithfully capture the unpredictable multi agent maintaining the illusion of interacting within a 3d virtual space it is widely thought to more or less a degree that a sense of presence may be induced in users of new and emerging media technologies such as the internet digital television and cinema supporting interaction teleconferencing and 3d virtual reality systems in this paper it is argued that presence presupposes that participants are absorbed in the illusion of interacting within the visual spaces created by these media that is prior to the possibility of any inducement of presence participants need to be absorbed in the illusion conveyed by the media without this participants attention is broken and the illusion is lost hence the potential to induce presence in participants ceases to encourage participants to lose sight of the means of representation and be drawn into the illusion conveyed by these media this paper proposes the development of design principles to increase participants experience in an attempt to inform design principles this paper focuses on another artificial although highly successful visual medium film by way of example this paper concentrates on one medium virtual reality and proposes design principles that attempt to maintain the illusion of interacting within 3d virtual space this attempts to provide a platform through the resourceful blend of hardware and software virtual reality vr enabling technologies on which to support a well designed virtual environment and hence from which the inducement of presence in participants may develop using cinematography conventions to inform guidelines for the design and evaluation of virtual off screen space many usability problems are associated with navigation and exploration of virtual space in an attempt to find methods that support navigation within virtual space this paper describes an investigation of cinematography conventions in particular this will focus on conventions that suggest to spectators the existence of additional space other than that contained within the confines or borders of the projection screen referred to as off screen space this paper builds upon these conventions and proposes guidelines to inform the design of visual cues to suggest virtual off screen space visual cues will appear natural and transparent they will help to guide participants through the smooth and continuously animated ve and thus maintain the illusion of interacting within a larger 3d virtual space than that contained within the restricted field of view fov of the display screen introduction the 3 rd dimension of a virtual environment ve creates a space within evaluating guidelines for reducing user disorientation when navigating in virtual environments navigation in virtual environments can be difficult one contributing factor is user disorientation guiding user navigation in virtual environments using awareness of virtual off screen space navigation in virtual environments can be difficult one contributing factor is the problem of user disorientation two major causes of this are the lack of navigation cues in the environment and problems with navigating too close to or through virtual world objects previous work has developed guidelines informed by cinematography conventions for the construction of virtual environments to aid user comprehension of virtual space to reduce user disorientation this paper describes the validation of these guidelines via a user study involving a navigation task in a virtual maze results suggest that the use of the guidelines can help reduce the incidences of user disorientation however the guidelines seemed to have little impact on users abilities to construct cognitive maps of the environment co operative evaluation of a desktop virtual reality system a summative usability evaluation of a desktop virtual reality vr system was developed and a preliminary study then conducted the purpose of the study was twofold firstly to test whether the traditional evaluation technique co operative evaluation is effective in the evaluation of desktop vr systems co operative evaluation is a variation on a think aloud verbal protocol whereby in addition to concurrently thinking aloud users are encouraged to ask any questions about an evaluation relating to the computer based system the application or the tasks that they are required to perform during the evaluation as well as this the evaluator may ask questions of the user at any time during the evaluation results from the study indicate that this additional probing technique enables an evaluator to elicit further usability problems that may not have otherwise been exteriorized by the user additionally a method is developed which attempts to turn round the qualitative think aloud type data into quantitative data this provides a way of evaluating empirical think aloud evaluation methods and will be useful for comparing their effectiveness to evaluate 3d virtual reality systems adaptive collaboration for wired and wireless platforms a data centric architecture for collaboration environments uses xml to adapt shared data dynamically between devices with widely disparate capabilities this article begins by introducing a data centric architecture that abstracts collaborative tasks as editing of data repositories followed by descriptions of the role of xml in managing heterogeneity and intelligent software agents in discovering network and computing environment conditions a logic programming framework for component based software prototyping the paper presents caselp a logic based prototyping environment for specifying and verifying complex distributed applications caselp provides a set of languages for modeling intelligent and interacting components agents at different levels of abstraction it also furnishes tools for integrating legacy software into a prototype the possibility of integrating into the same executable prototype agents which are only specified as well as already developed components can prove extremely useful in the engineering process of complex applications in fact the reusability of existing components can be verified before the application has been implemented and the developer can be more confident on the correctness of the new components specification if it has been executed and tested by means of an interaction with the existing components besides the aspects of integration and reuse caselp also faces another fundamental issue of nowadays applications namely distribution the specification and simulation of multi agent systems in caselp nowadays software applications are characterized by a great complexity it arises from the need of reusing existing components and properly integrating them the distribution of the involved entities and their heterogeneity makes it very useful the adoption of the agent oriented technology the paper presents the state of the art of caselp an experimental logic based prototyping environment for multi agent systems caselp provides a prototyping method and a set of tools and languages which support the prototype realization at the system specification level an architectural description language can be adopted to describe the prototype in terms of agents classes instances their provided and required services and their communication links at the agent specification level a rule based not executable language can be used to easily define reactive and proactive agents an executable linear logic language can define more sophisticated agents and the system in which they operate at t knowledge retrieval and the world wide web figure 5 images knowledge indexations and a customized query interface contained within one document the sample query shows how the command spec which looks for specializations of a conceptual graph can be used to retrieve images cgs indexed figure 7 gives the results cat on table in wordnet cat has five meanings feline gossiper x ray beat and vomit and table has five meanings array furniture tableland food and postpone in the webkb ontology the relation type on connects a concept of type spatial entity to another concept of the same type thus webkb can infer that beat and vomit are not the intended meanings for cat and array and postpone are not the intended meanings for table to further identify the intended meanings webkb could prompt the following questions to the user does cat refer to feline gossiper x ray or something else and does table refer to furniture tableland food or something else finally knowledge state embedding knowledge in web documents the paper argues for the use of general and intuitive knowledge representation languages and simpler notational variants e g subsets of natural languages for indexing the content of web documents and representing knowledge within them we believe that these languages have advantages over metadata languages based on the extensible mark up language xml indeed the retrieval of precise information is better supported by languages designed to represent semantic content and support logical inference and the readability of such a language eases its exploitation presentation and direct insertion within a document thus also avoiding information duplication we advocate the use of conceptual graphs and simpler notational variants that enhance knowledge readability to further ease the representation process we propose techniques allowing users to leave some knowledge terms undeclared we also show how lexical structural and knowledge based techniques may be combined to retrieve or recognition of partially occluded and or imprecisely localized faces using a probabilistic approach new face recognition approaches are needed because although much progress has been recently achieved in the field e g within the eigenspace domain still many problems are to be robustly solved two of these problems are occlusions and the imprecise localization of faces which ultimately imply a failure in identification while little has been done to account for the first problem almost nothing has been proposed to account for the second this paper presents a probabilistic approach that attempts to solve both problems while using an eigenspace representation to resolve the localization problem we need to find the subspace within the feature space e g eigenspace that represents this error for each of the training image to resolve the occlusion problem each face is divided into n local regions which are analyzed in isolation in contrast with other previous approaches where a simple voting space is used we present a probabilistic method that analyzes how good a loca face image retrieval using hmms this paper introduces a new face recognition system that can be used to index and thus retrieve images and videos of a database of faces new face recognition approaches are needed because although much progress has been made to identify face taken from different viewpoints we still cannot robustly identify faces under different illumination conditions or when the facial expression changes or when a part of the face is occluded on account of glasses or parts of clothing when face recognition methods have worked in the past it was only when all possible image variations were learned principal components analysis pca and fisher discriminant analysis fda are well known cases of such methods in this paper we present a different approach to the indexing of face images our approach is based on identifying frontal faces and it allows reasonable variability in facial expressions illumination conditions and occlusions caused by eye wear or items of clothing such as scarves w an agent based approach to distributed simulation distributed prototyping and software integration due to nowadays huge availability of data and software a software developer must be able not only to invent good algorithms and implement them efficiently but also to assemble existing components to create timely and economically a new application prototyping is a software engineering paradigm particularly suitable for the compositional approach to software development a working prototype embedding the heterogeneous software which will be used in the final application proves useful for at least two reasons ffl the prototype is definitely closer to the final application ffl the re usability of the legacy software can be evaluated before the final application is built the distribution of the software to be integrated within the prototype must be taken into account as well as the distribution of the prototype execution this would help to gain in efficiency and closeness to the final application the aim of this thesis is to exploit the multi agent system abstracti a fine grained model for code mobility in this paper we take the extreme view that every line of code is potentially mobile i e may be duplicated and or moved from one program context to another on the same host or across the network our motivation is to gain a better understanding of the range of constructs and issues facing the designer of a mobile code system in a setting that is abstract and unconstrained by compilation and performance considerations traditionally associated with programming language design incidental to our study is an evaluation of the expressive power of mobile unity a notation and proof logic for mobile computing 1 introduction the advent of world wide networks the emergence of wireless communication and the growing popularity of the java language are contributing to a growing interest in dynamic and reconfigurable systems code mobility is viewed by many as a key element of a class of novel design strategies which no longer assume that all the resources needed to accomplish a task are towards hybrid interface specification for virtual environments many new multi modal interaction techniques have been proposed for interaction in a virtual world often these techniques are of a hybrid nature combining continuous interaction such as gestures and moving video with discrete interaction such as pushing buttons to select items unfortunately the description of the behavioural aspects of these interaction techniques found in the literature is informal and incomplete this can make it hard to compare and evaluate their usability this paper investigates the use of hynet to give concise and precise specifications of hybrid interaction techniques hynet is an extension of high level petri nets developed for specification and verification of hybrid systems i e mathematical models including both continuous and discrete elements 1 introduction new technologies for virtual environments ves have been eagerly embraced by ve users and developers the process of diffusing this technology into a wider range of products has i sensory motor primitives as a basis for imitation linking perception to action and biology to robotics ing away from the specific coding of the spinal fields the examples from neurobiology provide the framework for a motor control system based on a small number of additive primitives or basis behaviors sufficient for a rich output movement repertoire our previous work matari c 1995 matari c 1997 inspired by the same biological results has successfully applied the idea of basis behaviors to control of mobile robots 6 by fitting it directly into the modular behavior based control paradigm applictions of schema theory arbib 1992 to behavior based mobile robots arkin 1987 have employed a similar notion of composable behaviors stemming from foundations in neuroscience arbib 1981 arbib 1989 the idea of using such primitives for articulator control has been recently studied in robotics williamson 1996 and marjanovi c scassellati williamson 1996 developed a 6 dof degrees of freedom robot arm controller while in the biological and mobile robotics work primitives c making complex articulated agents dance an analysis of control methods drawn from robotics animation and biology we discuss the tradeoffs involved in control of complex articulated agents and present three implemented controllers for a complex task a physically based humanoid torso dancing the macarena the three controllers are drawn from animation biological models and robotics and illustrate the issues of joint space vs cartesian space task specification and implementation we evaluate the controllers along several qualitative and quantitative dimensions considering naturalness of movement and controller flexibility finally we propose a general combination approach to control aimed at utilizing the strengths of each alternative within a general framework for addressing complex motor control of articulated agents key words articulated agent control motor control robotics animation 1 introduction control of humanoid agents dynamically simulated or physical is an extremely difficult problem due to the high dimensionality of the control space i e the many degrees of freed highly available firewall service using virtual redirectors this paper introduces the dependable decentralised system architecture developed by the phds group it describes the virtual redirector utilised by this architecture and how a decentralised network layer firewall application is implemented on it dependability of the design is achieved through the use of fault tolerant protocols the virtual redirector is implemented by a dynamic hashing algorithm which uses load information and the fault working status of nodes in the system the focus of this paper is the implementation of a testbed which is used to compare the performance and availability of the decentralised firewall against a monolithic firewall with equivalent functionality keywords dependable computing internet security firewall fault tolerant protocol category research paper student paper contents 1 introduction 2 2 the phds architecture 3 2 1 autonomous decentralised systems 3 2 2 the phds system for inter fuzzy concepts and formal methods a fuzzy logic toolkit for z it has been recognised that formal methods are useful as a modelling tool in requirements engineering specification languages such as z permit the precise and unambiguous modelling of system properties and behaviour however some system problems particularly those drawn from the is problem domain may be difficult to model in crisp or precise terms it may also be desirable that formal modelling should commence as early as possible even when our understanding of parts of the problem domain is only approximate this paper suggests fuzzy set theory as a possible representation scheme for this imprecision or approximation we provide a summary of a toolkit that defines the operators measures and modifiers necessary for the manipulation of fuzzy sets and relations we also provide some examples of the laws which establishes an isomorphism between the extended notation presented here and conventional z when applied to boolean sets and relations fuzzy concepts and formal methods some illustrative examples it has been recognised that formal methods are useful as a modelling tool in requirements engineering specification languages such as z permit the precise and unambiguous modelling of system properties and behaviour however some system problems particularly those drawn from the is problem domain may be difficult to model in crisp or precise terms it may also be desirable that formal modelling should commence as early as possible even when our understanding of parts of the problem domain is only approximate this paper identifies the problem types of interest and argues that they are characterised by uncertainty and imprecision it suggests fuzzy set theory as a useful formalism for modelling aspects of this imprecision the paper illustrates how a fuzzy logic tooklit for z can be applied to such problem domains several examples are presented illustrating the representation of imprecise concepts as fuzzy sets and relations soft pre conditions and system requirements as a series of linguistically quantified propositions 1 on computational representations of herbrand models finding computationally valuable representations of models of predicate logic formulas is an important issue in the field of automated theorem proving e g for automated model building or semantic resolution in this article we treat the problem of representing single models independently of building them and discuss the power of different mechanisms for this purpose we start with investigating context free languages for representing single herbrand models we show their computational feasibility and prove their expressive power to be exactly the finite models we show an equivalence with ground atoms and ground equations concluding equal expressive power finally we indicate how various other well known techniques could be used for representing essentially infinite models i e models of not finitely controllable formulas thus motivating our interest in relating model properties with syntactical properties of corresponding herbrand models and in investigating connections betwe second order sufficient conditions for optimal control problems with free final time the riccati approach second order sufficient conditions ssc for control problems with control state constraints and free final time are presented instead of deriving such ssc de initio the control problem with free final time is tranformed into an augmented control problem with fixed final time for which well known ssc exist ssc are then expressed as a condition on the positive definiteness of the second variation a convenient numerical tool for verifying this condition is based on the riccati approach where one has to find a bounded solution of an associated riccati equation satisfying specific boundary conditions the augmented riccati equations for the augmented control problem are derived and their modifications on the boundary of the control state constraint are discussed two numerical examples 1 the classical earth mars orbit transfer in minimal time 2 the rayleigh problem in electrical engineering demonstrate that the riccati equation approach provides a viable numerical test of ss how to write f logic programs in florid a tutorial for the database language f logic contents 2 contents 1 introduction 4 2 a first example 4 3 objects and their properties 6 3 1 object names and variable names 6 3 1 1 methods 6 3 1 2 class membership and subclass relationship 8 3 2 expressing information about an object f molecules 8 3 3 behavioral inheritance 9 3 4 signatures 9 4 nesting object properties 11 4 1 f molecules without any properties 12 5 predicate symbols 12 6 built in features 13 6 1 equality 13 6 2 integers comparisons and arithmetics named entity recognition from diverse text types current research in information extraction tends to be focused on application specific systems tailored to a particular domain the muse system is a multi purpose named entity recognition system which aims to reduce the need for costly and time consuming adaptation of systems to new applications with its capability for processing texts from widely di ering domains and genres although the system is still under development preliminary results are encouraging showing little degradation when processing texts of lower quality or of unusual types the system currently averages 93 precision and 95 recall across a variety of text types wearable visual robots this paper presents a wearable active visual sensor which is able to achieve a level of decoupling of camera movement from the wearer s posture and movements this decoupling is the result of a combination of an active sensing approach inertial information and visual sensor feedback the issues of sensor placement robot kinematics and their relation to wearability are discussed the performance of the prototype head is evaluated for some essential visual tasks the paper also discusses potential application scenarios for this kind of wearable robot 1 designing a miniature wearable visual robot in this paper we report on two methods we have developed to aid in the design of a wearable visual robot a body mounted robot for which the main sensor is a camera specifically we have first refined the analysis of sensor placement through the computation of the field of view and body motion using a 3d model of the human form second we have improved the design of the robot s morphology with the help of an optimization algorithm based on the pareto front within constraints set by the overall choice of robot kinematic chain and the need to specify obtainable actuators and sensors the methods could be of use for the design and performance evaluation of rather different kinds of wearable robots and devices achieving consistency in mobile databases through localization in pro motion there is great need and potential for traditional transaction support in a mobile computing environment however owing to the inherent limitations of mobile computing we need to augment the well developed techniques of database management systems with new approaches in this paper we focus on the challenge of assuring data consistency our approach of localization is to reformulate global constraints so as to enhance the autonomy of the mobile hosts we show how this approach unifies techniques of maintaining replicated data with methods of enforcing polynomial inequalities we also discuss how localization can be implemented in pro motion a flexible infrastructure for transaction processing in a mobile environment 1 introduction thanks to the relentless advances in semiconductors the number of users with mobile computers we will refer to these machines as mobile hosts or mhs continues to increase these users have discovered that exciting developments in wireless technology evaluating humanoid synthetic agents in e retail applications this paper presents three experiments designed to empirically evaluate humanoid synthetic agents in electronic retail applications firstly human like agents were evaluated in a single e retail application a home furnishings service the second experiment explored application dependency effects by evaluating the same human like agents in a different e retail application a personalized cd service the third experiment evaluated the effectiveness of a range of humanoid cartoon like agents participants eavesdropped on spoken dialogues between a customer and each of the agents which played the role of conversational sales assistants results showed participants expected a high level of realistic human like verbal and nonverbal communicative behavior from the human like agents overall ratings of the agents showed no significant application dependency two different groups of participants rated the human like agents in similar ways in a different application further results showed participants have a preference for three dimensional 3 d rather than two dimensional 2 d cartoon like agents and have a desire to interact with fully embodied agents i maximum entropy markov models for information extraction and segmentation hidden markov models hmms are a powerful probabilistic tool for modeling sequential data and have been applied with success to many text related tasks such as part of speech tagging text segmentation and information extraction in these cases the observations are usually modeled as multinomial distributions over a discrete vocabulary and the hmm parameters are set to maximize the likelihood of the observations this paper presents a new markovian sequence model closely related to hmms that allows observations to be represented as arbitrary overlapping features such as word capitalization formatting part of speech and defines the conditional probability of state sequences given observation sequences it does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state we present positive experimental results on the segmentation of faq s 1 introdu heterogeneous database integration using agent oriented information systems the department of defense dod has an extensive family of models used to simulate the mission level interaction of weapon systems interoperability and reuse of the underlying data files used to create simulation scenarios pose great challenges in this regard unlike traditional data integration methods common to federated database research the emerging field of agent oriented information systems aois views data as the central focus of an application while also providing an overall architectural framework for application development we develop an aois solution relevant to this problem domain by combining object oriented data modeling omt a persistent programming language using a commercial objectoriented database objectstore and an agentoriented analysis and design methodology mase requirements from a contractor led effort at the air force research laboratory afrl known as certcort are the basis for analysis and design of our system we implement prototypical information layer applications to conceptually demonstrate the reusability and integration of scenarios across simulation models keywords aois agents modeling and simulations heterogeneous database integration 1 automatic discovery of subgoals in reinforcement learning using diverse density this paper presents a method by which a reinforcement learning agent can automatically discover certain types of subgoals online by creating useful new subgoals while learning the agent is able to accelerate learning on the current task and to transfer its expertise to other related tasks through the reuse of its ability to attain subgoals the agent discovers subgoals based on commonalities across multiple paths to a solution we cast the task of finding these commonalities as a multiple instance learning problem and use the concept of diverse density to find solutions we illustrate this approach using several gridworld tasks 1 who do you want to be today web personae for personalised information access personalised context sensitivity is the holy grail of web information retrieval as a first step towards this goal we present the web personae personalised search and browsing system we use well known information retrieval techniques to develop and track user models web personae differ from previous approaches in that we model users with multiple profiles each corresponding to a distinct topic or domain such functionality is essential in heterogeneous environments such as the web we introduce web personae describe an algorithm for learning such models from browsing data and discuss applications and evaluation methods indexing semistructured data this paper describes techniques for building and exploiting indexes on semistructured data data that may not have a fixed schema and that may be irregular or incomplete we first present a general framework for indexing values in the presence of automatic type coercion then based on lore a dbms for semistructured data we introduce four types of indexes and illustrate how they are used during query processing our techniques and indexing structures are fully implemented and integrated into the lore prototype 1 introduction we call data that is irregular or that exhibits type and structural heterogeneity semistructured since it may not conform to a rigid predefined schema such data arises frequently on the web or when integrating information from heterogeneous sources in general semistructured data can be neither stored nor queried in relational or object oriented database management systems easily and efficiently we are developing lore 1 a database management system d optimizing branching path expressions path expressions form the basis of most query languages for semistructured data and xml specifying traversals through graph based data we consider the query optimization problem for path expressions that branch or specify traversals through two or more related subgraphs such expressions are common in nontrivial queries over xml data searching the entire space of query plans for branching path expressions is generally infeasible so we introduce several heuristic algorithms and postoptimizations that generate query plans for branching path expressions all of our algorithms have been implemented in the lore database system for xml and we report experimental results over a variety of database and query shapes we compare optimization and execution times across our suite of algorithms and post optimizations and for small queries we compare against the optimal plan produced by an exhaustive search of the plan space 1 introduction wo r k i n semistructured data abi97 semantic web services hose properties capabilities interfaces and effects are encoded in an unambiguous machine understandable form the realization of the semantic web is underway with the development of new ai inspired content markup languages such as oil 3 daml oil www daml org 2000 10 daml oil and daml l the last two are members of the darpa agent markup language daml family of languages 4 these languages have a well defined semantics and enable the markup and manipulation of complex taxonomic and logical relations between entities on the web a fundamental component of the semantic web will be the markup of web services to make them computer interpretable use apparent and agent ready this article addresses precisely this component we present an approach to web service markup that provides an agent independent declarative api capturing the data and metadata associated with a service together with specifications of its pro the repair of speech act misunderstandings by abductive inference this paper we have concentrated on the repair of mis understanding our colleagues heeman and edmonds have looked at the repair of non understanding the difference between the two situations is that in the former the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem in the latter the agent derives either more than one interpretation with no way to choose between them or no interpretation at all and so the problem is immediately apparent heeman and edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other heeman and hirst 1995 edmonds 1994 hirst et al 1994 clark and his colleagues clark and wilkes gibbs 1986 clark 1993 have shown that in such situations conversants will collaborate on repairing the problem by in effect negotiating a reconstruction or elaboration of the referring expression heeman and edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them thus as in our own model two copies of the system can converse with each other negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of the other repairing them where necessary and presenting the new referring plan to the other for approval achieving robust human computer communication this paper describes a computational approach to robust human computer interaction the approach relies on an explicit declarative representation of the content and structure of the interaction that a computer system builds over the course of the interaction in this paper we will show how this representation allows the system to recognize and repair misunderstandings between the human and the computer we demonstrate the utility of the representations by showing how they facilitate the repair process 1 introduction in dialogs between people or between people and machines understanding is an uncertain process if the goals or beliefs of two discourse 1 participants differ one of them might interpret an event in the dialog in a way that she believes is complete and correct although her interpretation is not the one that the other one had intended when this happens we as analysts or observers would say that a misunderstanding has occurred the participants themselves might multimodal man machine interface for mission planning this paper presents a multimodal interface featuring fusion of multiple modalities for natural human computer interaction the architecture of the interface and the methods applied are described and the results of the real time multimodal fusion are analyzed the research in progress concerning a mission planning scenario is discussed and other possible future directions are also presented keywords multimodal interfaces speech recognition microphonearray force feedback tactile glove gaze tracking military maps introduction current human machine communication systems predominantly use keyboard and mouse inputs that inadequately approximate human abilities for communication more natural communication technologies such as speech sight and touch are capable of freeing computer users from the constraints of keyboard and mouse although they are not sufficiently advanced to be used individually for robust human machine communication they have adequately advanced to serve simul trends in evolutionary robotics a review is given on the use of evolutionary techniques for the automatic design of adaptive robots the focus is on methods which use neural networks and have been tested on actual physical robots the chapter also examines the role of simulation and the use of domain knowledge in the evolutionary process it concludes with some predictions about future directions in robotics appeared in soft computing for intelligent robotic systems edited by l c jain and t fukuda physicaverlag new york ny 215 233 1998 1 introduction to be truly useful robots must be adaptive they should have a collection of basic abilities that can be brought to bear in tackling a variety of tasks in a wide range of environments these fundamental abilities might include navigation to a goal location obstacle avoidance object recognition and object manipulation however to date this desired level of adaptability has not been realized instead robots have primarily been successful when deploye towards proximity group communication group communication will undoubtedly be a useful paradigm for many applications of wireless networking in which reliability and timeliness are requirements moreover location awareness is clearly central to mobile applications such as traffic management and smart spaces in this paper we introduce our definition of proximity groups in which group membership depends on location and then discuss some requirements for a group management service suitable for proximity groups atlas a generic software platform for speech technology based applications atlas is a java software library that provides a framework for building multilingual and multi modal applications especially dialogue systems on top of speech technology components the design is based on a layered system model where atlas sits as a middleware between an application dependent layer and the speech technology components and implements much of application independent functionality in the system atlas is itself layered with interfaces to speech technology components at the bottom and self contained dialogue components at the top the layered design is both efficient and flexible and is suitable for a research environment the framework also provides support for applicationdependent layers through a structure of an application with sessions interacting with users through terminals the terminal concept supports creating audio deviceindependent applications that run transparently in both telephone and desktop environments several speech technology components are available for use with the atlas framework including text to speech speech recognition and speaker verification systems four applications that use atlas have so far been developed within student and research projects at the centre for speech technology ctt including a speech controlled telephone banking system ctt bank and an automated entrance receptionist per 1 evaluating topic driven web crawlers due to limited bandwidth storage and computational resources and to the dynamic nature of the web search engines cannot index every web page and even the covered portion of the web cannot be monitored continuously for changes therefore it is essential to develop effective crawling strategies to prioritize the pages to be indexed the issue is even more important for topic specific search engines where crawlers must make additional decisions based on the relevance of visited pages however it is difficult to evaluate alternative crawling strategies because relevant sets are unknown and the search space is changing we propose three different methods to evaluate crawling strategies we apply the proposed metrics to compare three topic driven crawling algorithms based on similarity ranking link analysis and adaptive agents topic driven crawlers machine learning issues topic driven crawlers are increasingly seen as a way to address the scalability limitations of universal search engines by distributing the crawling process across users queries or even client computers adaptive retrieval agents internalizing local context and scaling up to the web this paper focuses on two machine learning abstractions springing from ecological models i evolutionary adaptation by local selection and ii selective query expansion by internalization of environmental signals we first outline a number of experiments pointing to the feasibility and performance of these methods on a general class of graph environments we then describe how these methods have been applied to the intelligent retrieval of information distributed across networked environments in particular the paper discusses a novel distributed evolutionary algorithm and representation used to construct populations of adaptive web agents these infospiders search on line for information relevant to the user by traversing hyperlinks in an autonomous and intelligent fashion they can adapt to the spatial and temporal regularities of their local context our results suggest that infospiders could complement current search engine technology by starting up where search engines stop data visualization indexing and mining engine a parallel computing architecture for information processing over the internet ion 9 3 2 spatial representation of the system s networks 10 3 3 display and interaction mechanisms 11 3 3 1 overviews orientation and network abstraction 11 3 3 2 other navigation and orientation tools 12 3 3 3 head tracked stereoscopic display 12 4 web access of geographic information system data 13 4 1 functions provided by gis2web 13 4 2 structure 14 5 paracrawler parallel web searching with machine learning 15 5 1 a machine learning approach towards web searching 15 6 dusie interactive content based web structuring 16 6 1 creating t database query formation from natural language using semantic modeling and statistical keyword meaning disambiguation this paper describes a natural language interface to database systems which is based on the query formation capabilities of a high level query formulator the formulator relies on the semantic graph of the database which is a model of the data stored in the database the natural language interface accepts a user input in natural language and extracts the necessary information needed by the formulator this extraction process is performed using keywords obtained from the semantic graph and the database because keywords may have several meanings within a given domain keyword meaning disambiguation is done using a statistical approach which involves comparing vectors of n grams n grams are n contiguous words within a given text of natural language and they are capable of capturing lexical context traditionally natural language interfaces have been heavy with grammars and other knowledge but have been wide ranging in functionality the interface presented in this paper is more porta estimating the usefulness of search engines in this paper we present a statistical method to estimate the usefulness of a search engine for any given query the estimates can be used by a metasearch engine to choose local search engines to invoke for a given query the usefulness of a search engine in this paper is defined to be a combination of the number of documents in the search engine that are sufficiently similar to the query and the average similarity of these documents experimental results indicate that the proposed estimation method is quite accurate 1 introduction many search engines have been created on the internet to help ordinary users find desired data each search engine has a corresponding database that defines the set of documents that can be searched by the search engine usually an index for all documents in the database is created and stored in the search engine to speed up query processing the amount of data in the internet is huge it is believed that by the end of 1997 there were more than 300 mil issues with meta knowledge this article approaches these questions from our di erent viewpoints a the constructors motta and kalfoglou focus on the construction of knowledge base systems kbs using two special kinds of meta knowledge ontologies 14 33 and problem solving methods psms 26 6 psms model the useful inference patterns seen in previous applications such patterns it is argued simplify and clarify future implementations ontologies model common domain terminology this terminology might include the data structures required by a psm using a good ontology it is argued can guide developers in the construction of new systems for more information on ontologies see http www dai ed ac uk daidb people homes yannisk seke99panelhtml html the maintainers altho and menzies focus on the maintenance and modification of kbs using case based reasoning cbr 3 1 and continual testing 10 22 cbr researchers argue that people scalable association based text classification nave bayes nb classifier has long been considered a core methodology in text classification mainly due to its simplicity and computational efficiency there is an increasing need however for methods that can achieve higher classification accuracy while maintaining the ability to process large document collections in this paper we examine text categorization methods from a perspective that considers the tradeoff between accuracy and scalability to large data sets and large feature sizes we start from the observation that support vector machines one of the best text categorization methods cannot scale up to handle the large document collections involved in many real word problems we then consider bayesian extensions to nb that achieve higher accuracy by relaxing its strong independence assumptions our experimental results show that lb an association based lazy classifier can achieve a good tradeoff between high classification accuracy and scalability to large document collections bisimulation congruences in safe ambients we study a variant of levi and sangiorgi s safe ambients sa enriched with passwords sap in sap by managing passwords for example generating new ones and distributing them selectively an ambient may now program who may migrate into its computation space and when moreover in sap an ambient may provide different services depending on the passwords exhibited by its incoming clients we give an lts based operational semantics for sap and a labelled bisimulation based equivalence which is proved to coincide with barbed congruence our notion of bisimulation is used to prove a set of algebraic laws which are subsequently exploited to prove more significant examples 1 the influence of design techniques on user interfaces the digistrips experiment for air traffic control graphical user interfaces have limitations in terms of the information bandwidth they provide between users and systems this can impede the redesign of systems previously based on more physical media information may be less appropriately displayed and shared cognition between users can be reduced however in parallel with research on new user interaction techniques a more systematic use of visual design techniques can relieve those limitations this article explores some of those techniques and how they can be applied through a design experiment virtuosi and digistrips are two user interface prototypes developed within a research program on air traffic control workstations which make use of touch screens and served as a basis for research on the use of graphical design techniques in user interfaces this paper describes the lessons learnt in that experience and argues that techniques such as animation font design careful use of graphical design techniques can augment the p pushing the limits of atc user interface design beyond s m interaction the digistrips experience most designs proposed for air traffic control workstations are based on user interface designs from the early 1980s though research in user interaction has produced innovations since then project toccata federates a series of research held at cena on new user interfaces and services for atc virtuosi and digistrips are two prototypes developed for toccata which make use of touch screens and served as a basis for research on the use of graphical design techniques in user interfaces this paper describes the lessons learnt in that experience and argues that techniques such as animation font design careful use of graphical design techniques can augment the possibilities of user interface design and improve the usability of systems we finally analyze the possible implications on atc workstation design key words touch screen animation graphical design feedback gesture recognition informal assessment mutual awareness electronic flight strips introduction in the last deca conception par maquettage rapide application des crans tactiles pour le contrle arien in this paper we present a joint use of tactile screen and animation we first recall why this two techniques are valuable for air traffic controller computer interaction and then describe the current trends for these techniques we then describe the methodology we used based on paper and video fast prototyping it allowed us to quickly design the first computer based prototypes these prototypes demonstrated that tightly coupling tactile screen and animation make the computer human interaction more natural these results can easily be applied in future air traffic controller computer interfaces studied at the cena keywords paper prototype video prototype touch input screen gestures animation air traffic control computer human interactions mutual awareness introduction le dplacement et l organisation d objets informatiques sur un cran peuvent ils tre aussi naturels efficaces explicites rapides que la manipulation du papier aujourd hui non cette question est import genetic algorithms for binary quadratic programming in this paper genetic algorithms for the unconstrained binary quadratic programming problem bqp are presented it is shown that for small problems a simple genetic algorithm with uniform crossover is sufficient to find optimum or best known solutions in short time while for problems with a high number of variables n 200 it is essential to incorporate local search to arrive at high quality solutions a hybrid genetic algorithm incorporating local search is tested on 40 problem instances of sizes containing between n 200 and n 2500 the results of the computer experiments show that the approach is comparable to alternative heuristics such as tabu search for small instances and superior to tabu search and simulated annealing for large instances new best solutions could be found for 14 large problem instances 1 introduction in the unconstrained binary quadratic programming problem bqp a symmetric rational n theta n matrix q q ij is given and a binary vector of leng dag matching techniques for information retrieval on structured documents with the establishment of international standards for document representation like sgml oda or xml attention in information retrieval has shifted to representation models and query languages that make active use both of the logical structure and the contents of the documents in a document database at the same time representation of structure has become more and more important in other types of databases as well among several related approaches kilpelainen s tree matching is one of the most expressive and intuitive formalisms for querying databases with treestructured entities however in its original formulation it leaves aside most of the problems that arise in real life applications of information retrieval in this paper we extend tree matching to dag matching and suggest various techniques that should be useful when using the formalism in a practical ir system in particular we suggest a representation of answers that can cope with the potentially huge number of entities in improving index structures for structured document retrieval structured document retrieval has established itself as a new research area in the overlap between database systems and information retrieval this work proposes a filtering technique that can be added to already existing index structures of many structured document retrieval systems this new technique takes the contextual structure information of query and document database into account and reduces the occurrence sets returned by the original index structure drastically this improves the performance of query evaluation a measure is introduced that allows to quantify the added value of the proposed index structure based on this measure a heuristic is presented that allows to include only valuable context information in the index structure 1 introduction with the growing importance of information retrieval in the presence of a vast amount of structured documents in formalisms like sgml iso86 or the future www language xml w3c99 sophisticated and efficient indexing techn automatic construction of intelligent diagrammatic environments introduction graphical user interfaces have become an integral part of almost every modern application type and it can be claimed that they are among the driving forces that have made the computer accessible to non expert users however comparing the use of graphics in existent user interfaces with that in non computer based work the inadequacy of standard guis for complex visual communication is revealed most guis are still wimp interfaces centered around such simple interaction devices like icons buttons menus or image maps on the contrary in non computerbased work rich and highly structured graphical notations prevail there are diagrammatic languages in almost every technical discipline for example circuit diagrams architectural floor plans or chemical formulas and modern software engineering is embracing all kinds of diagrammatic specification methods likewise non technical fields use their own well established diagrammatic systems for example choreography no constraint diagram reasoning diagrammatic human computer interfaces are now becoming standard in the near future diagrammatic front ends such as those of uml based case tools will be required to offer a much more intelligent behavior than just editing yet there is very little formal support and there are almost no tools available for the construction of such environments the present paper introduces a constraint based formalism for the specification and implementation of complex diagrammatic environments we start from grammar based definitions of diagrammatic languages and show how a constraint solver for diagram recognition and interpretation can automatically be constructed from such grammars in a second step the capabilities of these solvers are extended by allowing to axiomatise formal diagrammatic systems such as venn diagrams so that they can be regarded as a new constraint domain the ultimate aim of this schema is to establish a language of type clp diagram for diagrammatic reasoni a generalized approach to handling parameter interdependencies in probabilistic modeling and reinforcement learning optimization algorithms this paper generalizes our research on parameter interdependencies in reinforcement learning algorithms for optimization problem solving this generalization expands the work to a larger class of search algorithms that use explicit search statistics to form feasible solutions our results suggest that genetic algorithms can both enrich and benefit from probabilistic modeling reinforcement learning ant colony optimization or other similar algorithms using values to encode preferences for parameter assignments the approach is shown to be effective on both the asymmetric traveling salesman and the quadratic assignment problems introduction there has been a recent upsurge of interest in a family of search algorithms that store past experience not only as the best solutions generated but also abstract representations of the decision processes employed this interest is provoked by a number of factors first since solution memory is always limited search algorithms which store only b a uniform approach to programming the world wide web we propose a uniform model for programming distributed web applications the model is based on the concept of web computation places and provides mechanisms to coordinate distributed computations at these places including peer to peer communication between places and a uniform mechanism to initiate computation in remote places computations can interact with the flow of http requests and responses typically as clients proxies or servers in the web architecture we have implemented the model using the global pointers and remote service requests provided by the nexus communication library we present the model and its rationale with some illustrative examples and we describe the implementation 1 introduction many web applications require a significant amount of computation which may be distributed and requires coordination these applications use the web infrastructure to good advantage but are often constrained by the architecture which is fundamentally client server a variety assessing software libraries by browsing similar classes functions and relationships comparing and contrasting a set of software libraries is useful for reuse related activities such as selecting a library from among several candidates or porting an application from one library to another the current state of the art in assessing libraries relies on qualitative methods to reduce costs and or assess a large collection of libraries automation is necessary although there are tools that help a developer examine an individual library in terms of architecture style etc we know of no tools that help the developer directly compare several libraries with existing tools the user must manually integrate the knowledge learned about each library automation to help developers directly compare and contrast libraries requires matching of similar components such as classes and functions across libraries this is different than the traditional component retrieval problem in which components are returned that best match a user s query rather we need to find those component using motives and artificial emotion for long term activity of an autonomous robot to operate over a long period of time in the real world autonomous mobile robots must have the capabilityofrecharging themselves whenever necessary in addition to be able to nd and dockintoacharging station robots must be able to decide when and for how long to recharge this decision is inuenced by the energetic capacity of their batteries and the contingencies of their environments to deal with this temporality issue and based on researchworks in psychology this paper investigates the use of motives and articial emotions to regulate the recharging need of autonomous robots a bipolar model of articial emotion is presented designed to be generic and not specically congured for a particular task the paper also describes the use of the approach in two specic applications the aaai mobile robot challenge and experiments involving a group of robots share one charging station in an enclosed area 1 learning from history for behavior based mobile robots in non stationary conditions learning in the mobile robot domain is a very challenging task especially in nonstationary conditions the behavior based approach has proven to be useful in making mobile robots work in real world situations since the behaviors are responsible for managing the interactions between the robots and its environment observing their use can be exploited to model these interactions in our approach the robot is initially given a set of behavior producing modules to choose from and the algorithm provides a memory based approach to dynamically adapt the selection of these behaviors according to the history of their use the approach is validated using a vision and sonar based pioneer i robot in non stationary conditions in the context of a multirobot foraging task results show the effectiveness of the approach in taking advantage of any regularities experienced in the world leading to fast and adaptable specialization for the learning robot keywords multi robot learning histor managing robot autonomy and interactivity using motives and visual communication an autonomous mobile robot operating in everyday life conditions will have to face a huge variety of situations and to interact with other agents living or artificial such a robot needs flexible and robust methods for managing its goals and for adapting its control mechanisms to face the contingencies of the world it also needs to communicate with others in order to get useful information about the world this paper describes an approach based on a general architecture and on internal variables called motives to manage the goals of an autonomous robot these variables are also used as a basis for communication using a visual communication system experiments using a vision and sonar based pioneer i robot equipped with a visual signaling device are presented 1 introduction designing an autonomous mobile robot to operate in unmodified environments i e environments that have not been specifically engineered for the robot is a very challenging problems dynamic and unpredic representation of behavioral history forl earningin nonstationary conditions a robot having to operate in nonstationary conditions needs to learn how to modify its control policy to adapt to the changing dynamics of the environment using the behavior based approach to manage the interactions between the robot and its environment we propose a method that models these interactions and adapts the selection of behaviors according to the history of behavior use the learning and the use of this quot interaction model quot are validated using a vision and sonar based pioneer i robot in the context of a multi robot foraging task results show the effectiveness of the approach in taking advantage of any regularities experienced in the world leading to fast and adaptable specialization for the learning robot 1 capturing knowledge of user preferences ontologies in recommender systems tools for filtering the world wide web exist but they are hampered by the difficulty of capturing user preferences in such a dynamic environment we explore the acquisition of user profiles by unobtrusive monitoring of browsing behaviour and application of supervised machine learning techniques coupled with an ontological representation to extract user preferences a multi class approach to paper classification is used allowing the paper topic taxonomy to be utilised during profile construction the quickstep recommender system is presented and two empirical studies evaluate it in a real work setting measuring the effectiveness of using a hierarchical topic ontology compared with an extendable flat list exploiting synergy between ontologies and recommender systems recommender systems learn about user preferences over time automatically finding things of similar interest this reduces the burden of creating explicit queries recommender systems do however suffer from cold start problems where no initial information is available early on upon which to base recommendations semantic knowledge structures such as ontologies can provide valuable domain knowledge and user information however acquiring such knowledge and keeping it up to date is not a trivial task and user interests are particularly difficult to acquire and maintain this paper investigates the synergy between a web based research paper recommender system and an ontology containing information automatically extracted from departmental databases available on the web the ontology is used to address the recommender systems cold start problem the recommender system addresses the ontology s interest acquisition problem an empirical evaluation of this approach is conducted and the performance of the integrated systems measured word sense disambiguation based on semantic density this paper presents a word sense disambiguation method based on the idea of semantic density between words the disambiguation is done in the context of wordnet the internet is used as a raw corpora to provide statistical information for word associations a metric is introduced and used to measure the semantic density and to rank all possible combinations of the senses of two words this method provides a precision of 58 in indicating the correct sense for both words at the same time the precision increases as we consider more choices 70 for top two ranked and 73 for top three ranked 1 introduction word sense disambiguation wsd is an open problem in natural language processing its solution impacts other tasks such as discourse reference resolution coherence inference and others wsd methods can be broadly classified into three types 1 wsd that make use of the information provided by machine readable dictionaries cowie et al 1992 miller et al 1994 agirre and rig word sense disambiguation and its application to internet search ambiguation method presented here is that it provides a ranking of possible associations between words senses rather than a binary yes no decision for a possible sense combination this proves to be particularly useful for natural language processing tasks such as retrieving information related to a particular input question an important task which can highly benet from a word sense disambiguation method is the internet search this thesis presents a possible application of word sense disambiguation techniques for improving the quality of the search on the internet knowing the sense of the words in the input question enables the creation of similarity lists which contain words semantically related to the original keywords and which can be further used for query extension the extended query together with the new lexical operators dened for information extraction improve both the precision and the resolution of a search on the internet iv speci designing agent oriented systems by analysing agent interactions we propose a preliminary methodology for agent oriented software engineering based on the idea of agent interaction analysis this approach uses interactions between undetermined agents as the primary component of analysis and design agents as a basis for software engineering are useful because they provide a powerful and intuitive abstraction which can increase the comprehensiblity of a complex design the paper describes a process by which the designer can derive the interactions that can occur in a system satisfying the given requirements and use them to design the structure of an agent based system including the identification of the agents themselves we suggest that this approach has the flexibility necessary to provide agent oriented designs for open and complex applications and has value for future maintenance and extension of these systems 1 socratenon and its application to the learning of italian language tudents the product was developed through a cooperation between universities in salerno and belgrade this paper presents the basic elements of the socratenon application and implementation philosophy and discusses its possibilities in the general languagelearning environment it describes three different experiments and explains the lessons learned the stress is on the statistical analysis of success of those who used our web based product and those who relied on the classical approaches 1 introduction rapid growth of internet as a medium and internet technologies has led to the point when education can be detached from humans and books as the only possessors of knowledge from the early days internet has been exploited in educational institutions for dissemination of research results and knowledge in general first shapes were unpolished and required a lot of attention from the users such sources of knowledge also included wrestling with several resources of training teams with collaborative agents training teams is an activity that is expensive time consuming hazardous in some cases and can be limited by availability of equipment and personnel in team training the focus is on optimizing interactions such as efficiency of communication conflict resolution and prioritization group situation awareness resource distribution and load balancing etc this paper presents an agent based approach to designing intelligent team training systems we envision a computer based training system in which teams are trained by putting them through scenarios which allow them to practice their team skills there are two important roles that intelligent agents can play these are as virtual team members and as coach to carry out these functions these agents must be equipped with an understanding of the task domain the team structure the selected decision making process and their belief about other team members mental states 1 introduction an integral element of large c training for teamwork teams train by practicing basic interaction skills in order to develop a shared mental model between team members a computer based training system must have a model of teamwork to train a new team member to act as part of a team this model of teamwork can be used to identify weaknesses in team interaction skills in the trainee existing multi agent models for teamwork are limited in their ability to support proactive information exchange among teammates to address this issue we have developed and implemented a multi agent architecture called cast that simulates teamwork and supports proactive information exchange in a dynamic environment to this we are adding a tutoring system that monitors evaluates and corrects the trainee in the performance of teamwork skills keywords its teamwork multi agent collaboration hci the genetic algorithm as a discovery engine strange circuits and new principles this paper examines the idea of a genetic or evolutionary algorithm being an inspirational or discovery engine this is illustrated in the particular context of designing electronic circuits we argue that by connecting pieces of logic together and testing them to see if they carry out the desired function it may be possible to discover new principles of design and new algebraic techniques this is illustrated in the design of binary circuits particularly arithmetic functions where we demonstrate that by evolving a hierarchical series of examples it becomes possible to re discover the well known ripple carry principle for building adder circuits of any size we also examine the much harder case of multiplication we show also that extending the work into the field of multiple valued logic the genetic algorithm is able to produce fully working circuits that lie outside conventional algebra in addition we look at the issue of principle extraction from evolved data 1 introduction typechecking for xml transformers we study the typechecking problem for xml transformers given an xml transformation program and a dtd for the input xml documents check whether every result of the program conforms to a specified output dtd we model xml transformers using a novel device called a k pebble transducer that can express most queries without data value joins in xml ql xslt and other xml query languages types are modeled by regular tree languages a robust extension of dtds the main result of the paper is that typechecking for k pebble transducers is decidable consequently typechecking can be performed for a broad range of xml transformation languages including xmlql and a fragment of xslt 1 introduction traditionally database query languages have focused on data retrieval with complex data transformations left to applications the new xml data exchange standard for the web and emerging applications requiring data wrapping and integration have shifted the focus towards data transformations cooperating mobile agents for dynamic network routing this paper we present a contrasting model a dynamic wireless peer to peer network with routing tasks performed in a decentralized and distributed fashion by mobile software agents that cooperate to accumulate and distribute connectivity information our agents determine system topology by exploring the network then store this information in the nodes on the network other agents use this stored information to derive multi hop routes across the network we study these algorithms in simulation as an example of using populations of mobile agents to manage networks hive distributed agents for networking things hive is a distributed agents platform a decentralized system for building applications by networking local system resources this paper presents the architecture of hive concentrating on the idea of an ecology of distributed agents and its implementation in a practical java based system hive provides ad hoc agent interaction ontologies of agent capabilities mobile agents and a graphical interface to the distributed system we are applying hive to the problems of networking things that think putting computation and communication in everyday places such as your shoes your kitchen or your own body ttt shares the challenges and potentials of ubiquitous computing and embedded network applications we have found that the flexibility of a distributed agents architecture is well suited for this application domain enabling us to easily build applications and to reconfigure our systems on the fly hive enables us to make our environment and network more alive this paper is dedic extracting low resolution text with an active camera for ocr reading text in any scene is useful in the context of wearable computing robotic vision or as an aid for visually handicapped people here we present a novel automatic text reading system using an active camera focused on text regions already located in the scene using our recent work a region of text found is analysed to determine the optimal zoom that would foveate onto it then a number of images are captured over the text region to reconstruct a high resolution mosaic of the whole region this magnified image of the text is good enough for reading byhumans or for recognition by ocr even with a low resolution camera we obtained very good results materialized view selection and maintenance using multi query optimization materialized views have been found to be very effective at speeding up queries and are increasingly being supported by commercial databases and data warehouse systems however whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing the time window available for maintaining materialized views is shrinking these trends necessitate efficient techniques for the maintenance of materialized views in this paper we show how to find an efficient plan for the maintenance of a set of materialized views by exploiting common subexpressions between different view maintenance expressions in particular we show how to efficiently select a expressions and indices that can be effectively shared by transient materialization b additional expressions and indices for permanent materialization and c the best maintenance plan incremental or recomputation for each view these three decisions are highly interdependent and the choice of data mining in soft computing framework a survey the present article provides a survey of the available literature on data mining using soft computing a categorization has been provided based on the di erent soft computing tools and their hybridizations used the data mining function implemented and the preference criterion selected by the model the utility of the di erent soft computing methodologies is highlighted generally fuzzy sets are suitable for handling the issues related to understandability of patterns incomplete noisy data mixed media information and human interaction and can provide approximate solutions faster neural networks are non parametric robust and exhibit good learning and generalization capabilities in data rich environments genetic algorithms provide e cient search algorithms to select a model from mixed media data based on some preference criterion objective function rough sets are suitable for handling di erent types of uncertainty in data some challenges to data mining and the application of soft computing methodologies are indicated an extensive bibliography is also included keywords knowledge discovery rule extraction fuzzy logic neural networks genetic algorithms rough sets neuro fuzzy computing i selecting text spans for document summaries heuristics and metrics human quality text summarization systems are difficult to design and even more difficult to evaluate in part because documents can differ along several dimensions such as length writing style and lexical usage nevertheless certain cues can often help suggest the selection of sentences for inclusion in a summary this paper presents an analysis of news article summaries generated by sentence extraction sentences are ranked for potential inclusion in the summary using a weighted combination of linguistic features derived from an analysis of news wire summaries this paper evaluates the relative effectiveness of these features in order to do so we discuss the construction of a large corpus of extraction based summaries and characterize the underlying degree of difficulty of summarization at different compression levels on articles in this corpus results on our feature set are presented after normalization by this degree of difficulty estimating resemblance of midi documents abstract search engines often employ techniques for determining syntactic similarity of web pages such a tool allows them to avoid returning multiple copies of essentially the same page when a user makes a query here we describe our experience extending these techniques to midi music files the music domain requires modification to cope with problems introduced in the musical setting such as polyphony our experience suggests that when used properly these techniques prove useful for determining duplicates and clustering databases in the musical setting as well 1 using ontological engineering to overcome common ai ed problems this paper discusses long term prospects of ai ed research with the aim of giving a clear view of what we need for further promotion of the research from both the ai and ed points of view an analysis of the current status of ai ed research is done in the light of intelligence conceptualization standardization and theory awareness following this an ontology based architecture with appropriate ontologies is proposed ontological engineering of is id is next discussed followed by a road map towards an ontology aware authoring system heuristic design patterns and xml based documentation are also discussed 1 introduction among ai ed research done to date several paradigms such as cai icai micro world its ile and cscl have been proposed and many systems have been built within each paradigm additionally innovative computer technologies such as hyper media virtual reality internet www have significantly affected the ai ed community in general we really have learned a lot implementing clinical practice guidelines while taking account of changing evidence athena dss an easily modifiable decision support system for managing hypertension in primary care this paper describes the athena decision support system dss which operationalizes guidelines for hypertension using the eon architecture athena dss encourages blood pressure control and recommends guideline concordant choice of drug therapy in relation to comorbid diseases athena dss has an easily modifiable knowledge base that specifies eligibility criteria risk stratification blood pressure targets relevant comorbid diseases guideline recommended drug classes for patients with comorbid disease preferred drugs within each drug class and clinical messages because evidence for best management of hypertension evolves continually athena dss is designed to allow clinical experts to customize the knowledge base to incorporate new evidence or to reflect local interpretations of guideline ambiguities together with its database mediator athenaeum athena dss has physical and logical data independence from the legacy computerized patient record system cprs supplying the patient data so it can be integrated into a variety of electronic medical record systems turning yahoo into an automatic web page classifier the paper describes an approach to automatic web page classification based on the yahoo hierarchy machine learning techniques developed for learning on text data are used here on the hierarchical classification structure the high number of features is reduced by taking into account the hierarchical structure and using feature subset selection based on the method known from information retrieval documents are represented as feature vectors that include n grams instead of including only single words unigrams as commonly used when learning on text data based on the hierarchical structure the problem is divided into subproblems each representing one on the categories included in the yahoo hierarchy the result of learning is a set of independent classifiers each used to predict the probability that a new example is a member of the corresponding category experimental evaluation on real world data shows that the proposed approach gives good results for more than a half of testing clustering hypertext with applications to web searching clustering separates unrelated documents and groups related documents and is useful for discrimination disambiguation summarization organization and navigation of unstructured collections of hypertext documents we propose a novel clustering algorithm that clusters hypertext documents using words contained in the document out links from the document and in links to the document the algorithm automatically determines the relative importance of words out links and in links for a given collection of hypertext documents we annotate each cluster using six information nuggets summary breakthrough review keywords citation and reference these nuggets constitute high quality information resources that are representatives of the content of the clusters and are extremely effective in compactly summarizing and navigating the collection of hypertext documents we employ web searching as an application to illustrate our results keywords cluster annotation feature comb an on line em algorithm applied to kernel pca kernel principal component analysis pca is a recent method for non linear feature extraction applying kernel pca to a data set with n patterns requires storing and nding the eigenvectors of a n n kernel matrix this paper describes how an expectation maximization em algorithm for standard pca can be adapted to kernel pca without having to store the kernel matrix experimental results are given where em for kernel pca extracts up to 512 non linear features from a data set with 15 000 examples the extracted features lead to good performance when used as pre processed data for a linear classifier a novel on line em algorithm for pca is presented and shown to further speed up the learning process beyond eigenfaces probabilistic matching for face recognition we propose a novel technique for direct visual matching of images for the purposes of face recognition and database search specifically we argue in favor of a probabilistic measure of similarity in contrast to simpler methods which are based on standard l2 norms e g template matching or subspace restricted norms e g eigenspace matching the proposed similarity measure is based on a bayesian analysis of image differences we model two mutually exclusive classes of variation between two facial images intra personal variations in appearance of the same individual due to different expressions or lighting and extra personal variations in appearance due to a difference in identity the high dimensional probability density functions for each respective class are then obtained from training data using an eigenspace density estimation technique and subsequently used to compute a similarity measure based on the a posteriori probability of membership in the intrapersonal class an efficient method for performing record deletions and updates using index scans we present a method for efficiently performing deletions and updates of records when the records to be deleted or updated are chosen by a range scan on an index the traditional method involves numerous unnecessary lock calls and traversals of the index from root to leaves especially when the qualifying records keys span more than one leaf page of the index customers have suffered performance losses from these inefficiencies and have complained about them our goal was to minimize the number of interactions with the lock manager and the number of page fixes comparison operations and possibly i os some of our improvements come from increased synergy between the query planning and data manager components of a dbms our patented method has been implemented in db2 v7 to address specific customer requirements it has also been done to improve performance on the tpc h benchmark 1 repeating history beyond aries in this paper i describe first the background behind the development of the original aries recovery method and its significant impact on the commercial world and the research community next i provide a brief introduction to the various concurrency control and recovery methods in the aries family of algorithms subsequently i discuss some of the recent developments affecting the transaction management area and what these mean for the future in aries the concept of repeating history turned out to be an important paradigm as i examine where transaction management is headed in the world of the internet i observe history repeating itself in the sense of requirements that used to be considered significant in the mainframe world e g performance availability and reliability now becoming important requirements of the broader information technology community as well 1 introduction transaction management is one of the most important functionalities provided by a design of a classification system for rectangular shapes using a co design environment pattern localization and classification are cpu time intensive being normally implemented in software custom implementations in hardware allow real time processing in practice in asic or fpga implementations the digitization process introduces errors that should be taken into account this paper presents initially the state of the art in this field analyzing the performance and implementation of each work after we propose a system for rectangular shapes localization and classification using reconfigurable devices fpga and a signal processor dsp available in a flexible codesign platform the system will be described using c and vhdl languages for the software and hardware parts respectively finally it is described the classification block of the system implemented as an artificial neural network ann in a rapid prototyping platform 1 scalable algorithms for large temporal aggregation the ability to model time varying natures is essential to many database applications such as data warehousing and mining however the temporal aspects provide many unique characteristics and challenges for query processing and optimization among the challenges is computing temporal aggregates which is complicated by having to compute temporal grouping in this paper we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work first for small scale aggregations both the worst case and average case processing time have been improved significantly second for large scale aggregations the proposed algorithms can deal with a database that is substantially larger than the size of available memory scalable algorithms for large scale temporal aggregation the ability to model time varying natures is essential to many database applications such as data warehousing and mining however the temporal aspects provide many unique characteristics and challenges for query processing and optimization among the challenges is computing temporal aggregates which is complicated by having to compute temporal grouping in this paper we introduce a variety of temporal aggregation algorithms that overcome major drawbacks of previous work first for small scale aggregations both the worst case and average case processing time have been improved significantly second for large scale aggregations the proposed algorithms can deal with a database that is substantially larger than the size of available memory third the parallel algorithm designed on a shared nothing architecture achieves scalable performance by delivering nearly linear scale up and speed up the contributions made in this paper are particularly important because the rate of increase content based book recommending using learning for text categorization recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user s likes and dislikes most existing recommender systems use collaborative filtering methods that base recommendations on other users preferences by contrast content based methods use information about an item itself to make suggestions this approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations we describe a content based book recommending system that utilizes information extraction and a machine learning algorithm for text categorization initial experimental results demonstrate that this approach can produce accurate recommendations keywords recommender systems information filtering machine learning text categorization introduction there is a growing interest in recommender systems that suggest music films books and othe improvement in a lazy context an operational theory for call by need the standard implementation technique for lazy functional languages is call by need which ensures that an argument to a function in any given call is evaluated at most once a significant problem with call by need is that it is difficult even for compiler writers to predict the effects of program transformations the traditional theories for lazy functional languages are based on call by name models and offer no help in determining which transformations do indeed optimize a program in this article we present an operational theory for callby need based upon an improvement ordering on programs m is improved by n if in all program contexts c when c m terminates then c n terminates at least as cheaply we show that this improvement relation satisfies a context lemma and supports a rich inequational theory subsuming the call by need lambda calculi of ariola et al afm 95 the reduction based call by need calculi are inadequate as a theory of lazy program tran tertiary storage organization for large multidimensional datasets mobile objects in java mobile objects in java provides support for object mobility in java similarly to the rmi technique a notion of client side stub called startpoint is used to communicate transparently with a server side stub called endpoint objects and associated endpoints are allowed to migrate our approach takes care of routing method calls using an algorithm that we studied in 22 the purpose of this paper is to present and evaluate the implementation of this algorithm in java in particular two different strategies for routing method invocations are investigated namely call forwarding and referrals the result of our experimentation shows that the latter can be more efficient by up to 19 1 an alisis din amico de las creencias el modelo de los mundos posibles y su sem antica de kripke asociada dan una sem antica intuitiva para las l ogicas dox asticas pero parecen llevar inevitablemente a modelizar agentes l ogicamente omniscientes y razonadores perfectos estos problemas son evitables si los mundos posibles dejan de considerarse como descripciones completas y consistentes del mundo real adoptando una definici on sint actica de mundos posibles en este art iculo se sugiere c omo se pueden analizar las creencias de una forma puramente l ogica usando el m etodo de los tableros anal iticos con ciertas modificaciones este an alisis constituye un primer paso hacia la modelizaci on de la investigaci on racional palabras clave l ogicas de creencias y de conocimiento omnisciencia l ogica mundos posibles relaci on de accesibilidad tableros anal iticos temas representaci on del conocimiento razonamiento 1 introducci on las l ogicas de creencias y de conocimiento halp92 fagi95 son herramienta limited logical belief analysis the process of rational inquiry can be defined as the evolution of a rational agent s belief set as a consequence of its internal inference procedures and its interaction with the environment these beliefs can be modelled in a formal way using doxastic logics the possible worlds model and its associated kripke semantics provide an intuitive semantics for these logics but they seem to commit us to model agents that are logically omniscient and perfect reasoners these problems can be avoided with a syntactic view of possible worlds defining them as arbitrary sets of sentences in a propositional doxastic logic in this paper this syntactic view of possible worlds is taken and a dynamic analysis of the agent s beliefs is suggested in order to model the process of rational inquiry in which the agent is permanently engaged one component of this analysis the logical one is summarily described this dimension of analysis is performed using a modified version of the analytic tableaux dynamic belief analysis the process of rational inquiry can be defined as the evolution of the beliefs of a rational agent as a consequence of its internal inference procedures and its interaction with the environment these beliefs can be modelled in a formal way using belief logics the possible worlds model and its associated kripke semantics provide an intuitive semantics for these logics but they commit us to model agents that are logically omniscient and perfect reasoners these problems can be avoided with a syntactic view of possible worlds defining them as arbitrary sets of sentences in a propositional belief logic in this article this syntactic view of possible worlds is taken and a dynamic analysis of the beliefs of the agent is suggested in order to model the process of rational inquiry in which the agent is permanently engaged 1 introduction the aim of this work 1 is to model the process of rational inquiry i e the rationally controlled transformation of the beliefs of an intelligent how to avoid knowing it all beliefs have been formally modelled in the last decades using doxastic logics the possible worlds model and its associated kripke semantics provide an intuitive semantics for these logics but they seem to commit us to model agents that are logically omniscient they believe every classical tautology and perfect reasoners their beliefs are closed under classical deductive closure thus this model would not be appropriate to model non ideal agents that have resource limitations that prevent them from attaining such levels of doxastic competence this report contains a statement of these problems and a brief survey of some of the most interesting approaches that have been suggested to overcome them contents 1 formal models of belief 3 1 1 possible worlds and kripke semantics 3 1 2 logical omniscience and perfect reasoning 5 2 avoiding logical omniscience 7 2 1 syntactic approaches semantics of bdi agents and their environment this paper describes an approach for reasoning about the interactions of multiple agents in moderately complex environments the semantics of belief desire intention bdi agents has been investigated by many researchers and the gap between theoretical specification and practical design is starting to be bridged however the research has concentrated on single agent semantics rather than multiagent semantics and has not emphasised the semantics of the environment and its interaction with the agent this paper describes a class of simple bdi agents and uses a recently introduced logic of actions to provide semantics for these agents independent of the environment in which they may be embedded the same logic is used to describe the semantics of the environment itself and the interactions between the agent and the environment as there is no restriction on the number of agents the environment may interact with the approach can be used to address the semantics of multiple interacting ag collablogger a tool for visualizing groups at work the collablogger is a visual tool that supports usability analyses of human computer interaction in a team environment participants in our computer mediated activity were engaged in a small scale manufacturing testbed project interactions of the group were mediated by teamwave workplace 1 and the members performed both synchronous and asynchronous activities depending on their availability project requirements and due to chance meetings in the collaborative space the software was instrumented to log users interactions with the system and each other the collablogger addresses the problem of helping investigators analyze the volumes of log data that groupware tools can generate visual tools are powerful when large amounts of diverse data present themselves the place based collaboration environment offered by teamwave workplace provided a level of organization that allowed us to create a visual interface with which to perform exploratory sequential data analysis preliminary discovering and mining user web page traversal patterns as the popularity of www explodes a massive amount of data is gathered by web servers in the form of web access logs this is a rich source of information for understanding web user surfing behavior web usage mining also known as web log mining is an application of data mining algorithms to web access logs to find trends and regularities in web users traversal patterns the results of web usage mining have been used in improving web site design business and marketing decision support user profiling and web server system performance in this thesis we study the application of assisted exploration of olap data cubes and scalable sequential pattern mining algorithms to web log analysis in multidimensional olap analysis standard statistical measures are applied to assist the user at each step to explore the interesting parts of the cube in addition a scalable sequential pattern mining algorithm is developed to discover commonly traversed paths in large data sets our experimental and performance studies have demonstrated the effectiveness and efficiency of the algorithm in comparison to previously developed sequential pattern mining algorithms in conclusion some further research avenues in web usage mining are identified as well iv dedication to my parents v acknowledgments i would like to thank my supervisor dr jiawei han for his support sharing of his knowledge and the opportunities that he gave me his dedication and perseverance has always been exemplary to me i am also grateful to telelearning for getting me started in web log analysis i owe a depth of gratitude to dr jiawei han dr tiko kameda and dr wo shun luk for supporting my descision to continue my graduate studies i am also grateful to dr tiko kameda for accepting to be my supervis amalthaea information discovery and filtering using a multiagent evolving ecosystem agents are semi intelligent programs that assist the user in performing repetitive and time consuming tasks information discovery and information filtering are a suitable domain for applying agent technology ideas drawn from the field of autonomous agents and artificial life are combined in the creation of an evolving ecosystem composed of competing and cooperating agents a co evolution model of information filtering agents that adapt to the various user s interests and information discovery agents that monitor and adapt to the various on line information sources is analyzed results from a number of experiments are presented and discussed keywords agents information filtering evolution world wide web 1 introduction the exponential increase of computer systems that are interconnected in on line networks has resulted in a corresponding exponential increase in the amount of information available on line this information is distributed among heterogeneous sources and is bilateral negotiation with incomplete and uncertain information a decision theoretic approach using a model of the opponent the application of software agents to e commerce has made a radical change in the way businesses and consumer to consumer transactions take place agent negotiation is an important aspect of e commerce to bring satisfactory agreement in business transactions we approach e commerce and negotiation in the context of a distributed multiagent peer help system i help supporting students in a university course personal agents keep models of student preferences and negotiate on their behalf to acquire resources help from other agents we model negotiation among personal agents by means of influence diagram a decision theoretic tool to cope with the uncertainty inherent in a dynamic market with self interested participants the agents create models of their opponents during negotiation which help them predict better their opponents actions we carried out experiments comparing the proposed negotiation mechanism with influence diagram one using in addition a model of t convergence theory and applications of the factorized distribution algorithm the paper investigates the optimization of additively decomposable functions adf by a new evolutionary algorithm called factorized distribution algorithm fda fda is based on a factorization of the distribution to generate search points first separable adfs are considered these are mapped to generalized linear functions with metavariables defined for multiple alleles the mapping transforms fda into an univariate marginal frequency algorithm umda for umda the exact equation for the response to selection is computed under the assumption of proportionate selection for truncation selection an approximate equation for the time to convergence is used derived from an analysis of the onemax function fda is also numerically investigated for non separable functions the time to convergence is very similar to separable adfs fda outperforms the genetic algorithm with recombination of strings by far keywords response to selection fisher s theorem additively decomposable functions an event driven sensor architecture for low power wearables in this paper we define a software architecture for low power wearable computers the architecture is event driven and is designed so that application programs have access to sensor data without the need for polling the software architecture is designed to allow for an underlying hardware architecture that can achieve maximal power efficiency by switching off parts of the hardware that are known not to be required introduction wearable computing places particular demands on software it is characterised by having to accommodate a potentially large number of input devices which are simultaneously providing contextual data a need to minimise processor activity to reduce battery consumption 1 and the requirement for simplicity to aid debugging applications which may be used in adverse conditions as a result of our experiences with software which had none of the above features we have developed an eventdriven sensor architecture and used it successfully for several applications the cub e a novel virtual 3d display device we have designed and are in the process of building a visualisation device the cub e the cub e consists of six tft screens arranged in a perspex cube with a strongarm processor and batteries inside it is a multipurpose device with applications including teleconferencing interaction with virtual worlds and games 1 towards a virtual operating environment exploring immersive virtual interface design using a simple vr image viewer this paper explores the virtual interface from a design perspective a simple virtual reality image viewer application is tested with a variety of users producing a virtual interface design reference model and a set of virtual interface design guidelines 2 acknowledgements a massive thank you to shaun bangay my supervisor for priceless guidance thank you to mike rorke for help within the corgi system and as an expert user fred fourie for help with 3d studio max and the corgi ase readers and colin dembovski for collision detection that always did work thanks also to the rhodes vrsig members all the testers the corgi documentation team and phatwax 3 contents 1 introduction 7 1 1 problem statement 7 1 2 introduction to virtual reality 7 1 3 research goals identifying linkage groups by nonlinearity non monotonicity detection this paper presents and discusses direct linkage identification procedures based on nonlinearity non monotonicity detection the algorithm we propose checks arbitrary nonlinearity non monotonicity of fitness change by perturbations in a pair of loci to detect their linkage we first discuss condition of the linkage identification by nonlinearity check linc procedure munetomo goldberg 1998 and its allowable nonlinearity then we propose another condition of the linkage identification by nonmonotonicity detection limd and prove its equality to the linc with allowable nonlinearity linc an the procedures can identify linkage groups for problems with at most order k difficulty by checking o 2 k strings and the computational cost for each string is o l 2 where l is the string length 1 introduction the definition of linkage in genetics is the tendency for alleles of different genes to be passed together from one generation to the next winter hickey footprints in the snow er than use more formalised information artefacts when navigating cities people tend to ask other people for advice rather than study maps streeter and vitello 1985 when trying to find information about pharmaceuticals medical doctors tend to ask other doctors for advice tiimpka and hallberg 1996 if your child has red spots you might phone your mother or talk to a friend for an opinion even when we are not directly looking for information we use a wide range of cues both from features of the environment and from the behaviour of other people to manage our activities alan munro observed how people followed crowds or simply sat around at a venue when deciding which shows and street events to attend at the edinburgh arts festival munro 1998 we might be influenced to pick up a book because it appears well thumbed we walk into a sunny courtyard because it looks attractive or we might decide to see a film because our friends enjoyed it not only do we find our ways through bbq a visual interface for integrated browsing and querying of xml in this paper we present bbq blended browsing and querying a graphic user interface for seamlessly browsing and querying xml data sources bbq displays the structure of multiple data sources using a paradigm that resembles drilling down in windows directory structures bbq allows queries incorporating one or more of the sources queries are constructed in a query by example qbe manner where dtds play the role of schema the queries are arbitrary conjunctive queries with groupby and their results can be subsequently used and refined to support query refinement bbq introduces virtual result views standalone virtual data sources that i are constructed by user queries from elements in other data sources and ii can be used in subsequent queries as first class data sources themselves furthermore bbq allows users to query data sources with loose or incomplete schema and can augment such schema with a dtd inference mechanism visualizing the structure of the world wide web in 3d hyperbolic space we visualize the structure of sections of the world wide web by constructing graphical representations in 3d hyperbolic space the felicitous property that hyperbolic space has more room than euclidean space allows more information to be seen amid less clutter and motion by hyperbolic isometries provides for mathematically elegant navigation the 3d graphical representations available in the weboogl or vrml file formats contain link anchors which point to the original pages on the web itself we use the geomview weboogl 3d web browser as an interface between the 3d representation and the actual documents on the web the web is just one example of a hierarchical tree structure with links back up the tree i e a directed graph which contains cycles our information visualization techniques are appropriate for other types of directed graphs with cycles such as filesystems with symbolic links 1 introduction the dominant paradigm for world wide web navigation is pointing and click the bayes net toolbox for matlab the bayes net toolbox bnt is an open source matlab package for directed graphical models bnt supports many kinds of nodes probability distributions exact and approximate inference parameter and structure learning and static and dynamic models bnt is widely used in teaching and research the web page has received over 28 000 hits since may 2000 in this paper we discuss a broad spectrum of issues related to graphical models directed and undirected and describe at a high level how bnt was designed to cope with them all we also compare bnt to other software packages for graphical models and to the nascent openbayes effort specifying agents with uml in robotic soccer the use of agents and multiagent systems is widespread in computer science nowadays thus the need for methods to specify agents in a clear and simple manner arises in this paper we propose an approach to specifying agents with the help of uml statecharts agents are specified on different levels of abstraction in addition a method for specifying multiagent plans with explicit cooperation is shown as an example domain we chose robotic soccer which lays the basis of the annual robocup competitions robotic soccer is an ideal testbed for research in the fields of robotics and multiagent systems in the robocup simulation league the research focus is laid on agents and multiagent systems and we will demonstrate our approach by using examples from this domain keywords multiagent systems unified modeling language uml specification robocup robotic soccer 1 kaleidoquery a visual query language for object databases in this paper we describe kaleidoquery a visual query language for object databases with the same expressive power as oql we will describe the design philosophy behind the filter flow nature of kaleidoquery and present each of the language s constructs giving examples and relating them to oql the kaleidoquery language is described independent of any implementation details but a brief description of a 3d interface currently under construction for kaleidoquery is presented the queries in this implementation of the language are translated into oql and then passed to the object database o 2 for evaluation keywords visual query language oql object databases three dimensional interface introduction the lack of a generally accepted and widely supported query language has probably had a significant effect in slowing the uptake of early commercial object oriented databases however the emergence of the object query language oql which is being standardised by the object databas concept network a structure for context sensitive document representation in this paper we propose a directed acyclic graphical structure called concept network cnw for context sensitive document representation for use in information filtering nodes of cnw represent concepts and links represent the relationships between the concepts a concept can either be a phrase or a topic of discourse or a mode of discourse an important feature of the cnw based scheme cnwbs is context filters murthy and keerthi 1999 which are employed on the links of the graph to enable context sensitive analysis and representation of documents context filters are intended to filter the noise in the inputs of the concepts based on the context of appearance of their inputs the cnwbs automatically finds the paragraphs related to all concepts in the document it also provides good comprehensibility in representation allows sharing of cnw among a group of users and reduces the credit assignment problem during its construction this representation scheme is used for context filters for document based information filtering in this paper we propose a keyphrase sense disambiguation methodology called context filters for use in keyphrase based information filtering systems a context filter finds whether an input keyphrase has occurred in the required context context filters consider various factors of ambiguity some of these factors are special to information filtering and they are handled in a structured fashion the proposed context filters are very comprehensibile context filters consider varieties of contexts which are not considered in existing word sense disambiguation methods but these are all needed for information filtering the ideas on context filters that we report in this paper form important elements of an instructible information filtering agent that we are developing 1 introduction information filtering is the process of separating out irrelevant documents from relevant ones its importance has motivated several researchers to develop software agents such as sift infoscan iagent reformulating temporal plans for efficient execution the simple temporal network formalism permits significant flexibility in specifying the occurrence time of events in temporal plans however to retain this flexibility during execution there is a need to propagate the actual execution times of past events so that the occurrence windows of future events are adjusted appropriately unfortunately this may run afoul of tight real time control requirements that dictate extreme efficiency the performance may be improved by restricting the propagation however a fast locally propagating execution controller may incorrectly execute a consistent plan to resolve this dilemma we identify a class of dispatchable networks that are guaranteed to execute correctly under local propagation we show that every consistent temporal plan can be reformulated as an equivalent dispatchable network and we present an algorithm that constructs such a network moreover the constructed network is shown to have a minimum number of edges among all such n selective sampling with redundant views selective sampling a form of active learning reduces the cost of labeling training data by asking only for the labels of the most informative unlabeled examples we introduce a novel approach to selective sampling which we call co testing cotesting can be applied to problems with redundant views i e problems with multiple disjoint sets of attributes that can be used for learning we analyze the most general algorithm in the co testing family naive co testing which can be used with virtually any type of learner naive co testing simply selects at random an example on which the existing views disagree we applied our algorithm to a variety of domains including three real world problems wrapper induction web page classification and discourse trees parsing the empirical results show that besides reducing the number of labeled examples naive co testing may also boost the classification accuracy introduction in order to learn a classifier supervised learn hierarchical wrapper induction for semistructured information sources with the tremendous amount of information that becomes available on the web on a daily basis the abilitytoquickly develop information agents has become a crucial problem a vital componentofanyweb based information agent is a set of wrappers that can extract the relevant data from semistructured information sources our novel approach to wrapper induction is based on the idea of hierarchical information extraction which turns the hard problem of extracting data from an arbitrarily complex documentinto a series of simpler extraction tasks we introduce an inductive algorithm stalker that generates high accuracy extraction rules based on user labeled training examples labeling the training data represents the major bottleneck in using wrapper induction techniques and our experimental results showthatstalker requires up to two orders of magnitude fewer examples than other algorithms furthermore can wrap information sources that could not be wrapped by existing inductivetechniques a hierarchical approach to wrapper induction with the tremendous amount of information that becomes available on the web on a daily basis the ability to quickly develop information agents has become a crucial problem a vital component of any web based information agent is a set of wrappers that can extract the relevant data from semistructured information sources our novel approach to wrapper induction is based on the idea of hierarchical information extraction which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks we introduce an inductive algorithm stalker that generates high accuracy extraction rules based on user labeled training examples labeling the training data represents the major bottleneck in using wrapper induction techniques and our experimental results show that stalker does significantly better then other approaches on one hand stalker requires up to two orders of magnitude fewer examples than other algorithms while on the other hand integrating light weight workflow management systems within existing business environments workflow management systems support the efficient largely au tomated execution of business processes however using a workflow management system typically requires implementing the application s control flow exclusively by the workflow management system this approach is powerful if the control flow is specified and implemented from scratch but it has severe drawbacks if a workflow management system is to be integrated within environments with existing solutions for implementing control flow usual ly the existing solutions are too complex to be substituted by the workflow management system at once hence the workflow management system must support an incremental integration i e the reuse of existing implementations of control flow as well as their in cremental substitution extending the workflow management system s functionality ac cording to future application needs e g by worklist and history management must also be possible in particular at the beginning of interacting at a distance using semantic snarfing it is difficult to interact with computer displays that are across the room which can be important in meetings and when controlling computerized devices a popular approach is to use laser pointers tracked by a camera but interaction techniques using laser pointers tend to be imprecise error prone and slow therefore we have developed a new interaction style where the laser pointer or other pointing technique such pointing with a finger or even eye tracking indicates the region of interest and then the item there is copied snarfed to the user s handheld device such as a palm or pocketpc handheld if the content changes on the pc the handheld s copy will be updated as well interactions can be performed on the handheld using familiar direct manipulation techniques and then the modified version is sent back to the pc the content often must be reformatted to fit the properties of the handheld to facilitate natural interaction 1 easily adding animations to interfaces using constraints adding animation to interfaces is a very difficult task with today s toolkits even though there are many situations in which it would be useful and effective the amulet toolkit contains a new form of animation constraint that allows animations to be added to interfaces extremely easily without changing the logic of the application or the graphical objects themselves an animation constraint detects changes to the value of the slot to which it is attached and causes the slot to instead take on a series of values interpolated between the original and new values the advantage over previous approaches is that animation constraints provide significantly better modularity and reuse the programmer has independent control over the graphics to be animated the start and end values of the animation the path through value space and the timing of the animation animations can be attached to any object even existing widgets from the toolkit and any type of value can be animated scalars the amulet environment new models for effective user interface software development abstract the amulet user interface development environment makes it easier for programmers to create highly interactive graphical user interface software for unix windows and the macintosh amulet uses new models for objects constraints animation input output commands and undo the object system is a prototype instance model in which there is no distinction between classes and instances or between methods and data the constraint system allows any value of any object to be computed by arbitrary code and supports multiple constraint solvers animations can be attached to existing objects with a single line of code input from the user is handled by interactor objects which support reuse of behavior objects the output model provides a declarative definition of the graphics and supports automatic refresh command objects encapsulate all of the information needed about operations including support for various ways to undo them a key feature of the amulet design is that all graphical objects and behaviors of those objects are explicitly represented at run time so the system can provide a number of high level built in functions including automatic display and editing of objects and external analysis and control of interfaces amulet integrates these capabilities in a flexible and effective manner index terms toolkits user interface tools user interface development environments user interface management systems uimss 1 a brief history of human computer interaction technology this article summarizes the historical development of major advances in humancomputer interaction technology emphasizing the pivotal role of university research in the advancement of the field copyright 1996 carnegie mellon university a short excerpt from this article appeared as part of strategic directions in human computer interaction edited by brad myers jim hollan isabel cruz acm computing surveys 28 4 december 1996 this research was partially sponsored by nccosc under contract no n66001 94 c 6037 arpa order no b326 and partially by nsf under grant number iri 9319969 the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies either expressed or implied of nccosc or the u s government keywords human computer interaction history user interfaces interaction techniques brief history of hci 1 1 introduction research in human computer interaction hci has been spec a mutually beneficial integration of data mining and information extraction text mining concerns applying data mining techniques to unstructured text information extraction ie is a form of shallow text understanding that locates specific pieces of data in natural language documents transforming unstructured text into a structured database this paper describes a system called discotex that combines ie and data mining methodologies to perform text mining as well as improve the performance of the underlying extraction system rules mined from a database extracted from a corpus of texts are used to predict additional information to extract from future documents thereby improving the recall of ie encouraging results are presented on applying these techniques to a corpus of computer job announcement postings from an internet newsgroup using information extraction to aid the discovery of prediction rules from text text mining and information extraction ie are both topics of significant recent interest text mining concerns applying data mining a k a knowledge discovery from databases kdd techniques to unstructured text information extraction ie is a form of shallow text understanding that locates specific pieces of data in natural language documents transforming unstructured text into a structured database this paper describes a system called discotex that combines ie and kdd methods to perform a text mining task discovering prediction rules from natural language corpora an initial version of discotex is constructed by integrating an ie module based on rapier and a rule learning module ripper we present encouraging results on applying these techniques to a corpus of computer job postings from an internet newsgroup mining soft matching rules from textual data text mining concerns the discovery of knowledge from unstructured textual data one important task is the discovery of rules that relate specific words and phrases although existing methods for this task learn traditional logical rules soft matching methods that utilize word frequency information generally work better for textual data this paper presents a rule induction system textrise that allows for partial matching of text valued features by combining rule based and instance based learning we present initial experiments applying tex trise to corpora of book descriptions and patent documents retrieved from the web and compare its results to those of traditional rule and instance based methods 1 introduction text mining discovering knowledge from unstructured natural language text is an important data mining problem attracting increasing attention hearst 1999 feldman 1999 mladenic 2000 existing methods for mining rules from text use a hard logical text mining with information extraction the popularity of the web and the large number of documents available in electronic form has motivated the search for hidden knowledge in text collections consequently there is growing research interest in the general topic of text mining in this paper we develop a text mining system by integrating methods from information extraction ie and data mining knowledge discovery from databases or kdd by utilizing existing ie and kdd techniques text mining systems can be developed relatively rapidly and evaluated on existing text corpora for testing ie systems we present a general text mining framework called discotex which employs an ie module for transforming natural language documents into structured data and a kdd module for discovering prediction rules from the extracted data when discovering patterns in extracted text strict matching of strings is inadequate because textual database entries generally exhibit variations due to typographical errors misspellings abbreviations and other breadth first search crawling yields high quality pages this paper examines the average page quality over time of pages downloaded during a web crawl of 328 million unique pages we use the connectivity based metric pagerank to measure the quality of a page we show that traversing the web graph in breadth first search order is a good crawling strategy as it tends to discover high quality pages early on in the crawl high performance web crawling src s charter is to advance the state of the art in computer systems by doing basic and applied research in support of our company s business objectives our interests and projects span scalable systems including hardware networking distributed systems and programming language technology the internet including the web e commerce and information retrieval and human computer interaction including user interface technology computer based appliances and mobile computing src was established in 1984 by digital equipment corporation we test the value of our ideas by building hardware and software prototypes and assessing their utility in realistic settings interesting systems are too complex to be evaluated solely in the abstract practical use enables us to investigate their properties in depth this experience is useful in the short term in refining our designs and invaluable in the long term in advancing our knowledge most of the major advances in information systems have come through this approach including personal computing distributed systems and the internet we also perform complementary work of a more mathematical character some of benchmarking xml management systems the xoo7 way the effectiveness of existing xml query languages has been studied by many who focused on the comparison of linguistic features implicitly reflecting the fact that most xml tools exist only on paper in this paper with a focus on efficiency and concreteness we propose a pragmatic first step toward the systematic benchmarking of xml query processing platforms we begin by identifying the necessary functionalities an xml data management system should support we review existing approaches for managing xml data and the query processing capabilities of these approaches we then compare three xml query benchmarks xmach 1 xmark and xoo7 and discuss the applicability strengths and limitations of these benchmarks we highlight the bias of these benchmarks towards the data centric view of xml and motivate our selection of xoo7 to extend with document centric queries we complete xoo7 to capture the information retrieval capabilities of xml management systems finally we summarize our contributions and discuss future directions a proposal for an agent architecture for proactive persistent non player characters in the past games development has been driven by the need to achieve more realistic graphics evaluation of access structures for discretely moving points several applications require management of data which is spatially dynamic e g tracking of battle ships or moving cells in a blood sample the capability of handling the temporal aspect i e the history of such type of data is also important this paper presents and evaluates three temporal extensions of the r tree the 3d r tree the 2 3 r tree and the hr tree which are capable of indexing spatiotemporal data our experiments focus on discretely moving points i e points standing at a specific location for a time period and then moving instantaneously and so on and so forth we explore several parameters e g initial spatial distribution spatial query area and temporal query length we found out that the hr tree usually outperforms the other candidates in terms of query processing cost specially when querying time points and small time intervals however the main side effect of the hr tree is its storage requirement whichismuch larger than that of the o assessment methods for information quality criteria information quality iq is one of the most important aspects of information integration on the internet many projects realize and address this fact by gathering and classifying iq criteria hardly ever do the projects address the immense difficulty of assessing scores for the criteria this task must precede any usage of criteria for qualifying and integrating information after reviewing previous attempts to classify iq criteria in this paper we also classify criteria but in a new assessment oriented way we identify three sources for iq scores and thus three iq criterion classes each with different general assessment possibilities additionally for each criterion we give detailed assessment methods finally we consider confidence measures for these methods confidence expresses the accuracy lastingness and credibility of the individual assessment methods 1 introduction low information quality is one of the most pressing problems for consume rs of information that is di completeness of information sources information quality plays a crucial role in every application that integrates data from autonomous sources however information quality is hard to measure and complex to consider for the tasks of information integration even if the integrating sources cooperate we present a systematic and formal approach to the measurement of information quality and the combination of such measurements for information integration our approach is based on a value model that incorporates both extensional value coverage and intensional value density of information for both aspects we provide merge functions for adequately scoring integrated results also we combine the two criteria to an overall completeness criterion that formalizes the intuitive notion of completeness of query results this completeness measure is a valuable tool to assess source size and to predict result sizes of queries in integrated information systems we propose this measure as an important step towards the usage of information quality for source selection query planning query optimization and quality feedback to users from databases to information systems information quality makes the difference research and business is currently moving from centralized databases towards in formation systems integrating distributed and autonomous data sources simultane ously it is a well acknowledged fact that consideration of information quality iq reasoning is an important issue for large scale integrated information systems we show that iq reasoning can be the driving force of the current shift from databases to integrated information systems in this paper we explore the implications and consequences of this shift all areas of answering user queries are affected from user input to query planning and query optimization and finally to building the query result the application of iq reasoning brings both challenges such as new cost models for optimization and opportunities such as improved query planning we highlight several emerging aspects and suggest solutions toward a pervasion of information quality in information systems adding compression to block addressing inverted indexes inverted index compression block addressing and sequential search on compressed text are three techniques that have been separately developed for efficient low overhead text retrieval modern text compression techniques can reduce the text to less than 30 of its size and allow searching it directly and faster than the uncompressed text inverted index compression obtains significant reduction of their original size at the same processing speed block addressing makes the inverted lists point to text blocks instead of exact positions and pay the reduction in space with some sequential text scanning in this work we combine the three ideas in a single scheme we present a compressed inverted file that indexes compressed text and uses block addressing we consider different techniques to compress the index and study their performance with respect to the block size we compare the index against three separate techniques for varying block sizes showing that our index is superior to each isolated approach for instance with just 4 of extra space overhead the index has to scan less than 12 of the text for exact searches and about 20 allowing one error in the matches keywords text compression inverted files block addressing text databases 1 dynamic connection of wearable computers to companion devices using near field radio hewlett packard laboratories bristol and the university of bristol department of computer science are engaged in an initiative to explore the design technology and use of wearable computers we describe a way of connecting a wearable computer to companion devices such as displays or cameras using near field radio technology the shortrange nature of near field radio allows relatively high data rates 300 kbps 1mbit low power consumption and the interpretation of gestures as configuration requests keywords near field radio dynamic connectivity introduction we are particularly interested in communication technologies that exhibit low power short range up to 1 foot and modest data rates 300 kbps 1 mbs the action of picking up a companion device such as a display establishes the communication link due to the very short range an important aspect of a suitable communication technology is that the user is not required to touch an electrode and therefore handling of constructive biology and approaches to temporal grounding in post reactive robotics constructive biology as opposed to descriptive biology means understanding biological mechanisms through building systems that exhibit life like properties applications include learning engineering tricks from biological systems as well as the validation in biological modelling in particular biological systems unlike reactive robots in the course of development and experience become temporally grounded researchers attempting to transcend mere reactivity have been inspired by the drives motivations homeostasis hormonal control and emotions of animals in order to contextualize and modulate behavior these ideas have been introduced into robotics and synthetic agents while further flexibility is achieved by introducing learning broadening scope of the temporal horizon further requires post reactive techniques that address not only the action in the now although such action may perhaps be modulated by drives and affect support is needed for expressing and benefitting from meaning for observers and agents claude shannon formalized the notion of information transmission rate and capacity for pre existing channels wittgenstein in his later work insisted that linguistic meaning be defined in terms of use in language games c s peirce the father of semiotics realized the importance of sign signified and interpretant in processes of semiosis in particular the connection between sign and signified does not take place in a platonic vacuum but is situated embodied embedded and must be mediated by an interpretant we introduce a rigorous mathematical notion of meaning as 1 agent and observer perceptible information in interaction games between an agent and its environment or between an agent and other agents that is 2 useful for satisfying homeostatic and other drives needs goals or intentions with this framework it is possible to address issues of sensor and actuator design origins evolution and maintenance for biological and artificial systems moreover correspondences between channels of meaning are exploited by biological entities in predicting the behavior or reading the intent of others as in predator prey and social interaction social learning imitation communication of experience also develop and can be developed on this substrate of shared meaning perceptual interfaces for information interaction joint processing of audio and visual information for human computer interaction we are exploiting the human perceptual principle of sensory integration the joint use of audio and visual information to improve the recognition of human activity speech recognition speech event detection and speaker change intent intent to speak and human identity speaker recognition particularly in the presence of acoustic degradation due to noise and channel in this paper we present experimental results in a variety of contexts that demonstrate the benefit of joint audio visual processing augmented reality tracking in natural environments tracking or camera pose determination is the main technical challenge in creating augmented realities constraining the degree to which the environment may be altered to support tracking heightens the challenge this paper describes several years of work at the usc computer graphics and immersive technologies cgit laboratory to develop self contained minimally intrusive tracking systems for use in both indoor and outdoor settings these hybrid technology tracking systems combine vision and inertial sensing with research in fiducial design feature detection motion estimation recursive filters and pragmatic engineering to satisfy realistic application requirements constructing a realistic head animation mesh for a specific person this paper addresses the problem of constructing an realistic and complete animation mesh that portrays a specific person s head geometry and texture our approach deforms a prototype mesh containing vector based delineated muscles to fit one or more geometric models obtained from stereo image pairs of a specific person s head the resulting personalized mesh facilitates animation with the same realism and predictability as the original prototype mesh the model construction requires some manual interaction however automatic refinement methods reduce the need for precision the sensing process is passive and no physical markers are needed on the person s face models produced by our method are suited to realistic animations of specific individuals for applications in special effects games and 3d teleconferencing 1 introduction ever since the pioneering work of frederic i parke 1 in 1972 researchers have attempted to generate realistic facial models and animation recent inte experience with emerald to date after summarizing the emerald architecture and the evolutionary process from which emerald has evolved this paper focuses on our experience to date in designing implementing and applying emerald to various types of anomalies and misuse the discussion addresses the fundamental importance of good software engineering practice and the importance of the system architecture in attaining detectability interoperability general applicability and future evolvability it also considers the importance of correlation among distributed and hierarchical instances of emerald and needs for additional detection and analysis components 1 introduction emerald event monitoring enabling responses to anomalous live disturbances 6 8 9 is an environment for anomaly and misuse detection and subsequent analysis of the behavior of systems and networks emerald is being developed under darpa ito contract number f30602 96 c 0294 and applied under darpa iso contract number f30602 98 c 0059 emer stable algorithms for link analysis the kleinberg hits and the google pagerank algorithms are eigenvector methods for identifying authoritative or influential articles given hyperlink or citation information that such algorithms should give reliable or consistent answers is surely a desideratum and in 10 we analyzed when they can be expected to give stable rankings under small perturbations to the linkage patterns in this paper we extend the analysis and show how it gives insight into ways of designing stable link analysis methods this in turn motivates two new algorithms whose performance we study empirically using citation data and web hyperlink data 1 on reconfiguring query execution plans in distributed object relational dbms massive database sizes and growing demands for decision support and data mining result in long running queries in extensible object relational dbms particularly in decision support and data warehousing analysis applications parallelization of query evaluation is often required for acceptable performance yet queries are frequently processed suboptimally due to 1 only coarse or inaccurate estimates of the query characteristics and database statistics available prior to query evaluation 2 changes in system configuration and resource availability during query evaluation in a distributed environment dynamically reconfiguring query execution plans qeps which adapts qeps to the environment as well as the query characteristics is a promising means to significantly improve query evaluation performance based on an operator classification we propose an algorithm to coordinate the steps in a reconfiguration and introduce alternatives for execution context checkpointing and restorin strict archimedean t norms and t conorms as universal approximators in knowledge representation when we have to use logical connectives various continuous t norms and t conorms are used in this paper we show that every continuous t norm and t conorm can be approximated to an arbitrary degree of accuracy by a strict archimedean t norm t conorm address correspondence to vladik kreinovich department of computer science university of texas at el paso el paso tx 79968 usa email vladik cs utep edu international journal of approximate reasoning 199 c fl 199 elsevier science inc 655 avenue of the americas new york ny 10010 0888 613x 9 7 00 2 1 introduction brief idea when we represent expert knowledge in expert systems and in intelligent control it is important to adequately describe not only the experts statements themselves but also the experts degrees of confidence in the corresponding statements it is also important to adequately describe which operations with these degrees of confidence are best representing the expe using handhelds as controls for everyday appliances a paper prototype study everyday appliances including telephones copiers and home stereos increasingly contain embedded computers which enable greater functionality if the interfaces to these appliances were easy to use people might benefit from these new functions unfortunately it is rare to find a well designed appliance interface this study shows that existing appliance interfaces could be improved by using a remote control interface on a handheld computer keywords handheld computers remote control appliance pebbles introduction the problem with many appliances is that they are too complex some appliances need thirty or more buttons to cover all of their functions this complexity can even make relatively simple tasks like setting the clock on a vcr so difficult that people avoid them most appliances do not provide unambiguous feedback to users indicators of appliance state can be confusing for example on a stereo that combines a cd and tape player it may be difficult to decide wh increasing the expressiveness of analytical performance models for replicated databases the vast number of design options in replicated databases requires efficient analytical performance evaluations so that the considerable overhead of simulations or measurements can be focused on a few promising options a review of existing analytical models in terms of their modeling assumptions replication schemata considered and network properties captured shows that data replication and intersite communication as well as workload patterns should be modeled more accurately based on this analysis we define a new modeling approach named 2rc 2 dimensional replication model with integrated communication we derive a complete analytical queueing model for 2rc and demonstrate that it is of higher expressiveness than existing models 2rc also yields a novel bottleneck analysis and permits to evaluate the trade off between throughput and availability 1 introduction replication management in distributed databases concerns the decision when and where to allocate physical copies of joint optimization of cost and coverage of query plans in data integration existing approaches for optimizing queries in data integration use decoupled strategies attempting to optimize coverage and cost in two separate phases since sources tend to have a variety of access limitations such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans in this paper we present techniques for joint optimization of cost and coverage of the query plans our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct the refinement of the partial plans takes into account the potential parallelism between source calls and the binding compatibilities between the sources included in the plan we start by introducing and motivating our query plan representation we then briefly review how to compute the cost and coverage of a parallel plan next we provide both a system r style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better both in terms of planning cost and in terms of plan execution cost compared to the existing approaches 1 mining source coverage statistics for data integration recent work in data integration has shown the importance of statistical information about the coverage and overlap of sources for efficient query processing despite this recognition there are no effective approaches for learning the needed statistics the key challenge in learning such statistics is keeping the number of needed statistics low enough to have the storage and learning costs manageable naive approaches can become infeasible very quickly in this paper we present a set of connected techniques that estimate the coverage and overlap statistics while keeping the needed statistics tightly under control our approach uses a hierarchical classification of the queries and threshold based variants of familiar data mining techniques to dynamically decide the level of resolution at which to learn the statistics we describe the details of our method and present preliminary experimental results showing the feasibility of the approach 1 analyzing the effectiveness and applicability of co training recently there has been significant interest in supervised learning algorithms that combine labeled and unlabeled data for text learning tasks the co training setting 1 applies to datasets that have a natural separation of their features into two disjoint sets we demonstrate that when learning from labeled and unlabeled data algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not when a natural split does not exist co training algorithms that manufacture a feature split may out perform algorithms not using a split these results help explain why co training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers categories and subject descriptors i 2 6 artificial intelligence learning h 3 3 information storage and retrieval information search and retrieval information filtering keywords co training expectation maximization learning with labeled and unlabeled using unlabeled data to improve text classification one key difficulty with text classification learning algorithms is that they require many hand labeled examples to learn accurately this dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high accuracy text classifiers by assuming that documents are created by a parametric generative model expectation maximization em finds local maximum a posteriori models and classifiers from all the data labeled and unlabeled these generative models do not capture all the intricacies of text however on some domains this technique substantially improves classification accuracy especially when labeled data are sparse two problems arise from this basic approach first unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated in this case the assumptions can be made more representative in two ways by modeling sub topic class structure and by modeling super topic hierarchical class relationships by doing so model probability and classification accuracy come into correspondence allowing unlabeled data to improve classification performance the second problem is that even with a representative model the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data here limited labeled data provide em initializations that lead to low probability models performance can be significantly improved by using active learning to select high quality initializations and by using alternatives to em that avoid low probability local maxima text classification from labeled and unlabeled documents using em this paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents this is important because in many text classification problems obtaining training labels is expensive while large quantities of unlabeled documents are readily available we introduce an algorithm for learning from labeled and unlabeled documents based on the combination of expectation maximization em and a naive bayes classifier the algorithm first trains a classifier using the available labeled documents and probabilistically labels the unlabeled documents it then trains a new classifier using the labels for all the documents and iterates to convergence this basic em procedure works well when the data conform to the generative assumptions of the model however these assumptions are often violated in practice and poor performance can result we present two extensions to the algorithm that improve classification accuracy under these conditions 1 a weighting factor to modulate the contribution of the unlabeled data and 2 the use of multiple mixture components per class experimental results obtained using text from three different real world tasks show that the use of unlabeled data reduces classification error by up to 30 multimodal interactions with agents in virtual worlds introduction world wide web allows interactions and transactions through web pages using speech and language either by inanimate or live agents image interpretation and generation and of course the more traditional ways of presenting explicitly predefined information of text tables figures pictures audio animation and video in a task or domain oriented way of interaction current technology allows the recognition and interpretation of rather natural speech and language in dialogues however rather than the current two dimensional web pages many interesting parts of the web will become three dimensional allowing the building of virtual worlds inhabited by user and task agents with which the user can interact using different types of modalities including speech and language interpretation and generation agents can work on behalf of users hence human computer interaction will make use of indirect management rather than interacting through direct manipulation of data repository support for multi perspective requirements engineering abstract relationships among di erent modeling perspectives have been systematically investigated focusing either on given notations e g uml or on domain reference models e g aris sap in contrast many successful informal methods for business analysis and requirements engineering e g jad emphasize team negotiation goal orientation and exibility of modeling notations this paper addresses the question how much formal and computerized support can be provided in such settings without destroying their creative tenor our solution is based on a novel modeling language design m telos that integrates the adaptability and analysis advantages of the logic based meta modeling language telos with a module concept covering the structuring mechanisms of scalable software architectures it comprises four components 1 a modular conceptual modeling formalism organizes individual perspectives and their interrelationships 2 perspective schemata are linked to a conceptual meta meta model of shared domain terms thus giving the architecture a semantic meaning and enabling adaptability and extensibility of the network of perspectives 3 inconsistency management across perspectives is handled in a goal oriented manner by formalizing analysis goals as meta rules which are automatically customized to perspective schemata 4 continuous incremental maintenance of inconsistency information is provided by exploiting recent view maintenance techniques from deductive databases the approach has been fully implemented as an extension to the conceptbase meta database management system and is currently experimentally applied in the context of business analysis and data warehouse design an overview of active information gathering in infosleuth infosleuth is a system of collaborating software agents that can be configured to perform many different information management activities in a distributed environment infosleuth agents advertise semantic constraints about themselves to infosleuth brokers using a global domain ontology when queried a broker reasons over these constraints to determine the minimal set of agents that can provide a solution to the query infosleuth s architecture is based on a generic agent shell that provides basic agent communication behaviors over a subset of knowledge query manipulation language individual agents are subclasses of this generic shell that provide specific kinds of functionality infosleuth agents perform a number of complex query activities that require resolving ontology based queries over dynamically changing distributed heterogeneous resources including distributed query location independent single resource updates event monitoring by means of subscription notification servi active information gathering in infosleuth infosleuth 1 is an agent based system that can be configured to perform many different information management activities in a distributed environment infosleuth agents provide a number of complex query services that require resolving ontology based queries over dynamically changing distributed heterogeneous resources these include distributed query processing location independent single resource updates event and information monitoring statistical or inferential data analysis and trend discovery in complex event streams it has been used in numerous applications including the environmental data exchange network and the competitive intelligence system 1 introduction in the past 15 20 years numerous products and prototypes have regularly appeared to provide uniform access to heterogeneous data sources as a result that access to heterogeneous sources is now taken as a given by customers current mcc studies indicate that given the availability of products that achieve uncertain knowledge representation and communicative behavior in coordinated defense this paper reports on results we obtained on communication among artificial and human agents interacting in a simulated air defense domain in our research we postulate that the artificial agents use a decision theoretic method to select optimal communicative acts given the characteristics of the particular situation thus the agents we implemented compute the expected utilities of various alternative communicative acts and execute the best one the agents use a probabilistic frame based knowledge formalism to represent the uncertain information they have about the domain and about the other agents present we build on our earlier work that uses the recursive modeling method rmm for coordination and apply rmm to rational communication in an anti air defense domain in this domain distributed units coordinate and communicate to defend a specified territory from a number of attacking missiles we measure the benefits of rational communication by showing the improvement in the qua learning to notice adaptive models of human operators agent based technologies have been used for a number of years to model human operators in complex simulated environments the bdi agent framework has proven to be particularly suited to this sort of modelling due to its natural composition of beliefs goals and plans however one of the weaknesses of the bdi agent model and many other human operator models agent based or otherwise is its inability to support agent learning human operators naturally adapt their behaviour over time particularly to avoid repeating mistakes this paper introduces an enhancement to the bdi framework which is based on a descriptive psychological model of decision making called recognition primed decision making this enhancement allows the development of agents that adapt their behaviour in real time in the same manner as a person would providing more realistic human operator models argument in multi agent systems multi agent systems research is concerned both with the modelling of human and animal societies and with the development of principles for the design of practical distributed information management systems this position paper will rather than examine the various dierences in perspective within this area of research discuss issues of communication and commitment that are of interest to multi agent systems research in general 1 introduction a computational society is a collection of autonomous agents that are loosely dependent upon each other the intentional stance 12 is often taken in describing the state of these agents an agent may have beliefs desires intentions and it may adopt a role or have relationships with others thus multi agent systems mas as with most ai research is signi cantly inuenced at least in its vocabulary by philosophy and cognitive psychology 1 so what s the point computational societies are developed for two primary reasons mode goal creation in motivated agents goal creation is an important consideration for an agent that is required to behave autonomously in a real world domain this paper describes an agent that is directed not by a conjunction of top level goals but by a set of motives the agent is motivated to create and prioritise different goals at different times as a part of an on going activity under changing circumstances goals can be created both in reaction to and in anticipation of a situation while there has been much work on the creation of reactive goals i e goals created in reaction to a situation the issues involved in the creation of anticipatory or proactive goals have not been considered in depth the solution to the goal creation problem outlined here provides an agent with an effective method of creating goals both reactively and proactively giving the agent a greater degree of autonomy 1 introduction the focus of planning research has principally been concerned with the creation of good plans to satisfy alarms an implementation of motivated agency autonomous agents in the real world must be capable of asynchronous goal generation however one consequence of this ability is that the agent may generate a substantial number of goals but only a small number of these will be relevant at any one time therefore there is a need for tractable mechanisms to manage a changing and potentially large number of goals this paper presents both a framework for the design of agents that have the capability to generate and manage their own top level goals motivated agency and an implementation of part of this agent architecture alarms the alarm processing machinery serves to focus the attention of an agent on a limited number of the most salient goals regardless of the number of possible goals that the agent can pursue or their distribution in time in this way a resource bounded autonomous agent can employ modern planning methods to greater effect 1 introduction if an autonomous agent is required to interact with an environment t motivation based direction of planning attention in agents with goal autonomy the action of an agent with goal autonomy will be driven by goals generated with reference to its own beliefs and desires this ability is essential for agents that are required to act in their own interests in a domain that is not entirely predictable at any time the situation may warrant the generation of new goals however it is not always the case that changes in the domain that lead to the generation of a goal are detected immediately before the goal should be pursued action may not be appropriate for some time furthermore an agent may be influenced by goals that tend to recur periodically or at particular times of the day or week for example such goals serve to motivate an agent towards interacting with other agents or processes with certain types of predictable behaviour patterns this thesis provides a model of a goal autonomous agent that may generate goals in response to unexpected changes in its domain or cyclically through automatic processes an important effect of an extended entity relationship approach to data management in object oriented systems database programming in object oriented systems can be supported by combining data modelling and programming technologies such that a data model supports the management of collections of objects where those objects are as specified by the underlying object oriented programming language this approach is the basis of the object data management services odms of the comandos system the odms data model provides constructs for the representation of both entities and their relationships and further supports rich classification structures to complement the structural model there is an operational model based on an algebra over collections of objects 1 introduction object oriented technologies are gaining in popularity as the basis for software development platforms meanwhile the family of entity relationship data models retain their wide spread use and popularity for conceptual modelling how then can these two successful technologies be combined to support the development of web based integration of printed and digital information the affordances of paper have ensured its retention as a key information medium in spite of dramatic increases in the use of digital technologies for information storage processing and delivery recent developments in paper printing and wand technologies may lead to the widespread use of digitally augmented paper in the near future thereby enabling the paper and digital worlds to be linked together we are interested in using these technologies to achieve a true integration of printed and digital information sources such that users may browse freely back and forth between paper and digital resources we present web based server technologies that support this integration by allowing users to dynamically link areas of printed documents to objects of an application database the server component is implemented using the extensible information management architecture xima and is independent of the particular paper printing and reader technologies used to realise the digitally augmented paper the framework presented manages semantic information about application objects documents users links and client devices and it supports universal client access learning probabilistic datalog rules for information classification and transformation probabilistic datalog is a combination of classical datalog i e function free horn clause predicate logic with probability theory therefore probabilistic weights may be attached to both facts and rules but it is often impossible to assign exact rule weights or even to construct the rules themselves instead of specifying them manually learning algorithms can be used to learn both rules and weights in practice these algorithms are very slow because they need a large example set and have to test a high number of rules we apply a number of extensions to these algorithms in order to improve efficiency several applications demonstrate the power of learning probabilistic datalog rules showing that learning rules is suitable for low dimensional problems e g schema mapping but inappropriate for higher dimensions like e g in text classification mind an architecture for multimedia information retrieval in federated digital libraries introduction today people have routine access to a huge number of heterogeneous and distributed digital libraries to satisfy an information need relevant libraries have to be selected the information need has to be reformulated for every library w r t its schema and query syntax and the results have to be fused this is an ineffective manual task for which accurate tools are desirable mind which we are currently developing in an eu project is an end to end solution for federated digital libraries which covers all these issues we start from information retrieval approaches which focus on retrieval quality but mostly only consider monomedial and homogeneous sources we will extend these approaches for dealing with different kinds of media text facts images and transcripts of speech recognition as well as handling heterogeneous libraries e g with different schemas another innovation is that mind also considers non co operating libraries which only provide the an analytical study of object identifier indexing the object identifier index of an object oriented database system is typically 20 of the size of the database itself and for large databases only a small part of the index fits in main memory to avoid index retrievals becoming a bottleneck efficient buffering strategies are needed to minimize the number of disk accesses in this report we develop analytical cost models which we use to find optimal sizes of index page buffer and index entry cache for different memory sizes index sizes and access patterns because existing buffer hit estimation models are not applicable for index page buffering in the case of tree based indexes we have also developed an analytical model for index page buffer performance the cost gain from using the results in this report is typically in the order of 200 300 thus the results should be of valuable use in optimizers and tools for configuration and tuning of object oriented database systems 1 introduction in a large oodb with logical object i optimizing oid indexing cost in temporal object oriented database systems in object oriented database systems oodb with logical oids an oid index oidx is needed to map from oid to the physical location of the object in a transaction time temporal oodb the oidx should also index the object versions in this case the index entries which we call object descriptors od also include the commit timestamp of the transaction that created the object version in this report we develop an analytical model for oidx access costs in temporal oodbs the model includes the index page buffer as well as an od cache we use this model to study access cost and optimal use of memory for index page buffer and od cache with different access patterns the results show that 1 the oidx access cost can be high and can easy become a bottleneck in large temporal oodbs 2 the optimal od cache size can be relatively large and 3 the gain from using an optimal size is considerable and because access pattern in a database system can be very dynamic the system should be ab the vagabond parallel temporal object oriented database system versatile support for future applications in this paper we discuss features that future database systems should support to deliver the required functionality and performance to future applications the most important features are efficient support for 1 large objects 2 isochronous delivery of data 3 queries on large data sets 4 full text indexing 5 multidimensional data 6 sparse data and 7 temporal data and versioning to efficiently support these features in one integrated system a new database architecture is needed we describe an architecture suitable for this purpose the vagabond parallel temporal object oriented database system we also describe techniques we have developed to avoid some possible bottlenecks in a system based on this new architecture 1 introduction the recent years have brought computers into almost every office and this availability of powerful computers connected in global networks has made it possible to utilize powerful data management systems in new application areas the persistent cache improving oid indexing in temporal object oriented database systems in a temporal oodb an oid index oidx is needed to map from oid to the physical location of the object in a transaction time temporal oodb the oidx should also index the object versions in this case the index entries which we call object descriptors od also include the commit timestamp of the transaction that created the object version the oidx in a non temporal oodb only needs to be updated when an object is created but in a temporal oodb the oidx have to be updated every time an object is updated we have in a previous study shown that this can be a potential bottleneck and in this report we present the persistent cache pcache a novel approach which reduces the index update and lookup costs in temporal oodbs in this report we develop a cost model for the pcache and use this to show that the use of a pcache can reduce the average access cost to only a fraction of the cost when not using the pcache even though the primary context of this report is oid indexing in a perspective on software agents research this paper sets out ambitiously to present a brief reappraisal of software agents research evidently software agent technology has promised much however some five years after the word agent came into vogue in the popular computing press it is perhaps time the efforts in this fledgling area are thoroughly evaluated with a view to refocusing future efforts we do not pretend to have done this in this paper but we hope we have sown the first seeds towards a thorough first 5 year report of the software agents area the paper contains some strong views not necessarily widely accepted by the agent community 1 1 introduction the main goal of this paper is to provide a brief perspective on the progress of software agents research though agents research had been going on for more than a fifteen years before agents really became a buzzword in the popular computing press and also within the artificial intelligence and computing communities around 1994 during this year sev putting the feel in look and feel haptic devices are now commercially available and thus touch has become a potentially realistic solution to a variety of interaction design challenges we reportonanexperimental investigation of the use of touch as a way of reducing visual overload in the conventional desktop in a two phase study we investigated the use of the phantom haptic device as a means of interacting with a conventional graphical user interface the first experiment compared the effects of four different haptic augmentations on user performance in a simple targeting task the second experiment involved a more ecologically oriented searching and scrolling task results indicated that the haptic effects did not improve users performance in terms of task completion time however the number of errors made was significantly reduced subjective workload measures showed that participants perceived many aspects of workload as significantly less with haptics the results are described and the implications for the use of haptics in user interface design are discussed subject areas multimodal interaction augmented reality empirical quantitative evaluation input devices interaction technology chi 2000 home page 1of2 13 09 99 11 36 chi 2000 call for papers http msrconf microsoft com sigchi authorlisteditprocess asp tactile or gestural i o click here to view the anonymous version of your paper that we have received your paper must be viewable by the standard adobe acrobat viewer if you cannot get this to work then your hardcopy version will be sent to reviewers remember in addition to your electronic submission you must submit your paper in hardcopy form to chi 2000 c o mary czerwinski microsoft research one microsoft way redmond the clef 2003 interactive track the clef 2003 interactive track iclef was the third year of a shared experiment design to compare strategies for cross language search assistance two kinds of experiments were performed a experiments in cross language document selection where the user task is to scan a ranked list of documents written in a foreign language selecting those which seem relevant to a given query the aim here is to compare di erent translation strategies for an indicative purpose and b full cross language search experiments where the user task is to maximize the number of relevant documents that can be found in a foreignlanguage collection with the help of an end to end cross language search system participating teams could choose to focus on any aspects of the search task e g query formulation query translation and or relevance feedback this paper describes the shared experiment design and briefly summarizes the experiments run by the five teams that participated evaluating interactive cross language information retrieval document selection the problem of finding documents that are written in a language that the searcher cannot read is perhaps the most challenging application of cross language information retrieval clir technology the first cross language evaluation forum clef provided an excellent venue for assessing the performance of automated clir techniques but little is known about how searchers and systems might interact to achieve better cross language search results than automated systems alone can provide this paper explores the question of how interactive approaches to clir might be evaluated suggesting an initial focus on evaluation of interactive document selection important evaluation issues are identified the structure of an interactive clef evaluation is proposed and the key research communities that could be brought together by such an evaluation are introduced 1 introduction cross language information retrieval clir has somewhat uncharitably been referred to as the problem ontologies description and applications the word ontology has gained a good popularity within the ai community ontology is usually viewed as a high level description consisting of concepts that organize the upper parts of the knowledge base constraints and agents in madesmart as part of the darpa rapid design exploration and optimization radeo program boeing philadelphia is involved in an on going concurrent design engineering research project called madesmart which seeks to partially automate the integrated product team ipt concept used by boeing for organizing the design engineering process with the aid of intelligent agent technology although currently only in an early stage of development the project is expected to crucially employ a constraint centered system design management agent developed by the university of toronto s ie department in conjunction with boeing the sdma will use the constraint based toronto ontologies for a virtual enterprise tove ontologies and its domain theories for design engineering and dependent underlying theories phrased as kif ontolingua assertions in an axiomatic system running in the constraint logic system eclipse as its primary knowledge resource to monitor an ongoing design project offering resource all specifying rational agents with statecharts and utility functions abstract to aid the development of the robotic soccer simulation league team robolog 2000 a method for the specification of multi agent teams by statecharts has been introduced the results in the last years competitions showed that though the team was competitive it did not behave adaptive in unknown situations the design of adaptive agents with this method is possible but not in a straightforward manner the purpose of this paper is to extend the approach by a more adaptive action selection mechanism and to facilitate a more explicit representation of goals of an agent 1 preliminary investigation of wearable computers for task guidance in aircraft inspection this paper describes a preliminary investigation of how the capabilities of wearable computers may be used to provide task guidance in mobile environments specifically this study examined how the capabilities of wearable computers may be used to aid a user in an inspection task using as a case study the procedural task of preflight inspection of a general aviation aircraft two different configurations of a computer based voiceactivated task guidance system and the current method of preflight inspection were compared and evaluated initial results demonstrate an over reliance on the computer by the pilots and indicate the importance of the user interface design to the performance of the inspectors the paper concludes with recommendations on promising directions of research keywords task guidance procedural tasks aircraft inspection computerized procedures decision aiding wearable computers 1 introduction wearable computers combine portable voiceactivated wireless netw efficient and cost effective techniques for browsing and indexing large video databases we present in this paper a fully automatic content based approach to organizing and indexing video data our methodology involves three steps ffl step 1 we segment each video into shots using a camera tracking technique this process also extracts the feature vector for each shot which consists of two statistical variances v ar ba and v ar oa these values capture how much things are changing in the background and foreground areas of the video shot ffl step 2 for each video we apply a fully automatic method to build a browsing hierarchy using the shots identified in step 1 ffl step 3 using the v ar ba and v ar oa values obtained in step 1 we build an index table to support a variance based video similarity model that is video scenes shots are retrieved based on given values of v ar ba and v ar oa the above three inter related techniques offer an integrated framework for modeling browsing and searching large video databases our experimental results indic supporting cross cultural communication with a large screen system as opportunities for international collaboration and crosscultural communication among people from heterogeneous cultures increase the importance of electronic communication support is increasing to support cross cultural communication we believe it is necessary to offer environments in which participants enjoy conversations which allow them to share one another s background and profile visually we believe that the following three functions are important 1 showing topics based on participants profiles and cultural background 2 life sized large screen interface and 3 displaying objects which show feelings of identify in this paper we discuss the implementation and the empirical evaluation of two systems that were designed to support cross cultural communication in the real world or between remote locations from the empirical evaluation of these systems we conclude that these systems add new functionality to support conversation contents which may be especially useful in a cross cultural context where language skills are an issue and this type of environment may be especially useful in a pre collaboration context temporal aspects of semistructured data in many applications information about the history of data and their dynamic aspects are just as important as static information during the last years the increasing amount of information accessible through the web has presented new challenges to academic and industrial research on database in this context data are either structured when coming from relational or object oriented databases or partially or completely unstructured when they consist of simple collections of text or image files in the context of semistructured data model and query languages must be extended in order to consider dynamic aspects we present a model based on labeled graphs for representing changes in semistructured data and a sql like query language for querying it agents advanced features for negotiation in electronic commerce and virtual organisations formation process electronic commerce technology has changed the way traditional business is being done transactions complexity is increased due both to the huge amount of available information and also to the environment dynamics moreover electronic commerce has enabled the arising of new economical structures as it is the case of virtual organisations our research aims at providing flexible and general purpose systems for intelligent negotiation both for electronic commerce and virtual organisation formation this paper proposes an electronic market architecture implemented through a multi agent system this architecture includes both a specific market agent which plays the role of market coordinator as well as agents representing the individual business partners with their own goals and strategies we also include a sophisticated negotiation protocol through multi criteria and distributed constraint formalisms an online continuous reinforcement learning algorithm has been designed to enab graphical models for recognizing human interactions we describe a real time computer vision and machine learning system for modeling and recognizing human actions and interactions two different domains are explored recognition of two handed motions in the martial art tai chi and multiple person interactions in a visual surveillance task our system combines top down with bottom up information using a feedback loop and is formulated with a bayesian framework two different graphical models hmms and coupled hmms are used for modeling both individual actions and multiple agent interactions and chmms are shown to work more efficiently and accurately for a given amount of training finally to overcome the limited amounts of training data we demonstrate that synthetic agents alife style agents can be used to develop flexible prior models of the person to person interactions 1 introduction we describe a real time computer vision and machine learning system for modeling and recognizing human behaviors in two different scenari statistical modeling of human interactions in this paper we describe a real time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task the system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction examples of interesting interaction behaviors include following another person altering one s path to meet another and so forth our system combines top down with bottom up information in a closed feedback loop with both components employing a statistical bayesian approach we propose and compare two different state based learning architectures namely hmms and chmms for modeling behaviors and interactions the chmm model is shown to work much more efficiently and accurately finally a synthetic agent training system is used to develop a priori models for recognizing human behaviors and interactions we demonstrate the ability to use these a priori models to accurately classify real human beha a bayesian computer vision system for modeling human interactions abstract we describe a real time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task 1 the system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction examples of interesting interaction behaviors include following another person altering one s path to meet another and so forth our system combines top down with bottom up information in a closed feedback loop with both components employing a statistical bayesian approach 2 we propose and compare two different state based learning architectures namely hmms and chmms for modeling behaviors and interactions the chmm model is shown to work much more efficiently and accurately finally to deal with the problem of limited training data a synthetic alife style training system is used to develop flexible prior models for recognizing human interactions we demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training index terms visual surveillance people detection tracking human behavior recognition hidden markov models 1 probabilistic self localization for mobile robots localization is a critical issue in mobile robotics if the robot does not know where it is it cannot effectively plan movements locate objects or reach goals in this paper we describe probabilistic self localization techniques for mobile robots that are based on the principal of maximum likelihood estimation the basic method is to compare a map generated at the current robot position to a previously generated map of the environment to prohabilistically maximize the agreement between the maps this method is able to operate in both indoor and outdoor environments using either discrete features or an occupancy grid to represent the world map the map may be generated using any method to detect features in the robot s surroundings including vision sonar a d laser range finder a global search of the pose space is performed that guarantees that the best position in a discretized pose space is found according to the probabilistic map agreement measure in addition fitting the likelihood function with a parameterized smface allows both subpixel localization and uncertainty estimation to be performed the application of these techniques in several experiments is described including experimental localization results for the sojourner mars rover 1 a probabilistic formulation for hausdorff matching matching images based on a hausdorff measure has become popular for computer vision applications however no probabilistic model has been used in these applications this limits the formal treatment of several issues such as feature uncertainties and prior knowledge in this paper we develop a probabilistic formulation of image matching in terms of maximum likelihood estimation that generalizes a version of hausdorff matching this formulation yields several benefits with respect to previous hausdorff matching formulations in addition we show that the optimal model position in a discretized pose space can be located efficiently in this formation and we apply these techniques to a mobile robot self localization problem 1 introduction the use of variants of the hausdorff distance has recently become popular for image matching applications see for example 6 9 11 16 18 19 while these methods have been largely successful they have lacked a probabilistic formulation of th subpixel localization and uncertainty estimation using occupancy grids we describe techniques for performing mobile robot localization using occupancy grids that allow subpixel localization and uncertainty estimation in the pixelized pose space the techniques are based on a localization method where matching is performed between the visible landmarks at the current robot position and a previously generated map of the environment a likelihood function over the space of possible robot positions is formulated as a function of the probability distribution for the map matching error subpixel localization and uncertainty estimation are performed by fitting the likelihood function with a parameterized surface the performance of the method is analyzed using synthetic experiments and an example is given using the rocky 7 mars rover prototype 1 introduction localization is a critical issue in mobile robotics if the robot does not know where it is it cannot effectively plan movements locate objects or reach goals it is important to not only perform accur a java corba based visual program composition environment for pses a problem solving environment pse is a complete integrated computing environment for composing compiling and running applications in a specific problem area or domain parts of the pse are domain independent such as the visual programming composition environment vpce which may be used for constructing application in a number of different domains however other parts are domain specific such as rules to support particular types of components a domain independent vpce is first described which serves as a user interface for a pse and uses java and corba to provide a framework of tools to enable the construction of scientific applications from components the vpce consists of a component repository from which the user can select off the shelf or in house components a graphical composition area on which components can be combined various tools that facilitate the configuration of components the integration of legacy codes into components and the design and bui equivalence in knowledge representation automata recurrent neural networks and dynamical fuzzy systems neurofuzzy systems the combination of artificial neural networks with fuzzy logic have become useful in many application domains however conventional neurofuzzy models usually need enhanced representation power for applications that require context and state e g speech time series prediction control some of these applications can be readily modeled as finite state automata previously it was proved that deterministic finite state automata dfa can be synthesized by or mapped into recurrent neural networks by directly programming the dfa structure into the weights of the neural network based on those results a synthesis method is proposed for mapping fuzzy finite state automata ffa into recurrent neural networks furthermore this mapping is suitable for direct implementation in very large scale integration vlsi i e the encoding of ffa as a generalization of the encoding of dfa in vlsi systems the synthesis method requires ffa to undergo a transformation prior to being mapped into recurrent networks the neurons are provided with an enriched functionality in order to accommodate a fuzzy representation of ffa states this enriched neuron functionality also permits fuzzy parameters of ffa to be directly represented as parameters of the neural network we also prove the stability of fuzzy finite state dynamics of the constructed neural networks for finite values of network weight and through simulations give empirical validation of the proofs hence we prove various knowledge equivalence representations between neural and fuzzy systems and models of automata an asymptotic analysis of adaboost in the binary classification case recent work has shown that combining multiple versions of weak classifiers such as decision trees or neural networks results in reduced test set error to study this in greater detail we analyze the asymptotic behavior of adaboost type algorithms the theoretical analysis establishes the relation between the distribution of margins of the training examples and the generated voting classification rule the paper shows asymptotic experimental results for the binary classification case underlining the theoretical findings finally the relation between the model complexity and noise in the training data and how to improve adaboost type algorithms in practice are discussed 1 introduction an ensemble is a collection of neural networks or other types of classifiers predictors that are trained for the same task boosting and other ensemble learning methods have been used recently with great success for several applications e g ocr 6 4 in this work we investigate the functioning o discovering interesting association rules in medical data we are presently exploring the idea of discovering association rules in medical data there are several technical aspects which make this problem challenging in our case medical data sets are small but have high dimensionality information content is rich there exist numerical categorical time and even image attributes data records are generally noisy we explain how to map medical data to a transaction format suitable for mining rules the combinatorial nature of association rules matches our needs but current algorithms are unsuitable for our purpose we thereby introduce an improved algorithm to discover association rules in medical data which incorporates several important constraints some interesting results obtained by our program are discussed and we explain how the program parameters were set we believe many of the problems we come across are likely to appear in other domains 1 introduction data mining is an active research area one of the most popular approaches t mining constrained association rules to predict heart disease this work describes our experiences on discovering association rules in medical data to predict heart disease we focus on two aspects in this work mapping medical data to a transaction format suitable for mining association rules and identifying useful constraints based on these aspects we introduce an improved algorithm to discover constrained association rules we present an experimental section explaining several interesting discovered rules 1 discovering association rules based on image content our focus for data mining in this paper is concerned with knowledge discovery in image databases we present a data mining algorithm to find association rules in 2 dimensional color images the algorithm has four major steps feature extraction object identification auxiliary image creation and object mining our emphasis is on data mining of image content without the use of auxiliary domain knowledge the purpose of our experiments is to explore the feasibility of this approach a synthetic image set containing geometric shapes was generated to test our initial algorithm implementation our experimental results show that there is promise in image mining based on content we compare these results against the rules obtained from manually identifying the shapes we analyze the reasons for discrepancies we also suggest directions for future work 1 introduction discovering knowledge from data stored in typical alphanumeric databases such as relational databases has been the focal p defining views in an image database system a view mechanism can help handle the complex semantics in emerging application areas such as image databases this paper presents the view mechanism we defined for the disima image database system since disima is being developed on top of an object oriented database system we first propose apowerful object oriented view mechanism based on the separation between types interface functions and classes that manage objects of the same type the image view mechanism uses our object oriented view mechanism to allow us to give differentsemantics to the same image the solution is based on the distinction between physical salient objects which are interesting objects in an image and logical salient objects which are the meanings of these objects 14 1 introduction views have been widely used in relational database management systems to extend modeling capabilities and to provide data independence basically views in a relational database can be seen as formulae defining virtua visualmoql the disima visual query language multimedia data are now available to a variety of users ranging from naive to sophisticated to make querying easy visual query languages have been proposed most of these languages have a low expressive power and have their own query processors efforts have been made to design query languages with proper semantics to facilitate query optimization and processing in existing database systems the majority of multimedia database systems are built on top of object or object relational database systems with the underlying query facilities inherited the disima system is being built on top of a commercial oodbms and we have chosen to extend the standard object oriented query language oql with some multimedia functionalities the resulting language is called moql this paper presents visualmoql a visual query language implementing the image component of moql 1 introduction in this paper we present the visual query interface visualmoql of the disima distributed image database managemen learning and tracking cyclic human motion we present methods for learning and tracking human motion in video we estimate a statistical model of typical activities from a large set of 3d periodic human motion data by segmenting these data automatically into cycles then the mean and the principal components of the cycles are computed using a new algorithm that accounts for missing information and enforces smooth transitions between cycles the learned temporal model provides a prior probability distribution over human motions that can be used in a bayesian framework for tracking human subjects in complex monocular video sequences and recovering their 3d motion a cellular neural associative array for symbolic vision a system which combines the descriptional power of symbolic representations with the parallel and distributed processing model of cellular automata and the speed and robustness of connectionist symbol processing is described following a cellular automata based approach the aim of the system is to transform initial symbolic descriptions of patterns to corresponding object level descriptions in order to identify patterns in complex or noisy scenes a learning algorithm based on a hierarchical structural analysis is used to learn symbolic descriptions of objects the underlying symbolic processing engine of the system is a neural based associative memory aura which use enables the system to operate in high speed in addition the use of distributed representations allow both efficient inter cellular communications and compact storage of rules 1 introduction one of the basic features of syntactic and structural pattern recognition systems is the use of the structure of the patterns a cellular system for pattern recognition using associative neural networks a cellular system for pattern recognition is presented in this paper the cells are placed in a two dimensional array and they are capable of performing basic symbolic processing and exchanging messages about their state following a cellular automata like operation the aim of the system is to transform an initial symbolic description of a pattern to a correspondent object level representation to this end a hierarchical approach for the description of the structure of the patterns is followed the underlying processing engine of the system is the aura model of associative memory the system is endowed with a learning mechanism utilizing the distributed nature of the architecture a dedicated hardware platform is also available 1 introduction one of the basic characteristics of cellular automata 1 is their ability for parallel and distributed processing this is due to the co operation of relatively simple interconnected processing units called cells these are connected followin the smart floor a mechanism for natural user identification and tracking we have created a system for identifying people based on their footstep force profiles and have tested its accuracy against a large pool of footstep data this floor system may be used to identify users transparently in their everyday living and working environments we have created user footstep models based on footstep profile features and have been able to achieve a recognition rate of 93 we have also shown that the effect of footwear is negligible on recognition accuracy keywords interaction technology ubiquitous computing user identification biometrics novel input introduction in the smart floor project we have created and validated a system for biometric user identification based on footstep profiles we have outfitted a floor tile with force measuring sensors and are using the data gathered as users walk over the tile to identify them we rely on the uniqueness of footstep profiles within a small group of people to provide recognition accuracy similar to other biome multi agent support for internet scale grid management internet scale computational grids are emerging from various research projects most notably are the us national technology grid and the european data grid projects one specific problem in realizing wide area distributed computing environments as proposed in these projects is effective management of the vast amount of resources that are made available within the grid environment this paper proposes an agent based approach to resource management in grid environments and describes an agent infrastructure that could be integrated with the grid middleware layer this agent infrastructure provides support for mobile agents that is scalable in the number of agents and the number of resources ten myths of multimodal interaction cooperative information agents for digital cities this paper presents an architecture for digital cities and shows the roles of agent keyword spices a new method for building domain specific web search engines this paper presents a new method for building domain specific web search engines previous methods eliminate irrelevant documents from the pages accessed using heuristics based on human knowledge about the domain in question accordingly they are hard to build and can not be applied to other domains the keyword spice method in contrast improves search performance by adding cyclic association rules we study the problem of discovering association rules that display regular cyclic variation over time for example if we compute association rules over monthly sales data we may observe seasonal variation where certain rules are true at approximately the same month each year similarly association rules can also display regular hourly daily weekly etc variation that is cyclical in nature we demonstrate that existing methods cannot be naively extended to solve this problem of cyclic association rules we then present two new algorithms for discovering such rules the first one which we call the sequential algorithm treats association rules and cycles more or less independently by studying the interaction between association rules and time we devise a new technique called cycle pruning which reduces the amount of time needed to find cyclic association rules the second algorithm which we call the interleaved algorithm uses cycle pruning and other optimization techniques f a simple neural network models categorical perception of facial expressions the performance of a neural network that categorizes facial expressions is compared with human subjects over a set of experiments using interpolated imagery the experiments for both the human subjects and neural networks make use of interpolations of facial expressions from the pictures of facial affect database ekman and friesen 1976 the only difference in materials between those used in the human subjects experiments young et al 1997 and our materials are the manner in which the interpolated images are constructed image quality morphs versus pixel averages nevertheless the neural network accurately captures the categorical nature of the human responses showing sharp transitions in labeling of images along the interpolated sequence crucially for a demonstration of categorical perception harnad 1987 the model shows the highest discrimination between transition images at the crossover point the model also captures the shape of the reaction time curves of the human s the pagerank citation ranking bringing order to the web the importance of a web page is an inherently subjective matter which depends on the readers interests knowledge and attitudes but there is still much that can be said objectively about the relative importance of web pages this paper describes pagerank a method for rating web pages objectively and mechanically effectively measuring the human interest and attention devoted to them we compare pagerank to an idealized random web surfer we show how to efficiently compute pagerank for large numbers of pages and we show how to apply pagerank to search and to user navigation evolving personal agent environments to reduce internet information overload initial considerations this paper we will address the other major information overload problem arising from habitual surfing we will present our proposed research programme and also outline and support a potential solution so far this issue has been far less visited and researched though with the ever increasing use of computers at home and in the workplace as well as the imminent arrival of information appliances this may well change 6 rough fuzzy mlp modular evolution rule generation and evaluation a methodology is described for evolving a rough fuzzy multi layer perceptron with modular concept using a genetic algorithm to obtain a structured network suitable for both classification and rule extraction the modular concept based on divide and conquer strategy provides accelerated training and a compact network suitable for generating a minimum number of rules with high certainty values the concept of variable mutation operator is introduced for preserving the localized structure of the constituting knowledge based subnetworks while they are integrated and evolved rough set dependency rules are generated directly from the real valued attribute table containing fuzzy membership values two new indices viz certainty and confusion in a decision are defined for evaluating quantitatively the quality of rules the effectiveness of the model and the rule extraction algorithm is extensively demonstrated through experiments alongwith comparisons web mining in soft computing framework relevance state of the art and future directions this paper summarizes the different characteristics of web data the basic components of web mining and its different types and their current states of the art the reason for considering web mining a separate field from data mining is explained the limitations of some of the existing web mining methods and tools are enunciated and the significance of soft computing comprising fuzzy logic fl artificial neural networks anns genetic algorithms gas and rough sets rss highlighted a survey of the existing literature on soft web mining is provided along with the commercially available systems the prospective areas of web mining where the application of soft computing needs immediate attention are outlined with justification scope for future research in developing soft web mining systems is explained an extensive bibliography is also provided the isomorphism between a class of place transition nets and a multi plane state machine agent model recently we introduced a multi plane state machine model of an agent released an implementation of the model and designed several applications of the agent framework in this paper we address the translation from the petri net language to the blueprint language used for agent description as well as the translation from blueprint to petri nets the simulation of a class of place transition nets is part of an effort to create an agent based workflow management system contents 1 introduction 1 2 a multi plane state machine agent model 3 2 1 bond core 3 2 2 bond services 4 2 3 bond agents 5 2 4 using planes to implement facets of behavior 5 3 simulating a class of place transition nets on the bond social individual technological issues for groupware calendar systems designing and deploying groupware is difficult groupware evaluation and design are often approached from a single perspective with a technologically individually or socially centered focus a study of groupware calendar systems gcss highlights the need for a synthesis of these multiple perspectives to fully understand the adoption challenges these systems face first gcss often replace existing calendar artifacts which can impact users calendaring habits and in turn influence technology adoption decisions second electronic calendars have the potential to easily share contextualized information publicly over the computer network creating opportunities for peer judgment about time allocation and raising concerns about privacy regulation however this situation may also support coordination by allowing others to make useful inferences about one s schedule third the technology and the social environment are in a reciprocal co evolutionary relationship the use context is affected by the constraints andaffordances of the technology and the technology also co adapts to the environment in important ways finally gcss despite being below the horizon of everyday notice can affect the nature of temporal coordination beyond the expected meeting scheduling practice prosody modeling in concept to speech generation methodological issues generation of intensive care data a system that generates multimedia briefings of a patient s status after having a bypass operation dalal et al 1996 mckeown et al 1997 we first describe information magic generates in the process of producing language turning next to the corpora we collected we then provide a description of the more traditional approach to prosody modeling using machine learning that generalizes over many examples followed by a description of our memory based approach our results show the memory based approach yields a better improvement in quality measured through subjective judgments of output 2 information from language generation in the course of producing language language generators typically produce a variety of intermediate linguistic representations that contain information which potentially could influence prosody some of this information is similar to the kind of information used in tts such as part of speech pos tags or syntactic cons an intelligent agent framework in vrml worlds actions e g move to next room are received by it and consequently send to the eac finally the abstract action arrives at the virtual reality management unit that specifies in detail the received actions it provides specific values concerning the orientation and position of the avatar e g it specifies the coordinates orientation and path so that it can successfully move to the next room and sends them as commands to the virtual reality world browser the browser executes the command by altering the virtual environment appropriately when changes have been performed the aec unit notifies the logical core that the action has been successfully executed and the logical core goes on by updating its internal and external state consequently the agent looks around into the virtual space gathers any additional information and decides the next step it should take to satisfy its goals using pagerank to characterize web structure recent work on modeling the web graph has dwelt on capturing the degree distributions observed on the web pointing out that this represents a heavy reliance on local properties of the web graph we study the distribution of pagerank values used in the google search engine on the web this distribution is of independent interest in optimizing search indices and storage we show that pagerank values on the web follow a power law we then develop detailed models for the web graph that explain this observation and moreover remain faithful to previously studied degree distributions we analyze these models and compare the analyses to both snapshots from the web and to graphs generated by simulations on the new models to our knowledge this represents the first modeling of the web that goes beyond fitting degree distributions on the web exploration versus exploitation in topic driven crawlers topic driven crawlers are increasingly seen as a way to address the scalability limitations of universal search engines by distributing the crawling process across users queries or even client computers the context available to a topic driven crawler allows for informed decisions about how to prioritize the links to be explored given time and bandwidth constraints we have developed a framework and a number of methods to evaluate the performance of topic driven crawler algorithms in a fair way under limited memory resources quality metrics are derived from lexical features link analysis and a hybrid combination of the two in this paper we focus on the issue of how greedy a crawler should be given noisy quality estimates of links in a frontier we investigate what is an appropriate balance between a crawler s need to exploit this information to focus on the most promising links and the need to explore links that appear suboptimal but might lead to more relevant pages we show that exploration is essential to locate the most relevant pages under a number of quality measures in spite of a penalty in the early stage of the crawl myspiders evolve your own intelligent web crawlers abstract the dynamic nature of the world wide web makes it a challenge to find information that is bothrelevant and recent intelligent agents can complement the power of searchengines to meet this challenge we present a web tool called myspiders which implements an evolutionary algorithm managing a population of adaptive crawlers who browse the web autonomously each agent acts as an intelligent client on behalf of the user driven by a user query and by textual and linkage clues in the crawled pages agents autonomously decide which links to follow which clues to internalize when to spawn offspring to focus the search near a relevant source and when to starve the tool is available to the public as a threaded java applet we discuss the development and deployment of such a system keywords web informational retrieval topic driver crawlers online search infospiders myspiders applet expert system for automatic analysis of facial expressions this paper discusses our expert system called integrated system for facial expression recognition isfer which performs recognition and emotional classification of human facial expression from a still full face image the system consists of two major parts the first one is the isfer workbench which forms a framework for hybrid facial feature detection multiple feature detection techniques are applied in parallel the redundant information is used to define unambiguous face geometry containing no missing or highly inaccurate data the second part of the system is its inference engine called hercules which converts low level face geometry into high level facial actions and then this into highest level weighted emotion labels the organisation of sociality a manifesto for a new science of multiagent systems in this paper we pose and motivate a challenge namely the need for a new science of multiagent systems we propose that this new science should be grounded theoretically on a richer conception of sociality and methodologically on the extensive use of computational modelling for real world applications and social simulations here the steps we set forth towards meeting that challenge are mainly theoretical in this respect we provide a new model of multi agent systems that reflects a fully explicated conception of cognition both at the individual and the collective level finally the mechanisms and principles underpinning the model will be examined with particular emphasis on the contributions provided by contemporary organisation theory 1 social mental shaping modelling the impact of sociality on the mental states of autonomous agents this paper presents a framework that captures how the social nature of agents that are situated in a multi agent environment impacts upon their individual mental states roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping this allows us to extend the standard belief desireintention model to account for how common social phenomena e g cooperation collaborative problem solving and negotiation can be integrated into a unified theoretical perspective that reflects a fully explicated model of the autonomous agent s mental state keywords multi agent systems agent interactions bdi models social influence 3 1 formalizing collaborative decision making and practical reasoning in multi agent systems in this paper we present an abstract formal model of decision making in a social setting that covers all aspects of the process from recognition of a potential for cooperation through to joint decision in a multi agent environment where self motivated autonomous agents try to pursue their own goals a joint decision cannot be taken for granted in order to decide effectively agents need the ability to a represent and maintain a model of their own mental attitudes b reason about other agents mental attitudes and c influence other agents mental states social mental shaping is advocated as a general mechanism for attempting to have an impact on agents mental states in order to increase their cooperativeness towards a joint decision our approach is to specify a novel high level architecture for collaborative decision making in which the mentalistic notions of belief desire goal intention preference and commitment play a central role in guiding the individual agent s and the group s decision making behaviour we identify preconditions that must be fulfilled before collaborative decision making can commence and prescribe how cooperating agents should behave in terms of their own decision making apparatus and their interactions with others when the decision making process is progressing satisfactorily the model is formalized through a new many sorted multi modal logic modeling sociality in the bdi framework we present a conceptual model for how the social nature of agents impacts upon their individual mental states roles and social relationships provide an abstraction upon which we develop the notion of social mental shaping 1 introduction belief desire intention bdi architectures for deliberative agents are based on the physical symbol system assumption that agents maintain and reason about internal representations of their world 2 however while such architectures conceptualise individual intentionality and behaviour they say nothing about the social aspects of agents being situated in a multi agent system the main reason for this limitation is that mental attitudes are taken to be internal to a particular agent or team and are modeled as a relation between the agent or a team and a proposition the purpose of this paper is therefore to extend bdi models in order to investigate the problem of how the social nature of agents can impact upon their individual mental efficient olap operations in spatial data warehouses abstract spatial databases store information about the position of individual objects in space in many applications however such as traffic supervision or mobile communications only summarized data like the number of cars in an area or phones serviced by a cell is required although this information can be obtained from transactional spatial databases its computation is expensive rendering online processing inapplicable driven by the non spatial paradigm spatial data warehouses can be constructed to accelerate spatial olap operations in this paper we consider the star schema and we focus on the spatial dimensions unlike the non spatial case the groupings and the hierarchies can be numerous and unknown at design time therefore the wellknown materialization techniques are not directly applicable in order to address this problem we construct an ad hoc grouping hierarchy based on the spatial index at the finest spatial granularity we incorporate this hierarchy in the lattice model and present efficient methods to process arbitrary aggregations we finally extend our technique to moving objects by employing incremental update methods 1 indexing spatio temporal data warehouses spatio temporal databases store information about the positions of individual objects over time in many applications however such as traffic supervision or mobile communication systems only summarized data like the average number of cars in an area for a specific period or phones serviced by a cell each day is required although this information can be obtained from operational databases its computation is expensive rendering online processing inapplicable a vital solution is the construction of a spatiotemporal data warehouse in this paper we describe a framework for supporting olap operations over spatiotemporal data we argue that the spatial and temporal dimensions should be modeled as a combined dimension on the data cube and present data structures which integrate spatiotemporal indexing with pre aggregation while the well known materialization techniques require a priori knowledge of the grouping hierarchy we develop methods that utilize the proposed structures for efficient execution of ad hoc group bys our techniques can be used for both static and dynamic dimensions 1 constraint based processing of multiway spatial joins a multiway spatial join combines information found in three or more spatial relations with respect to some spatial predicates motivated by their close correspondence with constraint satisfaction problems csps we show how multiway spatial joins can be processed by systematic search algorithms traditionally used for csps this paper describes two different strategies window reduction and synchronous traversal that take advantage of underlying spatial indexes to effectively prune the search space in addition we provide cost models and optimization methods that combine the two strategies to compute more efficient execution plans finally we evaluate the efficiency of the proposed techniques and the accuracy of the cost models through extensive experimentation with several query and data combinations key words spatial databases spatial joins constraint satisfaction r trees 1 introduction spatial dbmss and giss store large amounts of multi dimensional data such as points manufacturing systems integration and agility can mobile agents help mobile code is being championed as a solution to a plethora of software problems this paper investigates whether mobile agents and mobile objects support improved system integration and agility in the manufacturing domain we describe two systems built to support the sales order process of a distributed manufacturing enterprise using ibm s aglets software development kit the sales order process model and the requirements for agility used as the basis for these implementations are derived from data collected in an industrial case study both systems are evaluated using the goal question metric methodology two new metrics for semantic alignment and change capability are presented and used to evaluate each system with respect to the degree of system agility supported the work described provides evidence that both mobile agent and mobile object systems have inherent properties that can be used to build agile distributed systems further mobile agents with their additional autonomy query rewriting using semistructured views we address the problem of query rewriting for msl a semistructured language developed at stanford in the tsimmis project for information integration we develop and present an algorithm that given a semistructured query q and a set of semistructured views v finds rewriting queries i e queries that access the views and produce the same result as q our algorithm is based on appropriately generalizing containment mappings the chase and unification techniques which were developed for structured relational data at the same time we develop an algorithm for equivalence checking of msl queries we show that the rewriting algorithm is sound and complete i e it always finds every conjunctive msl rewriting query of q and we discuss its complexity we currently incorporate the algorithm in the tsimmis system 1 introduction recently many semistructured data models query and view definition languages have been proposed gm 97 mag 97 bdhs96 av97a mm97 ks95 pgmu96 contextualizing the information space in federated digital libraries rapid growth in the volume of documents their diversity and terminological variations render federated digital libraries increasingly difficult to manage suitable abstraction mechanisms are required to construct meaningful and scalable document clusters forming a cross digital library information space for browsing and semantic searching this paper addresses the above issues proposes a distributed semantic framework that achieves a logical partitioning of the information space according to topic areas and provides facilities to contextualize and landscape the available document sets in subject specific categories a language for publishing virtual documents on the web the web is creating exciting new possibilities for direct and instantaneous publishing of information however the apparent ease with which one can publish documents on the web hides more complex issues such updating and maintaining web pages we believe one of the crucial requirements to document delivery is the ability to extract and reuse information from other documents or sources in this paper we present a descriptive language that allows users to write virtual documents where dynamic information can be retrieved from various sources transformed and included along with static information in html documents the language uses a tree like structure for the representation of information and defines a database like query language for extracting and combining information without a complete knowledge of the structure or the types of information the data structures and the syntax of the language are presented along with examples 1 introduction in recent years the web has grown fr a virtual document interpreter for reuse of information the importance of reuse of information is well recognised for electronic publishing however it is rarely achieved satisfactorily because of the complexity of the task integrating different formats handling updates of information addressing document author s need for intuitiveness and simplicity etc an approach which addresses these problems is to dynamically generate and update documents through a descriptive definition of virtual documents in this paper we present a document interpreter that allows gathering information from multiple sources and combining it dynamically to produce a virtual document two strengths of our approach are the generic information objects that we use which enables access to distributed heterogeneous data sources and the interpreter s evaluation strategy which permits a minimum of re evaluation of the information objects from the data sources keywords virtual documents information reuse active documents document synthesis 1 introducti design and implementation of expressive footwear as an outgrowth of our interest in dense wireless sensing and expressive applications of wearable computing we have developed the world s most versatile human computer interface for the foot by dense wireless sensing we mean the remote acquisition of many different parameters with a compact autonomous sensor cluster we have developed such a low power sensor card to measure over 16 continuous quantities and transmit them wirelessly to a remote base station updating all variables at 50 hz we have integrated a pair of these devices onto the feet of dancers and athletes measuring continuous pressure at 3 points near the toe dynamic pressure at the heel bidirectional bend of the sole height of each foot off conducting strips in the stage angular rate of each foot about the vertical angular position of each foot about the earth s local magnetic field as well as their tilt and low g acceleration 3 axis shock acceleration from kicks and jumps and position via an integrated s footnotes personal reflections on the development of instrumented dance shoes and their musical applications this paper describes experiences in designing and developing an extremely versatile multimodal sensor interface built entirely into a pair of shoes i discuss the system design trace its motivations and goals then describe its applications in interactive music for dance performance summarizing lessons learned and future possibilities 1 the inspiration although the idea of instrumenting shoes for interactive music performance had crossed my mind before the moment at which i decided to pursue this project can be traced to a demonstration that i attended with my media lab colleague tod machover in november of 1996 we were visiting some of our research sponsors and colleagues at a yamaha development laboratory in the shinjuku section of tokyo where they showed us the latest version of their miburi musical controller 1 the miburi is an electronic vest with bend sensors at various joints to monitor dynamic articulation and a pair of ha the cybershoe a wireless multisensor interface for a dancer s feet as a bridge between our interest in wearable computer systems and new performance interfaces for digital music we have built a highly instrumented pair of sneakers for interactive dance these shoes each measure 16 different continuous parameters expressed by each foot and are able to transmit them wirelessly to a base station placed well over 30 meters away updating all values up to 60 times per second this paper describes our system illustrates its performance and outlines a few musical mappings that we have created for demonstrations in computer augmented dance electronic sensors have been incorporated into footwear for several different applications over the last several years employing force sensing resistor arrays or pixelated capacitive sensing insoles with very dense pressure sampling have been developed for research at the laboratories of footwear manufacturers and pediatric treatment facilities cavanaugh et al 1992 alth interactive music for instrumented dancing shoes we have designed and built a pair of sneakers that each sense 16 different tactile and free gesture parameters these include continuous pressure at 3 points in the forward sole dynamic pressure at the heel bidirectional bend of the sole height above instrumented portions of the floor 3 axis orientation about the earth s magnetic field 2 axis gravitational tilt and low g acceleration 3 axis shock angular rate about the vertical and translational position via a sonar transponder both shoes transfer these parameters to a base station across an rf link at 50 hz state updates as they are powered by a local battery there are no tethers or wires running off the shoe a pc monitors the data streaming off both shoes and translates it into real time interactive music the shoe design is introduced and the interactive music mappings that we have developed for dance performances are discussed 1 introduction a trained dancer is capable of expressing highly dexterous control constructive neural network learning algorithms for multi category pattern classification constructive learning algorithms offer an attractive approach for the incremental construction of near minimal neural network architectures for pattern classification they help overcome the need for ad hoc and often inappropriate choices of network topology in algorithms that search for suitable weights in a priori fixed network architectures several such algorithms are proposed in the literature and shown to converge to zero classification errors under certain assumptions on tasks that involve learning a binary to binary mapping i e classification problems involving binary valued input attributes and two output categories we present two constructive learning algorithms mpyramid real and mtiling real that extend the pyramid and tiling algorithms respectively for learning real to m ary mappings i e classification problems involving real valued input attributes and multiple output classes we prove the convergence of these algorithms and empirically demonstrate their applicability to practical pattern classification problems additionally we show how the incorporation of a local pruning step can eliminate several redundant neurons from mtiling real networks common sense and maximum entropy this paper concerns the question of how to draw inferences common sensically from uncertain knowledge since the early work of shore and johnson 10 paris and vencovsk a 6 and csisz r 1 it has been known that the maximum entropy inference process is the only inference process which obeys certain common sense principles of uncertain reasoning in this paper we consider the present status of this result and argue that within the rather narrow context in which we work this complete and consistent mode of uncertain reasoning is actually characterised by the observance of just a single common sense principle or slogan analysis and extraction of useful information across networks of web databases contents 1 introduction 2 2 problem statement 2 3 literature review 3 3 1 retrieving text 3 3 2 understanding music 7 3 3 identifying images 9 3 4 extracting video 11 4 work completed and in progress 12 5 research plan and time line 14 a list of published work 15 1 1 introduction 2 1 introduction the world wide web of documents on the internet contains a huge amount of information and resources it has been growing at a rapid rate for nearly a decade and is now one of the main resources of information for many people the large interest in the web is due to the fact that it is uncontrolled and easily accessible no single person owns it and anyone can add to it the web has also brought with it a lot of controversy also due to the segment based approach for subsequence searches in sequence databases this paper investigates the subsequence searching problem under time warping in sequence databases time warping enables to find sequences with similar changing patterns even when they are of different lengths our work is motivated by the observation that subsequence searches slow down quadratically as the total length of data sequences increases to resolve this problem we propose the segmentbased approach for subsequence searches sbass which modifies the similarity measure from time warping to piecewise time warping and limits the number of possible subsequences to be compared with a query sequence for efficient retrieval of similar subsequences without false dismissal 1 we extract feature vectors from all data segments exploiting their monotonically changing properties and build a multi dimensional index such as r tree or r tree using this index queries are processed with four steps 1 index filtering 2 feature filtering 3 successor filtering and 4 post proce alternative correctness criteria for multiversion concurrency control and a locking protocol via freezing concurrency control protocols based on multiversions have been used in some commercial transaction processing systems in order to provide the serializable executions of transactions in the existing protocols transactions are allowed to read only the most recent version of each data item in order to ensure the correct execution of transactions however this feature is not desirable in some advanced database systems which have more requirements such as timing or security constraints besides serializability in this paper we propose a new correctness criteria called f serializability for multiversion concurrency control protocols it is the extended definition of 1 serial and relaxes the condition so that a protocol provides not only the most recent version but also the previous one to transactions if necessary we prove that whenever a multiversion schedule is f serializable the schedule is also one copy serializable this is the first contribution of our paper next we propos a cost based object buffer replacement algorithm for object oriented database systems many object oriented database systems manage object buffers to provide fast access to objects traditional buffer replacement algorithms based on fixed length pages simply assume that the cost incurred by operating a buffer is proportional to the number of buffer faults however this assumption no longer holds in an object buffer where objects are of variable lengths and the cost of replacing an object varies for each object in this paper we propose a cost based replacement algorithm for object buffers the proposed algorithm replaces the objects that have minimum costs per unit time and unit space the cost model extends the previous page based one to include the replacement costs and the sizes of objects the performance tests show that the proposed algorithm is almost always superior to the lru 2 algorithm and in some cases is more than twice as fast the idea of cost based replacement can be applied to any buffer management architectures that adopt earlier algorithms it is espe current state of the art in distributed autonomous mobile robotics as research progresses in distributed robotic systems more and more aspects of multi robot systems are being explored this article surveys the current state of the art in distributed mobile robot systems our focus is principally on research that has been demonstrated in physical robot implementations we have identi ed eight primary research topics within multi robot systems biological inspirations communication architectures localization mapping exploration object transport and manipulation motion coordination recon gurable robots and learning and discuss the current state of research in these areas as we describe each research area we identify some key open issues in multi robot team research we conclude by identifying several additional open research issues in distributed mobile robotic systems 1 introduction the eld of distributed robotics has its origins in the late 1980 s when several researchers began investigating issues in multiple mobile robot systems adaptive heterogeneous multi robot teams this research addresses the problem of achieving fault tolerant cooperation within small to medium sized teams of heterogeneous mobile robots we describe a novel behavior based fully distributed architecture called alliance that utilizes adaptive action selection to achieve fault tolerant cooperative control in robot missions involving loosely coupled tasks the robots in this architecture possess a variety of high level functions that they can perform during a mission and must at all times select an appropriate action based on the requirements of the mission the activities of other robots the current environmental conditions and their own internal states since such cooperative teams often work in dynamic and unpredictable environments the software architecture allows the team members to respond robustly and reliably to unexpected environmental changes and modi cations in the robot team that may occur due to mechanical failure the learning of new skills or the addition or removal of robots from the team by human intervention after presenting alliance we describe in detail our experimental results of an implementation of this architecture on a team of physical mobile robots performing a cooperative box pushing demonstration these experiments illustrate the ability ofalliance to achieve adaptive fault tolerant cooperative control amidst dynamic changes in the capabilities of the robot team agents that reason and negotiate by arguing the need for negotiation in multi agent systems stems from the requirement for agents to solve the problems posed by their interdependence upon one another negotiation provides a solution to these problems by giving the agents the means to resolve their conflicting objectives correct inconsistencies in their knowledge of other agents world views and coordinate a joint approach to domain tasks which benefits all the agents concerned we propose a framework based upon a system of argumentation which permits agents to negotiate in order to establish acceptable ways of solving problems the framework provides a formal model of argumentation based reasoning and negotiation details a design philosophy which ensures a clear link between the formal model and its practical instantiation and describes a case study of this relationship for a particular class of architectures namely those for belief desire intention agents 1 introduction an increasing number of software app argumentation and multi agent decision making this paper summarises our on going work on mixedinitiative decision making which extends both classical decision theory and a symbolic theory of decision making based on argumentation to a multi agent domain introduction one focus of our work at queen mary and westfield college is the development of multi agent systems which deal with real world problems an example being the diagnosis of faults in electricity distribution networks jennings et al 1996 these systems are mixed initiative in the sense that they depend upon interactions between agents no single agent has sufficient skills or resources to carry out the tasks which the multi agent system as a whole is faced with because the systems are built to operate in the real world the agents are forced to deal with the usual problems of incomplete and uncertain information and increasingly we are turning to the use of techniques from decision theory both classical and non standard in order to ensure that our agents make so an approach to using degrees of belief in bdi agents the past few years have seen a rise in the popularity of the use of mentalistic attitudes such as beliefs desires and intentions to describe intelligent agents many of the models which formalise such attitudes do not admit degrees of belief desire and intention we see this as an understandable simplification but as a simplification which means that the resulting systems cannot take account of much of the useful information which helps to guide human reasoning about the world this paper starts to develop a more sophisticated system based upon an existing formal model of these mental attributes 1 introduction in the past few years there has been a lot of attention given to building formal models of autonomous software agents pieces of software which operate to some extent independently of human intervention and which therefore may be considered to have their own goals and the ability to determine how to achieve those goals many of these formal models are based on the use of tackling multimodal problems in hybrid genetic algorithms a method is proposed to address the issue of multimodality while using hybrid genetic algorithms gas the hybrid ga framework that is used is one in which a local searcher is employed during erim s approach to fine grained agents traditional software agents an extension of artificial intelligence seek human level intelligence in each agent for over 15 years inspired by artificial life erim has been devising architectures in which useful intelligence emerges at the system level from interactions of fine grained agents we have applied such architectures to a wide variety of domains including business industrial and military this white paper outlines three major principles that characterize our approach for each we discuss what the principle is why it is important and how it works in practical implementations adding generic contextual capabilities to wearable computers context awareness has an increasingly important role to play in the development of wearable computing systems in order to better define this role we have identified four generic contextual capabilities sensing adaptation resource discovery and augmentation a prototype application has been constructed to explore how some of these capabilities could be deployed in a wearable system designed to aid an ecologist s observations of giraffe in a kenyan game reserve however despite the benefits of context awareness demonstrated in this prototype widespread innovation of these capabilities is currently stifled by the difficulty in obtaining the contextual data to remedy this situation the contextual information service cis is introduced installed on the user s wearable computer the cis provides a common point of access for clients to obtain manipulate and model contextual information independently of the underlying plethora of data formats and sensor interface mechanisms 1 int active database systems exception clock externalg granularity ae fmember subset setg type ae fprimitive composite g operators ae for and seq closure times not g consumption mode ae frecent chronicle cumulative continuous g role 2 fmandatory optional noneg condition role 2 fmandatory optional noneg context ae fdb t binde dbe dbc g action options ae fstructure operation behavior invocation update rules abort inform external do instead g context ae fdb t binde bindc dbe dbc dba g behavior invocation in which case the event is raised by the execution of some user defined operation e g the message display is sent to an object of type widget it is common for event languages to allow events to be raised before or after an operation has been executed transaction in which case the event is raised by transaction commands e g abort commit begin transaction abstract or user defined in which case a programming mechanism is used that allows an appli a context for assisted cognition assisted cognition is introduced as an idea which leverages four technologies ubiquitous computing state reduction plan recognition and decision theory to develop solutions to problems faced by alzheimer patients and their caregivers a patient population with general dementia scale rating of less than 4 is targeted and user interface methods from clinical studies are identified a review of the literature on the component technologies is surveyed and a general architecture for assisted cognition is described the inductive approach to verifying cryptographic protocols informal arguments that cryptographic protocols are secure can be made rigorous using inductive definitions the approach is based on ordinary predicate calculus and copes with infinite state systems proofs are generated using isabelle hol the human e ort required to analyze a protocol can be as little as a week or two yielding a proof script that takes a few minutes to run protocols are inductively defined as sets of traces a trace is a list of communication events perhaps comprising many interleaved protocol runs protocol descriptions incorporate attacks and accidental losses the model spy knows some private keys and can forge messages using components decrypted from previous tra c three protocols are analyzed below otwayrees which uses shared key encryption needham schroeder which uses public key encryption and a recursive protocol 9 which is of variable length one can prove that event ev always precedes event ev or that property p holds provided x remains gestural interface to a visual computing environment for molecular biologists in recent years there has been tremendous progress in 3d immersive display and virtual reality vr technologies scientific visualization of data is one of many applications that has benefited from this progress to fully exploit the potential of these applications in the new environment there is a need for natural interfaces that allow the manipulation of such displays without burdensome attachments this paper describes the use of visual hand gesture analysis enhanced with speech recognition for developing a bimodal gesture speech interface for controlling a 3 d display the interface augments an existing application vmd which is a vr visual computing environment for molecular biologists the free hand gestures are used for manipulating the 3 d graphical display together with a set of speech commands we concentrate on the visual gesture analysis techniques used in developing this interface the dual modality of gesture speech is found to greatly aid the interaction capability visual interpretation of hand gestures for human computer interaction a review the use of hand gestures provides an attractive alternative to cumbersome interface devices for human computer interaction hci in particular visual interpretation of hand gestures can help in achieving the ease and naturalness desired for hci we survey the literature on vision based hand gesture recognition within the context of its role in hci the number of approaches to video based hand gesture recognition has grown in recent years thus the need for systematization and analysis of different aspects of gestural interaction has developed we discuss a complete model of hand gestures that possesses both spatial and dynamic properties of human hand gestures and can accommodate for all their natural types two classes of models that have been employed for interpretation of hand gestures for hci are considered the first utilizes 3d models of the human hand while the second relies on the appearance of the human hand in the image investigation of model parameters and analysis feat representation of electronic mail filtering profiles a user study electronic mail offers the promise of rapid communication of essential information however electronic mail is also used to send unwanted messages a variety of approaches can learn a profile of a user s interests for filtering mail here we report on a usability study that investigates what types of profiles people would be willing to use to filter mail keywords mail filtering user studies 1 introduction while electronic mail offers the promise of rapid communication of essential information it also facilitates transmission of unwanted messages such as advertisements solicitations light bulb jokes chain letters urban legends etc software that automatically sorts mail into categories e g junk talk announcements homework questions would help automate the process of sorting through mail to prioritize messages or suggest actions such as deleting junk mail or forwarding urgent messages to a handheld device such software maintains a profile of the user s interests supporting imprecision in multidimensional databases using granularities on line analytical processing olap technologies are being used widely for business data analysis and these technologies are also being used increasingly in medical applications e g for patient data analysis the lack of effective means of handling data imprecision which occurs when exact values are not known precisely or are entirely missing represents a major obstacle in applying olap technology to the medical domain as well as many other domains olap systems are mainly based on a multidimensional model of data and include constructs such as dimension hierarchies and granularities this paper develops techniques for the handling of imprecision that aim to maximally reusing these already existing constructs with imprecise data now available in the database queries are tested to determine whether or not they may be answered precisely given the available data if not alternative queries that are unaffected by the imprecision are suggested when a user elects to proceed with a query that is affected by imprecision techniques are proposed that take into account the imprecision in the grouping of the data in the subsequent aggregate computation and in the presentation of the imprecise result to the user the approach is capable of exploiting existing multidimensional query processing techniques such as pre aggregation yielding an effective approach with low computational overhead and that may be implemented using current technology the paper illustrates how to implement the approach using sql databases the bivariate marginal distribution algorithm the paper deals with the bivariate marginal distribution algorithm bmda bmda is an extension of the univariate marginal distribution algorithm umda it uses the pair gene dependencies in order to improve algorithms that use simple univariate marginal distributions bmda is a special case of the factorization distribution algorithm but without any problem specic knowledge in the initial stage the dependencies are being discovered during the optimization process itself in this paper bmda is described in detail bmda is compared to dierent algorithms including the simple genetic algorithm with dierent crossover methods and umda for some tness functions the relation between problem size and the number of tness evaluations until convergence is shown 1 introduction genetic algorithms work with populations of strings of xed length in this paper binary strings will be considered from current population better strings are selected at the expense of worse ones new strings ar boa the bayesian optimization algorithm in this paper an algorithm based on the concepts of genetic algorithms that uses an estimation of a probability distribution of promising solutions in order to generate new candidate solutions is proposed to estimate the distribution techniques for modeling multivariate data by bayesian networks are used the proposed algorithm identifies reproduces and mixes building blocks up to a specified order it is independent of the ordering of the variables in the strings representing the solutions moreover prior information about the problem can be incorporated into the algorithm however prior information is not essential preliminary experiments show that the boa outperforms the simple genetic algorithm even on decomposable functions with tight building blocks as a problem size grows 1 introduction recently there has been a growing interest in optimization methods that explicitly model the good solutions found so far and use the constructed model to guide the fu parameter less genetic algorithm a worst case time and space complexity analysis in this paper the worst case analysis of the time and space complexity of the parameter less genetic algorithm versus the genetic algorithm with an optimal population size is provided and the results of the analysis are discussed since the assumptions in order for the analysis to be correct are very weak the result is applicable to a wide range of problems various configurations of the parameter less genetic algorithm are considered and the results of their time and space complexity are compared 1 introduction a parameter less genetic algorithm harik lobo 1999 is an alternative to a common trialand error method of tweaking the values of the parameters of the genetic algorithm in order to find a set up to accurately and reliably solve a given problem the algorithm manages a number of independent runs of the genetic algorithm with different population sizes with the remaining parameters set to fixed values according to the theory of genetic algorithms control maps introduce continuous based heuristics for graph and tree isomorphisms with application to computer vision we present a new continuous quadratic programming approach for graph and tree isomorphism problems which is based on an equivalent maximum clique formulation the approach is centered around a fundamental result proved by motzkin and straus in the mid 1960s and recently expanded in various ways which allows us to formulate the maximum clique problem in terms of a standard quadratic program the attractive feature of this formulation is that a clear one to one correspondence exists between the solutions of the quadratic programs and those in the original combinatorial problems to approximately solve the program we use the so called replicator equations a class of straightforward continuous and discrete time dynamical systems developed in various branches of theoretical biology we show how despite their inherent inability to escape from local solutions they nevertheless provide experimental results which are competitive with those obtained using more sophisticated mean fiel automatic multi lingual information extraction information extraction ie is a burgeoning technique because of the explosion of internet so far most of the ie systems are focusing on english text and most of them are in the supervised learning framework which requires large amount of human labor and most of them can only work in narrow domain which is domain dependent these systems are difficult to be ported to other languages other domains because of these inherent shortcomings currently besides western languages like english there are many other asian languages which are much di erent from english in english words are delimited by white spaces so computer can easily tokenize the input text string in many languages like chinese japanese thai and korea they do not have word boundaries between words this poses a difficult problem for the information extraction for those languages in this thesis we intend to implement a self contained language independent automatic ie system the system is automatic because we are using a unsupervised learning framework in which no labeled data is required for training or a semi supervised learning framework in which small amount of labeled data and large amount of unlabeled data are used specifically we deal with chinese and english languages name entity recognition and entity relation extraction but the system can be easily extended to any other languages and other tasks we implement an unsupervised chinese word segmenter a chinese pos tagger and we extend maximum entropy models to incorporate unlabeled data for general information extraction the digital doctor an experiment in wearable telemedicine consultation with various specialists and review of medical literature are key elements in superior modern medical care because this information can be expensive and inconvenient to access physicians and patients must typically compromise ideal care practices with practical realities awearable computer with the ability to transmit and receive text and image data without a direct connection during an examination can remove the need for such compromise potentially allowing both better care and lower cost in this paper we report on experiments conducted at the university of rochester s strong hospital in which a wearable computer is usedtocreate patient records and provide remote consultations during dermatological examinations using semantic networks for knowledge representation in an intelligent environment introduction for many years now research in intelligent spaces has grown exploring different ways that a room can react to one or more users and their actions as usage of these intelligent environments ies grows however they will by necessity collect ever increasing amounts of data about their users in order to adapt to the user s desires information will be collected on the users interests who they communicate with their location web pages they visit and numerous other details that we may not even notice all this information needs to be collected and organized so that the ie can make quick correct assumptions about what the user would like to do next at the intelligent room project hanssens et al 2002 we are beginning to define one such knowledge representation kr using semantic networks as the basis for the representation this creates inherent advantages both in ease of adding and changing information as well as inference generation 2 knowledge repre an object oriented case based learning system this thesis first gives an overview of the subfield of classification in the area of machine learning the numerous variants of case based learning algorithms are compared according to what kind of data is processed how knowledge and hypotheses are represented and what kind of reasoning or learning is performed the strengths and weaknesses of these learning methods are compared to each other and to other groups of learning methods a modular object oriented lisp environment vie cbr2 is introduced that implements a number of algorithms for case based learning this system allows to easily combine preprogrammed learning algorithms and provides a framework for simple integration of new learning algorithms and other components that make use of the basic system rooms protocols and nets metaphors for computer supported cooperative learning of distributed groups we discuss an integrative design for computer supported cooperative learning cscl environments three common problems of cscl are addressed how to achieve social orientation and group awareness how to coordinate goal directed interaction and how to construct a shared knowledge base with respect to each problem we propose a guiding metaphor which links theoretical technical and usability requirements if appropriately implemented each metaphor resolves one problem virtual rooms support social orientation learning protocols guide interactions aimed at knowledge acquisition and learning nets represent socially shared knowledge theoretically the metaphor of virtual rooms originates in work on virtual spaces in human computer interaction learning protocols are related to speech act theory and learning nets are based on models of knowledge representation a prototype system implementing the virtual room metaphor is presented we argue that by further integrating these thre novel approaches to the indexing of moving object trajectories the domain of spatiotemporal applications is a treasure trove of new types of data and queries however work in this area is guided by related research from the spatial and temporal domains so far with little attention towards the true nature of spatiotemporal phenomena in this work the focus is on a spatiotemporal sub domain namely the trajectories of moving point objects we present new types of spatiotemporal queries as well as algorithms to process those further we introduce two access methods this kind of data namely the spatio temporal r tree str tree and the trajectory bundle tree tb tree the former is an r tree based access method that considers the trajectory identity in the index as well while the latter is a hybrid structure which preserves trajectories as well as allows for r tree typical range search in the data we present performance studies that compare the two indices with the r tree appropriately modified for a fair comparison under a varying set of spatiotemporal queries and we provide guidelines for a successful choice among them a new twist on mobile computing two way interactive session transfer the ubiquitous use of computer resources for daily productivity is a goal that presently remains unrealised we believe that the convergence of desktop and mobile applications into a seamless computing experience will provide a strong motivation for future anytime anywhere computing in this paper we describe this convergence as the capability to perform the handoff of application sessions across heterogeneous platforms using the network as a conduit in addition to discussing the architecture and protocols to facilitate this capability in this paper we also provide a taxonomy for describing a variety of different session handoff schemes in particular we have identified an important two way interactive session transfer twist behaviour for communication between heterogeneous clients and servers to demonstrate our concepts we have implemented the handoff capability with twist semantics into a real world application that serves as a teaching tool for radiology clinicians from experimental data we will show that the handoff mechanism incurs little delay to transfer large dataladen sessions 1 a scalable distributed middleware service architecture to support mobile internet applications middleware layers placed between user clients and application servers have been used to perform a variety of functions in previous work we have used middleware to perform a new capability application session handoff using a single middleware server to provide all functionality however to improve the scalability of our architecture we have designed an efficient distributed middleware service layer that properly maintains application session handoff semantics while being able to service a large number of clients we show that this service layer improves the scalability of general clientto application server interaction as well as the specific case of application session handoff we detail protocols involved in performing handoff and analyse an implementation of the architecture that supports the use of a real medical teaching tool from experimental results it can be seen that our middleware service effectively provides scalability as a response to increased workload 1 an extensible and scalable content adaptation pipeline architecture to support heterogeneous clients the importance of middleware and content adaptation has previously been demonstrated for pervasive use of web based applications in this paper we propose a modular extensible and scalable middleware component called the content adaptation pipeline that performs content adaptation on arbitrarily complex data types not limited to text and graphic images furthermore the architecture can be used as part of many client server applications not just web browsers in our work we leverage the xml language as a uniform means to describe all the elements in our architecture including the client device and user profiles the data characteristics the transcoding operations performed on the data and the resultant adapted data we illustrate the flexibility of our architecture to support new data types and adaptation operations by first showing its use with data from a real world medical application and then extending its capabilities to handle animated graphics and also real time streaming rtp data finally we demonstrate scalability in our architecture by executing the content adaptation pipeline over a distributed set of servers running an efficient protocol distinctive features should be learned most existing machine vision systems perform recognition based on a xed set of hand crafted features geometric models or eigen subspace decomposition drawing from psychology neuroscience and intuition we show that certain aspects of human performance in visual discrimination cannot be explained by any of these techniques we argue that many practical recognition tasks for articial vision systems operating under uncontrolled conditions critically depend on incremental learning loosely motivated by visuocortical processing we present feature representations and learning methods that perform biologically plausible functions the paper concludes with experimental results generated by our method 1 introduction how exible are the representations for visual recognition encoded by the neurons of the human visual cortex are they predetermined by a xed developmental schedule or does their development depend on their stimulation does their development cease at some poin toward learning visual discrimination strategies humans learn strategies for visual discrimination through interaction with their environment discrimination skills are refined as demanded by the task at hand and are not a priori determined by any particular feature set tasks are typically incompletely specified and evolve continually this work presents a general framework for learning visual discrimination that addresses some of these characteristics it is based on an infinite combinatorial feature space consisting of primitive features such as oriented edgels and texture signatures and compositions thereof features are progressively sampled from this space in a simple to complex manner a simple recognition procedure queries learned features one by one and rules out candidate object classes that do not sufficiently exhibit the queried feature training images are presented sequentially to the learning system which incrementally discovers features for recognition experimental results on two databases of geometric objects ill human computer coupling this article lime linda meets mobility lime is a system designed to assist in the rapid development of dependable mobile applications over both wired and ad hoc networks mobile agents reside on mobile hosts and all communication takes place via transiently shared tuple spaces distributed across the mobile hosts the decoupled style of computing characterizing the linda model is extended to the mobile environment at the application level both agents and hosts perceive movement as a sudden change of context the set of tuples accessible by a particular agent residing on a given host is altered transparently in response to changes in the connectivity pattern among the mobile hosts in this paper we present the key design concepts behind the lime system 1 introduction today s users demand ubiquitous network access independent of their physical location this style of computation often referred to as mobile computing is enabled by rapid advances in the wireless communication technology the networking scenarios enabled an architecture for outdoor wearable computers to support augmented reality and multimedia applications this paper describes an architecture to support a hardware and software platform for research into the use of wearable computers and augmented reality in an outdoor environment the architecture supports such devices as a gps compass and head mounted display a prototype system was built to support novel applications by drawing graphics such as maps building plans and compass bearings on the head mounted display projecting information over that normally seen by the user and hence augmenting a user s perception of reality this paper presents a set of novel augmented reality and multimedia applications operated in an outdoor environment 1 generating user interface code in a model based user interface development environment declarative models play an important role in most software design activities by allowing designs to be constructed that selectively abstract over complex implementation details in the user interface setting model based user interface development environments mb uides provide a context within which declarative models can be constructed and related as part of the interface design process however such declarative models are not usually directly executable and may be difficult to relate to existing software components it is therefore important that mb uides both fit in well with existing software architectures and standards and provide an effective route from declarative interface specification to running user interfaces this paper describes how user interface software is generated from declarative descriptions in the teallach mb uide distinctive features of teallach include its open architecture which connects directly to existing applications and widget sets and the genera umli the unified modeling language for interactive applications user interfaces uis are essential components of most software systems and significantly affect the effectiveness of installed applications in addition uis often represent a significant proportion of the code delivered by a development activity however despite this there are no modelling languages and tools that support contract elaboration between ui developers and application developers the unified modeling language uml has been widely accepted by application developers but not so much by ui designers for this reason this paper introduces the notation of the unified modelling language for interactive applications umli that extends uml to provide greater support for ui design ui elements elicited in use cases and their scenarios can be used during the design of activities and ui presentations a diagram notation for modelling user interface presentations is introduced activity diagram notation is extended to describe collaboration between interaction and domain objects further a case study using umli notation and method is presented user interface modelling with uml the unified modeling language uml is a natural candidate for user interface ui modelling since it is the standard notation for object oriented modelling of applications however it is by no means clear how to model uis using uml this paper presents a user interface modelling case study using uml this case study identifies some aspects of uis that cannot be modelled using uml notation and a set of uml constructors that may be used to model uis the modelling problems indicate some weaknesses of uml for modelling uis while the constructors exploited indicate some strengths the identification of such strengths and weaknesses can be used in the formulation of a strategy for extending uml to provide greater support for user interface design multi robot target acquisition using multiple objective behavior coordination in this paper we propose an approach to multi robot coordination in the context of cooperative target acquisition the approach is based on multiple objective behavior coordination extended to multiple robots it provides mechanisms for distributed command fusion across a group of robots to pursue multiple goals of multiple robots in parallel the mechanisms enable each robot to select actions that not only benefit itself but also benefit the group as a whole experimental results with two mobile robots validate that by using this method a group of robots can successfully track and acquire a moving target 1 introduction cooperation of a team of robots in unknown settings poses complex control problems which require solutions that guarantee a suitable trade off between a multitude of potentially conflicting task objectives within and among the robots for instance an action that is optimal with respect to a particular robot might be unacceptable with respect to the others thus behavior coordination mechanisms state of the art in behavior based robotics the control of a robot is shared between a set of purposive perception action units called behaviors based on selective sensory information each behavior produces immediate reactions to control the robot with respect to a particular objective i e a narrow aspect of the robot s overall task such as obstacle avoidance or wall following behaviors with di erent and possibly incommensurable objectives may produce con icting actions that are seemingly irreconcilable thus a major issue in the design of behavior based control systems is the formulation of e ective mechanisms for coordination of the behaviors activities into strategies for rational and coherent behavior this is known as the action selection problem also refereed to as the behavior coordination problem and is the primary focus of this overview paper numerous action selection mechanisms have been proposed over the last decade and the main objective of this document istogive a qualitative overview of these approaches 2 1 locating objects in mobile computing in current distributed systems the notion of mobility is emerging in many forms and applications mobility arises naturally in wireless computing since the location of users changes as they move besides mobility in wireless computing software mobile agents are another popular form of moving objects locating objects i e identifying their current location is central to mobile computing in this paper we present a comprehensive survey of the various approaches to the problem of storing querying and updating the location of objects in mobile computing the fundamental techniques underlying the proposed approaches are identified analyzed and classified along various dimensions keywords mobile computing location management location databases caching replication moving objects spatio temporal databases 1 introduction in current distributed systems the notion of mobility is emerging in many forms and applications increasingly many users are not tied to a fixed exploiting versions for handling updates in broadcast disks recently broadcasting has attracted considerable attention as a means of disseminating information to large client populations in both wired and wireless settings in this paper we exploit versions to increase the concurrency of client transactions in the presence of updates we consider three alternative mediums for storing versions a the air older versions are broadcast along with current data b the client s local cache older versions are maintained in cache and c a local database or warehouse at the client part of the server s database is maintained at the client in the form of a multiversion materialized view the proposed techniques are scalable in that they provide consistency without any direct communication from clients to the server performance results show that the overhead of maintaining versions can be kept low while providing a considerable increase in concurrency 1 introduction while traditionally data are delivered from servers to clients on demand a scalable processing of read only transactions in broadcast push recently push based delivery has attracted considerable attention as a means of disseminating information to large client populations in both wired and wireless settings in this paper we address the problem of ensuring the consistency and currency of client read only transactions in the presence of updates to this end additional control information is broadcast a suite of methods is proposed that vary in the complexity and volume of the control information transmitted and subsequently differ in response times degrees of concurrency and space and processing overheads the proposed methods are combined with caching to improve query latency the relative advantages of each method are demonstrated through both simulation results and qualitative arguments read only transactions are processed locally at the client without contacting the server and thus the proposed approaches are scalable i e their performance is independent of the number of clients 1 introduction in traditio tumor detection in colonoscopic images using hybrid methods for on line neural network training in this paper the effectiveness of a new hybrid evolutionary algorithm in on line neural network training for tumor detection is investigated to this end a lamarck inspired combination of evolutionary algorithms and stochastic gradient descent is proposed the evolutionary algorithm works on the termination point of the stochastic gradient descent thus the method consists in a stochastic gradient descent based on line training stage and an evolutionary algorithm based retraining stage on line training is considered eminently suitable for large or even redundant training sets and or networks it also helps escaping local minima and provides a more natural approach for learning nonstationary tasks furthermore the notion of retraining aids the hybrid method to exhibit reliable and stable performance and increases the generalization capability of the trained neural network experimental results suggest that the proposed hybrid strategy is capable to train on line efficiently and effectively here an artificial neural network architecture has been successfully used for detecting abnormalities in colonoscopic video images the design of history mechanisms and their use in collaborative educational simulations reviewing past events has been useful in many domains videotapes and flight data recorders provide invaluable technological help to sports coaches or aviation engineers similarly providing learners with a readable recording of their actions may help them monitor their behavior reflect on their progress and experiment with revisions of their experiences it may also facilitate active collaboration among dispersed learning communities learning histories can help students and professionals make more effective use of digital library searching word processing tasks computer assisted design tools electronic performance support systems and web navigation this paper describes the design space and discusses the challenges of implementing learning histories it presents guidelines for creating effective implementations and the design tradeoffs between sparse and dense history records the paper also presents a first implementation of learning histories for a simulation based engineer eye communication in a conversational 3d synthetic agent this paper we concentrate on the study and generation of coordinated linguistic and gaze communicative acts in this view we analyse gaze signals according to their functional meaning rather than to their physical actions we propose a formalism where a communicative act is represented by two elements a meaning that corresponds to a set of goals and beliefs that the agent has the purpose to transmit to the interlocutor and a signal that is the nonverbal expression of that meaning we also outline a methodology to generate messages that coordinate verbal with nonverbal signals machine learning and knowledge representation in the labour approach to user modeling in early user adaptive systems the use of knowledge representation methods for user modeling has often been the focus of research in recent years however the application of machine learning techniques to control user adapted interaction has become popular in this paper we present and compare adaptive systems that use either knowledge representation or machine learning for user modeling based on this comparison several dimensions are identified that can be used to distinguish both approaches but also to characterize user modeling systems in general the labour learning about the user approach to user modeling is presented which attempts to take an ideal position in the resulting multi dimensional space by combining machine learning and knowledge representation techniques finally an implementation of labour ideas into the information server elfi is sketched 1 introduction while striving to achieve user adapted interaction user modeling researchers have often m fast reinforcement learning through eugenic neuro evolution in this paper we introduce eusane a novel reinforcement learning algorithm based on the sane neuroevolution method it uses a global search algorithm the eugenic algorithm to optimize the selection of neurons to the hidden layer of sane networks the performance of eusane is evaluated in the two pole balancing benchmark task showing that eusane is significantly stronger than other reinforcement learning methods to date in this task adjustable autonomy for a plan management agent the plan management agent pma is an intelligent software system that is intended to aid a user in managing a potentially large and complex set of plans currently under development pma applies ai technology for modeling and reasoning about plans and processes to the development of automated support for work activities we have developed and implemented algorithms for reasoning about richly expressive plans which include explicit temporal constraints temporal uncertainty and observation actions and conditional branches we have also developed and implemented an approach to computing the cost of a new plan in the context of existing commitments the current version of pma has a low level of autonomy it makes suggestions to its user but it does not directly act on her behalf in this paper we first describe the pma system and then briefly raise some design questions we will need to address as we increase the level of pma s autonomy and have it vary with the situation introduc icrafter a service framework for ubiquitous computing environments in this paper we propose icrafter a framework for services and their user interfaces in a class of ubiquitous computing environments automatic labeling of document clusters automatically labeling document clusters with words which indicate their topics is difficult to do well the most commonly used method labeling with the most frequent words in the clusters ends up using many words that are virtually void of descriptive power even after traditional stop words are removed another method labeling with the most predictive words often includes rather obscure words we present two methods of labeling document clusters motivated by the model that words are generated by a hierarchy of mixture components of varying generality the first method assumes existence of a document hierarchy manually constructed or resulting from a hierarchical clustering algorithm and uses a 2 test of significance to detect different word usage across categories in the hierarchy the second method selects words which both occur frequently in a cluster and effectively discriminate the given cluster from the other clusters we compare these methods on abstracts of documents sel probabilistic models for unified collaborative and content based recommendation in sparse data environments recommender systems leverage product and community information to target products to consumers researchers have developed collaborative recommenders content based recommenders and a few hybrid systems we propose a unified probabilistic framework for merging collaborative and content based recommendations we extend hofmann s 1999 aspect model to incorporate three way co occurrence data among users items and item content the relative influence of collaboration data versus content data is not imposed as an exogenous parameter but rather emerges naturally from the given data sources however global probabilistic models coupled with standard em learning algorithms tend to drastically overfit in the sparsedata situations typical of recommendation applications we show that secondary content information can often be used to overcome sparsity experiments on data from the researchindex library of computer science publications show that appropriate mixture models incorporating secondary data produce significantly better quality recommenders than k nearest neighbors k nn global probabilistic models also allow more general inferences than local methods like k nn towards structural logistic regression combining relational and statistical learning inductive logic programming ilp techniques are useful for analyzing data in multi table relational databases learned rules can potentially discover relationships that are not obvious in flattened data statistical learners on the other hand are generally not constructed to search relational data they expect to be presented with a single table containing a set of feature candidates however statistical learners often yield more accurate models than the logical forms of ilp and can better handle certain types of data such as counts we propose a new approach which integrates structure navigation from ilp with regression modeling our approach propositionalizes the first order rules at each step of ilp s relational structure search generating features for potential inclusion in a regression model ideally feature generation by ilp and feature selection by stepwise regression should be integrated into a single loop preliminary results for scientific literature classification are presented using a relational form of the data extracted by researchindex formerly citeseer we use foil and logistic regression as our ilp and statistical components decoupled at this stage word counts and citation based features learned with foil are modeled together by logistic regression the combination often significantly improves performance when high precision classification is desired heterogeneity in the coevolved behaviors of mobile robots the emergence of specialists many mobile robot tasks can be most efficiently solved when a group of robots is utilized the type of organization and the level of coordination and communication within a team of robots affects the type of tasks that can be solved this paper examines the tradeoff of homogeneity versus heterogeneity in the control systems by allowing a team of robots to coevolve their high level controllers given different levels of difficulty of the task our hypothesis is that simply increasing the difficulty of a task is not enough to induce a team of robots to create specialists the key factor is not difficulty per se but the number of skill sets necessary to successfully solve the task as the number of skills needed increases the more beneficial and necessary heterogeneity becomes we demonstrate this in the task domain of herding where one or more robots must herd another robot into a confined space 1 virtual notepad handwriting in immersive vr we present virtual notepad a collection of interface tools that allows the user to take notes annotate documents and input text using a pen while still immersed in virtual environments ves using a spatially tracked pressure sensitive graphics tablet pen and handwriting recognition software virtual notepad explores handwriting as a new modality for interaction in immersive ves this paper reports details of the virtual notepad interface and interaction techniques discusses implementation and design issues reports the results of initial evaluation and overviews possible applications of virtual handwriting 1 introduction writing is a ubiquitous everyday activity we jot down ideas and memos scribble comments in the margins of a book or an article annotate blueprints and design plans using computers we type documents complete forms and enter database queries however writing taking notes or entering text in immersive ves is almost impossible cut off from conventional a feasible low power augmented reality terminal this paper studies the requirements for a truly wearable augmented reality ar terminal the requirements translate into a generic hardware architecture consisting of programmable modules communicating through a central interconnect careful selection of low power components shows that it is feasible to construct an ar terminal that weighs about 2 kg and roughly dissipates 26 w with stateof the art batteries and a 50 average resource utilization the terminal can operate for about 10 hours 1 introduction the goal of ubiquitous computing is to have computers act as human assistants that support us instantly computers should move out of our awareness instead of being at the center of our attention 14 for ubiquitous computing to become reality we need two important technologies to mature wireless communication wearability and augmented reality user interface wireless communication is obviously required to obtain services provided by an arbitrary computer regardless the from declarative signatures to misuse ids in many existing misuse intrusion detection systems intrusion signatures are very close to the detection algorithms as a consequence they contain too many cumbersome details recent work have proposed declarative signature languages that raise the level of abstraction when writing signatures however these languages do not always come with operational support in this article we show how to transform such declarative signatures into operational ones this process points out several technical details which must be considered with care when performing the translation by hand but which can be systematically handled the impact of database selection on distributed searching abstract the proliferation of online information resources increases the importance of effective and efficient distributed searching distributed searching is cast in three parts database selection query processing and results merging in this paper we examine the effect of database selection on retrieval performance we look at retrieval performance in three different distributed retrieval testbeds and distill some general results first we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database second we find that good performance can be achieved when only a few sites are selected and that the performance generally increases as more sites are selected finally we find that when database selection is employed it is not necessary to maintain collection wide information cwi e g global idf local information can be used to achieve superior performance this means that distributed systems can be engineered with more autonomy and less cooperation this work suggests that improvements in database selection can lead to broader improvements in retrieval performance even in centralized i e single database systems given a centralized database and a good selection mechanism retrieval performance can be improved by decomposing that database conceptually and employing a selection step 1 cooperative learning over composite search spaces experiences with a multi agent design system we suggest the use of two learning techniques short term and long term to enhance search efficiency in a multi agent design system by letting the agents learn about non local requirements on the local search process the first technique allows an agent to accumulate and apply constraining information about global problem solving gathered as a result of agent communication to further problem solving within the same problem instance the second technique is used to classify problem instances and appropriately index and retrieve constraining information to apply to new problem instances these techniques will be presented within the context of a multi agent parametricdesign application called steam we show that learning conclusively improves solution quality and processingtime results introduction in this article we study machine learning techniques that can be applied within multi agent systems mas to improve solution quality and processing time results a ubiquitous prob distributed case based learning multi agent systems exploiting case based reasoning techniques have to deal with the problem of retrieving episodes that are themselves distributed across a set of agents from a gestalt perspective a good overall case may not be the one derived from the summation of best subcases in this paper we deal with issues involved in learning and exploiting the learned knowledge in multiagent case based systems introduction case based reasoning cbr has been attracting much attention recently as a paradigm with a wide variety of applications kolodner 93 in this paper we discuss issues pertaining to cooperative retrieval and composition of a case in which subcases are distributed across different agents in a multi agent system a multi agent system comprises a group of intelligent agents working towards a set of common global goals or separate individual goals that may interact in such a system each of the agents may not be individually capable of achieving the global goal and or off line learning of coordination in functionally structured agents for distributed data processing when we design multi agent systems for realistic worth oriented environments coordination problems they present involve intricate and sophisticated interplay between the domain and the various system components achieving effective coordination in such systems is a difficult problem for a number of reasons like local views of problem solving task and uncertainty about the outcomes of interacting non local tasks in this paper we present a learning algorithm that endows agents with the capability to choose an appropriate coordination algorithm based on the present problem solving situation in the domain of distributed data processing 1 introduction achieving effective coordination in a multi agent system is a difficult problem for a number of reasons the first is that an agent s control decisions based only on its local view of problem solving task structures may lead to inappropriate decisions about which activity it should do next what results it should transmit to other agen learning situation specific coordination in cooperative multi agent systems achieving effective cooperation in a multi agent system is a difficult problem for a number of reasons such as limited and possiblyout dated views of activities of other agents and uncertainty about the outcomes of interacting non local tasks in this paper we present a learning system called collage that endows the agents with the capability to learn how to choose the most appropriate coordination strategy from a set of available coordination strategies collage relies on meta level information about agents problem solving situationsto guide them towards a suitable choice for a coordination strategy we present empirical results that strongly indicate the effectiveness of the learning algorithm keywords multi agent systems coordination learning 1 introduction coordination is the process of effectively managing interdependencies between activities distributed across agents so as to derive maximum benefit from them 21 6 based on structure and uncertainty in their environmen off policy temporal difference learning with function approximation we introduce the first algorithm for off policy temporal difference learning that is stable with linear function approximation off policy learning is of interest because it forms the basis for popular reinforcement learning methods such as q learning which has been known to diverge with linear function approximation and because it is critical to the practical utility of multi scale multi goal learning frameworks such as options hams and maxq our new algorithm combines td over state action pairs with importance sampling ideas from our previous work we prove that given training under any soft policy the algorithm converges w p 1 to a close approximation as in tsitsiklis and van roy 1997 tadic 2001 to the action value function for an arbitrary target policy variations of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent we also illustrate our method empirically on a small policy evaluation problem our current results are limited to episodic tasks with episodes of bounded length 1 although q learning remains the most popular of all reinforcement learning algorithms it has been known since about 1996 that it is unsound with linear function approximation see gordon 1995 bertsekas and tsitsiklis 1996 the most telling counterexample due to baird 1995 is a seven state markov decision process with linearly independent feature vectors for which an exact solution exists yet 1 this is a re typeset version of an article published in the proceedings the hyper system knowledge reformation for efficient first order hypothetical reasoning we present the hyper system that implements a new approach to knowledge compilation where function free first order acyclic horn theories are transformed to propositional logic the compilation method integrates techniques from deductive databases relevance reasoning and theory transformation via unfold fold transformations to obtain a compact propositional representation the transformed theory is more compact than the ground version of the original theory in terms of significantly less and mostly shorter clauses this form of compilation called knowledge base reformation is important since the most efficient reasoning methods are defined for propositional theories while knowledge is most naturally expressed in a first order language in particular we will show that knowledge reformation allows low order polynomial time inference to find a near optimal solution in cost based first order hypothetical reasoning or abduction problems we will also present ex social role awareness in animated agents this paper promotes social role awareness as a desirable capability of animated agents that are by now strong affective reasoners but otherwise often lack the social competence observed with humans in particular humans may easily adjust their behavior depending on their respective role in a socio organizational setting whereas their synthetic pendants tend to be driven mostly by attitudes emotions and personality our main contribution is the incorporation of social filter programs to mental models of animated agents those programs may qualify an agent s expression of its emotional state by the social context thereby enhancing the agent s believability as a conversational partner or virtual teammate our implemented system is entirely webbased and demonstrates socially aware animated agents in an environment similar to hayes roth s cybercaf e keywords believability social agents human like qualities of synthetic agents social dimension in communication affective reaso a web based its controlled by an expert system intelligent tutoring system its for teaching high school teachers how to use new technologies it offers course units covering the needs of users with different knowledge levels and characteristics it tailors the presentation of the educational material to the users diverse needs by using ai techniques to specify each user s model as well as to make pedagogical decisions this is achieved via an expert system that uses a hybrid knowledge representation formalism integrating symbolic rules with neurocomputing a web based intelligent tutoring system using hybrid rules as its representational basis in this paper we present the architecture and describe the functionality of a web based intelligent tutoring system its which uses neurules for knowledge representation neurules are a type of hybrid rules integrating symbolic rules with neurocomputing the use of neurules as the knowledge representation basis of the its results in a number of advantages a comparative evaluation of meta learning strategies over large and distributed data sets there has been considerable interest recently in various approaches to scaling up machine learning systems to large and distributed data sets we have been studying approaches based upon the parallel application of multiple learning programs at distributed sites followed by a meta learning stage to combine the multiple models in a principled fashion in this paper we empirically determine the best data partitioning scheme for a selected data set to compose appropriatelysized subsets and we evaluate and compare three di erent strategies voting stacking and stacking with correspondence analysis scann for combining classification models trained over these subsets we seek to find ways to e ciently scale up to large data sets while maintaining or improving predictive performance measured by the error rate a cost model and the tp fp spread keywords classification multiple models meta learning stacking voting correspondence analysis data partitioning email address of co cyberoos2000 experiments with emergent tactical behaviour this paper is that rather than defining situated or tactical reasoning ad hoc it is desirable to categorise agents according to their functionality and reactions to the environment and identify corresponding classes of action theories and agent architectures then the reasoning exhibited by agents of a certain type and validated by particular action theories can be declared to be situated tactical strategic social and so on in other words the principal target is a systematic description of increasing levels of agent reasoning abilities the results reported in 8 9 10 demonstrated that this is achievable at the situated level preliminary results on the systematic models for basic tactical behaviour were obtained as well 11 this work intends to use this framework in experimenting with emergent tactical teamwork and thus build up empirical results and intuition necessary to advance the systematic methodology towards collaborative goal oriented agents autoepistemic logic of knowledge and beliefs in recent years various formalizations of non monotonic reasoning and different semantics for normal and disjunctive logic programs have been proposed including autoepistemic logic circumscription cwa gcwa ecwa epistemic specifications stable well founded stationary and static semantics of normal and disjunctive logic programs in this paper we introduce a simple non monotonic knowledge representation framework which isomorphically contains all of the above mentioned non monotonic formalisms and semantics as special cases and yet is significantly more expressive than each one of these formalisms considered individually the new formalism called the autoepistemic logic of knowledge and beliefs aelb is obtained by augmenting moore s autoepistemic logic ael already employing the knowledge operator l with an additional belief operator b as a result we are able to reason not only about formulae f which are known to be true i e those for which lf holds but also abou map design and implementation of a mobile agents platform the recent development of telecommunication networks has contributed to the success of applications such as information retrieval and electronic commerce as well as all the services that take advantage of communication in distributed systems in this area the emerging technology of mobile agents aroused considerable interest mobile agents are applications that can move through the network for carrying out a given task on behalf of the user in this work we present a platform called map mobile agents platform for the development and the management of mobile agents the language used both for developing the platform and for carrying out the agents is java the platform gives the user all the basic tools needed for creating some applications based on the use of agents it enables us to create run suspend resume deactivate reactivate local agents to stop their execution to make them communicate each other and migrate keywords mobile agents distributed computing java net using mobile agents to implement flexible network management strategies due to their intrinsic complexity computer and communication systems require increasingly more sophisticated management starategies to be adopted in order to guarantee adequate levels of performance and reliability the centralized paradigm adopted by the snmp is appropriate in several network management applications but the quick expansion of networks has posed the problem of its scalability as well as for any other centralized model mobile agents represent a challenging approach to provide advanced network management functionalities due to the possibility to easily implement a decentralized and active monitoring of the system in this paper we discuss how to take advantage of this technology and identify some reference scenario where mobile agents represent a very promising approach we also describe a prototype implementation based on our mobile agent platform called map and show how it is possible to take advantages from using the features it provides keywords net web usage mining languages and algorithms we propose two new xml applications xgmml and logml xgmml is a graph description language and logml is a web log report description language we generate a web graph in xgmml format for a web site using the web robot of the wwwpal system developed for web visualization and organization we generate web log reports in logml format for a web site from web log files and the web graph in this paper we further illustrate the usefulness of these two xml applications with a web data mining example moreover we show the simplicity with which this mining algorithm can be specified and implemented efficiently using our two xml applications we provide sample results namely frequent patterns of users in a web site with our web data mining algorithm the use of classifiers in sequential inference we study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints in particular we develop two general approaches for an important subproblem identifying phrase structure the first is a markovian approach that extends standard hmms to allow the use of a rich observation structure and of general classifiers to model state observation dependencies the second is an extension of constraint satisfaction formalisms we develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing 1 introduction in many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints the sequential nature of the data or other domain specific constraints consider for example the problem of chunking natural language sentences a theory of proximity based clustering structure detection by optimization in this paper a systematic optimization approach for clustering proximity or similarity data is developed starting from fundamental invariance and robustness properties a set of axioms is proposed and discussed to distinguish different cluster compactness and separation criteria the approach covers the case of sparse proximity matrices and is extended to nested partitionings for hierarchical data clustering to solve the associated optimization problems a rigorous mathematical framework for deterministic annealing and mean field approximation is presented efficient optimization heuristics are derived in a canonical way which also clarifies the relation to stochastic optimization by gibbs sampling similarity based clustering techniques have a broad range of possible applications in computer vision pattern recognition and data analysis as a major practical application we present a novel approach to the problem of unsupervised texture segmentation which relies on statistical toward team oriented programming the promise of agent based systems is leading towards the development of autonomous heterogeneous agents designed by a variety of research industrial groups and distributed over a variety of platforms and environments argos efficient refresh in an xql based web caching system the web has become a major conduit to information repositories of all kinds web caches are employed to store a web view to provide an immediate response to recurring queries however the accuracy of the replicates in web caches encounters challenges due to the dynamicity of web data we are thus developing and evaluating a web caching system equipped with an efficient refresh strategy with the assistance of a novel index structure the aggregation path index apix we built argos a web cache system based on the gmd xql query engine argos achieves a high degree of self maintenance by diagnosing irrelevant data update cases and hence greatly improves the refresh performance of the materialized web view we also report preliminary experimental results assessing the performance of argos compared to from scratch evaluation 1 introduction the advent of the web has dramatically increased the proliferation of information of all kinds xml 17 is rapidly becoming popular for represen eyes in the interface computer vision has a significant role to play in the human computer interaction hci devices of the future all computer input devices serve one essential purpose they transduce some motion or energy from a human agent into machine useable signals one may therefore think of input devices as the perceptual organs by which computers sense the intents of their human users we outline the role computer vision will play highlight the impediments to the development of vision based interfaces and propose an approach for overcoming these impediments prospective vision research areas for hci include human face recognition facial expression interpretation lip reading head orientation detection eye gaze tracking three dimensional finger pointing hand tracking hand gesture interpretation and body pose tracking for vision based interfaces to make any impact we will have to embark on an expansive approach which begins with the study of the interaction modality we seek to implement foreign event handlers to maintain information consistency and system adequacy this paper is to describe novel applications of mobile code technology which have not appeared yet but should be feasible with our current knowledge of the domain these new applications contradict the often made observation that mobile code is just another technique that does not really bring much more possibilities than existing technologies for distributed applications there is a whole class of problems that have not received much attention yet and that are not well managed by current environments these are the problems of maintaining consistency of dynamic information and maintaining systems in adequacy with the ever changing requirements of customers our motivation is that besides the quantitative improvements that most people expect from using mobile code there is also a qualitative benefit which is even more important but not universally recognized now mobile code allows communication with less conventions than message passing 5 3 processes interconnected by mobile code still have to agree on high level encoding and synchronization primitives but these agreements are only a fraction of what is necessary to communicate many context dependent aspects can be encapsulated inside mobile code and changed when the context changes encapsulation has the same benefits here as in other software engineering domains it reduces the dependency between components thus reducing the number of modifications that we must make to software in order to adapt it to new requirements for this reason we think that it is the best way to cope with systems that are distributed hence not manageable by a single person or organization that are dynamic because the information they contain must change when the world itself changes and that are evolving since the users discover n building a xml based corporate memory this paper emphasizes the interest of xml meta language for corporate knowledge management taking into account the advantages of the world wide web and of ontologies for knowledge management we present osirix a tool enabling enterprise ontology guided search in xml documents that may consitute a part of a corporate memory keywords xml world wide web knowledge management document based corporate memory enterprise ontology information retrieval 1 introduction extending the definitions proposed by 28 20 we consider a corporate memory as an explicit disembodied persistent representation of knowledge and information in an organization in order to facilitate its access and reuse by members of the organization for their tasks we consider its building as relying on the following steps 11 1 detection of needs in corporate memory 2 construction of the corporate memory 3 diffusion of the corporate memory 4 use of the corporate memory 5 evaluation of maintaining unstructured case bases with the dramatic proliferation of case based reasoning systems in commercial applications many case bases are now becoming legacy systems they represent a significant portion of an organization s assets but they are large and difficult to maintain one of the contributing factors is that these case bases are often large and yet unstructured they are represented in natural language text adding to the complexity is the fact that the case bases are often authored and updated by different people from a variety ofknowledge sources making it highly likely for a case base to contain redundant and inconsistent knowledge in this paper we present methods and a system for maintaining large and unstructured case bases we focus on two difficult problems in case base maintenance redundancy and inconsistency detection these two problems are particularly pervasive when one deals with an unstructured case base we will discuss both algorithms and a system for solving these problems as the ability tocontain the knowledge acquisition problem is of paramount importance our methods allow one to express relevant domain expertise for detecting both redundancy and inconsistency naturally and effortlessly empirical evaluations of the system prove the effectiveness of the methods in several large domains probabilistic question answering on the web web based search engines such as google and northernlight return documents that are relevant to a user query not answers to user questions we have developed an architecture that augments existing search engines so that they support natural language question answering the process entails five steps query modulation document retrieval passage extraction phrase extraction and answer ranking in this paper we describe some probabilistic approaches to the last three of these stages we show how our techniques apply to a number of existing search en 1 radev et al 2 gines and we also present results contrasting three different methods for question answering our algorithm probabilistic phrase reranking ppr uses proximity and question type features and achieves a total reciprocal document rank of 20 on the trec8 corpus our techniques have been implemented as a web accessible system called nsir automated collaborative filtering applications for online recruitment services online recruitment services suffer from shortcomings due to traditional search techniques most users fail to construct queries that provide an adequate and accurate description of their job requirements leading to imprecise search results we investigate one potential solution that combines implicit profiling methods and automated collaborative filtering acf techniques to build personalised query less job recommendations two acf strategies are implemented and evaluated in the jobfinder domain 1 introduction online recruitment services have emerged as one of the most successful and popular information services on the internet providing job seekers with a comprehensive database of jobs and a dedicated search engine for example the award winning irish site jobfinder www jobfinder ie however like many similar internet applications jobfinder suffers from shortcomings due to its reliance on traditional database technology and the client pull information access mode crawling the hidden web current day crawlers retrieve content only from the publicly indexable web i e the set of web pages reachable purely by following hypertext links ignoring search forms and pages that require authorization or prior registration in particular they ignore the tremendous amount of high quality content hidden behind search forms in large searchable electronic databases in this paper we address the problem of designing a crawler capable of extracting content from this hidden web we introduce a generic operational model of a hidden web crawler and describe how this model is realized in hiwe hidden web exposer a prototype crawler built at stanford we introduce a new layout based information extraction technique lite and demonstrate its use in automatically extracting semantic information from search forms and response pages we also present results from experiments conducted to test and validate our techniques 1 adaptation techniques for intrusion detection and intrusion response systems this paper examines techniques for providing adaptation in intrusion detection and intrusion response systems as attacks on computer systems are becoming increasingly numerous and sophisticated there is a growing need for intrusion detection and response systems to dynamically adapt to better detect and respond to attacks the adaptive hierarchical agentbased intrusion detection system aha ids provides detection adaptation by adjusting the amount of system resources devoted to the task of detecting intrusive activities this is accomplished by dynamically invoking new combinations of lower level detection agents in response to changing circumstances and by adjusting the confidence associated with these lower level agents the adaptive agentbased intrusion response system aairs provides response adaptation by weighting those responses that have been successful in the past over those techniques that have not been as successful as a result the more successful responses are used virtual enterprise design bdi agents vs objects current research identifying architectures for a virtual enterprise has moved from information modelling to role modelling thus a high level of autonomy results from the distribution of responsibilities capabilities and knowledge among different business units in the virtual enterprise at the design stage current trends tend towards using object oriented technology as an effective abstract system design and implementation methodology we argue that applying the software agent paradigm to the virtual enterprise provides various advantages on both the design and operational levels we further show that the belief desire intention agent architecture has additional abilities of mapping real world business unit autonomy and interaction we also introduce the belief desire intention agent paradigm capability of facilitating highly flexible agile enterprise design and implementation 1 incremental learning of explanation patterns and their indices this paper describes how a reasoner can improve its understanding of an incompletely understood domain through the application of what it already knows to novel problems in that domain recent work in ai has dealt with the issue of using past explanations stored in the reasoner s memory to understand novel situations however this process assumes that past explanations are well understood and provide good lessons to be used for future situations this assumption is usually false when one is learning about a novel domain since situations encountered previously in this domain might not have been understood completely instead it is reasonable to assume that the reasoner would have gaps in its knowledge base by reasoning about a new situation the reasoner should be able to fill in these gaps as new information came in reorganize its explanations in memory and gradually evolve a better understanding of its domain we present a story understanding program that retrieves past explan similarity query processing in image databases chitra is a prototype cbir system we are building it uses a four layer data model we have developed and enables retrieval based on high level concepts such as retrieve images of mountains and retrieve images of mountains and sunset this paper deals with some issues about query processing encountered in the implementation of the system the contributions of this paper can be summarized in terms of processing the following four example queries i 1 i 2 i k are images q 1 retrieve images similar to i 1 based on color q 2 retrieve images similar to i 1 based on color and texture q 3 retrieve images similar to i 1 i 2 i k based on color q 4 retrieve images similar to i 1 i 2 i k based on color and texture first a brief review of basic cbir query processing literature is provided processing of q 1 processing of q 2 involves efficient evaluation of combining functions a problem that has attracted research attention in recent time supporting distributed cooperative work in cagis this paper describes how the cagis environment can be used to manage work processes cooperative processes and how to share and control information in a distributed heterogeneous environment we have used a conference organising process as a scenario and applied our cagis environment on this process the cagis environment consists of three main parts a document management system a process management system and a transaction management system keywords web based software engineering internet computing java xml intelligent agent software database systems document modelling process modelling transaction modelling 1 introduction after the introduction of the internet more and more projects are taking place in heterogeneous environments where both people information and working processes are distributed work is often dynamic and cooperative and involves multiple actors with different kinds of needs in these settings there is a need to help people coordinate their potter s wheel an interactive data cleaning system cleaning data of errors in structure and content is important for data warehousing and integration current solutions for data cleaning involve many iterations of data auditing to find errors and long running transformations to fix them users need to endure long waits and often write complex transformation scripts we present potter s wheel an interactive data cleaning system that tightly integrates transformation and discrepancy detection users gradually build transformations to clean the data by adding or undoing transforms on a spreadsheet like interface the effect of a transform is shown at once on records visible on screen these transforms are specified either through simple graphical operations or by showing the desired effects on example data values in the background potter s wheel automatically infers structures for data values in terms of user defined domains and accordingly checks for constraint violations thus users can gradually build a transformation as discrepancies are found and clean the data without writing complex programs or enduring long delays 1 the shopping jacket wearable computing for the consumer as part of the bristol wearable computing initiative we are exploring location sensing systems suitable for use with wearable computing in this paper we present our findings and in particular a wearable application the shopping jacket which relies on a minimal infrastructure to be effective we use two positioning devices pingers and gps the pinger is used to signal the presence of a shop and to indicate the type of shop and it s website the gps is used to disambiguate which branch of a high street chain we are passing the wearable uses this information to determine whether the wearer needs to be alerted that they are passing an interesting shop or to direct the wearer around a shopping mall the shopping jacket integrates a wearable cardpc gps and pinger receivers a near field radio link hand held display gsm data telephone and a speech interface into a conventional sports blazer keywords wearable computer location sensing gps pinger shoppin the well mannered wearable computer in this paper we describe continuing work being carried out as part of the bristol wearable computing initiative we are interested in the use of context sensors to improve the usefulness of wearable computers a cyberjacket incorporating a tourist guide application has been built and we have experimented with location and movement sensing devices to improve its performance in particular we have researched processing techniques for data from accelerometers which enable the wearable computer to determine the user s activity means end plan recognition towards a theory of reactive recognition this paper draws its inspiration from current work in reactive planning to guide plan recognition using plans as recipes the plan recognition process guided by such a library of plans is called means end plan recognition an extension of dynamic logic called dynamic agent logic is introduced to provide a formal semantics for means end plan recognition and its counterpart means end plan execution the operational semantics given by algorithms for means end plan recognition are then related to the provability of formulas in the dynamic agent logic this establishes the relative soundness and completeness of the algorithms with respect to a given library of plans some of the restrictive assumptions underlying means end plan recognition are then relaxed to provide a theory of reactive recognition that allows for changes in the external world during the recognition process reactive recognition when embedded with the mental attitudes of belief desire and intention leads to a po bdi agents from theory to practice the study of computational agents capable of rational behaviour has received a great deal of attention in recent years theoretical formalizations of such agents and their implementations have proceeded in parallel with little or no connection between them this paper explores a particular type of rational agent a beliefdesire intention bdi agent the primary aim of this paper is to integrate a the theoretical foundations of bdi agents from both a quantitative decision theoretic perspective and a symbolic reasoning perspective b the implementations of bdi agents from an ideal theoretical perspective and a more practical perspective and c the building of large scale applications based on bdi agents in particular an air traffic management application will be described from both a theoretical and an implementation perspective introduction the design of systems that are required to perform high level management and control tasks in complex dynamic environments is becoming formal models and decision procedures for multi agent systems the study of computational agents capable of rational behaviour has received a great deal of attention in recent years a number of theoretical formalizations for such multiagent systems have been proposed however most of these formalizations do not have a strong semantic basis nor a sound and complete axiomatization hence it has not been clear as to how these formalizations could be used in building agents in practice this paper explores a particular type of multi agent system in which each agent is viewed as having the three mental attitudes of belief b desire d and intention i it provides a family of multi modal branching time bdi logics with a semantics that is grounded in traditional decision theory and a possible worlds framework categorizes them provides sound and complete axiomatizations and gives constructive tableaubased decision procedures for testing the satisfiability and validity of formulas the computational complexity of these decision procedures is n agentspeak l bdi agents speak out in a logical computable language abstract belief desire intention bdi agents have been investigated by many researchers from both a theoretical specification perspectiveand a practical design perspective however there still remains a large gap between theory and practice the main reason for this has been the complexity of theorem proving or modelchecking in these expressive specification logics hence the implemented bdi systems have tended to use the three major attitudes as data structures rather than as modal operators in this paper we provide an alternative formalization of bdi agents by providing an operational and proof theoretic semantics of a language agentspeak l this language can be viewed as an abstraction of one of the implemented bdi systems i e prs and allows agent programs to be written and interpreted in a manner similar to that of horn clause logic programs we show how to perform derivations in this logic using a simple example these derivations can then be used to prove the properties satisfied by bdi agents 1 a unified view of plans as recipes plans as recipes or abstract structures as well as plans as mental attitudes that guide an agent in its planning process has been enthusiastically embraced by both philosophers and ai practitioners they play a central role in a class of rational agents called belief desire intention bdi agents this dual view of plans can not only be used for efficient planning but can also be used for recognizing the plans of other agents coordinating one s actions and achieving joint intentions with other members of a larger collective or team and finally recognizing the collective plans and intentions of other teams in this paper we start with a simple notion of execution plans and discuss its operational semantics we progressively extend this notion of plans to recognition plans joint execution plans and joint recognition plans the primary contribution of this paper is in providing an integrated view of plans that facilitate individual an collective planning and recognition 1 int guided crossover a new operator for genetic algorithm based optimization genetic algorithms gas have been extensively used in different domains as a means of doing global optimization in a simple yet reliable manner they have a much better chance of getting to global optima than gradient based methods which usually converge to local sub optima however gas have a tendency of getting only moderately close to the optima in a small number of iterations to get very close to the optima the ga needs a very large number of iterations whereas gradient based optimizers usually get very close to local optima in a relatively small number of iterations in this paper we describe a new crossover operator which is designed to endow the ga with gradient like abilities without actually computing any gradients and without sacrificing global optimality the operator works by using guidance from all members of the ga population to select a direction for exploration empirical results in two engineering design domains and across both binary and floating point representa an adaptive penalty approach for constrained genetic algorithm optimization in this paper we describe a new adaptive penalty approach for handling constraints in genetic algorithm optimization problems the idea is to start with a relatively small penalty coefficient and then increase it or decrease it on demand as the optimization progresses empirical results in several engineering design domains demonstrate the merit of the proposed approach 1 introduction genetic algorithms gas goldberg 1989 are search algorithms that mimic the behavior of natural selection gas attempt to find the best solution to some problem e g the maximum of a function by generating a collection population of potential solutions individuals to the problem through mutation and recombination crossover operations better solutions are hopefully generated out of the current set of potential solutions this process continues until an acceptably good solution is found gas have many advantages over other search techniques including the ability to deal with qualitativ interacting with spatially augmented reality traditional user interfaces for off the desktop applications are designed to display the output on flat 2d surfaces while the input is with 2d or 3d devices in this paper we focus on projectorbased augmented reality applications we describe a framework to easily incorporate the interaction on a continuum of display surfaces and input devices we first create a 3d understanding of the relationship between the user the projectors and the display surfaces then we use some new calibration and rendering techniques to create a simple procedure to effectively illuminate the surfaces we describe various underlying techniques and discuss the results in the context of three different applications automatically analyzing and organizing music archives we are experiencing a tremendous increase in the amount of music being made available in digital form with the creation of large multimedia collections however we need to devise ways to make those collections accessible to the users while music repositories exist today they mostly limit access to their content to query based retrieval of their items based on textual meta information with some advanced systems supporting acoustic queries what we would like to have additionally is a way to facilitate exploration of musical libraries we thus need to automatically organize music according to its sound characteristics in such a way that we nd similar pieces of music grouped together allowing us to nd a classical section or a hard rock section etc in a music repository in this paper we present an approach to obtain such an organization of music data based on an extension to our somlib digital library system for text documents particularly we employ the self organizing map to create a map of a musical archive where pieces of music with similar sound characteristics are organized next to each other on the two dimensional map display locating a piece of music on the map then leaves you with related music next to it allowing intuitive exploration of a music archive keywords multimedia music library self organizing map som exploration of information spaces user interface mp3 1 integrating automatic genre analysis into digital libraries with the number and types of documents in digital library systems increasing tools for automatically organizing and presenting the content have to be found while many approaches focus on topic based organization and structuring hardly any system incorporates automatic structural analysis and representation yet genre information unconsciously forms one of the most distinguishing features in conventional libraries and in information searches in this paper we present an approach to automatically analyze the structure of documents and to integrate this information into an automatically created content based organization in the resulting visualization documents on similar topics yet representing dierent genres are depicted as books in diering colors this representation supports users intuitively in locating relevant information presented in a relevant form keywords genre analysis self organizing map som somlib document clustering visualization metaphor graphics 1 a gesture based interaction technique for a planning tool for construction and design in this article we wish to show a method that goes beyond the established approaches of human computer interaction we first bring a serious critique of traditional interface types showing their major drawbacks and limitations promising alternatives are offered by virtual or immersive reality vr and by augmented reality ar the ar design strategy enables humans to behave in a nearly natural way natural interaction means human actions in the real world with other humans and or with real world objects guided by the basic constraints of natural interaction we derive a set of recommendations for the next generation of user interfaces the natural user interface nui our approach to nuis is discussed in the form of a general framework followed by a prototype the prototype tool builds on video based interaction and supports construction and plant layout a first empirical evaluation is briefly presented 1 introduction the introduction of computers in the work place has had build it a planning tool for construction and design it is time to go beyond the established approaches in humancomputer interaction with the augmented reality ar design strategy humans are able to behave as much as possible in a natural way behavior of humans in the real world with other humans and or real world objects following the fundamental constraints of natural way of interacting we derive a set of recommendations for the next generation of user interfaces the natural user interface nui the concept of nui is presented in form of a runnable demonstrator a computer vision based interaction technique for a planning tool for construction and design tasks keywords augmented reality digital desk natural user interface computer vision based interaction towards data warehouse design this paper focuses on data warehouse modelling the conceptual model we defined is based on object concepts extended with specific concepts like generic classes temporal classes and archive classes the temporal classes are used to store the detailed evolutions and the archive classes store the summarised data evolutions we also provide a flexible concept allowing the administrator to define historised parts and non historised parts into the warehouse schema moreover we introduce constraints which configure the data warehouse behaviour and these various parts to validate our propositions we describe a prototype dedicated to the data warehouse design keywords conceptual data warehouse model temporal data object modelling 1 introduction in order to make long term managerial decisions companies have to exploit very large volumes of data generally stored in their operational databases the exploitation of these data is sometimes carried out in an empirical way using trad representing sentence structure in hidden markov models for information extraction we study the application of hidden markov models hmms to learning information extractors for ary relations from free text we propose an approach to representing the grammatical structure of sentences in the states of the model we also investigate using an objective function during hmm training which maximizes the ability of the learned models to identify the phrases of interest we evaluate our methods by deriving extractors for two binary relations in biomedical domains our experiments indicate that our approach learns more accurate models than several baseline approaches 1 coordinating heterogeneous work information and representation in medical care introduction the concept of a common information space or cis has become an influential way to think about the use of shared information in collaboration originating in the work of schmidt and bannon 1992 and further explored by bannon and bdker 1997 it was designed to extend then current notions about the role of technology and shared information at the time this was originally proposed a great deal of technical attention was being paid to the development of shared workspace systems e g lu and mantei 1991 ishii et al 1992 these systems attempted to extend the workspaces of conventional single user applications such as word processors and drawing tools allowing synchronous or asynchronous collaboration across digital networks designing effective shared workspace systems presented a range of technical challenges concerning appropriate network protocols synchronisation concurrency control mechanisms and user interface design still over time con vision based speaker detection using bayesian networks the development of user interfaces based on vision and speech requires the solution of a challenging statistical inference problem the intentions and actions of multiple individuals must be inferred from noisy and ambiguous data we argue that bayesian network models are an attractive statistical framework for cue fusion in these applications bayes nets combine a natural mechanism for expressing contextual information with efficient algorithms for learning and inference we illustrate these points through the development of a bayes net model for detecting when a user is speaking the model combines four simple vision sensors face detection skin color skin texture and mouth motion we present some promising experimental results 1 introduction human centered user interfaces based on vision and speech present challenging sensing problems in which multiple sources of information must be combined to infer the user s actions and intentions statistical inference techniques therefore requirements for a group communication service for flare this document explores what the requirements for a group communication service for the framework for location aware augmented reality environments flare are this chapter provides an introduction to flare the next chapter will explain the game rules for the rst application called quazoom that we will build using flare since network partitions are important we rst describe the game rules in the case when there are no partitions and treat the partitioned case in a seperate section mobile collaborative augmented reality the combination of mobile computing and collaborative augmented reality into a single system makes the power of computer enhanced interaction and communication in the real world accessible anytime and everywhere this paper describes our work to build a mobile collaborative augmented reality system that supports true stereoscopic 3d graphics a pen and pad interface and direct interaction with virtual objects the system is assembled from offthe shelf hardware components and serves as a basic test bed for user interface experiments related to computer supported collaborative work in augmented reality a mobile platform implementing the described features and collaboration between mobile and stationary users are demonstrated a wearable 3d augmented reality workspace this poster describes our work to build a wearable augmented reality system that supports true stereoscopic 3d graphics through a pen and pad interface well known 2d user interfaces can be presented to the user whereas the tracking of the pen allows to use direct interaction with virtual objects the system is assembled from off the shelf hardware components and serves as a basic test bed for user interface experiments related to collaboration between stationary and mobile ar users 1 cavestudy an infrastructure for computational steering in virtual reality environments we present the cavestudy system that enables scientists to interactively steer a simulation from a virtual reality vr environment no modification to the source code is necessary cavestudy allows interactive and immersive analysis of a simulation running on a remote computer using a high level description of the simulation the system generates the communication layer based on cavernsoft needed to control the execution and to gather data at runtime we describe three case studies implemented with cavestudy soccer simulation diode laser simulation and molecular dynamics 1 introduction high speed networks and high performance graphics open opportunities for completely new types of applications as a result the world of scientific computing is moving away from the batch oriented management to interactive programs also virtual reality vr systems are now commercially available but so far scientists mainly use them for off line visualization of data sets produced by a simu improving multi class text classification with naive bayes the problem there are billions of text documents available in electronic form more and more are becoming available every day the web itself contains over a billion documents millions of people send e mail every day academic publications and journals are becoming available inelectronicform thesecollections and many others represent a massive amount of information that is easily accessible however seeking value in this huge collection requires organization many web sites offer a hierarchically organized view of the web e mail clients offer a systems for filtering e mail numerous academic communities have a web site that allows searching on papers and shows an organization of papers however organizing documents by hand or creating rules for filtering is painstaking and labor intensive this can be greatly aided by automated classifier systems the accuracy of such systems determines their usefulness we propose to use the support vector machine svm in conjunction with error correcting output codes ecoc to improve the state of the art in text classification motivation previous work in 1998 joachims published results on a set of binary text classification experiments using the svm 4 the svm yielded lower error than many other classification techniques yang followed later with experiments of her own on the same data set 5 she used improved versions of naive bayes nb and knn but still found that the svm performed at least as well as all other classifiers she tried she also found that the linear svm performed as well as polynomial and rbf versions both papers used the svm for binary text classification parallel strands a preliminary investigation into mining the web for bilingual text parallel corpora are a valuable resource for machine translation but at present their availability and utility is limited by genreand domain specificity licensing restrictions and the basic difficulty of locating parallel texts in all but the most dominant of the world s languages a parallel corpus resource not yet explored is the world wide web which hosts an abundance of pages in parallel translation offering a potential solution to some of these problems and unique opportunities of its own this paper presents the necessary first step in that exploration a method for automatically finding parallel translated documents on the web the technique is conceptually simple fully language independent and scalable and preliminary evaluation results indicate that the method may be accurate enough to apply without human intervention 1 introduction in recent years large parallel corpora have taken on an important role as resources in machine translation and multilingual natural la mining the web for bilingual text strand resnik 1998 is a language independent system for automatic discovery of text in parallel translation on the world wide web this paper extends the preliminary strand results by adding automatic language identification scaling up by orders of magnitude and formally ewluating performance the most recent end product is an automaticajly acquired parallel corpus comprising 2491 english french document pairs approximately 1 5 million words per language location systems for ubiquitous computing to serve us well emerging mobile computing applications will need to know the physical location of things so that they can record them and report them to us what lab bench was i standing by when i prepared these tissue samples how should our search and rescue team move to quickly locate all the avalanche victims can i automatically display this stock devaluation chart on the large screen i am standing next to researchers are working to meet these and similar needs by developing systems and technologies that automatically locate people equipment and other tangibles indeed many systems over the years have addressed the problem of automatic location sensing because each approach solves a slightly different problem or supports different applications they vary in many parameters such as the physical phenomena used for location determination the form factor of the sensing apparatus power requirements infrastructure versus portable elements and resolution in time and space to make sense of this domain we have developed a taxonomy to help developers wearable computing meets ubiquitous computing reaping the best of both worlds this paper describes what we see as fundamental diculties in both the pure ubiquitous computing and pure wearable computing paradigms when applied to context aware applications in particular ubiquitous computing and smart room systems tend to have dif culties with privacy and personalization while wearable systems have trouble with localized information localized resource control and resource management between multiple people these diculties are discussed and a peer to peer network of wearable and ubiquitous computing components is proposed as a solution this solution is demonstrated through several implemented applications 1 introduction ubiquitous computing and wearable computing have been posed as polar opposites even though they are often applied in very similar applications here we rst outline the advantages and disadvantages of each and propose that the two perspectives have complementary problems we then attempt to demonstrate that the failing of both ubiquitous enlightened agents in tucson in the network centric computing era applications often involve sets of autonomous unpredictable and possibly mobile entities interacting within open dynamic and possibly unreliable environments intelligent environments are a typical case the complexity of such scenarios requires novel engineering tools providing effective support from the analysis to the deployment stage in this paper we illustrate the impact of a general purpose coordination infrastructure for multiagent systems providing a model a run time and suitable deployment tools on the engineering of such applications as a case study we consider the intelligent management of lights inside a building despite its simplicity this problem endorses the typical challenges of this class of applications the case study is built upon the tucson coordination infrastructure which provides engineers with both the abstractions and the run time support for effectively managing the application complexity i infrastr extending local learners with error correcting output codes error correcting output codes ecocs represent classes with a set of output bits where each bit encodes a binary classification task corresponding to a unique partition of the classes algorithms that use ecocs learn the function corresponding to each bit and combine them to generate class predictions ecocs can reduce both variance and bias errors for multiclass classification tasks when the errors made at the output bits are not correlated they work well with global e g c4 5 but not with local e g nearest neighbor classifiers because the latter use the same information to predict each bit s value which yields correlated errors this is distressing because local learners are excellent classifiers for some types of applications we show that the output bit errors of local learners can be decorrelated by selecting different features for each bit this yields bit specific distance functions which causes different information to be used for each bit s prediction we presen animated agents for procedural training in virtual reality perception cognition and motor control this paper describes steve an animated agent that helps students learn to perform physical procedural tasks the student and steve cohabit a three dimensional simulated mock up of the student s work environment steve can demonstrate how to perform tasks and can also monitor students while they practice tasks providing assistance when needed this paper describes steve s architecture in detail including perception cognition and motor control the perception module monitors the state of the virtual world maintains a coherent representation of it and provides this information to the cognition and motor control modules the cognition module interprets its perceptual input chooses appropriate goals constructs and executes plans to achieve those goals and sends out motor commands the motor control module implements these motor commands controlling steve s voice locomotion gaze and gestures and allowing steve to manipulate objects in the virtual world 1 introduction to ma task oriented dialogs with animated agents in virtual reality we are working towards animated agents that can carry on tutorial task oriented dialogs with human students the agent s objective is to help students learn to perform physical procedural tasks such as operating and maintaining equipment although most research on such dialogs has focused on verbal communication nonverbal communication can play many important roles as well to allow a wide variety of interactions the student and our agent cohabit a threedimensional interactive simulated mock up of the student s work environment the agent steve can generate and recognize speech demonstrate actions use gaze and gestures answer questions adapt domain procedures to unexpected events and remember past actions this paper focuses on steve s methods for generating multi modal behavior contrasting our work with prior work in task oriented dialogs multimodal explanation generation and animated conversational characters introduction we are working towards animated agents that active storage for large scale data mining and multimedia the increasing performance and decreasing cost of processors and memory are causing system intelligence to move into peripherals from the cpu storage system designers are using this trend toward excess compute power to perform more complex processing and optimizations inside storage devices to date such optimiza tions have been at relatively low levels of the stor age protocol at the same time trends in storage density mechanics and electronics are eliminat ing the bottleneck in moving data off the media and putting pressure on interconnects and host processors to move data more efficiently we pro pose a system called active disks that takes advantage of processing power on individual disk drives to run application level code moving por tions of an application s processing to execute directly at disk drives can dramatically reduce data traffic and take advantage of the storage par allelism already present in large systems today we discuss several types of applications that would benefit from this capability with a focus on the areas of database data mining and multime dia we develop an analytical model of the speed ups possible for scan intensive applications in an active disk system we also experiment with a prototype active disk system using relatively low powered processors in comparison to a data base server system with a single fast processor our experiments validate the intuition in our model and demonstrate speedups of 2x on 10 disks across four scan based applications the model promises linear speedups in disk arrays of hundreds of disks provided the application data is large enough thts research was sponsored by darpaflto through order d306 and issued by indian head division nswc under contract noo174 96 0002 additional support was prowded by nsf under grants eec 94 02384 and iri 9625428 and nsf arpa and nasa under nsf agreement iri 9411299 we are also indebted to generous contributions from the member companies of the parallel data consortium hewlett trust and partial typing in open systems of mobile agents we present a partially typed semantics for dp a distributed p calculus the semantics is designed for mobile agents in open distributed systems in which some sites may harbor malicious intentions nonetheless the semantics guarantees traditional type safety properties at good locations by using a mixture of static and dynamic type checking we show how the semantics can be extended to allow trust between sites improving performance and expressiveness without compromising type safety 1 introduction in 13 we presented a type system for controlling the use of resources in a distributed system or network the type system guarantees two properties ffl resource access is always safe e g integer resources are always accessed with integers and string resources are always accessed with strings and ffl resource access is always authorized i e resources may only be accessed by agents that have been granted permission to do so while these properties are desirable they are prop statistical pattern recognition techniques for multimodal human computer interaction and multimedia information processing this paper presents an extensive overview on statistical pattern recognition methods for a variety of different tasks related to multimodal human computer interaction and multimedia information processing typical tasks in the area of human computer interaction include handwriting and gesture recognition as well as pen based retrieval of image databases multimedia information processing includes algorithms for document processing video indexing or face recognition the aim of the paper is to demonstrate to the speech community the usability of classical speech recognition algorithms such as hidden markov models and related statistical pattern recognition techniques for a much larger variety of related problems in man machine communication and the ecient processing and retrieval of multimedia information on behavior classification in adversarial environments in order for robotic systems to be successful in domains with other agents possibly interfering with the accomplishing of goals the agents must be able to adapt to the opponents behavior the more quickly the agents can respond to a new situation the better they will perform we present an approach to doing adaptation which relies on classification of the current adversary into predefined adversary classes for feature extraction we present a windowing technique to abstract useful but not overly complicated features in order to take into account the spatial locality of topological differences we use a previously developed similarity metric the feature extraction and classification steps are fully implemented in the domain of simulated robotic soccer and experimental results are presented a planning algorithm not based on directional search the initiative in strips planning has recently been taken by work on propositional satisfiability best current planners like graphplan and earlier planners originating in the partial order or refinement planning community have proved in many cases to be inferior to general purpose satisfiability algorithms in solving planning problems however no explanation of the success of programs like walksat or relsat in planning has been offered in this paper we discuss a simple planning algorithm that reconstructs the planner in the background of the sat csp approach 1 introduction many of the recent interesting results in ai planning did not originate in traditional planning research but in work on algorithms for checking the satisfiability of propositional formulae strips planning problems have been used as benchmarks to test sat algorithms based on greedy local search kautz and selman 1992 kautz and selman 1996 and new developments bayardo jr and schrag 1997 of the well personality driven social behaviors in believable agents agents are considered believable when they are viewed by an audience as endowed with thoughts desires and emotions typical of different personalities the paper describes our work in progress aimed at realizing believable agents that perform helping behaviors influenced by their own personalities the latter are represented as different clusters of prioritized goals and preferences over plans for achieving the goals the implementation is based on the integration of a statebased planner that serves as the reasoning tool for the agents and a situation driven execution system introduction there is a notion in the arts of believable character it does not mean an honest or reliable character but one that provides the illusion of life and thus permits the audience s suspension of disbelief 1 the idea of believability has long been studied and explored in literature theater film radio drama and other media bates 1994 believability therefore refers to a character s astrolabe a robust and scalable technology for distributed system monitoring management and data mining this paper we describe a new information management service called astrolabe astrolabe monitors the dynamically changing state of a collection of distributed resources reporting summaries of this information to its users like dns astrolabe organizes the resources into a hierarchy of domains which we call zones to avoid confusion and associates attributes with each zone unlike dns zones are not bound to specific servers the attributes may be highly dynamic and updates propagate quickly typically in tens of seconds requirements interaction management ion requirements may be distinguished based on the abstraction level of their description a requirement may be further defined by add new details defined in more specialized subrequirements through specialization of abstract requirements or generalization of detailed requirement a requirement abstraction hierarchy can be defined development p roperties requirements may be distinguished based on their development properties for example a requirement may have just been proposed late r it may be accepted or rejected representational properties requirements may be distinguished based on their representation a requirement may begin as an informal sketch then become a natural language sentence e g the system shall finall y more formal representations such as uml z or predicate cal requirements interaction management definition and scope 6 1999 william n robinson requirements interaction management gsu cis 99 7 culus may be used to express a requir biologically motivated distributed design for adaptive knowledge management we discuss how distributed designs that draw from biological network metaphors can largely improve the current state of information retrieval and knowledge management of distributed information systems in particular two adaptive recommendation systems named talkmine and apweb are discussed in more detail talkmine operates at the semantic level of keywords it leads different databases to learn new and adapt existing keywords to the categories recognized by its communities of users using distributed algorithms syntactic autonomy or why there is no autonomy without symbols and how self organization systems might evolve them two different types of agency are discussed based on dynamically coherent and incoherent couplings with an environment respectively i propose that until a private syntax syntactic autonomy is discovered by dynamically coherent agents there are no significant or interesting types of closure or autonomy when syntactic autonomy is established then because of a process of description based selected self organization open ended evolution is enabled at this stage agents depend in addition to dynamics on localized symbolic memory thus adding a level of dynamical incoherence to their interaction with the environment furthermore it is the appearance of syntactic autonomy which enables much more interesting types of closures amongst agents which share the same syntax to investigate how we can study the emergence of syntax from dynamical systems experiments with cellular automata leading to emergent computation to solve non trivial tasks are discussed rna editing is also mentio beyond schema versioning a flexible model for spatio temporal schema selection schema versioning provides a mechanism for handling change in the structure of database systems and has been investigated widely both in the context of static and temporal databases with the growing interest in spatial and spatio temporal data as well as the mechanisms for holding such data the spatial context within which data items are formatted also becomes an issue this paper presents a generalised model that accommodates temporal spatial and spatio temporal schema versioning within databases towards a model for spatio temporal schema selection schema versioning provides a mechanism for handling change in the structure of database systems and has been investigated widely both in the context of static and temporal databases with the growing interest in spatial and spatio temporal data as well as the mechanisms for holding such data the spatial context within which data is formatted also becomes an issue this paper presents a generalised model that accommodates schema versioning within static temporal spatial and spatio temporal relational and object oriented databases gaia enabling active spaces ubiquitous computing promotes physical spaces with hundreds of specialized embedded devices that increase our productivity alleviate some specific everyday tasks and provide new ways of interacting with the computational environment personal computers lose the focus of attention due to the fact that the computational environment is spread across the physical space therefore the users view of the computational environment is finally extended beyond the physical limits of the computer physical spaces become computer systems or in other terms active spaces however these active spaces require novel system software capable of seamlessly coordinating their hidden complexity our goal is to extend the model provided by current computer systems to allow interaction with physical spaces and their contained entities physical and virtual by means of a single abstraction called active space 1 introduction ubiquitous computing promotes the proliferation of embedded devices specializ legorb and ubiquitous corba the increasing popularity of ubiquitous computing and the new approaches for low consumption shortrange wireless connectivity will enable a future with hundreds of heterogeneous devices interconnected to achieve a common task however communication among those network enabled heterogeneous devices requires standard protocols and well defined interfaces while existing middleware architectures already offer standard mechanisms dcom corba jini they are in most of the cases not suitable for most of the heterogeneous devices the resources required by those middleware solutions normally exceed the computational limits of the heterogeneous devices we present in this paper a minimalist component based object request broker orb that can be dynamically reconfigured and requires for the smallest configuration 6kb of memory introduction the incoming ubiquitous computing trend allows the existence of collections of network enabled devices attached to rooms people and buildings smart playing cards a ubiquitous computing game abstract recent technological advances allow for turning parts of our everyday environment into so called smart environments in this paper we present the smart playing cards application a ubiquitous computing game that augments a classical card game with information technological functionality in contrast to developing new games around the abilities of available technology furthermore we present the requirements such an application makes on a supporting software infrastructure for ubiquitous computing comparison of learning approaches to appearance based 3d object recognition with and without cluttered background we re evaluate the application of support vector machines svm to appearance based 3d object recognition by comparing it to two other learning approaches the system developed at columbia university columbia and a simple image matching system using a nearest neighbor classifier nnc in a first set of experiments we compare correct recognition rates of the segmented 3d object images of the coil database we show that the performance of the simple nnc system compares to the more elaborated columbia and svm systems only when the experimental setting is more demanding i e when we reduce the number of views during the training phase some difference in performance can be observed in a second set of experiments we consider the more realistic task of 3d object recognition with cluttered background also in this case we obtain that the performance of the three systems are comparable only with the recently proposed black white background training scheme bw applied t directsvm a fast and simple support vector machine perceptron we propose a simple implementation of the support vector machine svm for pattern recognition that is not based on solving a complex quadratic optimization problem instead we propose a simple iterative algorithm that is based on a few simple heuristics the proposed algorithm nds high quality solutions in a fast and intuitively simple way in experiments on the coil database on the extended coil database and on the sonar database of the uci irvine repository directsvm is able to nd solutions that are similar to these found by the original svm however directsvm is able to nd these solutions substantially faster while requiring less computational resources than the original svm introduction support vector machines svms belong to the best performing learning algorithms available they have produced remarkable performance in a number of dicult learning tasks without requiring prior knowledge we mention amongst others the following examples in pattern recognition handwr view based 3d object recognition with support vector machines support vector machines have demonstrated excellent results in pattern recognition tasks and 3d object recognition in this contribution we confirm some of the results in 3d object recognition and compare it to other object recognition systems we use di erent pixel level representations to perform the experiments while we extend the setting to the more challenging and practical case when only a limited number of views of the object are presented during training we report high correct classification of unseen views especially considering that no domain knowledge is including into the proposed system finally we suggest an active learning algorithm to reduce further the required number of training views introduction humans are able to recognize everyday 3d objects when shown previously only one or at most a few views of the object in contrast artificial systems must either been shown many views of an object e g 8 or either a lot of knowledge of object structure must 3d hand pose reconstruction using specialized mappings a system for recovering 3d hand pose from monocular color sequences is proposed the system employs a non linear supervised learning framework the specialized mappings architecture sma to map image features to likely 3d hand poses the sma s fundamental components are a set of specialized forward mapping functions and a single feedback matching function the forward functions are estimated directly from training data which in our case are examples of hand joint configurations and their corresponding visual features the joint angle data in the training set is obtained via a cyberglove a glove with 22 sensors that monitor the angular motions of the palm and fingers in training the visual features are generated using a computer graphics module that renders the hand from arbitrary viewpoints given the 22 joint angles the viewpoint is encoded by two real values therefore 24 real values represent a hand pose we test our system both on synthetic sequences and on sequences taken with a color camera the system automatically detects and tracks both hands of the user calculates the appropriate features and estimates the 3d hand joint angles and viewpoint from those features results are encouraging given the complexity of the task trajectory guided tracking and recognition of actions a combined 2d 3d approach is presented that allows for robust tracking of moving people and recognition of actions it is assumed that the system observes multiple moving objects via a single uncalibrated video camera low level features are often insufficient for detection segmentation and tracking of non rigid moving objects therefore an improved mechanism is proposed that integrates low level image processing mid level recursive 3d trajectory estimation and high level action recognition processes a novel extended kalman filter formulation is used in estimating the relative 3d motion trajectories up to a scale factor the recursive estimation process provides a prediction and error measure that is exploited in higher level stages of action recognition conversely higherlevel mechanisms provide feedback that allows the system to reliably segment and maintain the tracking of moving objects before during and after occlusion the 3d trajectory occlusion and segmentation information are utilized in extracting stabilized views of the moving object that are then used as input to action recognition modules trajectory guided recognition tgr is proposed as a new and efficient method for adaptive classification of action the tgr approach is demonstrated using motion history images that are then recognized via a mixture of gaussians classifier the system was tested in recognizing various dynamic human outdoor activities running walking roller blading and cycling experiments with real and synthetic data sets are used to evaluate stability of the trajectory estimator with respect to noise a synthetic agent system for bayesian modeling human interactions when building statistical machine learning models from real data one of the most frequently encountered difficulties is the limited amount of training data compared to what is needed by the specific learning architecture in order to deal with this problem we have developed a synthetic simulated agent training system that let us develop flexible prior models for recognizing human interactions in a pedestrian visual surveillance task we demonstrate the ability to use these prior models to accurately classify real human behaviors and interactions with no additional tuning or training 1 introduction agent based solutions have been developed for many different application domains and field tested agent systems are steadily increasing in number agents are currently being applied in domains as diverse as computer games and interactive cinema information retrieval and filtering user interface design electronic commerce and industrial process control in this paper we propose a nove two decades of statistical language modeling where do we go from here statistical language models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies since the first significant model was proposed in 1980 many attempts have been made to improve the state of the art we review them here point to a few promising directions and argue for a bayesian approach to integration of linguistic theories with data 1 outline statistical language modeling slm is the attempt to capture regularities of natural language for the purpose of improving the performance of various natural language applications by and large statistical language modeling amounts to estimating the probability distribution of various linguistic units such as words sentences and whole documents statistical language modeling is crucial for a large variety of language technology applications these include speech recognition where slm got its start machine translation document classification and routing optical character recognition information retrieval handwriting recognition spelling correction and many more in machine translation for example purely statistical approaches have been introduced in 1 but even researchers using rule based approaches have found it beneficial to introduce some elements of slm and statistical estimation 2 in information retrieval a language modeling approach was recently proposed by 3 and a statistical information theoretical approach was developed by 4 slm employs statistical estimation techniques using language training data that is text because of the categorical nature of language and the large vocabularies people naturally use statistical techniques must estimate a large number of parameters and consequently depend critically on the availability of large amounts of training data continuous categories for a mobile robot autonomous agents make frequent use of knowledge in the form of categories categories of objects human gestures web pages and so on this paper describes a way for agents to learn such categories for themselves through interaction with the environment in particular the learning algorithm transforms raw sensor readings into clusters of time series that have predictive value to the agent we address several issues related to the use of an uninterpreted sensory apparatus and show specific examples where a pioneer 1 mobile robot interacts with objects in a cluttered laboratory setting introduction there is nothing more basic than categorization to our thought perception action and speech lakoff 1987 for autonomous agents categories often appear as abstractions of raw sensor readings that provide a means for recognizing circumstances and predicting effects of actions for example such categories play an important role for a mobile robot that navigates around obstacles view security as the basis for data warehouse security access permissions in a data warehouse are currently managed in a separate world from the sources policies the consequences are inconsistencies slow response to change and wasted administrative work we present a different approach which treats the sources exported tables and the warehouse as part of the same distributed database our main result is a way to control derived products by extending sql grants rather than creating entirely new mechanisms we provide a powerful sound inference theory that derives permissions on warehouse tables both materialized and virtual making the system easier to administer and its applications more robust we also propose a new permission construct suitable for views that filter data from mutually suspicious parties 1 introduction a key challenge for data warehouse security is how to manage the entire system coherently from sources and their export tables to warehouse stored tables conventional and cubes and vi administering permissions for distributed data factoring and automated inference we extend sql s grant revoke model to handle all administration of permissions in a distributed database the key idea is to factor permissions into simpler decisions that can be administered separately and for which we can devise sound inference rules the model enables us to simplify administration via separation of concerns between technical dbas and domain experts and to justify fully automated inference for some permission factors we show how this approach would coexist with current practices based on sql permissions keywords access permissions derived data view federation warehouse 1 complex aggregation at multiple granularities datacube queries compute simple aggregates at multiple granularities in this paper we examine the more general and useful problem of computing a complex subquery involving multiple dependent aggregates at multiple granularities we call such queries multi feature cubes an example is broken down by all combinations of month and customer find the fraction of the total sales in 1996 of a particular item due to suppliers supplying within 10 of the minimum price within the group showing all subtotals across each dimension we classify multi feature cubes based on the extent to which fine granularity results can be used to compute coarse granularity results this classification includes distributive algebraic and holistic multi feature cubes we provide syntactic sufficient conditions to determine when a multi feature cube is either distributive or algebraic this distinction is important because as we show existing datacube evaluation algorithms can be used to compute multif a snow based face detector a novel learning approach for human face detection using a network of linear units is presented the snow learning architecture is a sparse network of linear functions over a pre defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features a wide range of face images in different poses with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces experimental results on commonly used benchmark data sets of a wide range of face images show that the snow based approach outperforms methods that use neural networks bayesian methods support vector machines and others furthermore learning and evaluation using the snow based method are significantly more efficient than with other methods using handheld devices in synchronous collaborative scenarios in this paper we present a platform specially designed for groupware applications running on handheld devices common groupware platforms request desktop computers as underlying hardware platforms the fundamental different nature of handheld devices has a great impact on the platform e g resource limitations have to be considered the network is slow and unstable often personal data are stored on handheld devices thus mechanisms have to ensure privacy these considerations lead to the quickstep platform sample applications developed with quickstep demonstrate the strengths of the quickstep environment 1 introduction collaborative applications help a group to e g collaboratively create documents write agendas or schedule appointments a common taxonomy 3 classifies collaborative applications by time and space with same place and different places attributes on the space axis and same time synchronous and different time asynchronous ones on the time programming satan s agents mobile agent security is still a young discipline and most naturally the focus up to the time of writing was on inventing new cryptographic protocols for securing various aspects of mobile agents however past experience shows that protocols can be flawed and flaws in protocols can remain unnoticed for a long period of time the game of breaking and fixing protocols is a necessary evolutionary process that leads to a better understanding of the underlying problems and ultimately to more robust and secure systems although to the best of our knowledge little work has been published on breaking protocols for mobile agents it is inconceivable that the multitude of protocols proposed so far are all flawless as it turns out the opposite is true we identify flaws in protocols proposed by corradi et al karjoth et al and karnik et al including protocols based on secure coprocessors relational learning via propositional algorithms an information extraction case study this paper develops a new paradigm for relational learning which allows for the representation and learning of relational information using propositional means this paradigm suggests different tradeoffs than those in the traditional approach to this problem the ilp approach and as a result it enjoys several significant advantages over it in particular the new paradigm is more flexible and allows the use of any propositional algorithm including probabilistic algorithms within it we evaluate the new approach on an important and relation intensive task information extraction and show that it outperforms existing methods while being orders of magnitude more efficient 1 a scalable and secure global tracking service for mobile agents abstract in this paper we propose a global tracking service for mobile agents which is scalable to the internet and accounts for security issues as well as the particularities of mobile agents frequent changes in locations the protocols we propose address agent impersonation malicious location updates as well as security issues that arise from profiling location servers and threaten the privacy of agent owners we also describe the general framework of our tracking service and some evaluation results of the reference implementation we made learning to resolve natural language ambiguities a unified approach we analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space each of the methods makes a priori assumptions which it employs given the data when searching for its hypothesis nevertheless as we show it searches a space that is as rich as the space of all linear separators we use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space without further assumptions on the domain or a specific problem we present such an approach a sparse network of linear separators utilizing the winnow learning algorithm and show how to use it in a variety of ambiguity resolution problems the learning approach presented is attribute efficient and therefore appropriate for domains having very large number of attributes in particular we present an extensive experimental compact fuzzy models through complexity reduction and evolutionary optimization genetic algorithms gas and other evolutionary optimization methods to design fuzzy rules from data for systems modeling and classification have received much attention in recent literature we show that different tools for modeling and complexity reduction can be favorably combined in a scheme with ga based parameter optimization fuzzy clustering rule reduction rule base simplification and constrained genetic optimization are integrated in a data driven modeling scheme with low human intervention attractive models with respect to compactness transparency and accuracy are the result of this symbiosis i introduction we focus on learning fuzzy rules from data with low human intervention many tools to initialize tune and manipulate fuzzy models have been developed we show that different tools can be favorably combined to obtain compact fuzzy rule based models of low complexity with still good approximation accuracy a modeling scheme is presented that combine four pr learning fuzzy classification rules from data automatic design of fuzzy rule based classification systems based on labeled data is considered it is recognized that both classification performance and interpretability are of major importance and e ort is made to keep the resulting rule bases small and comprehensible an iterative approach for developing fuzzy classifiers is proposed the initial model is derived from the data and subsequently feature selection and rule base simplification are applied to reduce the model and a ga is used for model tuning an application to the wine data classification problem is shown 1 introduction rule based expert systems are often applied to classification problems in fault detection biology medicinem etc fuzzy logic improves classification and decision support systems by allowing the use of overlapping class definitions and improves the interpretability of the results by providing more insight into the classifier structure and decision making process 13 the automatic determination circumventing dynamic modeling evaluation of the error state kalman filter applied to mobile robot localization the mobile robot localization problem is treated as a two stage iterative estimation process the attitude is estimated first and is then available for position estimation the indirect error state form of the kalman filter is developed for attitude estimation when applying gyro modeling the main benefit of this choice is that complex dynamic modeling of the mobile robot and its interaction with the environment is avoided the filter optimally combines the attitude rate information from the gyro and the absolute orientation measurements the proposed implementation is independent of the structure of the vehicle or the morphology of the ground the method can easily be transfered to another mobile platform provided it carries an equivalent set of sensors the 2d case is studied in detail first results of extending the approach to the 3d case are presented in both cases the results demonstrate the efficacy of the proposed method 1 introduction on july 4th 1997 the mars pathfinde personality in synthetic agents id a043 personality in synthetic agents rousseau daniel ksl stanford university hayes roth barbara ksl stanford university abstract personality characterizes an individual through a set of psychological traits that influence his or her behavior combining visions from psychology artificial intelligence and theater we are studying the use of personality by intelligent automated actors able to improvise their behavior in order to portray characters and to interact with users in a multimedia environment we show how psychological personality traits can be exploited to produce a performance that is theatrically interesting and believable without being completely predictable we explain how personality can influence moods and interpersonal relationships we describe the model of a synthetic actor that takes into account those concepts to choose its behavior in a given context in order to test our approach we observe the performance of autonomous actors portraying waiters with di improvisational synthetic actors with flexible personalities we provide synthetic agents as intelligent actors that can improvise their behaviors in interactive environments without detailed planning just as human improvisers do their behavior is based on the directions they receive high level scenarios user commands and personalities of the characters portrayed we propose a social psychological model that enables an author to define a character s personality influenced by moods and interpersonal relationships using examples of characters and experiments with users we show how such a model can be exploited by synthetic actors to produce performances that are theatrically interesting believable and diverse keywords synthetic actors personality improvisation believability entertainment 1 introduction personality is the set of psychological traits that distinguish an individual from all others and characterize his or her behavior hayes roth et al 1996 such traits can easily be recognized by others and people are commonly a social psychological model for synthetic actors in the virtual theater project we provide synthetic actors that portray fictive characters by improvising their behavior in a multimedia environment actors are either autonomous or avatars directed by users their improvisation is based on the directions they receive and the context directions can take different forms high level scenarios user commands and personality changes in the character portrayed in this paper we look at this last form of direction we propose a social psychological model in which we can define personality traits that depend on the values of moods and attitudes we show how such a model can be exploited by synthetic actors to produce performances that are theatrically interesting believable and diverse an application the cybercaf is used to test those features content areas synthetic actors improvisation believability 3 1 introduction in the virt cache digests this paper presents cache digest a novel protocol and optimization technique for cooperative web caching cache digest allows proxies to make information about their cache contents available to peers in a compact form a peer uses digests to identify neighbors that are likely to have a given document cache digest is a promising alternative to traditional per request query reply schemes such as icp we discuss the design ideas behind cache digest and its implementation in the squid proxy cache the performance of cache digest is compared to icp using real world web caches operated by nlanr our analysis shows that cache digest outperforms icp in several categories finally we outline improvements to the techniques we are currently working on 1 introduction one of the most difficult problems in the design of web cache hierarchies is efficiently locating objects held in neighbor caches when a cache needs to forward a request how does it know whether to use a sibling a parent or p rotation invariant neural network based face detection in this paper we present a neural network based face detection system unlike similar systems which are limited to detecting upright frontal faces this system detects faces at any degree of rotation in the image plane the system employs multiple networks the first is a router network which processes each input window to determine its orientation and then uses this information to prepare the window for one or more detector networks we present the training methods for both types of networks we also perform sensitivity analysis on the networks and present empirical results on a large test set finally we present preliminary results for detecting faces which are rotated out of the image plane such as profiles and semi profiles this work was partially supported by grants from hewlett packard corporation siemens corporate research inc the department of the army army research office under grant number daah04 94 g 0006 and by the office of naval research under grant number a computational model of word learning from multimodal sensory input how do infants segment continuous streams of speech to discover words of their language current theories emphasize the role of acoustic evidence in discovering word boundaries cutler 1991 brent 1999 de marcken 1996 friederici wessels 1993 see also bolinger gertsman 1957 to test an alternate hypothesis we recorded natural infant directed speech from caregivers engaged in play with their pre linguistic infants centered around common objects we also recorded the visual context in which the speech occurred by capturing images of these objects we analyzed the data using two computational models one of which processed only acoustic recordings and a second model which integrated acoustic and visual input the models were implemented using standard speech and vision processing techniques enabling the models to process sensory data we show that using visual context in conjunction with spoken input dramatically improves learning when compared with using acoustic evidence alone integration of speech and vision using mutual information we are developing a system which learns words from co occurring spoken and visual input the goal is to automatically segment continuous speechatword boundaries without a lexicon and to form visual categories which correspond to spoken words mutual information is used to integrate acoustic and visual distance metrics in order to extract an audio visual lexicon from raw input wereport results of experiments with a corpus of infant directed speech and images 1 introduction we are developing systems which learn words from co occurring audio and visual input 5 4 input consists of naturally spoken mutliword utterances paired with visual representations of object shapes figure 1 output of the system is an audio visual lexicon of sound shape associations which encode acoustic forms of words or phrases and their visually grounded referents we assume that in general the audio and visual signals are uncorrelated in time however when a wordisspoken its visual representatio toward optimal active learning through sampling estimation of error reduction this paper presents an active learning method that directly optimizes expected future error this is in contrast to many other popular techniques that instead aim to reduce version space size these other methods are popular because for many learning models closed form calculation of the expected future error is intractable our approach is made feasible by taking a sampling approach to estimating the expected reduction in error due to the labeling of a query in experimental results on two real world data sets we reach high accuracy very quickly sometimes with four times fewer labeled examples than competing methods 1 coastal navigation mobile robot navigation with uncertainty in dynamic environments ships often use the coasts of continents for navigation in the absence of better tools such as gps since being close to land allows sailors to determine with high accuracy where they are similarly for mobile robots in many environments global and accurate localization is not always feasible environments can lack features and dynamic obstacles such as people can confuse and block sensors in this paper we demonstrate a technique for generating trajectories that take into account both the information content of the environment and the density of the people in the environment these trajectories reduce the average positional certainty as the robot moves reducing the likelihood the robot will become lost at any point our method was successfully implemented and used by the mobile robot minerva a museum tourguide robot for a 2 week period in the smithsonian national museum of american history 1 introduction one essential component of any operational mobile robot system is the ab optimizing learning in image retrieval combining learning with vision techniques in interactive image retrieval has been an active research topic during the past few years however existing learning techniques either are based on heuristics or fail to analyze the working conditions furthermore there is almost no in depth study on how to effectively learn from the users when there are multiple visual features in the retrieval system to address these limitations in this paper we present a vigorous optimization formulation of the learning process and solve the problem in a principled way by using lagrange multipliers we have derived explicit solutions which are both optimal and fast to compute extensive comparisons against state ofthe art techniques have been performed experiments were carried out on a large size heterogeneous image collection consisting of 17 000 images retrieval performance was tested under a wide range of conditions various evaluation criteria including precision recall curve and rank measure have demonstrated the effectiveness and robustness of the proposed technique 1 image retrieval current techniques promising directions and open issues this paper provides a comprehensive survey of the technical achievements in the research area of image retrieval especially content based image retrieval an area that has been so active and prosperous in the past few years the survey includes 100 papers covering the research aspects of image feature representation and extraction multidimensional indexing and system design three of the fundamental bases of content based image retrieval furthermore based on the state of the art technology available now and the demand from real world applications open research issues are identified and future promising research directions are suggested c 1999 academic press 1 capacity augmenting schema changes on object oriented databases towards increased interoperability the realization of capacity augmenting schema changes on a shared database while providing continued interoperability to active applications has been recognized as a hard open problem a novel three pronged process called transparent object schema evolution tose is presented that successfully addresses this problem tose uses the combination of views and versioning to simulate schema changes requested by one application without affecting other applications interoperating on a shared oodb the approach is of high practical relevance as it builds upon schema evolution support offered by commercial oodbmss keywords transparent schema evolution object oriented views object oriented databases application migration 1 introduction current schema evolution technology suffers from the problem that schema updates on a database shared by interoperating applications often have catastrophic consequences bkkk87 kc88 ms93 ps87 ts93 zic91 in such a multi user environment a schema c finding location using omnidirectional video on a wearable computing platform in this paper we present a framework for a navigation system in an indoor environment using only omnidirectional video within a bayesian framework we seek the appropriate place and image from the training data to describe what we currently see and infer a location the posterior distribution over the state space conditioned on image similarity is typically not gaussian the distribution is represented using sampling and the location is predicted and verified over time using the condensation algorithm the system does not require complicated feature detection but uses a simple metric between two images even with low resolution input the system may achieve accurate results with respect to the training data when given favorable initial conditions 1 introduction and previous work recognizing location is a difficult but often essential part of identifying a wearable computer user s context location sensing may be used to provide mobility aids for the blind 13 spatially based not application of moving objects and spatiotemporal reasoning in order to predict future variations of moving objects which general attributes locations and regions of spatial objects are changed over time spatiotemporal data domain knowledge and spatiotemporal operations are required to process together with temporal and spatial attributes of data however conventional researches on temporal and spatial reasoning cannot be applied directly to the inference using moving objects because they have been studied separately on temporal or spatial attribute of data therefore in this paper we not only define spatial objects in time domain but also propose a new type of moving objects and spatiotemporal reasoning model that has the capability of operation and inference for moving objects the proposed model is made up of spatiotemporal database gis tool and inference engine for application of spatiotemporal reasoning using moving objects and they execute operations and inferences for moving objects finally to show the applicability of the proposed model a proper domain is established for the battlefield analysis system to support commander s decision making in the army operational situation and it is experimented with this domain engineering executable agents using multi context systems in the area of agent based computing there are many proposals for specific system architectures and a number of proposals for general approaches to building agents as yet however there are comparatively few attempts to relate these together and even fewer attempts to provide methodologies which relate designs to architectures and then to executable agents this paper provides a first attempt to address this shortcoming we propose a general method of specifying logic based agents which is based on the use of multi context systems and give examples of its use the resulting specifications can be directly executed and we discuss an implementation which makes this direct execution possible explorations in asynchronous teams the subject of this thesis is the a teams formalism this formalism facilitates the organization of multiple algorithms encapsulated as autonomous agents into cooperating teams to solve difficult problems the ateams formalism is one of many agent based systems and i start by providing a taxonomy of agent based systems that allows us to see they how a teams relate to other agent based systems a teams are constructed from memories that store solutions and agents that work on those solutions ateams are open to the addition of new memories as well as of new agents sets of memories and agents can also be combined in different ways to create a variety of customized a teams as new memories and agents are created they can be added to existing repositories and reused for future applications the automatic construction of problem specific custom a teams from repositories of components has been a long standing goal of research in a teams current guidelines for a team construction requir 3d spatial layouts using a teams spatial layout is the problem of arranging a set of components in an enclosure such that a set of objectives and constraints is satisfied the constraints may include non interference of objects accessibility requirements and connection cost limits spatial layout problems are found primarily in the domains of electrical engineering and mechanical engineering in the design of integrated circuits and mechanical or electromechanical artifacts traditional approaches include ad hoc or specialized heuristics genetic algorithms and simulated annealing the a teams approach provides a way of synergistically combining these approaches in a modular agent based fashion a teams are also open to the addition of new agents modifications in the task requirements translate to modifications in the agent mix in this paper we describe how modular a team based optimization can be used to solve 3 dimensional spatial layout problems abduction with negation as failure for active and reactive rules recent work has suggested abductive logic programming as a suitable formalism to represent active databases and intelligent agents in particular abducibles in abductive logic programs can be used to represent actions and integrity constaints in abductive logic programs can be used to represent active rules of the kind encountered in active databases and reactive rules incorporating reactive behaviour in agents one would expect that in this approach abductive proof procedures could provide the engine underlying active database management systems and the behaviour of agents we analyse existing abductive proof procedures and argue that they are inadequate in handling these applications the inadequacy is due to the inappropriate treatment of negative literals in integrity constraints we propose a new abductive proof procedure and give examples of how this proof procedure can be used to achieve active behaviour in deductive databases and reactivity in agents final abduction with negation as failure for active databases and agents recent work has suggested abductive logic programming as a suitable formalism to represent active databases and intelligent agents in particular abducibles and integrity constaints in abductive logic programs can be used respectively to represent actions and active reactive rules one would expect that in this approach abductive proof procedures could provide the engine underlying active database management systems and agents we analyse existing abductive proof procedures and argue that they are inadequate in handling these applications the inadequacy is due to the incorrect treatment of negative literals in integrity constraints we propose a new abductive proof procedure and give examples of how this proof procedure can be used to achieve active behaviour in deductive databases and proactivity and reactivity in agents computational logic and multi agent systems a roadmap agent based computing is an emerging computing paradigm that has proved extremely successful in dealing with a number of problems arising from new technological developments and applications in this paper we report the role of computational logic in modeling intelligent agents by analysing existing agent theories agent oriented programming languages and applications as well as identifying challenges and promising directions for future research 1 introduction in the past ten years the eld of agent based computing has emerged and greatly expanded due to new technological developments such as ever faster and cheaper computers fast and reliable interconnections between them as well as the emergence of the world wide web these developments have at the same time opened new application areas such as electronic commerce and posed new problems such as that of integrating great quantities of information and building complex software embedding legacy code the establishment o visual looming as a range sensor for mobile robots this paper describes and evaluates visual looming as a method for monocular range estimation the looming algorithm is based on the relationship between displacements of the observer relative to an object and the resulting change in the size of the object s image on the focal plane of the camera though the looming algorithm has been described in detail in prior reports its usefulness for inexpensive robust ranging has not been realized widely in this paper we analyze visual looming as a visual range sensor for autonomous mobile robots systematic experiments with a pioneer 1 mobile robot show that visual looming can be used to extract ranging information much as with sonar the accuracy of the looming algorithm is found to be significantly more robust than sonar when the object whose distance is being measured is slanted relative to the robot s line of sight on the other hand sonar is better suited for objects that cannot be visually segmented from their background or objects communication primitives for ubiquitous systems or rpc considered harmful rpc is widely used to access and modify remote state its procedural call semantics are argued as an efficient unifying paradigm for both local and remote access our experience with ubiquitous device control systems has shown otherwise rpc semantics of a synchronous blocking invocation on a statically typed interface are overly restrictive inflexible and fail to provide an efficient unifying abstraction for accessing and modifying state in ubiquitous systems this position paper considers other alternatives and proposes the use of comvets conditional mobility aware events as the unifying generic communication paradigm for such systems keywords rpc rmi events comvets corba jini 1 introduction ubiquitous environments or active spaces are the next generation of device control networks a user interacts with an active space by using novel interfaces like speech and gesture input 1 to control her environment and the system interacts with the user using audio video outpu learning languages by collecting cases and tuning parameters we investigate the problem of case based learning of formal languages case based reasoning and learning is a currently booming area of artificial intelligence the formal framework for case based learning of languages has recently been developed by jl93 in an inductive inference manner in this paper we first show that any indexed class of recursive languages in which finiteness is decidable is case based representable but many classes of languages including the class of all regular languages are not case based learnable with a fixed universal similarity measure even if both positive and negative examples are presented next we consider a framework of case based learning where the learning algorithm is allowed to learn similarity measures too to avoid trivial encoding tricks we carefully examine to what extent the similarity measure is going to be learned then by allowing only to learn a few parameters in the similarity measures we show that any indexed class of recursive updating extended logic programs through abduction this paper introduces techniques for updating knowledge bases represented in extended logic programs three di erent types of updates view updates theory updates and inconsistency removal are considered we formulate these updates through abduction and provide methods for computing them with update programs an update program is an extended logic program which specifies changes on abductive hypotheses then updates are computed by the u minimal answer sets of an update program the proposed technique provides a uniform framework for these di erent types of updates and each update is computed using existing procedures of logic programming 1 introduction a knowledge base must be updated when new information arrives there are three cases in updating a knowledge base the first one is that a knowledge base contains two di erent kinds of knowledge variable knowledge and invariable knowledge in this case updates are permitted only on variable knowledge updates on the invari development of a computer aided geographic database design system this paper presents a prototype being developed at ic unicamp to help environmental planners specify their application databases the ultimate goal of the system presented is to reduce the impedance mismatch between the end user s view of the geographic reality and its implementation in geographic information systems gis the prototype offers users the possibility of specifying their application databases using concepts closer to their view of the world by means of an object oriented geographic data model this specification is mapped by the system to an intermediate object oriented schema which can then be transformed into different underlying target geographic dbms the prototype was implemented using c and o2c on the o2 object oriented dbms 1 introduction a geographic information system gis for short is a software that performs data management and retrieval operations for georeferenced data the term refers to data about geographic phenomena associated with its location design and implementation of a deductive query language for odmg compliant object databases introduction deductive object oriented databases doods seek to provide the combined support for the expressive modelling features available in the object oriented data model and the powerful query language features available in deductive databases when successfully engineered this combination can broaden the spectrum of declarative queries that can be supported by the dbms and ease their implementation due to the increased functionality in the query capabilities of the resulting system the extra leverage obtained from support for deductive functionality is relevant to building database middleware for distributed information systems 19 managing semistructured data 11 and for building decision support and knowledge discovery systems 4 unlike deductive relational database systems drdbs which were designed and implemented based on the formal denition of the relational data model by codd and on the widely researched deductive query language model language cons deductive queries in odmg databases the doql approach the deductive object query language doql is a rule based query language designed to provide recursion aggregates grouping and virtual collections in the context of an odmg compliant object database system this paper provides a description of the constructs supported by doql and the algebraic operational semantics induced by doql s query translation approach to implementation the translation consists of a logical rewriting step used to normalise doql expressions into molecular forms and a mapping step that transforms the canonical molecular form into algebraic expressions the paper thus not only describes a deductive language for use with odmg databases but indicates how this language can be implemented using conventional query processing techniques 1 introduction the odmg standard is an important step forward due to the provision of a reference architecture for object databases this architecture encompasses an object model and type system a set of imperative lan recent advances and research problems in data warehousing in the recent years the database community has witnessed the emergence of a new technology namely data warehousing a data warehouse is a global repository that stores pre processed queries on data which resides in multiple possibly heterogeneous operational or legacy sources the information stored in the data warehouse can be easily and efficiently accessed for making effective decisions the on line analytical processing olap tools access data from the data warehouse for complex data analysis such as multidimensional data analysis and decision support activities current research has lead to new developments in all aspects of data warehousing however there are still a number of problems that need to be solved for making data warehousing effective in this paper we discuss recent developments in data warehouse modelling view maintenance and parallel query processing a number of technical issues for exploratory research are presented and possible solutions are discusse agora enhancing group awareness and collaboration in floristic digital libraries digital libraries can be regarded as virtual spaces in which collaborative scholarly research can be conducted floristic digital libraries provide such collaboration spaces for scientists working on solutions for earth s biodiversity problems however group awareness and collaboration are not easily achieved in an increasingly distributed environment such as the virtual space in which digital library users particularly botanists and biologists do their work we describe an environment that enables group awareness communication and collaboration among users in a globally accessible floristic digital library this is achieved by extending existing library facilities with recommendation and alerting services as well as various communication interfaces keywords recommendation services group awareness agents floristic digital libraries 1 introduction digital libraries comprise highly complex and dynamic information spaces on top of which a variety of services are provide algorithm for optimal winner determination in combinatorial auctions combinatorial auctions i e auctions where bidders can bid on com binations of items tend to lead to more e cient allocations than tra ditional auctions in multi item auctions where the agents valuations of the items are not additive however determining the winners so as to maximize revenue is np complete first existing approaches for tackling this problem are reviewed exhaustive enumeration dynamic programming approximation algorithms and restricting the allow able combinations then we present our search algorithm for optimal winner determination experiments are shown on several bid distri butions the algorithm allows combinatorial auctions to scale up to signi cantly larger numbers of items and bids than prior approaches to optimal winner determination by capitalizing on the fact that the space of bids is necessarily sparsely populated in practice the algo rithm does this by provably su cient selective generation of children in the search tree by using a secondary search for fast child genera tion by heuristics that are accurate and optimized for speed and by four methods for preprocessing the search space patent pending a highly optimized implementation of the algorithm is available for licensing both for research and commercial purposes please contact the author 1 1 cabob a fast optimal algorithm for combinatorial auctions combinatorial auctions where bidders can bid on bundles of items can lead to more economical allocations but determining the winners is np complete and inapproximable we present cabob a sophisticated search algorithm for the problem it uses decomposition techniques upper and lower bounding also across components elaborate and dynamically chosen bid ordering heuristics and a host of structural observations experiments against cplex 7 0 show that cabob is usually faster never drastically slower and in many cases with special structure drastically faster we also uncover interesting aspects of the problem itself first the problems with short bids that were hard for the first generation of specialized algorithms are easy second almost all of the cats distributions are easy and become easier with more bids third we test a number of random restart strategies and show that they do not help on this problem because the run time distribution does not have a heavy tail at least not for cabob 1 market clearability market mechanisms play a central role in ai as a coordination tool in multiagent systems and as an application area for algorithm design mechanisms where buyers are directly cleared with sellers and thus do not require an external liquidity provider are highly desirable for electronic marketplaces for several reasons in this paper we study the inherent complexity of and design algorithms for clearing auctions and reverse auctions with multiple indistinguishable units for sale we consider settings where bidders express their preferences via price quantity curves and settings where the bids are price quantity pairs we show that markets with piecewise linear supply demand curves and non discriminatory pricing can always be cleared in polynomial time surprisingly if discriminatory pricing is used to clear the market the problem becomes np complete even for step function curves if the price quantity curves are all linear then in most variants the problem admits a poly time solution even for discriminatory pricing when bidders express their preferences with price quantity pairs the problem is np complete but solvable in pseudo polynomial time with free disposal the problem admits a poly time approximation scheme but no such approximation scheme is possible without free disposal we also present pseudo polynomial algorithms for xor bids and or of xors bids and analyze the approximability 1 side constraints and non price attributes in markets in most real world electronic marketplaces there are other considerations besides maximizing immediate economic value we present a sound way of taking such considerations into account via side constraints and non price attributes side constraints have a significant impact on the complexity of market clearing budget constraints a limit on the number of winners and xor constraints make even noncombinatorial markets complete to clear the latter two make markets complete to clear even if bids can be accepted partially this is surprising since as we show even combinatorial markets with a host of very similar side constraints can be cleared in polytime an extreme equality constraint makes combinatorial markets polytime clearable even if bids have to be accepted entirely or not at all finally we present a way to take into account additional attributes using a bid re weighting scheme and prove that it does not change the complexity of clearing all of the results hold for auctions as well as exchanges with and without free disposal 1 winner determination in combinatorial auction generalizations combinatorial markets where bids can be submitted on bundles of items can be economically desirable coordination mechanisms in multiagent systems where the items exhibit complementarity and substitutability there has been a surge of recent research on winner determination in combinatorial auctions in this paper we study a wider range of combinatorial market designs auctions reverse auctions and exchanges with one or multiple units of each item with and without free disposal we first theoretically characterize the complexity the most interesting results are that reverse auctions with free disposal can be approximated and in all of the cases without free disposal even finding a feasible solution is complete we then ran experiments on known benchmarks as well as ones which we introduced to study the complexity of the market variants in practice cases with free disposal tended to be easier than ones without on many distributions reverse auctions with free disposal were easier than auctions with free disposal as the approximability would suggest but interestingly on one of the most realistic distributions they were harder single unit exchanges were easy but multi unit exchanges were extremely hard algorithms for optimizing leveled commitment contracts in automated negotiation systems consisting of self interested agents contracts have traditionally been binding leveled commitment contracts i e contracts where each party can decommit by paying a predetermined penalty were recently shown to improve pareto efficiency even if agents rationally decommit in nash equilibrium using inflated thresholds on how good their outside offers must be before they decommit this paper operationalizes the four leveled commitment contracting protocols by presenting algorithms for using them algorithms are presented for computing the nash equilibrium decommitting thresholds and decommitting probabilities given the contract price and the penalties existence and uniqueness of the equilibrium are analyzed algorithms are also presented for optimizing the contract itself price and penalties existence and uniqueness of the optimum are analyzed using the algorithms we offer a contract optimization service on the web as part of mediator our next generation electronic commerce server finally the algorithms are generalized to contracts involving more than two agents 1 a query paradigm to discover the relation between text and images this paper studies the relation between images and text in image databases an analysis of this relation results in the definition of three distinct query modalities 1 linguistic scenario images are part of a whole including a self contained linguistic discourse and their meaning derives from their interaction with the linguistic discourse a typical case of this scenario is constituted by images on the world wide web 2 closed world scenario images are defined in a limited domain and their meaning is anchored by conventions and norms in that domain 3 user scenario the linguistic discourse is provided by the user this is the case of highly interactive systems with relevance feedback this paper deals with image databases of the first type it shows how the relation between images or parts of images and text can be inferred and exploited for search the paper develops a similarity model in which the similarity between two images is given by both their visual similarity similarity measures with complex multimedia data we see the emergence of database systems in which the fundamental operation is similarity assessment before database issues can be addressed it is necessary to give a definition of similarity as an operation in this paper we develop a similarity measure based on fuzzy logic that exhibit several features that match experimental findings in humans the model is dubbed fuzzy feature contrast ffc and is an extension to a more general domain of the feature contrast model due to tversky we show how the ffc model can be used to model similarity assessment from fuzzy judgment of properties and we address the use of fuzzy measures to deal with dependencies among the properties 1 introduction comparing two images or an image and a model is the fundamental operation for many visual information retrieval systems in most systems of interest a simple pixel by pixel comparison won t do the difference that we determine must bear some correlation with the p an introductory course on visualization a visualization course offered twice 1997 98 and 98 99 as an elective in the msc degree on electronics and telecommunications at the university of aveiro is presented its contents bibliography and teaching methods are described some difficulties encountered during the preparation and lecturing of this course are identified 1 introduction taking into consideration that visualization is becoming very important and useful in many areas an introductory course on vizualization seems a valid contribution to the curriculum of any postgraduation in science or technology and thus it was considered adequate as an elective course of the msc in electronics and telecommunications offered at the university of aveiro this post graduation program includes several courses and a thesis and aims to be a large spectrum degree encompassing mainly one of four areas of electrical engineering electronics telecommunications signal analysis and processing and computer science this means that it dynamic user model construction with bayesian networks for intelligent information queries the complexity of current software applications is overwhelming users the need exists for intelligent interface agents to address the problems of increasing taskload that is overwhelming the human user interface agents could help alleviate user taskload by extracting and analyzing relevant information and providing information abstractions of that information and providing timely beneficial assistance to users central to providing assistance to a user is the issue of correctly determining the user s intent the clavin project is to build an intelligent natural language query information management system clavin must maintain a dynamic user model of the relevant concepts in the user inquiries as they relate to the information sources the primary goal of clavin is to autonomously react to changes in user intent as well as the information sources by dynamically constructing the appropriate queries relative to the changes identified in this paper we discuss the data mining models as services on the internet the goal of this article is to raise a debate on the usefulness of providing data mining models as services on the internet these services can be provided by anyone with adequate data and expertise and made available on the internet for anyone to use for instance yahoo or altavista given their huge categorized document collection can train a document classifier and provide the model as a service on the internet this way data mining can be made accessible to a wider audience instead of being limited to people with the data and the expertise a host of practical problems need to be solved before this idea can be made to work we identify them and close with an invitation for further debate and investigation 1 discovery driven exploration of olap data cubes analysts predominantly use olap data cubes to identify regions of anomalies that may represent problem areas or new opportunities the current olap systems support hypothesis driven exploration of data cubes through operations such as drill down roll up and selection using these operations an analyst navigates unaided through a huge search space looking at large number of values to spot exceptions we propose a new discovery driven exploration paradigm that mines the data for such exceptions and summarizes the exceptions at appropriate levels in advance it then uses these exceptions to lead the analyst to interesting regions of the cube during navigation we present the statistical foundation underlying our approach we then discuss the computational issue of finding exceptions in data and making the process efficient on large multidimensional data bases 1 introduction on line analytical processing olap characterizes the operations of summarizing consolidating viewing a applying co training methods to statistical parsing we propose a novel co training method for statistical parsing the algorithm takes as input a small corpus 9695 sentences annotated with parse trees a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text the algorithm iteratively labels the entire data set with parse trees using empirical results based on parsing the wall street journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data 1 item based collaborative filtering recommendation algorithms recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for information products or services during a live interaction these systems especially the k nearest neighbor collaborative filtering based ones are achieving widespread success on the web the tremendous growth in the amount of available information and the number of visitors to web sites in recent years poses some key challenges for recommender systems these are producing high quality recommendations performing many recommendations per second for millions of users and items and achieving high coverage in the face of data sparsity in traditional collaborative filtering systems the amount of work increases with the number of participants in the system new recommender system technologies are needed that can quickly produce high quality recommendations even for very large scale problems to address these issues we have explored item based collaborative filtering techniques itembased techniques first analyze the user item matrix to identify relationships between different items and then use these relationships to indirectly compute recommendations for users in this paper we analyze different item based recommendation generation algorithms we look into different techniques for computing item item similarities e g item item correlation vs cosine similarities between item vectors and different techniques for obtaining recommendations from them e g weighted sum vs regression model finally we experimentally evaluate our results and compare them to the basic k nearest neighbor approach our experiments suggest that item based algorithms provide dramatically better performance than user based algorithms while at the same time providing better quality than the best available user based algorithms 1 cluster optimization using extended compact genetic algorithm this study presents an ecient atomic cluster optimization algorithm that utilizes a hybrid extended compact genetic algorithm along with an eciency enhancement technique called seeding empirical results indicate that the population size and total number of function evaluations scale up with the cluster size as o n 0 83 and o n 2 45 respectively the results also indicate that the proposed algorithm is not only very reliable in predicting lowest energy structures but also has a better scale up of number of function evaluations with the cluster size efficient atomic cluster optimization using a hybrid extended compact genetic algorithm with seeded population a recent study sastry xiao 2001 proposed a highly reliable cluster optimization algorithm parameter learning of logic programs for symbolic statistical modeling we propose a logical mathematical framework for statistical parameter learning of parameterized logic programs i e de nite clause programs containing probabilistic facts with a parameterized distribution it extends the traditional least herbrand model semantics in logic programming to distribution semantics possible world semantics with a probability distribution which is unconditionally applicable to arbitrary logic programs including ones for hmms pcfgs and bayesian networks we also propose a new em algorithm the graphical em algorithm thatrunsfora class of parameterized logic programs representing sequential decision processes where each decision is exclusive and independent it runs on a new data structure called support graphs describing the logical relationship between observations and their explanations and learns parameters by computing inside and outside probability generalized for logic programs the complexity analysis shows that when combined with oldt search for all explanations for observations the graphical em algorithm despite its generality has the same time complexity as existing em algorithms i e the baum welch algorithm for hmms the inside outside algorithm for pcfgs and the one for singly connected bayesian networks that have beendeveloped independently in each research eld learning experiments with pcfgs using two corpora of moderate size indicate that the graphical em algorithm can signi cantly outperform the inside outside algorithm 1 flying emulator rapid building and testing of networked applications for mobile computers this paper presents a mobile agent framework for building and testing mobile computing applications when a portable computing device is moved into and attached to a new network the proper functioning of an application running on the device often depends on the resources and services provided locally in the current network to solve this problem this framework provides an applicationlevel emulator of portable computing devices since the emulator is constructed as a mobile agent it can carry target applications across networks on behalf of a device and it allows the applications to connect to local servers in its current network in the same way as if they were moved with and executed on the device itself this paper also demonstrates the utility of this framework by describing the development of typical location dependent applications in mobile computing settings network processing of mobile agents by mobile agents for mobile agents this paper presents a framework for building network protocols for migrating mobile agents over a network the framework allows network protocols for agent migration to be naturally implemented within mobile agents and to be constructed in a hierarchy as most data transmission protocols are these protocols are given as mobile agents and they can transmit other mobile agents to remote hosts as first class objects since they can be dynamically deployed at remote hosts by migrating the agents that carry them these protocols can dynamically and flexibly customize network processing for agent migration according to the requirements of respective visiting agents and changes in the environments a prototype implementation was built on a java based mobile agent system and several practical protocols for agent migration were designed and implemented the framework can make major contributions to mobile agent technology for telecommunication systems 1 a data preparation framework based on a multidatabase language integration and analysis of data from different sources have to deal with several problems resulting from potential heterogeneities the activities addressing these problems are called data preparation and are supported by various available tools however these tools process mostly in a batch like manner not supporting the iterative and explorative nature of the integration and analysis process in this work we present a framework for important data preparation tasks based on a multidatabase language this language offers features for solving common integration and cleaning problems as part of query processing combining data preparation mechanisms and multidatabase query facilities permits applying and evaluating different integration and cleaning strategies without explicit loading and materialization of data the paper introduces the language concepts and discusses their application for individual tasks of data preparation nomadic radio scaleable and contextual notification for wearable audio messaging mobile workers need seamless access to communication and information services on portable devices however current solutions overwhelm users with intrusive and ambiguous notifications in this paper we describe scaleable auditory techniques and a contextual notification model for providing timely information while minimizing interruptions user s actions influence local adaptation in the model these techniques are demonstrated in nomadic radio an audio only wearable computing platform building behaviors developmentally a new formalism this paper advocates a developmental approach to building complex interactive behaviors for robotic systems a developmental methodology is advantageous because it provides a structured decomposition of complex tasks because it facilitates learning and because it allows for a gradual increase in task complexity the developmental approach provides a structured means both of dividing a task among research units as well as a metric for evaluating the progress of the task initial work with developmental modeling has also hinted that these skill decompositions may make the overall task easier to accomplish through the re use of knowledge gained from developmental precursors we report here on two projects of building behaviors developmentally for a humanoid robot in the first project the robot learned to reach to a visual target by following a developmental progression similar to those observed in infants the second project outlines a proposal for building social skills developmenta knowing what to imitate and knowing when you succeed if we are to build robots that can imitate the actions of a human instructor the robotics community must address a variety of issues in this paper we examine two of these issues first how does the robot know which things it should imitate second how does the robot know when its actions are an adequate imitation of the original we further describe an on going research effort to implement systems for a humanoid robot that address these issues 1 introduction humans and other animals acquire new skills from social interactions with others through direct tutelage observational conditioning goal emulation imitation and other methods galef 1988 these social learning skills provide a powerful mechanism for children to acquire skills and knowledge from their parents other adults and other children in particular imitation is an extremely powerful mechanism for social learning which has received a great deal of interest from researchers in the fields of animal behavior the ease actor development environment in interactive simulations it is often desirable to have intelligent actors playing the roles of humans drawing on a wide range of previous work this paper presents a system that is intended to reduce some of the diculties involved in the development of actors we present a system called ease end user actor specication environment that provides tools and methods to support end user development of intelligent actors the tools support the whole development process from design to testing the ease actor architecture is a multi agent system where a process of contract making and negotiation between agents determines the actions of the actor 1 introduction in modern complex interactive simulations it is often highly desirable to have intelligent actors playing the roles of humans the actors task is dicult sensing the simulated environment choosing a course of action that exibly and intelligently follows designer intentions and sending appropriate commands back to extensible and similarity based grouping for data integration data integration as required in a variety of applications like data warehousing information system integration etc makes great demands regarding features to deal with overlapping and inconsistent data object relational and other data management systems available today provide only limited concepts to deal with these requirements the general concept of grouping and aggregation appears to be a fitting paradigm for various of the current issues in data integration but in its common form of equality based grouping a number of problems remain unsolved various extensions to this concept have been introduced over the last years regarding user defined functions for aggregation and grouping objectivity db benchmark objectivity read write benchmarks and storage overhead measurements are presented keywords 1 introduction most of the coming hep experiments will use object oriented database management systems odbmss as data store a detailed understanding of the the odbms performance is crucial to enable highperformance analysis scenarios this paper presents performance measurements for di erent access patterns using the commercial odbms objectivity db measurements were made for sequential reading sequential writing and selective reading furthermore an experimental and analytical analysis of the storage overhead introduced by objectivity db is given the odbms performance depends on the underlying data medium at the moment there are no alternatives to hard drives for secondary storage existing although the performance price ratios of both processors and disks are improving the rate of improvement is greater for processors hence the disk subsystem is emerging as a bottleneck factor in s reclustering of high energy physics data the coming high energy physics experiments will store petabytes of data into object databases analysis jobs will frequently traverse collections containing millions of stored objects clustering is one of the most effective means to enhance the performance of these applications this paper presents a reclustering algorithm for independent objects contained in multiple possibly overlapping collections on secondary storage the algorithm decomposes the stored objects into a number of independent chunks and then maps these chunks to a traveling salesman problem under a set of realistic assumptions the number of disk seeks is reduced almost to the theoretical minimum experimental results obtained from a prototype are included 1 introduction we consider data analysis on secondary storage in certain kinds of scientific data analysis e g high energy physics hep a very large number of preselected independent objects is repeatedly processed such a set of preselected objects read b boosting and rocchio applied to text filtering we discuss two learning algorithms for text filtering modified rocchio and a boosting algorithm called adaboost we show how both algorithms can be adapted to maximize any general utility matrix that associates cost or gain for each pair of machine prediction and correct label we first show that adaboost significantly outperforms another highly effective text filtering algorithm we then compare adaboost and rocchio over three large text filtering tasks overall both algorithms are comparable and are quite effective adaboost produces better classifiers than rocchio when the training collection contains a very large number of relevant documents however on these tasks rocchio runs much faster than adaboost 1 introduction with the explosion in the amount of information available electronically information filtering systems that automatically send articles of potential interest to a user are becoming increasingly important if users indicate their interests to a filtering system planning agents in james abstract testing is an obligatory step in developing multi agent systems for testing multi agent systems in virtual dynamic environments simulation systems are required that support a modular declarative construction of experimental frames that facilitate the embeddence of a variety of agent architectures and that allow an efficient parallel distributed execution we introduce the system james a java based agent modeling environment for simulation in james agents and their dynamic environment are modeled as reflective time triggered state automata its possibilities to compose experimental frames based on predefined components to express temporal interdependencies to capture the phenomenon of pro activeness and reflectivity of agents are illuminated by experiments with planning agents the underlying planning system is a general purpose system about which no empirical results exist besides traditional static benchmark tests we analyze the interplay between heuristics for selecting goals viewing range commitment strategies explorativeness and trust in the persistence of the world and uncover properties of the agent the planning engine and the chosen test scenario tileworld i active hidden markov models for information extraction information extraction from html documents requires a classifier capable of assigning semantic labels to the words or word sequences to be extracted if completely labeled documents are available for training well known markov model techniques can be used to learn such classifiers in this paper we consider the more challenging task of learning hidden markov models hmms when only partially sparsely labeled documents are available for training we first give detailed account of the task and its appropriate loss function and show how it can be minimized given an hmm we describe an em style algorithm for learning hmms from partially labeled data we then present an active learning algorithm that selects difficult unlabeled tokens and asks the user to label them we study empirically by how much active learning reduces the required data labeling effort or increases the quality of the learned model achievable with a given amount of user effort active learning of partially hidden markov models we consider the task of learning hidden markov models hmms when only partially sparsely labeled observation sequences are available for training this setting is motivated by the information extraction problem where only few tokens in the training documents are given a semantic tag while most tokens are unlabeled we first describe the partially hidden markov model together with an algorithm for learning hmms from partially labeled data we then present an active learning algorithm that selects difficult unlabeled tokens and asks the user to label them we study empirically by how much active learning reduces the required data labeling effort or increases the quality of the learned model achievable with a given amount of user effort generative models for cold start recommendations systems for automatically recommending items e g movies products or information to users are becoming increasingly important in e commerce applications digital libraries and other domains where personalization is highly valued such recommender systems typically base their suggestions on 1 collaborative data encoding which users like which items and or 2 content data describing item features and user demographics systems that rely solely on collaborative data fail when operating from a cold start that is when recommending items e g first run movies that no member of the community has yet seen we develop several generative probabilistic models that circumvent the cold start problem by mixing content data with collaborative data in a sound statistical manner we evaluate the algorithms using movielens movie ratings data augmented with actor and director information from the internet movie database we find that maximum likelihood learning with the expectation maximization em algorithm and variants tends to overfit complex models that are initialized randomly however by seeding parameters of the complex models with parameters learned in simpler models we obtain greatly improved performance we explore both methods that exploit a single type of content data e g actors only and methods that leverage multiple types of content data e g both actors and directors simultaneously data partitioning and load balancing in parallel disk systems parallel disk systems provide opportunities for exploiting i o parallelism in two possible ways namely via inter request and intra request parallelism in this paper we discuss the main issues in performance tuning of such systems namely striping and load balancing and show their relationship to response time and throughput we outline the main components of an intelligent self reliant file system that aims to optimize striping by taking into account the requirements of the applications and performs load balancing by judicious file allocation and dynamic redistributions of the data when access patterns change our system uses simple but effective heuristics that incur only little overhead we present performance experiments based on synthetic workloads and real life traces keywords parallel disk systems performance tuning file striping data allocation load balancing disk cooling 1 introduction tuning issues in parallel disk systems parallel disk systems are of great imp attentional objects for visual context understanding this paper exploits wearable computers unique opportunity to record and index the visual environment of the user from the first person perspective we propose to use a hat mounted wearable camera to record what the user sees during the day with a wearable computer this camera can be used to make the computer more contextually aware of the user and their actions furthermore the camera can be used to record analyze and index the visual environment of the user by keeping track of the actions of the user upon and within the environment the system can be more aware of the interactions of the user within the environment an important aspect of the system is to automatically extract objects of user interest and their motion within the environment and relative to the user 1 introduction wearable computers have the potential to see as the user sees hear as the user hears and experience the life and the environment of the user in a first person sense as has been pointed out situation aware computing with wearable computers 1 motivation for contextual aware computing for most computer systems even virtual reality systems sensing techniques are a means of getting input directly from the user however wearable sensors and computers offer a unique opportunity to re direct sensing technology towards recovering more general user context wearable computers have the potential to see as the user sees hear as the user hears and experience the life of the user in a first person sense this increase in contextual and user information may lead to more intelligent and fluid interfaces that use the physical world as part of the interface wearable computers are excellent platforms for contextually aware applications but these applications are also necessary to use wearables to their fullest wearables are more than just highly portable computers they perform useful work even while the wearer isn t directly interacting with the system in such environments the user needs to concentrate on his environment not on the computer interface so the wearable needs to use information from the wearer s context to be the least distracting for example imagine an interface which is aware of the user s location while being in the subway the system might alert him with a on maintaining code mobility we introduce the aspect of maintenance to code mobility with its major problem of keeping track of code migrating through computer networks our approach introduces the concept of mobile lightweight knowledge repositories to support the maintenance of applications deploying mobile code in highly distributed computing environments our proposed system establishes a virtual global database with information about the mobile application and its surrounding environment based on distributed structured xml based knowledge repositories these distributed databases provide well defined structural query and retrieval capabilities and ensure the abstraction from programming languages and proprietary hardware platforms finally we point out several future research issues in the field of maintaining mobile code with respect to automatic maintenance of applications and automatic quality checking among others bridging multiple user interface dimensions with augmented reality studierstube is an experimental user interface system which uses collaborative augmented reality to incorporate true 3d interaction into a productivity environment this concept is extended to bridge multiple user interface dimensions by including multiple users multiple host platforms multiple display types multiple concurrent applications and a multi context i e 3d document interface into a heterogeneous distributed environment with this architecture we can explore the user interface design space between pure augmented reality and the popular ubiquitous computing paradigm we report on our design philosophy centered around the notion of contexts and locales as well as the underlying software and hardware architecture contexts encapsulate a live application together with 3d visual and other data while locales are used to organize geometric reference systems by separating geometric relationships locales from semantic relationships contexts we achieve a great amou using transparent props for interaction with the virtual table the virtual table presents stereoscopic graphics to a user in a workbench like setting this paper reports on a user interface and new interaction techniques for the virtual table based on transparent props a tracked hand held pen and a pad these props but in particular the pad are augmented with 3d graphics from the virtual table s display this configuration creates a very powerful and flexible interface for two handed interaction that can be applied to other back projected stereographic displays as well the pad can serve as a palette for tools and controls as well as a window like see through interface a plane shaped and throughthe plane tool supporting a variety of new interaction techniques 1 introduction while the desktop metaphor is well understood and represents an effective approach to human computer interaction for documentoriented 2d tasks transplanting it to 3d reveals inherent limitations e g 8 in contrast interfaces that incorporate true 3d input and implicit human computer interaction through context this paper we concentrate on the last case knowing that in most scenarios a combination of all four cases is the way of choice a novel sensor for dynamic tactile information we present a novel tactile sensor which is useful for dextrous grasping with a simple robot gripper the novel part consists of an array of capacitive sensors which couple to the object by means of little brushes of fibers these sensor elements are very sensitive with a threshold of about 5 mn but robust enough not to be damaged during grasping they yield two types of dynamical tactile information corresponding roughly to two types of tactile sensors in the human skin the complete sensor consists of a foil based static force sensor which yields the total force and the center of the two dimensional force distribution and is surrounded by an array of the dynamical sensor elements one such sensor has been mounted on each of the two gripper jaws of our humanoid robot and equipped with the necessary read out electronics and a can bus interface as first applications we describe experiments to evaluate the quality of a grip using the sensor measurements and a utility that allows to how to build smart appliances in this article smart appliances are characterized as devices that are attentive to their environment we introduce a terminology for situation sensor data context and context aware applications because it is important to gain a thorough understanding of these concepts to successfully build such artifacts in the article the relation between a real world situation and the data read by sensors is discussed furthermore an analysis of available sensing technology is given then we introduce an architecture that supports the transformation from sensor data to cues then to contexts as a foundation to make context aware applications the article suggests a method to build context aware devices the method starts from situation analysis offers a structured way for selection of sensors and finally suggests steps to determine recognition and abstraction methods in the final part of the article the question of how this influences the applications is raised and the areas of user int the xml benchmark project with standardization efforts of a query language for xml documents drawing to a close researchers and users increasingly focus their attention on the database technology that has to deliver on the new challenges that the sheer amount of xml documents produced by applications pose to data management validation performance evaluation and optimization of xml query processors are the upcoming issues following a long tradition in database research the xml store benchmark project provides a framework to assess an xml database s abilities to cope with a broad spectrum of different queries typically posed in real world application scenarios the benchmark is intended to help both implementors and users to compare xml databases independent of their own specific application scenario to this end the benchmark o ers a set queries each of which is intended to challenge a particular primitive of the query processor or storage engine the overall workload wepropose consists of a scalable document database and a concise yet comprehensive set of queries which covers the major aspects of query processing the queries challenges range from stressing the textual character of the document to data analysis queries but include also typical ad hoc queries we complement our research with results obtained from running the benchmark on our xml database platform they are intended to give a rst baseline illustrating the state of the art advanced interaction in context mobile information appliances are increasingly used in numerous different situations and locations setting new requirements to their interaction methods when the user s situation place or activity changes the functionality of the device should adapt to these changes in this work we propose a layered real time architecture for this kind of context aware adaptation based on redundant collections of low level sensors two kinds of sensors are distinguished physical and logical sensors which give cues from environment parameters and host information a prototype board that consists of eight sensors was built for experimentation the contexts are derived from cues using real time recognition software which was constructed after experiments with kohonen s self organizing maps and its variants a personal digital assistant pda and a mobile phone were used with the prototype to demonstrate situational awareness on the pda font size and backlight were changed depending learned models for continuous planning we are interested in the nature of activity structured behavior of nontrivial duration in intelligent agents we believe that the development of activity is a continual process in which simpler activities are composed via planning to form more sophisticated ones in a hierarchical fashion the success or failure of a planner depends on its models of the environment and its ability to implement its plans in the world we describe an approach to generating dynamical models of activity from real world experiences and explain how they can be applied towards planning in a continuous state space 1 introduction we are interested in the problem of how activity emerges in an intelligent agent we believe that activity plays a critical role in the development of many high level cognitive structures classes concepts and language to name a few thus it is our goal to derive and implement a theory of the development of activity in intelligent agents and implement it using the pioneer relating chemical structure to activity an application of the neural folding architecture this paper is based on the neural folding architecture fa the fa is a recurrent neural network architecture which is especially suited for adaptive structure processing i e learning approximations of mappings from symbolic term structures to ir n the main objective of this paper is to demonstrate that the fa can be successfully applied to approximate quantitative structure activity relationships qsars which play an important role during a drug design process several approaches for the conversion of a qsar problem to suitable learning tasks for the fa are presented finally the fa is applied to a well known qsar benchmark viz the inhibition of e coli dihydrofolate reductase by triazines the achieved results are compared with results of other machine learning approaches on the same qsar benchmark and prove that the fa is significantly better keywords recurrent neural networks folding architecture drug design quantitative structure activity relationships inhibit quality based learning we introduce a methodology for automating the maintenance of domain specific taxonomies based on natural language text understanding a given ontology is incrementally updated as new concepts are acquired from real world texts the acquisition process is centered around the linguistic and conceptual quality of various forms of evidence underlying the generation and refinement of concept hypotheses on the basis of the quality of evidence concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base appeared in ecaii 98 proceedings of the 13th biennial european conference on artificial intelligence em 23 28 august 1998 brighton centre brighton uk pp 160 164 c fl 1998 ecai 98 13th european conference on artificial intelligence edited by henri prade published in 1998 by john wiley sons ltd quality based learning klemens schnattinger and udo hahn 1 abstract we introduce a formal model f the augurscope a mixed reality interface for outdoors the augurscope is a portable mixed reality interface for outdoors a tripod mounted display is wheeled to different locations and rotated and tilted to view a virtual environment that is aligned with the physical background video from an onboard camera is embedded into this virtual environment our design encompasses physical form interaction and the combination of a gps receiver electronic compass accelerometer and rotary encoder for tracking an initial application involves the public exploring a medieval castle from the site of its modern replacement analysis of use reveals problems with lighting movement and relating virtual and physical viewpoints and shows how environmental factors and physical form affect interaction we suggest that problems might be accommodated by carefully constructing virtual and physical content disseminating trust information in wearable communities this paper describes a framework for managing and distributing trust information in a community of mobile and wearable computer users trust information in the form of reputations are used to aid users during their social interactions with the rest of the community keywords wearable computing social networks social interaction trust introduction in our modern world the use of communication technologies like phone fax and email has become commonplace despite this fact most social interactions between individuals still occur when we meet people face to face many of our daily interactions are actually the result of a chance encounter i e a situation in which we meet someone unexpectedly for example in a hallway or an elevator in most cases the majority of the people we encounter every day we don t know and have never met before however some are familiar any encounter with another person friend or stranger is a chance for striking up a conversation and for exchang distributed value functions many interesting problems such as power grids network switches and traffic flow that are candidates for solving with reinforcement learning rl also have properties that make distributed solutions desirable we propose an algorithm for distributed reinforcement learning based on distributing the representation of the value function across nodes each node in the system only has the ability to sense state locally choose actions locally and receive reward locally the goal of the system is to maximize the sum of the rewards over all nodes and over all time however each node is allowed to give its neighbors the current estimate of its value function for the states it passes through we present a value function learning rule using that information that allows each node to learn a value function that is an estimate of a weighted sum of future rewards for all the nodes in the network with this representation each node can choose actions to improve the performance of the overall less is more active learning with support vector machines we describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machines svms on several practical document classification tasks we observe a number of benefits the most surprising of which is that a svm trained on a wellchosen subset of the available corpus frequently performs better than one trained on all available data the heuristic for choosing this subset is simple to compute and makes no use of information about the test set given that the training time of svms depends heavily on the training set size our heuristic not only offers better performance with fewer data it frequently does so in less time than the naive approach of training on all available data 1 introduction there are many uses for a good document classifier sorting mail into mailboxes filtering spam or routing news articles the problem is that learning to classify documents requires manually labelling more documents than a typical compression of inverted indexes for fast query evaluation compression reduces both the size of indexes and the time needed to evaluate queries in this paper we revisit the compression of inverted lists of document postings that store the position and frequency of indexed terms considering two approaches to improving retrieval efficiency better implementation and better choice of integer compression schemes first we propose several simple optimisations to well known integer compression schemes and show experimentally that these lead to significant reductions in time second we explore the impact of choice of compression scheme on retrieval efficiency in experiments on large collections of data we show two surprising results use of simple byte aligned codes halves the query evaluation time compared to the most compact golomb rice bitwise compression schemes and even when an index fits entirely in memory byte aligned codes result in faster query evaluation than does an uncompressed index emphasising that the cost of transferring data from memory to the cpu cache is less for an appropriately compressed index than for an uncompressed index moreover byte aligned schemes have only a modest space overhead the most compact schemes result in indexes that are around 10 of the size of the collection while a byte aligned scheme is around 13 we conclude that fast byte aligned codes should be used to store integers in inverted lists fuzzy argumentation and extended logic programming we define a fuzzy version of extended logic programming under well founded semantics with explicit negation we develop a fuzzy fixpoint argumentation semantics and an efficient top down goal directed proof procedure we show that the procedure is sound and complete with respect to the argumentation semantics concurrency control and recovery in transactional process management the classical theory of transaction management contains two different aspects namely concurrency control and recovery which ensure serializability and atomicity of transaction executions respectively although concurrency control and recovery are not independent of each other the criteria for these two aspects were developed orthogonally and as a result in most cases these criteria are incompatible with each other recently a unified theory of concurrency control and recovery for databases with read and write operations has been introduced in swy93 ava 94 that allows reasoning about serializability and atomicity within the same framework in swy93 ava 94 a class of schedules was introduced called prefix reducible which guarantees both serializability and atomicity in a failure prone environment with read write operations several protocols were developed to generate such schedules by a database concurrency control mechanism we present here a unified transaction transactional coordination agents for composite systems composite systems are collections of autonomous heterogeneous and distributed software applications in these systems data dependencies are continuously violated by local operations and therefore coordination processes are necessary to guarantee overall correctness and consistency such coordination processes must be endowed with some form of execution guarantees which require the participating subsystems to have certain database functionality such as atomicity of local operations order preservation and either compensation of operations or the deferment of their commit however this functionality is not present in many applications and must be implemented by a transactional coordination agent coupled with the application in this paper we discuss the requirements to be met by the applications and their associated transactional coordination agents we identify a minimal set of functionality the applications must provide in order to participate in transactional coordination pro transactions and electronic commerce electronic commerce is a rapidly growing area that is gaining more and more importance not only in the interrelation of businesses business to business electronic commerce but also in the everyday consumption of individuals performed via the internet business to customer electronic commerce since electronic commerce is a very interdisciplinary area it has a lot of impacts to various communities the goal of this paper is to identify and to summarize the impact of electronic commerce from a database transaction point of view and to highlight open problems in transaction management arising in electronic commerce applications by reflecting the discussions of the working group transactions and electronic commerce held at the tdd workshop 1 motivation the exchange of electronic data between companies has been an important issue in business interactions for quite a while however the recent proliferation of the internet together with the rapid propagation of pers spontaneous short term interaction with mobile robots major open research directions in mobile robotics this paper considers a specific type of interaction short term and spontaneous interaction with crowds of people such patterns of interactions are found when service robots operate in public places for example information kiosks receptionists tour guide robots applications we describe our approach to spontaneous short term interaction a robot designed to be a believable social agent the approach has been implemented using a mobile robot with a motorized face as focal point for interaction an architecture that suggests the robot has moods and a method for learning how to interact with people our system was recently deployed at a smithsonian museum in washington dc during a two week period it interacted with thousands of people the robot s interactive capabilities were essential for its high on task performance and thus its practical success neural networks for speech processing this article currently 1998 successful use of nns for speech processing is mainly limited to intention reconsideration in complex environments one of the key problems in the design of belief desire intention bdi agents is that of finding an appropriate policy for intention reconsideration in previous work kinny and georgeff investigated the effectiveness of several such reconsideration policies and demonstrated that in general there is no one best approach different environments demand different intention reconsideration strategies in this paper we further investigate the relationship between the effectiveness of an agent and its intention reconsideration policy in different environments we empirically evaluate the performance of different reconsideration strategies in environments that are to varying degrees dynamic inaccessible and nondeterministic in addition to our empirical results we are able to give preliminary analytical results to explain some of our findings principles of intention reconsideration we present a framework that enables a belief desire intention bdi agent to dynamically choose its intention reconsideration policy in order to perform optimally in accordance with the current state of the environment our framework integrates an abstract bdi agent architecture with the decision theoretic model for discrete deliberation scheduling of russell and wefald as intention reconsideration determines an agent s commitment to its plans this work increases the level of autonomy in agents as it pushes the choice of commitment level from design time to run time this makes it possible for an agent to operate effectively in dynamic and open environments whose behaviour is not known at design time following a precise formal definition of the framework we present an empirical analysis that evaluates the run time policy in comparison with design time policies we show that an agent utilising our framework outperforms agents with fixed policies learning to recommend from positive evidence in recent years many systems and approaches for recommending information products or other objects have been developed in these systems often machine learning methods that need training input to acquire a user interest profile are used such methods typically need positive and negative evidence of the user s interests to obtain both kinds of evidence many systems make users rate relevant objects explicitly others merely observe the user s behavior which fairly obviously yields positive evidence in order to be able to apply the standard learning methods these systems mostly use heuristics that attempt to find also negative evidence in observed behavior in this paper we present several approaches to learning interest profiles from positive evidence only as it is contained in observed user behavior thus both the problem of interrupting the user for ratings and the problem of somewhat artificially determining negative evidence are avoided the learning approaches were developed and tested in the context of the web based elfi information system it is in real use by more than 1000 people we give a brief sketch of elfi and describe the experiments we made based on elfi usage logs to evaluate the different proposed methods the use of artificial intelligence to improve the numerical optimization of complex engineering designs gradient based numerical optimization of complex engineering designs promises to produce better designs rapidly however such methods generally assume that the objective function and constraint functions are continuous smooth and defined everywhere unfortunately realistic simulators tend to violate these assumptions we present several artificial intelligence based techniques for improving the numerical optimization of complex engineering designs in the presence of such pathologies in the simulators we have tested the resulting system in several realistic engineering domains and have found that using our techniques can greatly decrease the cost of design space search and can also increase the quality of the resulting designs multilevel simulation and numerical optimization of complex engineering designs multilevel representations have been studied extensively by artificial intelligence researchers we present a general method that utilizes the multilevel paradigm to attack the problem of performing multidiscipline engineering design optimization in the presence of many local optima the method uses a multidisciplinary simulator at multiple levels of abstraction paired with a multilevel search space we tested the method in the domain of conceptual design of supersonic transport aircraft focusing on the airframe and the exhaust nozzle and using sequential quadratic programming as the optimizer at each level we found that using multilevel simulation and optimization can decrease the cost of design space search by an order of magnitude 1 introduction a major barrier to the use of gradient based search methods for engineering design is that complex multidisciplinary design spaces tend an earlier version of this article was presented at 6 th aiaa nasa usaf multidisciplinary ana automatic text representation classification and labeling in european law the huge text archives and retrieval systems of legal information have not achieved yet the representation in the wellknown subject oriented structure of legal commentaries content based classification and text analysis remains a high priority research topic in the joint konterm som and labelsom projects learning techniques of neural networks are used to achieve similar high compression rates of classification and analysis like in manual legal indexing the produced maps of legal text corpora cluster related documents in units that are described with automatically selected descriptors extensive tests with text corpora in european case law have shown the feasibility of this approach classification and labeling proved very helpful for legal research the growing hierarchical self organizing map represents very interesting generalities and specialties of legal text corpora the segmentation into document parts improved very much the quality of labeling the next challenge would be a change from tfxidf vector representation to a modified vector representation taking into account thesauri or ontologies considering learned properties of legal text corpora improving the quality of labels for self organising maps using fine tuning vector representation of legal documents is still the best way for computing classification clusters and labelling of its contents a very special problem occurs with self organising maps strong clusters tend to dominate neighbouring smaller clusters in terms of their weight vector structure which influences the labels extracted from these this unwelcome side effect can be overcome efficiently with a dedicated fine tuning phase at the end of the training process in which the neighbourhood radius of the training function is set to zero experiments with our text collection have shown the high improvement of the quality of labelling an architecture to guide crowds using a rule based behavior system this paper describes a client server architecture to combine the control of human agents performing intelligent actions guided by a rule based behavior system rbbs with the management of autonomous crowds which perform pre programmed actions our main goal being ability to model crowds formed by a large number of agents e g 1000 we have used pre programmed actions and basic behaviors in addition rbbs provides the user with an interface for real time behavior control of some groups of the crowd this paper presents how the server application deals with virtual human agent s behaviors using a rule based system keywords multi agent co ordination and collaboration agent architectures network agents real time performance synthetic agents rulebased system human crowds model 1 introduction virtual humans grouped together to form crowds populating virtual worlds allow a more intuitive feeling of presence however the crowd is not only needed to create an at websifter an ontological web mining agent for ebusiness the world wide web provides access to a great deal of information on a vast array of subjects a user can begin a search for information by selecting a web page and following the embedded links from page to page looking for clues to the desired information an alternative method is to use one of the web based search engines to select the web pages that refer to the general subject of the information desired in either case a vast amount of information is retrieved design specification of dynamic mobile and reconfigurable multiagent systems multiagent systems use the power of collaborative software agents to solve complex distributed problems there are many agent oriented software engineering aose methodologies available to assist system designers to create multiagent systems however none of these methodologies can specify agents with dynamic properties such as cloning mobility or agent instantiation this thesis starts the process to bridge the gap between aose methodologies and dynamic agent platforms by incorporating mobility into the current multiagent systems engineering mase methodology mobility was specified within all components composing a mobile agent class an agent component was also created that integrated the behavior of the components within an agent class and was transformed to handle most of the move responsibilities for a mobile agent those agent component and component mobility transformations were integrated into agenttool as a proof of concept and a demonstration system built on the mobility specifications was implemented for execution on the carolina mobile agent platform 1 design specification of dynamic mobile and reconfigurable multiagent systems i individual learning of coordination knowledge social agents both human and computational inhabiting a world con taining multiple active agents need to coordinate their activities this is because agents share resources and without proper coordination or rules of the road everybody will be interfering with the plans of others as such we need coordination schemes that allow agents to effectively achieve local goals without adversely affecting the problem solving capabilities of other agents researchers in the field of distributed artificial intelligence dai have de veloped a variety of coordination schemes under different assumptions about agent capabilities and relationships whereas some of these research have been motivated by human cognitive biases others have approached it as an engineering problem of designing the most effective coordination architec ture or protocol we evaluate individual and concurrent learning by mul tiple autonomous agents as a means for acquiring coordination knowledge we show that a uniform reinforcement learning algorithm suffices as a coor dination mechanism in both cooperative and adversarial situations using a number of multiagent learning scenarios with both tight and loose coupling between agents and with immediate as well as delayed feedback we demon strate that agents can consistently develop effective policies to coordinate their actions without explicit information sharing we demonstrate the vi ability of using both the q learning algorithm and genetic algorithm based classifier systems with different payoff schemes namely the bucket brigade algorithm bba and the profit sharing plan psp for developing agent coordination on two different multi agent domains in addition we show that a semi random scheme for action selection is preferable to the more traditional fitness proportionate selection scheme used in classifier systems 1 1 constructing and transforming cbr implementations techniques for corporate memory management achieving widespread case based reasoning support for corporate memories will require the flexibility to integrate implementations with existing organizational resources and infrastructure case based reasoning implementations as currently constructed tend to fall into three categories characterized by implementation constraints task based task constraints alone enterprise integrating databases and web based integrating web representations these implementation types represent the possible targets in constructing corporate memory systems and it is important to understand the strengths of each how they are built and how one may be constructed by transforming another this paper describes a framework that relates the three types of cbr implementation discusses their typical strengths and weaknesses and describes practical strategies for building corporate cbr memories to meet new requirements by transforming and synthesizing existing resources 1 introduction constructi a reinforcement learning agent for personalized information filtering this paper describes a method for learning user s interests in the web based personalized information filtering system called wair the proposed method analyzes user s reactions to the presented documents and learns from them the profiles for the individual users reinforcement learning is used to adapt the term weights in the user profile so that user s preferences are best represented in contrast to conventional relevance feedback methods which require explicit user feedbacks our approach learns user preferences implicitly from direct observations of user behaviors during interaction field tests have been made which involved 7 users reading a total of 7 700 html documents during 4 weeks the proposed method showed superior performance in personalized information filtering compared to the existing relevance feedback methods on mapping decision trees and neural networks there exist several methods for transforming decision trees to neural networks these methods typically construct the networks by directly mapping decision nodes or rules to the neural units as a result the networks constructed are often larger than necessary this paper describes a pruning based method for mapping decision trees to neural networks which can compress the network by removing unimportant and redundant units and connections in addition equivalent decision trees extracted from the pruned networks are simpler than those induced by well known algorithms such as id3 and c4 5 keywords decision trees neural networks pruning 1 introduction decision trees have been widely used for nonparametric pattern classification tasks which involve several pattern classes and a large number of features given an input pattern the tree classifier performs the comparisons stipulated in each decision node of the tree and then branches to either the left or the right subtree based on evolving rule based trading systems in this study a market trading rulebase is optimised using genetic programming gp the rulebase is comprised of simple relationships between technical indicators and generates signals to buy sell short and remain inactive the methodology is applied to prediction of the standard poor s composite index 02 jan 1990 to 18 oct 2001 two potential market systems are inferred a simple system using few rules and nodes and a more complex system results are compared with a benchmark buy and hold strategy neither trading system was found capable of consistently outperforming this benchmark more complicated rulebases in addition to being difficult to understand are susceptible to overfitting learning hidden markov model structure for information extraction statistical machine learning techniques while well proven in fields such as speech recognition are just beginning to be applied to the information extraction domain we explore the use of hidden markov models for information extraction tasks specifically focusing on how to learn model structure from data and how to make the best use of labeled and unlabeled data we show that a manually constructed model that contains multiple states per extraction field outperforms a model with one state per field and discuss strategies for learning the model structure automatically from data we also demonstrate that the use of distantly labeled data to set model parameters provides a significant improvement in extraction accuracy our models are applied to the task of extracting important fields from the headers of computer science research papers and achieve an extraction accuracy of 92 9 introduction hidden markov modeling is a powerful statistical machine learning technique that is just amdb a visual access method development tool the development process for access methods ams in database systems is complex and tedious amdb is a graphical tool that facilitates the design and tuning process for height balanced tree structured ams central to amdb s user interface is a suite of graphical views that visualize the entire search tree paths and subtrees within the tree and data contained in the tree these views animate search tree operations in order to visualize the behavior of an access method amdb provides metrics that characterize the performance of queries the tree structure and the structureshaping aspects of an am implementation the visualizations can be used to browse the performance metrics in the context of the tree structure the combination of these features allows a designer to locate the sources of performance loss reported by the metrics and investigate causes for those deficiencies 1 introduction the recent explosion in the volume and diversity of electronically available information has introduction to the relationlog system advanced applications require construction efficient access and management of large databases with rich data structures and inference mechanisms however such capabilities are not directly supported by the existing database systems in this paper we describe relationlog a persistent deductive database system that is able to directly support the storage efficient access and inference of data with complex structures 1 introduction advanced applications require construction efficient access and management of large databases with rich data structures and inference mechanisms however such capabilities are not directly supported by the existing database systems deductive databases have the potential to meet the demands of advanced applications they grew out of the integration of logic programming and relational database technologies they are intended to combine the best of the two approaches such as representational and operational uniformity inference capabilities recursion weight adjustment schemes for a centroid based classifier in recent years we have seen a tremendous growth in the volume of text documents available on the internet digital libraries news sources and company wide intra nets automatic text categorization which is the task of assigning text documents to pre specified classes topics or themes of documents is an important task that can help both in organizing as well as in finding information on these huge resources similarity based categorization algorithms such as k nearest neighbor generalized instance set and centroid based classification have been shown to be very effective in document categorization a major drawback of these algorithms is that they use all features when computing the similarities in many document data sets only a small number of the total vocabulary may be useful for categorizing documents a possible approach to overcome this problem is to learn weights for different features or words in document data sets in this report we present two fast iterativ efficient concurrency control for broadcast environments a crucial consideration in environments where data is broadcast to clients is the low bandwidth available for clients to communicate with servers advanced applications in such environments do need to read data that is mutually consistent aswell as current however given the asymmetric communication capabilities and the needs of clients in mobile environments traditional serializability based approaches are too restrictive unnecessary and impractical we thus propose the use of a weaker correctness criterion called update consistency and outline mechanisms based on this criterion that ensure 1 the mutual consistency of data maintained by the server and read by clients and 2 the currency of data read by clients using these mechanisms clients can obtain data that is current and mutually consistent off the air i e without contacting the server to say obtain locks experimental results show a substantial reduction in response times as compared to existing serializability based approaches a further attractive feature of the approach is that if caching is possible at a client weaker forms of currency can be obtained while still satisfying the mutual consistency of data embodied cassie we have enhanced a computational cognitive agent by embodying it with real and simulated bodies operating in real and simulated worlds this has allowed us to experiment with various ways that embodiment influences the creation and meaning of the agent s beliefs and other terms in its knowledge base including symbol grounding by perception and action firstperson privileged knowledge the representation and use of indexicals having a personal sense of time and low level bodily awareness introduction we have been engaged in a series of projects in which cassie the sneps cognitive agent shapiro rapaport 1987 shapiro 1989 shapiro rapaport 1991 1992 shapiro the sneps implementation group 1998 has been incorporated into a hardware or softwaresimulated cognitive robot the capabilities of the embodied cassie have included input and output in fragments of english reasoning performance of primitive and composite acts and vision in this paper i give an overview of the sneps a logic for natural language understanding and commonsense reasoning the use of logic for knowledge representation and reasoning systems is controversial there are indeed several ways that standard first order predicate logic is inappropriate for modelling natural language understanding and commonsense reasoning however a more appropriate logic can be designed this chapter presents several aspects of such a logic 1 introduction my colleagues students and i have been engaged in a long term project to build a natural language using intelligent agent while our approach to natural language understanding nlu and commonsense reasoning csr has been logic based we have thought that the logics developed for metamathematics e g kleene 1950 are not the best ones for our purpose instead we have designed new logics better suited for nlu and csr the current version of these logics constitutes the formal language and inference mechanism of the knowledge representation reasoning krr system sneps 2 4 shapiro and the sneps implementation gr speech gesture interface to a visual computing environment for molecular biologists in recent years there has been tremendous progress in 3d immersive display and virtual reality vr technologies scientific visualization of data is one of many applications that has benefited from this progress to fully exploit the potential of these applications in the new environment there is a need for natural interfaces that allow the manipulation of such displays without burdensome attachments this paper describes the use of visual hand gesture analysis enhanced with speech recognition for developing a bimodal gesture speech interface for controlling a 3 d display the interface augments an existing application vmd which is a vr visual computing environment for molecular biologists the free hand gestures are used for manipulating the 3 d graphical display together with a set of speech commands we concentrate on the visual gesture analysis techniques used in developing this interface the dual modality of gesture speech is found to greatly aid the interaction capability algorithmics and applications of tree and graph searching modern search engines answer keyword based queries extremely efficiently the impressive speed is due to clever inverted index structures caching a domain independent knowledge of strings and thousands of machines several research efforts have attempted to generalize keyword search to keytree and keygraph searching because trees and graphs have many applications in next generation database systems this paper surveys both algorithms and applications giving some emphasis to our own work building intelligent agents for web based tasks a theory refinement approach we present and evaluate an infrastructure with which to rapidly and easily build intelligent software agents for web based tasks our design is centered around two basic functions scorethislink and scorethispage if given highly accurate such functions standard heuristic search would lead to efficient retrieval of useful information our approach allows users to tailor our system s behavior by providing approximate advice about the above functions this advice is mapped into neural network implementations of the two functions subsequent reinforcements from the web e g dead links and any ratings of retrieved pages that the user wishes to provide are respectively used to refine the link and page scoring functions hence our agent architecture provides an appealing middle ground between nonadaptive agent programming languages and systems that solely learn user preferences from the user s ratings of pages we present a case study where we provide some simple advice and speci an instructable adaptive interface for discovering and monitoring information on the world wide web we are creating a customizable intelligent interface to the world wide web that assists a user in locating specific current and relevant information the wisconsin adaptive web assistant wawa is capable of accepting instructions regarding what type of information the users are seeking and how to go about looking for it wawa compiles these instructions into neural networks which means that the system s behavior can be modified via training examples users can create these training examples by rating pages retrieved by wawa but more importantly the system uses techniques from reinforcement learning to internally create its own examples users can also later provide additional instructions wawa uses these neural networks to guide its autonomous navigation of the web thereby producing an interface to the web that users periodically instruct and which in the background searches the web for relevant information including periodically revisiting pages that change regularly key clause aggregation using linguistic knowledge by combining multiple clauses into one single sentence a text generation system can express the same amount of information in fewer words and at the same time produce a great variety of complex constructions in this paper we describe hypotactic and paratactic operators for generating complex sentences from clause sized semantic representations these two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar 1 introduction an expression is more concise than another expression if it conveys the same amount of information in fewer words complex sentences generated by combining clauses are more concise than corresponding simple sentences because multiple references to the recurring entities are removed for example clauses like jones is a patient and jones has hypertension can be combined into a more concise sentence jones is a hypertensive patient to illustrate the common occurrence of such repeated enti emergent cooperative goal satisfaction in large scale automated agent systems cooperation among autonomous agents has been discussed in the dai community for several years papers about cooperation 6 45 negotiation 33 distributed planning 5 and coalition formation 28 48 have provided a variety of approaches and several algorithms and solutions to situations wherein cooperation is possible however the case of cooperation in large scale multi agent systems mas has not been thoroughly examined therefore in this paper we present a framework for cooperative goal satisfaction in large scale environments focusing on a low complexity physics oriented approach the multi agent systems with which we deal are modeled by a physics oriented model according to the model mas inherit physical properties and therefore the evolution of the computational systems is similar to the evolution of physical systems to enable implementation of the model we provide a detailed algorithm to be used by a single agent within the system the model and the algorithm are a feasible formation of coalitions among autonomous agents in non super additive environments cooperating and sharing resources by creating coalitions of agents are an important way for autonomous agents to execute tasks and to maximize payoff such coalitions will form only if each member of a coalition gains more if it joins the coalition than it could gain otherwise there are several ways of creating such coalitions and dividing the joint payoff among the members in this paper we present algorithms for coalition formation and payoff distribution in non super additive environments we focus on a low complexity kernel oriented coalition formation algorithm the properties of this algorithm were examined via simulations these have shown that the model increases the benefits of the agents within a reasonable time period and more coalition formations provide more benefits to the agents key words distributed ai coalition formation multi agent systems this material is based upon work supported in part by the nsf under grant no iri 9423967 arpa rome labs contract f30602 a graph query language and its query processing many new database applications involve querying of graph data in this paper we present an objectoriented graph data model and an oql like graph query language goql the data model and the language are illustrated in the application domain of multimedia presentation graphs we then discuss the query processing techniques for goql more specifically the translation of goql into an operatorbased language called o algebra extended with operators to deal with paths and sequences we also discuss different approaches for efficient implementations of algebra operators for paths and sequences 1 introduction many database applications such as hypertext applications geographic information systems world wide web searching and heterogeneous information integration etc require modeling and querying of graph data guti94 gbpv 94 memm 96 bdhs 96 aqmww 96 am 98 ffkls 98 lsbboo98 in this paper we present a data model and an oql like query language goql for querying graphs fusion of perceptual cues for robust tracking of the paradigm of perceptual fusion provides robust solutions to computer vision problems by combining the outputs of multiple vision modules the assumptions and constraints of each module are factored out to result in a more robust system overall the integration of dierent modules can be regarded as a form of data fusion to this end we propose a framework for fusing dierent information sources through estimation of covariance from observations the framework is demonstrated in a face and 3d pose tracking system that fuses similarity to prototypes measures and skin colour to track head pose and face position the use of data fusion through covariance introduces constraints that allow the tracker to robustly estimate head pose and track face position simultaneously key words data fusion pose estimation similarity representation face recognition 1 introduction the approach we have taken to computer vision referred to as perceptual fusion involves the integration of multip design and implementation of a high performance distributed web crawler broad web search engines as well as many more specialized search tools rely on web crawlers to acquire large collections of pages for indexing and analysis such a web crawler may interact with millions of hosts over a period of weeks or months and thus issues of robustness flexibility and manageability are of major importance in addition i o performance network resources and os limits must be taken into account in order to achieve high performance at a reasonable cost in this paper we describe the design and implementation of a distributed web crawler that runs on a network of workstations the crawler scales to at least several hundred pages per second is resilient against system crashes and other events and can be adapted to various crawling applications we present the software architecture of the system discuss the performance bottlenecks and describe efficient techniques for achieving high performance we also report preliminary experimental results based on a crawl of million pages on million hosts work supported by nsf career award nsf ccr 0093400 intel corporation and the new york state center for advanced technology in telecommunications catt at polytechnic university and by equipment grants from intel corporation and sun microsystems 1 1 direct annotation a drag and drop strategy for labeling photos annotating photos is such a time consuming tedious and error prone data entry task that it discourages most owners of personal photo libraries by allowing users to drag labels such as personal names from a scrolling list and drop them on a photo we believe we can make the task faster easier and more appealing since the names are entered in a database searching for all photos of a friend or family member is dramatically simplified we describe the user interface design and the database schema to support direct annotation as implemented in our photofinder prototype keywords direct annotation direct manipulation graphical user interfaces photo libraries drag and drop label placement 1 introduction adding captions to photos is a time consuming and error prone task for professional photographers editors librarians curators scholars and amateur photographers in many professional applications photos are worthless unless they are accurately described by date time loc creating creativity for everyone user interfaces for supporting innovation a challenge for human computer interaction researchers and user interface designers is to construct information technologies that support creativity this ambitious goal can be attained by building on an adequate understanding of creative processes this paper offers the four phase genex framework for generating excellence collect learn from previous works stored in digital libraries relate consult with peers and mentors at early middle and late stages create explore compose and evaluate possible solutions donate disseminate the results and contribute to the digital libraries within this integrated framework this paper proposes eight activities that require humancomputer interaction research and advanced user interface design a scenario about an architect illustrates the process of creative work within a genex environment 1 supporting creativity with advanced information abundant user interfaces a challenge for human computer interaction researchers and user interface designers is to construct information technologies that support creativity this ambitious goal can be attained if designers build on an adequate understanding of creative processes this paper describes a model of creativity the four phase genex framework for generating excellence collect learn from previous works stored in digital libraries the web etc relate consult with peers and mentors at early middle and late stages create explore compose discover and evaluate possible solutions donate disseminate the results and contribute to the digital libraries the web etc within this integrated framework there are eight activities that require human computer interaction research and advanced user interface design this paper concentrates on techniques of information visualization that support creative work by enabling users to find relevant information resources identify desired items in a se using constraint satisfaction for view update translation view update is the problem of translating update requests against a view into update requests against the base data in this paper we present a novel approach to this problem in relational databases using conditional tables to represent relational views we translate a view update problem into a disjunction of a number of constraint satisfaction problems csps solutions to the csps correspond to possible translations of the view update semantic information to resolve ambiguity can be handled as additional constraints in the csps this approach enables us to apply the rich results of the csp research to analyze and solve an important problem in database management shock graphs and shape matching we have been developing a theory for the generic representation of 2 d shape where structural descriptions are derived from the shocks singularities of a curve evolution process acting on bounding contours we now apply the theory to the problem of shape matching the shocks are organized into a directed acyclic shock graph and complexity is managed by attending to the most significant central shape components first the space of all such graphs is highly structured and can be characterized by the rules of a shock graph grammar the grammar permits a reduction of a shock graph to a unique rooted shock tree we introduce a novel tree matching algorithm which finds the best set of corresponding nodes between two shock trees in polynomial time using a diverse database of shapes we demonstrate our system s performance under articulation occlusion and changes in viewpoint keywords shape representation shape matching shock graph shock graph grammar subgraph isomorphism 1 i more realistic human behavior models for agents in virtual worlds emotion stress and value ontologies this paper focuses on challenges to improving the behavioral realism of computer generated agents and attempts to reflect the state of the art in human behavior modeling with particular attention to value ontologies emotion and stress in game theoretic settings the goal is to help those interested in constructing more realistic software agents for use in simulations in virtual reality environments and in training and performance aiding settings such as on the web or in embedded applications this paper pursues this goal by providing a framework for better integrating the theories and models contained in the diverse human behavior modeling literatures such as those that straddle physiological cognitive and emotive processes individual differences emergent group and crowd behavior and punctuated equilibria in social settings the framework is based on widely available ontologies of world values and how these and physiological factors might be construed emotively into subjective expected utilities to guide the reactions and deliberations of agents for example what makes one set of opponent groups differ from another this framework serves as an extension of markov decision processes appropriate for iterative play in game theoretic settings with particular emphasis on agent capabilities for redefining drama and for finding meta games to counter the human player this article presents the derivation of the framework and some initial results and lessons learned about integrating behavioral models into interactive dramas and meta games that stimulate systemic thought and training doctrine 1 to boldly go bayesian exploration for mobile robots this work addresses the problem of robot exploration that is the task of automatically learning a map of the environment which is useful for mobile robot navigation and localization the exploration mechanism is intended to be applicable to an arbitrary environment and is independent of the particular representation of the world we take an information theoretic approach and avoid the use of arbitrary heuristics preliminary results are presented and we discuss future directions for investigation learning visual landmarks for pose estimation we present an approach to vision based mobile robot localization even without an a priori pose estimate this is accomplished by learning a set of visual features called image domain landmarks the landmark learning mechanism is designed to be applicable to a wide range of environments each landmark is detected as a local extremum of a measure of uniqueness and represented by an appearance based encoding localization is performed using a method that matches observed landmarks to learned prototypes and generates independent position estimates for each match the independent estimates are then combined to obtain a final position estimate with an associated uncertainty quantitative experimental evidence is presented that demonstrates that accurate pose estimates can be obtained despite changes to the environment the computational theory of neural networks in the present paper a detailed taxonomy of neural network models with various restrictions is presented with respect to their computational properties the criteria of classification include e g feedforward and recurrent architectures discrete and continuous time binary and analog states symmetric and asymmetric weights finite size and infinite families of networks deterministic and probabilistic models etc the underlying results concerning the computational power of perceptron rbf winner take all and spiking neural networks are briey surveyed and completed by relevant references a computational taxonomy and survey of neural network models we survey and summarize the existing literature on the computational aspects of neural network models by presenting a detailed taxonomy of the various models according to their computational characteristics the criteria of classication include e g the architecture of the network feedforward vs recurrent time model discrete vs continuous state type binary vs analog weight constraints symmetric vs asymmetric network size nite nets vs infinite families computation type deterministic vs probabilistic etc the underlying results concerning the computational power of perceptron rbf winner take all and spiking neural networks are briefly surveyed with pointers to the relevant literature first results in the coordination of heterogeneous robots for large scale assembly while many multi robot systems rely on fortuitous cooperation between agents some tasks such as the assembly of large structures require tighter coordination we present a general software architecture for coordinating heterogeneous robots that allows for both autonomy of the individual agents as well as explicit coordination this paper presents recent results with three robots with very different configurations working as a team these robots are able to perform a high precision docking task that none could achieve individually 1 introduction as robots become more autonomous and sophisticated they are increasingly being used for more complex and demanding tasks often single robots are insufficient to perform the tasks for some types of tasks such as exploration or demining multiple robots can be used to increase efficiency and reliability for many other tasks however not only are multiple robots necessary but explicit coordination amongst the robots is imper the self organizing map in industry analysis the self organizing map som is a powerful neural network method for the analysis and visualization of high dimensional data it maps nonlinear statistical relationships between high dimensional measurement data into simple geometric relationships usually on a two dimensional grid the mapping roughly preserves the most important topological and metric relationships of the original data elements and thus inherently clusters the data the need for visualization and clustering occurs for instance in the data analysis of complex processes or systems in various engineering applications entire fields of industry can be investigated using som based methods the data exploration tool presented in this chapter allows visualization and analysis of large data bases of industrial systems forest industry is the rst chosen application for the tool to illustrate the global nature of forest industry the example case is used to cluster the pulp and paper mills of the world a case study in web search using trec algorithms web search engines rank potentially relevant pages sites for a user query ranking documents for user queries has also been at the heart of the text retrieval conference trec in short under the label retrieval the trec community has developed document ranking algorithms that are known to be the best for searching the document collections used in trec which are mainly comprised of newswire text however the web search community has developed its own methods to rank web pages sites many of which use link structure on the web and are quite dierentfrom the algorithms developed at trec this study evaluates the performance of a state of the art keyword based document ranking algorithm coming out of trec on a popular web search task nding the web page site of an entity companies universities organizations individuals etc this form of querying is quite prevalentonthe web the results from the trec algorithms are compared to four commercial web search engines results show that for nding the web page site of an entity commercial web search engines are notably better than a state of the art trec algorithm these results are in sharp contrast to results from several previous studies keywords search engines trec ad hoc keyword based ranking linkbased ranking 1 document expansion for speech retrieval advances in automatic speech recognition allow us to search large speech collections using traditional information retrieval methods the problem of aboutness for documents is a document about a certain concept has been at the core of document indexing for the entire history of ir this problem is more difficult for speech indexing since automatic speech transcriptions often contain mistakes in this study we show that document expansion can be successfully used to alleviate the effect of transcription mistakes on speech retrieval the loss of retrieval effectiveness due to automatic transcription errors can be reduced by document expansion from 15 27 relative to retrieval from human transcriptions to only about 7 13 even for automatic transcriptions with word error rates as high as 65 for good automatic transcriptions 25 word error rate retrieval effectiveness with document expansion is indistinguishable from retrieval from human transcriptions this makes speech learning visual models of social engagement we introduce a face detector for wearable computers that exploits constraints in face scale and orientation imposed by the proximity of participants in near social interactions using this method we describe a wearable system that perceives social engagement i e when the wearer begins to interact with other individuals our experimental system proved 90 accurate when tested on wearable video data captured at a professional conference over 300 individuals were captured during social engagement and the data was separated into independent training and test sets a metric for balancing the performance of face detection localization and recognition in the context of a wearable interface is discussed recognizing social engagement with a user s wearable computer provides context data that can be useful in determining when the user is interruptible in addition social engagement detection may be incorporated into a user interface to improve the quality of mobile face recognition software for example the user may cue the face recognition system in a socially graceful way by turning slightly away and then toward a speaker when conditions for recognition are favorable 1 symbiotic interfaces for wearable face recognition we introduce a wearable face detection method that exploits constraints in face scale and orientation imposed by the proximity of participants in near social interactions using this method we describe a wearable system that perceives social engagement i e when the wearer begins to interact with other individuals one possible application is improving the interfaces of portable consumer electronics such as cellular phones to avoid interrupting the user during face to face interactions our experimental system proved 90 accurate when tested on wearable video data captured at a professional conference over three hundred individuals were captured and the data was separated into independent training and test sets a goal is to incorporate user interface in mobile machine recognition systems to improve performance the user may provide real time feedback to the system or may subtly cue the system through typical daily activities such as turning to face a speaker as to when conditions for recognition are favorable 1 using information extraction rules for extending domain ontologies nt we lay special emphasis on considerations and methods which are necessary to realize such a scenario in industrial practice in each industrial environment besides the questions of smooth introduction of new technology regarding human factors and organizational processes and besides the question of modeling tools and method support for knowledge in particular ontologies for structuring oms or parts of oms acquisition at least two other factors are of utmost importance one is the predominance of informal i e essentially textbased representations of knowledge this is not only just a matter of fact but really useful because the cost of formalization is often not in the right relation to the potential benefits such that many informal parts of the scenario are economically reasonable 5 one implication is that also methods for building formal models must be affordable the other is the fact that ontologies are not a stand alone component built once and then remaining unt querying the uncertain position of moving objects in this paper we propose a data model for representing moving objects with uncertain positions in database systems it is called the moving objects spatio temporal most data model we also propose future temporal logic ftl as the query language for the most model and devise an algorithm for processing ftl queries in most 1 introduction existing database management systems dbms s are not well equipped to handle continuously changing data such as the position of moving objects the reason for this is that in databases data is assumed to be constant unless it is explicitly modified for example if the salary field is 30k then this salary is assumed to hold i e 30k is returned in response to queries until explicitly updated thus in order to represent moving objects e g cars in a database and answer queries about their position e g how far is the car with license plate rww860 from the nearest hospital the car s position has to be continuously updated this is unsa component based agent construction in this paper an agent architecture is proposed that can be used to integrate pre existing components that provide the domain dependent agent functionality the key integrating feature of the agent is an active message board that is used for inter component hence intra agent communication the board is active because it automatically forwards messages to components they do not have to poll the message board it does this on the basis of message pattern functions that components place on the board using advertisement messages these functions can contain component provided semantic tests on the content of the message they can also communicate with any other component whilst they are being applied in addition an agent management toolkit called alfa is described which offers a set of agent management services this toolkit consists of a number of servers for storing the code of the components and symbolic descriptions of agents regarding their component makeup a third server use a security architecture for application session handoff ubiquitous computing across a variety of wired and wireless connections still lacks an effective security architecture in our research work we address the specific issue of designing and building a security architecture for application session handoff a functionality which we envision will be a key component enabling ubiquitous computing our architecture incorporates a number of proven approaches into the new context of ubiquitous computing we employ the bell lapadula and capability models to realise access control and adopt public key infrastructure pki based approaches to provide efficient and authenticated end to end security to demonstrate the effectiveness of our design we implemented an application enabled with this security architecture and showed that it incurred low latency open video a framework for a test collection this paper provides a framework for such a test collection and describes the open video project that has begun to develop a test collection based on this framework the proposed test collection is meant to be used to study a wide range of problems such as tests of algorithms for creating surrogates for video content or interfaces that display result sets from queries an important challenge in developing such a collection is storing and distributing video objects this paper is meant to layout video management issues that may influence distributed storage solutions more specifically this paper describes the first phase for creating the test collection sets guidelines for building the collection and serves as a basis for discussion to inform subsequent phases and invite research community involvement 2000 academic press 1 introduction it is inevitable that the technical limitations that impede widespread usage of video libraries will dimi a foundation for conventional and temporal query optimization addressing duplicates and ordering abstract most real world databases contain substantial amounts of time referenced or temporal data recent advances in temporal query languages show that such database applications may benefit substantially from built in temporal support in the dbms to achieve this temporal query representation optimization and processing mechanisms must be provided this paper presents a foundation for query optimization that integrates conventional and temporal query optimization and is suitable for both conventional dbms architectures and ones where the temporal support is obtained via a layer on top of a conventional dbms this foundation captures duplicates and ordering for all queries as well as coalescing for temporal queries thus generalizing all existing approaches known to the authors it includes a temporally extended relational algebra to which sql and temporal sql queries may be mapped six types of algebraic equivalences concrete query transformation rules that obey different equivalences a procedure for determining which types of transformation rules are applicable for optimizing a query and a query plan enumeration algorithm the presented approach partitions the work required by the database implementor to develop a provably correct query optimizer into four stages the database implementor has to 1 specify operations formally 2 design and prove correct appropriate transformation rules that satisfy any of the six equivalence types 3 augment the mechanism that determines when the different types of rules are applicable to ensure that the enumeration algorithm applies the rules correctly and 4 ensure that the mapping generates a correct initial query plan index terms temporal databases query optimization transformation rules temporal algebra duplicate elimination coalescing 1 adaptable query optimization and evaluation in temporal middleware time referenced data are pervasive in most real world databases recent advances in temporal query languages show that such database applications may benefit substantially from built in temporal support in the dbms to achieve this temporal query optimization and evaluation mechanisms must be provided either within the dbms proper or as a source level translation from temporal queries to conventional sql this paper proposes a new approach using a middleware component on top of a conventional dbms this component accepts temporal sql statements and produces a corresponding query plan consisting of algebraic as well as regular sql parts the algebraic parts are processed by the middleware while the sql parts are processed by the dbms the middleware uses performance feedback from the dbms to adapt its partitioning of subsequent queries into middleware and dbms parts the paper describes the architecture and implementation of the temporal middleware component termed tango which is based on the volcano extensible query optimizer and the xxl query processing library experiments with the system demonstrate the utility of the middleware s internal processing capability and its cost based mechanism for apportioning the processing between the middleware and the underlying dbms index terms temporal databases query processing and optimization cost based optimization middleware 1 varieties of affect and the cogaff architecture schema in the last decade and a half the amount of work on affect in general and emotion in particular has grown in empirical psychology cognitive science and ai both for scientific purposes and for the purpose of designing synthetic characters e g in games and entertainments such work understandably starts from concepts of ordinary language e g emotion feeling mood etc however these concepts can be deceptive the words appear to have clear meanings but are used in very imprecise and systematically ambiguous ways this is often because of explicit or implicit pre scientific theories about mental states and process more sophisticated theories can provide a basis for deeper and more precise concepts as has happened in physics and chemistry in the cognition and affect project we have been attempting to explore the benefits of developing architecture based concepts i e starting with specifications of architectures for complete agents and then finding out what sorts of states and processes are supported by those architectures so instead of presupposing one theory of the architecture and explicitly or implicitly basing concepts on that we define a space of architectures generated by the cogaff architecture schema where each supports different collections of concepts in that space we focus on one architecture h cogaff a particularly rich instance of the cogaff architecture schema conjectured as a theory of normal adult human information processing the architecture based concepts that it supports provide a framework for defining with greater precision than previously a host of mental concepts including affective concepts we then find that these map more or less loosely onto various pre theoretical concepts such as emotion etc we indicate some of the variety of emotion concepts generated by the h cogaff architecture a different architecture supporting a different range of mental concepts might be appropriate for exploring affective states of other animals for instance insects reptiles or other mammals and young children 1 how many separately evolved emotional beasties live within us a problem which bedevils the study of emotions and the study of consciousness is that we assume a shared understanding of many everyday concepts such as emotion feeling pleasure pain desire awareness etc unfortunately these concepts are inherently very complex ill defined and used with different meanings by different people moreover this goes unnoticed so that people think they understand what they are referring to even when their understanding is very unclear consequently there is much discussion that is inherently vague often at cross purposes and with apparent disagreements that arise out of people unwittingly talking about different things we need a framework which explains how there can be all the diverse phenomena that different people refer to when they talk about emotions and other affective states and processes the conjecture on which this paper is based is that adult humans have a type of information processing architecture with components whi architectures and tools for human like agents 1 this paper discusses agent architectures which are describable in terms of the higher level mental concepts applicable to human beings e g believes desires intends and feels we conjecture that such concepts are grounded in a type of information processing architecture and not simply in observable behaviour nor in newell s knowledge level concepts nor dennett s intentional stance a strategy for conceptual exploration of architectures in design space and nichespace is outlined including an analysis of design tradeoffs the sim agent toolkit developed to support such exploration including hybrid architectures is described briefly keywords architecture hybrid mind emotion evolution toolkit mentalistic descriptions the usual motivation for studying architectures is to explain or replicate performance another less common reason is to account for concepts this paper discusses high level architectures which can provide a systematic non behavioural c damasio descartes alarms and meta management this paper discusses some of the requirements for the control architecture of an intelligent human like agent with multiple independent dynamically changing motives in a dynamically changing only partly predictable world the architecture proposed includes a combination of reactive deliberative and meta management mechanisms along with one or more global alarm systems the engineering design requirements are discussed in relation our evolutionary history evidence of brain function and recent theories of damasio and others about the relationships between intelligence and emotions 1 introduction stan franklin the organiser of this symposium wrote minds are the control structures of autonomous agents 5 p 412 the claim that minds are essentially concerned with control echoing the seminal ideas of norbert wiener 16 is one with which i strongly concur though as argued in 11 we need to go far beyond the early idea of control systems with fixed architecture and changes on building cognitively rich agents using the sim agent toolkit synthetic agents with varying degrees of intelligence and autonomy are being designed in many research laboratories the motivations include military training simulations games and entertainments educational software digital personal assistants software agents managing internet transactions or purely scientific curiosity different approaches are being explored including at one extreme research on the interactions between agents and at the other extreme research on processes within agents the first approach focuses on forms of communication requirements for consistent collaboration planning of coordinated behaviours to achieve collaborative goals extensions to logics of action and belief for multiple agents and types of emergent phenomena when many agents interact for instance taking routing decisions on a telecommunications network the second approach focuses on the internal architecture of individual agents required for social interaction collaborative behaviours complex decision making learning and emergent phenomena within complex agents agents with complex internal structure may for example combine perception motive generation planning plan execution execution monitoring and even emotional reactions document clustering using word clusters via the information bottleneck method we present a novel implementation of the recently introduced information bottleneck method for unsupervised document clustering given a joint empirical distribution of words and documents p x y we first cluster the words y so that the obtained word clusters y hat maximally preserve the information on the documents the resulting joint distribution p x y hat contains most of the original information about the documents i x y hat i x y but it is much less sparse and noisy using the same procedure we then cluster the documents x so that the information about the word clusters is preserved thus we first find word clusters that capture most of the mutual information about the set of documents and then find document clusters that preserve the information about the word clusters we tested this procedure over several document collections based on subsets taken from the standard 20newsgroups corpus the results were assessed by calculating the correlation between the document clusters and the correct labels for these documents finding from our experiments show that this double clustering procedure which uses the information bottleneck method yields significantly superior performance compared to other common document distributional clustering algorithms moreover the double clustering procedure improves all the distributional clustering methods examined here the power of word clusters for text classification the recently introduced information bottleneck method 21 provides an information theoretic framework for extracting features of one variable that are relevant for the values of another variable several previous works already suggested applying this method for document clustering gene expression data analysis spectral analysis and more in this work we present a novel implementation of this method for supervised text classification specifically we apply the information bottleneck method to find word clusters that preserve the information about document categories and use these clusters as features for classification previous work 1 used a similar clustering procedure to show that word clusters can significantly reduce the feature space dimensionality with only a minor change in classification accuracy in this work we present similar results and go further to show that when the training sample is small word clusters can yield significant improvement in classification accuracy up to over the performance using the words directly 1 theory of neuromata a finite automaton the so called neuromaton realized by a finite discrete recurrent neural network working in parallel computation mode is considered both the size of neuromata i e the number of neurons and their descriptional complexity i e the number of bits in the neuromaton representation are studied it is proved that a constant time delay of the neuromaton output does not play a role within a polynomial descriptional complexity it is shown that any regular language given by a regular expression of length n is recognized by a neuromaton with theta n neurons further it is proved that this network size is in the worst case optimal on the other hand generally there is not an equivalent polynomial length regular expression for a given neuromaton then two specialized constructions of neural acceptors of the optimal descriptional complexity theta n for a single n bit string recognition are described they both require o n 1 2 neurons and either o n con relevance feedback and query expansion for searching the web a model for searching a digital library a fully operational large scale digital library is likely to be based on a distributed architecture and because of this it is likely that a number of independent search engines may be used to index different overlapping portions of the entire contents of the library in any case different media text audio image etc will be indexed for retrieval by different search engines so techniques which provide a coherent and unified search over a suite of underlying independent search engines are thus likely to be an important part of navigating in a digital library in this paper we present an architecture and a system for searching the world s largest dl the world wide web what makes our system novel is that we use a suite of underlying web search engines to do the bulk of the work while our system orchestrates them in a parallel fashion to provide a higher level of information retrieval functionality thus it is our meta search engine and not the underlying direct search engines th conversation trees and threaded chats chat programs and instant messaging services are increasingly popular among internet users however basic issues with the interfaces and data structures of most forms of chat limit their utility for use in formal interactions like group meetings and decision making tasks in this paper we discuss threaded text chat a program designed to address some of the deficiencies of current chat programs standard forms of chat introduce ambiguity into interaction in a number of ways most profoundly by rupturing connections between turns and replies threaded chat presents a solution to this problem by actively supporting the basic turn taking structure of human conversation while the solution introduces interface design challenges of its own usability studies show that users patterns of interaction in threaded chat are equally effective but different and possibly more efficient than standard chat programs keywords chat programs turn taking conversation computer mediated communi neural networks in business techniques and applications for the operations researcher this paper presents an overview of the di erent types of neural network models which are applicable when solving business problems the history of neural networks in business is outlined leading to a discussion of the current applications in business including data mining as well as the current research directions the role of neural networks as a modern operations research tool is discussed scope and purpose neural networks are becoming increasingly popular in business many organisations are investing in neural network and data mining solutions to problems which have traditionally fallen under the responsibility of operations research this article provides an overview for the operations research reader of the basic neural network techniques as well as their historical and current use in business the paper is intended as an introductory article for the remainder of this special issue on neural networks in business 2000 elsevier science ltd all rights reserved keywords n management of xml documents in an integrated digital library we describe a generalized toolset developed by the perseus project to manage xml documents in the context of a large heterogeneous digital library the system manages multiple dtds through mappings from elements in the dtd to abstract document structures the abstraction of document metadata both structural and descriptive facilitates the development of application level tools for knowledge management and document presentation we discuss the implementation of the xml back end and describe applications for cross citation retrieval toponym extraction and plotting automatic hypertext generation morphology and word co occurrence 1 regularized principal manifolds many settings of unsupervised learning can be viewed as quantization problems the minimization of the expected quantization error subject to some restrictions this allows the use of tools such as regularization from the theory of supervised risk minimization for unsupervised settings this setting turns out to be closely related to principal curves the generative topographic map and robust coding we explore this connection in two ways 1 we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways 2 we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm in particular we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators experimental results demonstrate the feasibility of the approach keywords regularization uniform convergence kernels entropy numbers principal curves clustering generative topograph from regularization operators to support vector kernels we derive the correspondence between regularization operators used in regularization networks and hilbert schmidt kernels appearing in support vector machines more specifically we prove that the green s functions associated with regularization operators are suitable support vector kernels with equivalent regularization properties as a by product we show that a large number of radial basis functions namely conditionally positive definite functions may be used as support vector kernels 1 introduction support vector sv machines for pattern recognition regression estimation and operator inversion exploit the idea of transforming into a high dimensional feature space where they perform a linear algorithm instead of evaluating this map explicitly one uses hilbert schmidt kernels k x y which correspond to dot products of the mapped data in high dimensional space i e k x y phi x delta phi y 1 with phi r n f denoting the map into feature space mostly this m general cost functions for support vector regression the concept of support vector regression is extended to a more general class of convex cost functions moreover it is shown how the resulting convex constrained optimization problems can be efficiently solved by a primal dual interior point path following method both computational feasibility and improvement of estimation is demonstrated in the experiments 1 introduction 1 1 risk minimization in the following we will consider the problem of regression estimation for some probability density function p x y on r n omega r and some cost function c find a function f that minimizes the following risk functional r f z c y gamma f x p x y dxdy 1 however we do not know p x y instead we only have observations f x 1 y 1 x y g x i 2 r n y i 2 r at hand that have been drawn iid independent identially distributed from p x y hence the first guess would be to replace p by the empirical density derived from our observations and minimize the data mining at the interface of computer science and statistics this chapter is written for computer scientists engineers mathematicians and scientists who wish to gain a better understanding of the role of statistical thinking in modern data mining data mining has attracted considerable attention both in the research and commercial arenas in recent years involving the application of a variety of techniques from both computer science and statistics the chapter discusses how computer scientists and statisticians approach data from different but complementary viewpoints and highlights the fundamental differences between statistical and computational views of data mining in doing so we review the historical importance of statistical contributions to machine learning and data mining including neural networks graphical models and flexible predictive modeling the primary conclusion is that closer integration of computational methods with statistical thinking is likely to become increasingly important in data mining applications keywords data mining statistics pattern recognition transaction data correlation 1 surfing the digital wave generating personalised tv listings using collaborative case based recommendation abstract in the future digital tv will offer an unprecedented level of programme choice we are told that this will lead to dramatic increases in viewer satisfaction as all viewing tastes are catered for all of the time however the reality may be somewhat different we have not yet developed the tools to deal with this increased level of choice for example conventional tv guides will be virtually useless and viewers will face a significant and frustrating information overload problem this paper describes a solution in the form of the ptv system ptv employs user profiling and information filtering techniques to generate web based tv viewing guides that are personalised for the viewing preferences of individual users the paper explains how ptv constructs graded user profiles to drive a hybrid recommendation technique combining case based and collaborative information filtering methods the results of an extensive empirical study to evaluate the quality of ptv s casebased and collaborative filtering strategies are also described 1 a state of the art review on multimodal video indexing efficient and effective handling of video documents depends on the availability of indexes manual indexing is unfeasible for large video collections effective indexing requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion in this paper we focus on the similarities and differences between the modalities and survey several methods aiming at automating the time and resource consuming process of video indexing furthermore we put forward a unifying and multimodal framework which views a video document from the perspective of its author this framework forms the guiding principle for identifying index types for which automatic methods are found in literature it furthermore forms the basis for categorizing these different methods confseek a multi user multi threaded specialized search engine for conferences the explosive growth of the world wide web the latest estimates for its size are around 1 000 000 000 web pages has made critical the need to nd information more accurately than what the current generic search engines can deliver this project implements a prototype specialized search engine that allows user to submit queries for conferences in a specic eld of interest and returns the detailed information about those conferences deadlines etc it uses multiple existing search engines to provide better coverage of the information available on the internet it can interact with multiple users concurrently and makes use of multi threading to achieve faster information processing the goal is to make this tool available to the scientic community to provide researchers with improved access to conference information we discuss possible extensions e g ranking of conferences according to their quality trip planning etc contents 1 introduction 2 2 goals of this project building a machine learning based text understanding system text understanding systems are approaching the point of being a practical technology as long as the system is trained for a narrowly defined domain machine learning and statistical approaches can minimize the effort involved in adapting a text understanding system to a new domain this paper presents a system whose goal is deep understanding limited only by the necessity of designing a formal representation of the target concepts relevant to the domain this system is an advance over previous machine learning based systems because of its richer output representation and an advance over equally expressive text understanding systems because of its more extensive use of machine learning 1 information extraction from free text a variety of systems have been developed in recent years that extract information from text none of them attempts general purpose understanding but instead focus on narrowly defined information needs a domain is defined as a collection of docum learning information extraction rules for semi structured and free text a wealth of on line text information can be made available to automatic processing by information extraction ie systems each ie application needs a separate set of rules tuned to the domain and writing style whisk helps to overcome this knowledge engineering bottleneck by learning text extraction rules automatically whisk is designed to handle text styles ranging from highly structured to free text including text that is neither rigidly formatted nor composed of grammatical sentences such semistructured text has largely been beyond the scope of previous systems when used in conjunction with a syntactic analyzer and semantic tagging whisk can also handle extraction from free text such as news stories keywords natural language processing information extraction rule learning 1 information extraction as more and more text becomes available on line there is a growing need for systems that extract information automatically from text data an information extraction ie sys qos management in web based real time data services the demand for real time data services has been increasing recently many e commerce applications and webbased information services are becoming very sophisticated in their data needs they span the spectrum from low level status data e g stock prices to high level aggregated data e g recommended selling buying point in these applications it is desired to process user requests within their deadlines using fresh data which reflect the current market status current web based data services are poor at processing user requests in a timely manner using fresh data to address this problem we present a framework for guaranteed real time data services in unpredictable environments we also introduce a possible application of our approach in the distributed environment 1 internet search for indian languages with the internet growing at an exponential rate no single search engine can index all of the web it is therefore necessary to build specialized search engines that fulfill particular needs of a community of people an example is citeseer which indexes research papers on the web also as the web is increasingly hosting web pages in different languages it is essential to be able to search for information stored in a specific language for a search engine aimed at information in a particular language an easy to use user interface is as essential as good response time and relevance of results we introduce shodh a search engine for an indian language a prototype has been developed for selected set of pages and results are satisfactory the user interface of the search engine includes both querying facilities as well as display of query results in the same language in which the information is stored contents 1 automated derivation of complex agent architectures from analysis specifications multiagent systems have been touted as a way to meet the need for distributed software systems that must operate in dynamic and complex environments however in order for multiagent systems to be effective they must be reliable and robust engineering multiagent systems is a non trivial task providing ample opportunity for even experts to make mistakes formal transformation systems can provide automated support for synthesizing multiagent systems which can greatly improve their correctness and reliability this paper describes a semi automated transformation system that generates an agent s internal architecture from an analysis specification in the mase methodology 1 a theorem prover based analysis tool for object oriented databases we present a theorem prover based analysis tool for object oriented database systems with integrity constraints object oriented database specifications are mapped to higher order logic hol this allows us to reason about the semantics of database operations using a mechanical theorem prover such as isabelle or pvs the tool can be used to verify various semantics requirements of the schema such as transaction safety compensation and commutativity to support the advanced transaction models used in workflow and cooperative work we give an example of method safety analysis for the generic structure editing operations of a cooperative authoring system 1 introduction object oriented specification methodologies and object oriented programming have become increasingly important in the past ten years not surprisingly this has recently led to an interest in object oriented program verification in the theorem prover community mainly using higher order logic hol several dif man multi agent interaction in vr a case study with robocup we describe a virtual reality system that allows users at different locations to interact with a multi agent system in a natural way we use robocup robot soccer as a case study a human player who is immersed in a cave can interact with the robocup simulation in its natural domain by playing along with a virtual soccer game the system supports distributed collaboration by allowing humans at different geographic locations to participate and interact in real time the most difficult problem we address is how to deal with the latency that is induced by the multi agent simulation and by the wide area network between different caves our navigation software anticipates the movements of the human player and optimizes the interaction navigation kicking also it sends a minimal amount of state information over the wide area network 1 introduction multi agent systems are becoming increasingly important in our society the majority of such systems is in some way related to internet improving response time by search pruning in a content based image retrieval system using inverted file techniques this paper describes several methods for improving query evaluation speed in a content based image retrieval system cbirs response time is an extremely important factor in determining the usefulness of any interactive system as has been demonstrated by human factors studies over the past thirty years in particular response times of less than one second are often specified as a usability requirement it is shown that the use of inverted files facilitates the reduction of query evaluation time without significantly reducing the accuracy of the response the performance of the system is evaluated using precision vs recall graphs which are an established evaluation method in information retrieval ir and are beginning to be used by cbir researchers keywords content based image retrieval search pruning inverted file response time 1 introduction response times in the interaction between computer systems and human users are of great importance to user satisfaction at present dynamic on line clustering and state extraction an approach to symbolic learning researchers often try to understand the representations that develop in the hidden layers of a neural network during training interpretation is difficult because the representations are typically highly distributed and continuous by continuous we mean that if one constructed a scatter plot over the hidden unit activity space of patterns obtained in response to various inputs examination at any scale would reveal the patterns to be broadly distributed over the space such continuous representations are naturally obtained if the input space and activation dynamics are continuous continuous representations are not always appropriate many task domains might benefit from discrete representations representations selected from a finite set of alternatives example domains include finite state machine emulation data compression language and higher cognition involving discrete symbol processing and categorization in such domains standard neural target seeking crawlers and their topical performance topic driven crawlers can complement search engines by targeting relevant portions of the web a topic driven crawler must exploit the information available about the topic and its underlying context in this paper we extend our previous research on the design and evaluation of topic driven crawlers by comparing seven different crawlers on a harder problem namely seeking highly relevant target pages we find that exploration is an important aspect of a crawling strategy we also study how the performance of crawler strategies depends on a number of topical characteristics based on notions of topic generality cohesiveness and authoritativeness our results reveal that topic generality is an obstacle for most crawlers that three crawlers tend to perform better when the target pages are clustered together and that two of these also display better performance when topic targets are highly authoritative web crawling agents for retrieving biomedical information autonomous agents for topic driven retrieval of information from the web are currently a very active area of research the ability to conduct real time searches for information is important for many users including biomedical scientists health care professionals and the general public we present preliminary research on different retrieval agents tested on their ability to retrieve biomedical information whose relevance is assessed using both genetic and ontological expertise in particular the agents are judged on their performance in fetching information about diseases when given information about genes we discuss several key insights into the particular challenges of agent based retrieval learned from our initial experience in the biomedical domain from binary temporal relations to non binary ones and back in this paper a new approach towards temporal reasoning is presented that scales up from the temporal relations commonly used in allen s qualitative interval calculus and in quantitative temporal constraint satisfaction problems to include interval relations with distances temporal rules and other non binary relations into the reasoning scheme for this purpose we generalize well known methods for constraint propagation determination of consistency and computation of the minimal network from simpler schemes that only allow for binary relations thereby we nd that levels of granularity play a major role for applying these techniques in our more expressive framework indeed the technical preliminaries we provide are especially apt to investigate the switching between dierent granularities of representation hence illucitating and exploiting the tradeo between expressiveness and eciency of temporal reasoning schemes on the one side and between expressiveness and understandability intelligent support for enterprise modelling enterprise modelling integrating models of all pertinent aspects of an enterprise is essential to the management of change in organisations an integrated view of an organisation provides insight into what aspects may bechanged how they may be changed and what the overall e ect of speci c changes will be aiai at the university ofedinburgh has an ongoing research programme which focuses on the use ai techniques to cover the requirements of enterprise modelling and the tools to support it the ai techniques used in aiai s programme range from knowledge representation ontologies and process modelling techniques to visualisation techniques intelligent work ow and coordination technology the techniques are combined in an integrated toolset delivered on an agent based architecture part of aiai s programme is the enterprise project which has been instrumental in determining the requirements for enterprise modelling and in the development ofan integrated toolset to support it the results of the enterprise project show thatwhen combined with task management support enterprise models may directly control the operation of an organisation based on the results of the enterprise project aiai s tbpm project currently addresses coordination issues of enterprise modelling support in this paper we rst describe the requirements for enterprise modelling and enactment in general we then discuss the enterprise toolset which was designed and was implemented to address these requirements finally weevaluate the toolset and describe extensions that are currently being undertaken aiai tr 220 0 1 page 1 1 evolving neural networks through augmenting topologies an important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights we present a method neuroevolution of augmenting topologies neat which outperforms the best fixed topology method on a challenging benchmark reinforcement learning task we claim that the increased efficiency is due to 1 employing a principled method of crossover of different topologies 2 protecting structural innovation using speciation and 3 incrementally growing from minimal structure we test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other what results is significantly faster learning neat is also an important contribution to gas because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously offering the possibility of evolving increasingly complex solutions over generations and strengthening the analogy with biological evolution reverse nearest neighbor queries for dynamic databases in this paper we propose an algorithm for answering reverse nearest neighbor rnn queries a problem formulated only recently this class of queries is strongly related to that of nearest neighbor nn queries although the two are not necessarily complementary unlike nearest neighbor queries rnn queries find the set of database points that have the query point as the nearest neighbor there is no other proposal we are aware of that provides an algorithmic approach to answer rnn queries the earlier approach for rnn queries km99 is based on the pre computation of neighborhood information that is organized in terms of auxiliary data structures it can be argued that the precomputation of the rnn information for all points in the database can be too restrictive in the case of dynamic databases insert and update operations are expensive and can lead to modifications of large parts of the auxiliary data structures also answers to rnn queries for a set of data points depend on t weak consistency in distributed data warehouses we propose and analyze a novel multiple view model of a distributed data warehouse views are represented in a hierarchical fashion incorporating data from base sources as well as possibly other views current approaches to maintain consistency in such a model require that data stored in a view derived from base data via different paths be from the same state of the base relation this type of consistency criterion is too restrictive for some applications hence we propose relaxing the synchronization constraints at the view level and develop a model that allows views to set their own constraints by enforcing individual conditions for all pairs of paths we define a correctness criteria for updates in this particular model and analyze the new requirements necessary for maintaining the consistency of data finally we propose an algorithm to ensure that views are updated consistently keywords multi view data warehouse distributed system consistency updates 1 introduction a dat decentralized incremental maintenance of multi view data warehouses extended abstract i stanoi d agrawal a el abbadi department of computer science university of california santa barbara ca 93106 1 introduction decision support systems make up a big percentage of data base servers presently their size increases due to the necessity of more detailed information and the extended time range of interest as a result data mining queries also span larger sets of data to achieve fast response time a subset of the relevant information is sometimes materialized in views separate from the database sources 4 a data warehouse is an example of storage that integrates information from multiple sources which may be stand alone databases as well as sites such as the internet although data warehousing is a powerful concept for supporting analytical processing building a data warehouse runs into several pragmatic problems the cost of building a data warehouse that integrates disparate data sources in an organization can easily exceed millions of dollars a more sev the gesture pendant a self illuminating wearable infrared computer vision system for home automation control and medical monitoring in this paper we present a wearable device for control of home automation systems via hand gestures this solution has many advantages over traditional home automation interfaces in that it can be used by those with loss of vision motor skills and mobility by combining other sources of context with the pendant we can reduce the number and complexity of gestures while maintaining functionality as users input gestures the system can also analyze their movements for pathological tremors this information can then be used for medical diagnosis therapy and emergency services currently the gesture pendant can recognize control gestures with an accuracy of 95 and user defined gestures with an accuracy of 97 it can detect tremors above 2hz within hz real time american sign language recognition using desk and wearable computer based video hidden markov models hmm s have been used prominently and successfully in speech recognition and more recently in handwriting recognition consequently they seem ideal for visual recognition of complex structured hand gestures such as are found in sign language we describe two experiments that demonstrate a realtime hmm based system for recognizing sentence level american sign language asl without explicitly modeling the fingers the first experiment tracks hands wearing colored gloves and attains a word accuracy of 99 the second experiment tracks hands without gloves and attains a word accuracy of 92 both experiments have a 40 word lexicon 1 introduction while there are many different types of gestures the most structured sets belong to the sign languages in sign language each gesture already has assigned meaning and strong rules of context and grammar may be applied to make recognition tractable to date most work on sign language recognition has employed expensi visual contextual awareness in wearable computing small body mounted video cameras enable a different style of wearable computing interface as processing power increases a wearable computer can spend more time observing its user to provide serendipitous information manage interruptions and tasks and predict future needs without being directly commanded by the user this paper introduces an assistant for playing the real space game patrol this assistant tracks the wearer s location and current task through computer vision techniques and without off body infrastructure in addition this paper continues augmented reality research started in 1995 for binding virtual data to physical locations 1 introduction for most computer systems even virtual reality systems sensing techniques are a means of getting input directly from the user however wearable computers offer a unique opportunity to re direct sensing technology towards recovering more general user context wearable computers have the potential to see as the user sees finding counterexamples to inductive conjectures we present an implementation of a method for nding counterexamples to universally quanti ed inductive conjectures in rst order logic our method uses the proof by consistency strategy to guide a search for a counterexample and a standard rst order theorem prover to perform a concurrent check for inconsistency we explain brie y the theory behind the method describe our implementation and evaluate results achieved on a variety of incorrect conjectures from various sources some work in progress is also presented we are applying the method to the veri cation of cryptographic security protocols in this context a counterexample to a security property can indicate an attack on the protocol and our method extracts the trace of messages exchanged in order to effect the attack this application demonstrates the advantages of the method in that quite complex side conditions decide whether a particular sequence of messages is possible using a theorem prover provides a natural way of dealing with this some early results are presented and we discuss future work 1 oms java model extensibility of oodbms for advanced application domains we showhow model extensibility of object oriented data management systems can be achieved through the combination of a highlevel core object data model and an architecture designed with model extensibility in mind the resulting system oms java is both a general data management system and a framework for the developmentof advanced database application systems all aspects of the core model constructs query language and constraints can easily be generalised to support for example the management of temporal spatial and versioned data speci cally we showhow the framework was used to extend the core system to a temporal object oriented database management system 1 introduction extensibility has often been considered a purely architectural issue in database management systems dbms in the 1980s there was an increase in the various forms of dbms that appeared many of whichwere tailored to speci c application domains such as geographical information systems or optimizations of rough set model rough set methodology is based on concept set approximations constructed from available background knowledge represented in information systems 14 in many applications only partial knowledge about approximated concepts is given hence quite often first a parametrized family of concept approximations is built and next by tuning of the parameters the best in a sense approximation is chosen see e g variable precision rough set model 40 in approximation spaces in this paper we follow this approach in generalized approximation spaces we discuss rough set model based on approximation spaces with uncertainty functions and rough inclusions both elements of approximation space are parametrized and for the proper application of such model to a particular data set it is necessary to make optimization of the parameters we discuss basic properties of the mentioned model and also strategies of parameters optimization we also present different notions of rough relations exploring dynamic bayesian belief networks for intelligent fault management systems systems that are subject to uncertainty in their behaviour are often modelled by bayesian belief networks bbns these are probabilistic models of the system in which the independence relations between the variables of interest are represented explicitly a directed graph is used in which two nodes are connected by an edge if one is a direct cause of the other however the bayesian paradigm does not provide any direct means for modelling dynamic systems there has been a considerable amount of research effort in recent years to address this this paper reviews these approaches and proposes a new dynamic extension to the bbn this paper proceeds to discuss fault management of complex telecommunications and how the dynamic bayesian models can assist in the prediction of faults keywords dynamic bayesian belief networks telecommunication networks fault management intelligent systems 1 soft computing and fault management soft computing is a partnership between a i techniques that are tolerant of imprecision uncertainty and partial truth with the aim of obtaining a robust solution for complex systems telecommunication systems are built with extensive redundancy and complexity to ensure robustness and quality of service to facilitate this requires complex fault identification and management systems fault identification and management is generally handled by reducing the amount of alarm events symptoms presented to the operating engineer through monitoring filtering and masking the ultimate goal is to determine and present the actual underlying fault fault management is a complex task subject to uncertainty in the symptoms presented and as such is ideal for treatment by soft computing techniques the aim of this paper is to present a soft computing approach to fault management in telecommunication systems two key approaches are considered ai soft computing rule discovery and techniques to attempt to present less symptoms with greater diagnostic assistance for the more traditional rule based system approach and a hybrid soft computing approach which utilises a genetic algorithm to learn bayesian belief networks bbns it is also highlighted that research and development of the two target fault management systems are complementary keywords network management fault management knowledge discovery bayesian belief networks genetic algorithms soft computing 1 oiling the way to machine understandable bioinformatics resources the complex questions and analyses posed by biologists as well as the diverse data resources they develop require the fusion of evidence from different independently developed and heterogeneous resources the web as an enabler for interoperability has been an excellent mechanism for data publication and transportation successful exchange and integration of information however depends on a shared language for communication a terminology and a shared understanding of what the data means an ontology without this kind of understanding semantic heterogeneity remains a problem for both humans and machines one means of dealing with heterogeneity in bioinformatics resources is through terminology founded upon an ontology bioinformatics resources tend to be rich in human readable and understandable annotation with each resource using its own terminology these resources are machine readable but not machine understandable ontologies have a role in increasing this machine understanding reducing the semantic heterogeneity between resources and thus promoting the flexible and reliable interoperation of bioinformatics resources this paper describes a solution derived from the semantic web a machine understandable www the ontology inference layer oil as a solution for semantic bioinformatics resources the nature of the heterogeneity problems are presented along with a description of how metadata from domain ontologies can be used to alleviate this problem a companion paper in this issue gives an example of the development of a bioontology using oil keywords ontology oil semantics interoperation heterogeneity understanding 1 single display groupware a model for co present collaboration we introduce a model for supporting collaborative work between people that are physically close to each other we call this model single display groupware sdg in this paper we describe this model comparing it to more traditional remote collaboration we describe the requirements that sdg places on computer technology and our understanding of the benefits and costs of sdg systems finally we describe a prototype sdg system that we built and the results of a usability test we ran with 60 elementary school children keywords cscw single display groupware children educational applications input devices pad kidpad introduction in the early 1970 s researchers at xerox parc created an atmosphere in which they lived and worked with technology of the future when the world s first personal computer the alto was invented it had only a single keyboard and mouse this fundamental design legacy has carried through to nearly all modern computer systems although networks have design and implementation of bitmap indices for scientific data bitmap indices are efficient multi dimensional index data structures for handling complex adhoc queries in read mostly environments they have been implemented in several commercial database systems but are only well suited for discrete attribute values which are very common in typical business applications however many scientific applications usually operate on floating point numbers and cannot take advantage of the optimisation techniques offered by current database solutions we thus present a novel algorithm called genericrangeeval for processing one sided range queries over floating point values in addition we present a cost model for predicting the performance of bitmap indices for high dimensional search spaces we verify our analytical results by a detailed experimental study and show that the presented bitmap evaluation algorithm scales well also for high dimensional search spaces requiring only a fairly small index because of its simple arithmetic structure the cost model could easily be integrated into a query optimiser for deciding whether the current multi dimensional query shall be answered by means of a bitmap index or better by sequentially scanning the data values without using an index at all towards a cost model for distributed and replicated data stores large petabyte scale data stores need detailed design considerations about distributing and replicating particular parts of the data store in a cost effective way technical issues need to be analysed and based on these constraints an optimisation problem can be formulated in this paper we provide a novel cost model for building a world wide distributed petabyte data store which will be in place starting from 2005 at cern and its collaborating world wide distributed institutes we will elaborate on a framework for assessing potential system costs and influences which are essential for the design of the data store 1 introduction with the growth of the internet in the last couple of years and expanding technologies in database research data warehousing networking and data storage large distributed data stores with data amounts in the range of petabytes are emerging 16 not only the choice of the optimal data storage system relational or object oriented databases flat files conveying routes multimodal generation and spatial intelligence in embodied conversational agents in creating an embodied conversational agent eca capable of conveying routes it is necessary to understand how to present spatial information in an effective and natural manner when conveying routes to someone a person uses multiple modalities e g speech gestures and reference to a map to present information and it is important to know precisely how these modalities are coordinated with an understanding of how humans present spatial intelligence to give directions it is then possible to create an eca with similar capabilities two empirical studies were carried out to observe natural human to human direction giving interactions from the results a direction giving model was created and then implemented in the mack media lab autonomous conversational kiosk system spatial agents implemented in a logical expressible language in this paper we present a multi layered architecture for spatial and temporal agents the focus is laid on the declarativity of the approach which makes agent scripts expressive and well understandable they can be realized as constraint logic programs the logical description language is able to express actions or plans for one and more autonomous and cooperating agents for the robocup simulator league the system architecture hosts constraint technology for qualitative spatial reasoning but quantitative data is taken into account too the basic hardware layer processes the agent s sensor information an interface transfers this lowlevel data into a logical representation it provides facilities to access the preprocessed data and supplies several basic skills the second layer performs qualitative spatial reasoning on top of this the third layer enables more complex skills such as passing offside detection etc at last the fourth layer establishes acting as a team both by emergent and explicit cooperation logic and deduction provide a clean means to specify and also to implement teamwork behavior 1 from the specification of multiagent systems by statecharts to their formal analysis by model checking towards safety critical applications a formalism for the specification of multiagent systems should be expressive and illustrative enough to model not only the behavior of one single agent but also the collaboration among several agents and the influences caused by external events from the environment for this state machines 25 seem to provide an adequate means furthermore it should be easily possible to obtain an implementation for each agent automatically from this specification last but not least it is desirable to be able to check whether the multiagent system satisfies some interesting properties therefore the formalism should also allow for the verification or formal analysis of multiagent systems e g by model checking 6 in this paper a framework is introduced which allows us to express declarative aspects of multiagent systems by means of classical propositional logic and procedural aspects of these systems by means of state machines statecharts nowadays statecharts are a well accepted means to specify dynamic behavior of software systems they are a part of the unified modeling language uml we describe in a rigorously formal manner how the specification of spatial knowledge and robot interaction and its verification by model checking can be done integrating different methods from the field of artificial intelligence such as qualitative spatial reasoning and the situation calculus as example application domain we will consider robotic soccer see also 24 31 which present predecessor work towards a formal logic based approach for agents engineering a layered approach to learning client behaviors in the robocup soccer server in the past few years multiagent systems mas has emerged as an active subfield of artificial intelligence ai because of the inherent complexity of mas there is much interest in using machine learning ml techniques to help build multiagent systems robotic soccer is a particularly good domain for studying mas and multiagent learning our approach to using ml as a tool for building soccer server clients involves layering increasingly complex learned behaviors in this article we describe two levels of learned behaviors first the clients learn a low level individual skill that allows them to control the ball effectively then using this learned skill they learn a higher level skill that involves multiple players for both skills we describe the learning method in detail and report on our extensive empirical testing we also verify empirically that the learned skills are applicable to game situations 1 introduction in the past few years multiagent systems mas has emerge content integration for e business we define the problem of content integration for ebusiness and show how it differs in fundamental ways from traditional issues surrounding data integration application integration data warehousing and oltp content integration includes catalog integration as a special case but encompasses a broader set of applications and challenges we explore the characteristics of content integration and required services for any solution in addition we explore architectural alternatives and discuss the use of xml in this arena 1 reasoning within fuzzy description logics description logics dls are suitable well known logics for managing structured knowledge they allow reasoning about individuals and well defined concepts i e set of individuals with co hfip pro erties the experience in using dls inapplicatio has sho wn that in many cases we wo 6h like to extend their capabilities in particular their use in the co texto multimedia info mediafi retrieval mir leadsto the co vincement that such dlssho pf allo w the treatmento f the inherentimprecisio in multimediao ject co tent representatio and retrieval in this paper we will present a fuzzyextensio alc co bining zadeh s fuzzy lo zy with a classical dl in particular co rticu beco fk fuzzy and thus reaso ho6 ab o fi impreciseco recis is suppo ppfi6 we will define its syntax its semantics describe its pro erties and present a co phosfi9 tpro f fi9kfs calculus for reasoning in it roomware towards the next generation of human computer interaction based on an integrated design of real and virtual worlds in the past a central mainframe computer provided terminals for many users in the current age of the personal desktop computer there is one computer for one person observation of early adopters and predictions about the future point to an era where each person will have multiple devices and computational power will be ubiquitous against this background we present a vision for the workspaces of the future and a user centered approach for an integrated design of virtual information spaces and real architectural spaces the resulting environments are called cooperative buildings the design approach is based on the roomware concept by roomware we mean computer augmented objects resulting from the integration of room elements e g walls doors furniture tables chairs etc with computer based information devices they are part of the vision that the world around us will be the interface to information where the computer as a device will disappear and people s interaction w roomware for cooperative buildings integrated design of architectural spaces and information spaces in this paper we introduce the concepts of cooperative buildings and roomware and place them in the context of the integrated design of real physical resp architectural spaces and virtual resp digital information spaces by roomware we mean computer augmented things in rooms like doors walls furniture and others the general approach is detailed via examples from the i land project where we develop several roomware components in order to realize an interactive information and cooperation landscape e g an innovative work environment for creativity teams we describe the current realization of i land which includes an interactive electronic wall an interactive table computer augmented chairs and a mechanism for assigning physical objects as representatives of information objects in the virtual world keywords cooperative buildings shared workspaces physical space architecture virtual world information space augmented reality roomware furnitu i land an interactive landscape for creativity and innovation we describe the i land environment which constitutes an example of our vision of the workspaces of the future in this case supporting cooperative work of dynamic teams with changing needs i land requires and provides new forms of human computer interaction and new forms of computer supported cooperative work its design is based on an integration of information and architectural spaces implications of new work practices and an empirical requirements study informing our design i land consists of several roomware components i e computer augmented objects integrating room elements with information technology we present the current realization of i land in terms of an interactive electronic wall an interactive table two computer enhanced chairs and two bridges for the passage mechanism this is complemented by the description of the creativity support application and the technological infrastructure the paper is accompanied by a video figure in the chi 99 video program first order expressivity for s5 models modal vs two sorted languages this paper we are going to prove some results on the expressive power of the standard first order modal language by comparing it with its extensional counterpart we thereby restrict our attention to the case where the modal language is interpreted on s5 models moreover we decided to deal exclusively with constant domain models that is with models in which the domains of all worlds are the same it is worth mentioning however that our method can be applied to logics based on varying domain models as well before we describe the results of this paper in more detail we hasten to add that there exists some work done by other authors to which our results are related in 8 9 10 fine proves among other things a number of preservation results for modal first order formulas which are relevant for certain philosophical distinctions for instance he provides a semantical characterization of de dicto formulas within s5 and investigates conditions under which de re formulas are eliminable in certain extensions of s5 see also 6 18 in 3 it was shown 2 that a remarkable portion of classical model theory can be transferred to the domain of modal logic more closely related to our work are 14 and 15 they discuss a number of formulas from the two sorted language that are not expressible in the modal language a tableau calculus for temporal description logic the expanding domain case in this paper we present a tableau calculus for a temporal extension of the description logic alc called t lalc this logic is based on the temporal language with until interpreted over the natural numbers with expanding alc domains the tableau calculus forms an elaborated combination of wolper s tableau calculus for propositional linear temporal logic the standard tableau algorithm for alc and the method of quasimodels as it has been introduced by wolter and zakharyaschev based on those three ingredients the paper provides a new method of how tableau based decision procedures can be constructed for many dimensional logics which lack the finite model property the method can be applied to deal with other temporalized formalisms as well 1 introduction in many application domains of logic in computer science and artificial intelligence it is no longer enough to describe the static aspect of the world in particular there is a need to formalize its temporal evolution a prediction system for multimedia pre fetching in internet the rapid development of interact has resulted in more and more multimedia in web content however due to the limitation in the bandwidth and huge size of the multimedia data users always suffer from long time waiting on the other hand if we can predict the web object or page that the user most likely will view next while the user is viewing the current page and pre fetch the content then the perceived network latency can be significantly reduced in this paper we present an n gram based model to utilize path profiles of users from very large web log to predict the users future requests our model is based on a simple extension of existing point based models for such predictions but our results show that by sacrificing the applicability somewhat one can gain a great deal in prediction precision also we present an efficient method to compress the prediction model size so that it can be fitted into the main memory our result can potentially be applied to a wide range of applications on the web including pre fetehing enhancement of recommendation systems as well as web caching policies the experiments based on three realistic web logs have proved the effectiveness of the proposed scheme a replicable web based negotiation server for e commerce this paper describes our ongoing r d effort in developing a replicable web based negotiation server to conduct bargaining type negotiations between clients i e buyers and sellers in e commerce multiple copies of this server can be paired with existing web servers to provide negotiation capabilities each client can select a trusted negotiation server to represent his her interests web based gui tools are used by clients in a build time registration process to specify the requirements constraints negotiation strategic rules and preference scoring methods related to the buying or selling of a product the registration information is used by the negotiation servers to conduct negotiations automatically on behalf of the clients in this paper we present the architecture of the negotiation server and the framework for automated negotiations and describe a number of communication primitives which make up the negotiation protocol we have developed a constraint satisfaction processor csp to evaluate a negotiation proposal against the registered constraints an event trigger rule etr server manages events and triggers the execution of strategic rules which may relax constraints notify clients or perform other operations strategic rules can be added and modified at run time to deal with the dynamic nature of negotiations a cost benefit analysis performs quantitative analysis of alternative negotiation conditions we have implemented a prototype system to demonstrate automated negotiations among buyers and suppliers in a supply chain management system whatnext a prediction system for web requests using n gram sequence models as an increasing number of users access information on the web there is a great opportunity to learn from the server logs to learn about the users probable actions in the future in this paper we present an n gram based model to utilize path profiles of users from very large data sets to predict the users future requests since this is a prediction system we cannot measure the recall in a traditional sense we therefore present the notion of applicability to give a measure of the ability to predict the next document our model is based on a simple extension of existing point based models for such predictions but our results show for n gram based prediction when n is greater than three we can increase precision by 20 or more for two realistic web logs also we present an efficient method that can compress our model to 30 of its original size so that the model can be loaded in main memory our result can potentially be applied to a wide range of applications on the web inc cost based optimization of decision support queries using transient views next generation decision support applications besides being capable of processing huge amounts of data require the ability to integrate and reason over data from multiple heterogeneous data sources often these data sources differ in a variety of aspects such as their data models the query languages they support and their network protocols also typically they are spread over a wide geographical area the cost of processing decision support queries in such a setting is quite high however processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences minimizing these redundancies would significantly reduce the query processing cost in this paper we 1 propose an architecture for processing complex decision support queries involving multiple heterogeneous data sources automatic pattern acquisition for japanese information extraction one of the central issues for information extraction is the cost of customization from one scenario to another research on the automated acquisition of patterns is important for portability and scalability in this paper we introduce tree based pattern representation where a pattern is denoted as a path in the dependency tree of a sentence we outline the procedure to acquire tree based patterns in japanese from un annotated text the system extracts the relevant sentences from the training data based on tf idf scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns keywords information extraction pattern acquisition 1 embedding robots into the internet with the explosive growth of embedded computing hardware it is possible to conceive many new networked robotic applications for diverse domains ranging from urban search and rescue to house cleaning designing reliable software for such systems is a challenging problem however communication can facilitate robotics by reducing uncertainty and robotics can facilitate communication by providing physical mobility in this article we focus on methods for control and coordination of embedded mobile systems robots which interact with other computers on a wireless network situated in human environments 1 introduction ubiquitous embedded computing 12 is here to stay information appliances laptops palmtops and wearable computers are examples of the first wave of this new era two factors have contributed to the phenomenal increase in the number of computers in our environment moore s law and improved network connectivity it is now increasingly accepted that appliances of the futu tribeca a system for managing large databases of network traffic the engineers who analyze tra c on high bandwidth networks must lter and aggregate either recorded traces of network packets or live tra c from the network itself these engineers perform operations similar to database queries but cannot use conventional data managers because of performance concerns and a semantic mismatch between the analysis operations and the operations supported by commercial dbmss tra c analysis does not require fast random access transactional update or relational joins rather it needs fast sequential access to a stream of tra c records and the ability to lter aggregate de ne windows demultiplex and remultiplex the stream tribeca is an extensible stream oriented dbms designed to support network tra c analysis it combines ideas from temporal and sequence databases with an implementation optimized for databases stored on high speed id 1 tapes or arriving in real time from the network the paper describes tribeca s query language executor and optimizer as well as performance measurements of a prototype implementation 1 a model of collision perception for real time animation a model of human visual perception of collisions is presented based on twodimensional measures of eccentricity and separation the model is validated by performing psychophysical experiments we demonstrate the feasibility of using this model as the basis for perceptual scheduling of interruptible collision detection in a realtime animation of large numbers of visually homogeneous objects the user s point of fixation may be either tracked or estimated by using a priority queue scheduling algorithm perceived collision inaccuracy was approximately halved the ideas presented are applicable to other tasks where the processing of fine detail leads to a computational bottleneck individual action and collective function from sociology to multi agent learning how do we characterize the process and the dynamics of co learning conceptually mathematically or computationally how do social structures and relations interact with co learning of multiple agents 1 and so on a key question however is as follows which deserves some discussion here adam smith 1976 put it this way he generally indeed neither intends to promote the public interest nor knows how much he is promoting it he intends only his own gain and he is led by an invisible hand to promote an end which was not part of his intention this paradox have been troubling sociologists and economists for many decades and now computer scientists and psychologists as well the issue may be formulated as the apparent gap between the individual intention in deciding his her own action and the possibly largely unintended social function of his her action for example how may self interested action benet social welfare or world wide web information retrieval using web connectivity information gathering processing and distributing information from the world wide web will be a vital technology for the next century web search techniques have played a critical role in the development of information systems due to the diverse nature of web documents traditional search techniques must be improved hyperlink structure based methods have proved to be powerful ways of exploring the relationships between web documents in this project a prototype web search engine was developed to exploit the link structure of web documents based on the use of the companion algorithm the prototype consists of a web spider local database and search software the system was written using the java programming language our spider crawls and downloads web pages using lynx then saves the hyperlinks into an oracle database jdbc is used to implement the database processing search software makes a vicinity graph for the query url and returns the most related pages after calculating the hub and authority weights finally html web pages provide user interfaces and communicate with cgi using the perl language iii acknowledgments the author would like to express thanks to all of the members of his m s committee for their useful comments on the thesis assistance in scheduling the defense date and kind help during the final defense period the author would like to express his deepest appreciation to dr wen chen hu his thesis mentor for the depth of the training and the appropriate guidance he has provided the author would also like to acknowledge the department of computer science and software engineering of auburn university for financial support finally thanks especially go to the author s wife qifang his son alex and his father and mother for their support and love some experiments with a hybrid model for learning sequential decision making to deal with sequential decision tasks we present a learning model clarion which is a hybrid connectionist model consisting of both localist and distributed representations based on the two level approach proposed in sun 1995 the model learns and utilizes procedural and declarative knowledge tapping into the synergy of the two types of processes it unifies neural reinforcement and symbolic methods to perform on line bottom up learning experiments in various situations are reported that shed light on the working of the model 1 introduction this paper presents a hybrid model that unifies neural symbolic and reinforcement learning into an integrated architecture it addresses the following three issues 1 it deals with concurrent on line learning it allows a situated agent to learn continuously from on going experience in the world without the use of preconstructed data sets or preconceived concepts 2 the model learns not only low level procedural skills but also hi from implicit skills to explicit knowledge a bottom up model of skill learning this paper presents a skill learning model clarion different from existing models of mostly high level skill learning that use a top down approach that is turning declarative knowledge into procedural knowledge through practice we adopt a bottom up approach toward low level skill learning where procedural knowledge develops first and declarative knowledge develops later our model is formed by integrating connectionist reinforcement and symbolic learning methods to perform on line reactive learning it adopts a two level dual representation framework sun 1995 with a combination of localist and distributed representation we compare the model with human data in a minefield navigation task demonstrating some match between the model and human data in several respects a hybrid architecture for situated learning of reactive sequential decision making in developing autonomous agents one usually emphasizes only situated procedural knowledge ignoring more explicit declarative knowledge on the other hand in developing symbolic reasoning models one usually emphasizes only declarative knowledge ignoring procedural knowledge in contrast we have developed a learning model clarion which is a hybrid connectionist model consisting of both localist and distributed representations based on the two level approach proposed in sun 1995 clarion learns and utilizes both procedural and declarative knowledge tapping into the synergy of the two types of processes and enables an agent to learn in situated contexts and generalize resulting knowledge to different scenarios it unifies connectionist reinforcement and symbolic learning in a synergistic way to perform on line bottom up learning this summary paper presents one version of the architecture and some results of the experiments key words hybrid models sequential decision optional locking integrated with operational transformation in distributed real time group editors locking is a standard technique in traditional distributed computing and database systems to ensure data integrity by prohibiting concurrent conflicting updates on shared data objects operational transformation is an innovative technique invented by groupware research for consistency maintenance in real time group editors in this paper we will examine and explore the complementary roles of locking and operational transformation in consistency maintenance a novel optional locking scheme is proposed and integrated with operation transformation to maintain both generic and context specific consistency in a distributed interactive and collaborative environment the integrated optional locking and operational transformation technique is fully distributed highly responsive non blocking and capable of avoiding locking overhead in the most common case of collaborative editing keywords locking operational transformation consistency maintenance group editors groupware distribute symbol grounding a new look at an old idea symbols should be grounded as has been argued before but we insist that they should be grounded not only in subsymbolic activities but also in the interaction between the agent and the world the point is that concepts are not formed in isolation from the world in abstraction or objectively they are formed in relation to the experience of agents through their perceptual motor apparatuses in their world and linked to their goals and actions in this paper we will take a detailed look at this relatively old issue using a new perspective aided by our new work of computational cognitive model development to further our understanding we also go back in time to link up with earlier philosophical theories related to this issue the result is an account that extends from computational mechanisms to philosophical abstractions 1 symbol grounding a new look at an old idea symbols should be grounded as has been argued before but we insist that they should be ground ontoedit collaborative ontology development for the semantic web abstract ontologies now play an important role for enabling the semantic web they provide a source of precisely defined terms e g for knowledge intensive applications the terms are used for concise communication across people and applications typically the development of ontologies involves collaborative efforts of multiple persons ontoedit is an ontology editor that integrates numerous aspects of ontology engineering this paper focuses on collaborative development of ontologies with ontoedit which is guided by a comprehensive methodology 1 a five layer sensor architecture for autonomous robots in indoor environments autonomous mobile service robots for transportation tasks in indoor environments e g multistory buildings have to act in normal dynamic environments but with a huge number of components the robots total repertoire of skills is high according to the complexity of the building and its respective task difficult tasks can only be achieved on the base by immediate sensing of the environment this paper describes a five layer sensor architecture with an integrated world model for multistory buildings in contrast to grid based approaches we use a feature based approach the sensor architecture as well as the evaluation modules of the sensor data are based on natural landmarks the key features of the sensor architecture are reuseability modularity and portability to other multistory buildings as well as extendibility with different sensors keywords robot architectures sensor architectures collision avoidance and sensor based control robot sensing and data fusion behavior based robotics i learning feed forward and recurrent fuzzy systems a genetic approach in this paper we present a new learning method for rule based feed forward and recurrent fuzzy systems recurrent fuzzy systems have hidden fuzzy variables and can approximate the temporal relation embedded in dynamic processes of unknown order the learning method is universal i e it selects optimal width and position of gaussian like membership functions and it selects a minimal set of fuzzy rules as well as the structure of the rules a genetic algorithm is used to estimate the fuzzy systems which capture low complexity and minimal rule base optimization of the entropy of a fuzzy rule base leads to a minimal number of rules of membership functions and of sub premises together with an optimal input output behavior most of the resulting fuzzy systems are comparable to systems designed by an expert but offers a better performance the approach is compared to others by a standard benchmark a system identification process different results for feed forward and first order recurrent fuzzy systems with symmetric and non symmetric membership functions are presented key words fuzzy logic controller recurrent fuzzy systems genetic algorithm entropy of fuzzy rule machine learning dynamic processes 1 learning models of other agents using influence diagrams we adopt decision theory as a descriptive paradigm to model rational agents we use influence diagrams as a modeling representation of agents which is used to interact with them and to predict their behavior in this paper we provide a framework that an agent can use to learn the models of other agents in a multi agent system mas based on their observed behavior since the correct model is usually not known with certainty our agents maintain a number of possible models and assign a probability to each of them being correct when none of the available models is likely to be correct we modify one of them to better account for the observed behaviors the modification refines the parameters of the influence diagram used to model the other agent s capabilities preferences or beliefs the modified model is then allowed to compete with the other models and the probability assigned to it being correct can be arrived at based on how well it predicts the behaviors of the other agent alrea collaborative representations supporting face to face and online knowledge building discourse the present widespread interest in the use of electronic media for presents an unprecedented opportunity for leveraging the computational medium s strengths for learning however existing software tools provide only primitive support for online knowledge building discourse further work is needed in supporting coordinated use of disciplinary representations discourse representations and knowledge representations this paper introduces the concept of representational guidance for discourse along with results of an initial study of this phenomenon in face to face situations the paper then considers the requirements for supporting asynchronous online knowledge building discourse finding existing computer mediated communication tools to be particularly deficient in supporting artifact centered discourse a solution is proposed that coordinates discourse representations with disciplinary and knowledge representations 1 introduction there is a great deal of interest in the use of el online learning with random representations we consider the requirements of online learning learning which must be done incrementally and in realtime with the results of learning available soon after each new example is acquired despite the abundance of methods for learning from examples there are few that can be used effectively for online learning e g as components of reinforcement learning systems most of these few including radial basis functions cmacs kohonen s self organizing maps and those developed in this paper share the same structure all expand the original input representation into a higher dimensional representation in an unsupervised way and then map that representation to the final answer using a relatively simple supervised learner such as a perceptron or lms rule such structures learn very rapidly and reliably but have been thought either to scale poorly or to require extensive domain knowledge to the contrary some researchers rosenblatt 1962 gallant smith 1987 kanerva 1988 prager a world wide web meta search engine using an automatic query routing algorithm contents 1 introduction 6 2 literature review 9 2 1 overview of conventional search techniques 9 2 2 conventional query routing systems 11 2 2 1 manual query routing services 11 2 2 2 automated query routing systems based on centroids 12 2 2 3 automated query routing systems without centroids 12 3 system structure 14 3 1 system overview 14 3 2 off line operations system lag tests for augmented and virtual environments we describe a simple technique for accurately calibrating the temporal lag in augmented and virtual environments within the enhanced virtual hand lab evhl a collection of hardware and software to support research on goal directed human hand motion lag is the sum of various delays in the data pipeline associated with sensing processing and displaying information from the physical world to produce an augmented or virtual world our main calibration technique uses a modified phonograph turntable to provide easily tracked periodic motion reminiscent of the pendulum based calibration technique of liang shaw and green measurements show a three frame 50 ms lag for the evhl a second technique which uses a specialized analog sensor that is part of the evhl provides a closed loop calibration capable of sub frame accuracy knowing the lag to sub frame accuracy enables a predictive tracking scheme to compensate for the end toend lag in the data pipeline we describe both techniques and the evhl environment in which they are used using case based reasoning to acquire user scheduling preferences that change over time production manufacturing scheduling typically involves the acquisition of user optimization preferences the ill structuredness of both the problem space and the desired objectives make practical scheduling problems difficult to formalize and costly to solve especially when problem configurations and user optimization preferences change over time this paper advocates an incremental revision framework for improving schedule quality and incorporating user dynamically changing preferences through case based reasoning our implemented system called cabins records situation dependent tradeoffs and consequences that result from schedule revision to guide schedule improvement the preliminary experimental results show that cabins is able to effectively capture both user static and dynamic preferences which are not known to the system and only exist implicitly in a extensional manner in the case base 1 introduction scheduling deals with allocation of a limited set of resources to a nu in context information management through adaptive collaboration of intelligent agents although the number and availability of electronic information sources are increasing current information technology requires manual manipulation and userspecification of all details once accessed information must be filtered in the context of the user s task current systems lack the ability to get contextual information or use it to automate filtering at carnegie mellon university we have been engaged in the retsina project which aims to develop a reusable multiagent software infrastructure that allows heterogeneous agents on the internet possibly developed by different designers to collaborate with each other to manage information in the context of user specified tasks in this chapter we will provide a brief overview of the whole system and then focus on its capability for in context information management this research has been supported in part by darpa contract f30602 98 2 0138 and by onr grant n00014 96 1222 1 introduction the web is full of information resourc a virtual reality based system environment for intuitive walk throughs and exploration of large scale tourist information this paper describes the concept and prototype architecture for virtual reality vr based information systems virxis virxis may serve as a base architecture for different kind of is domains such as a vr based tourist information system virtis or a vr based geographic information system virgis finally potential application scenarios of virtis will be presented keywords tourist information systems virtual reality virtual worlds real time interactive 3d simulation information systems man machine interface object oriented database management systems spatial data access structures 1 introduction the ever increasing computing power and storage capacity of low cost computer systems enables the implementation of multimedia applications that integrate different media such as text image graphics voice music computer animation or video for the presentation and manipulation of tourist information at present the userinterface of such multimedia based tourist informati kernel expansions with unlabeled examples modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance we present a new tractable algorithm for exploiting unlabeled examples in discriminative classification this is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples the resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the em algorithm which in this case is both discriminative and achieves the optimal solution we provide in addition a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework we demonstrate that the proposed approach requires very few labeled examples for high classification accuracy 1 introduction in many modern classification problems such as text categorization very few labeled examples are available but a partially labeled classification with markov random walks to classify a large number of unlabeled examples we combine a limited number of labeled examples with a markov random walk representation over the unlabeled examples the random walk representation exploits any low dimensional structure in the data in a robust probabilistic manner we develop and compare several estimation criteria algorithms suited to this representation this includes in particular multi way classification with an average margin criterion which permits a closed form solution the time scale of the random walk regularizes the representation and can be set through a margin based criterion favoring unambiguous classification we also extend this basic regularization by adapting time scales for individual examples we demonstrate the approach on synthetic examples and on text classification problems hierarchical clustering for datamining this paper presents hierarchical probabilistic clustering methods for unsupervised and supervised learning in datamining applications the probabilistic clustering is based on the previously suggested generalizable gaussian mixture model a soft version of the generalizable gaussian mixture model is also discussed the proposed hierarchical scheme is agglomerative and based on a l 2 distance metric unsupervised and supervised schemes are successfully tested on artificially data and for segmention of e mails 1 feasible formations of multi agent systems formations of multi agent systems such as mobile robots satellites and aircraft require individual agents to satisfy their kinematic equations while constantly maintaining inter agent constraints in this paper we develop a systematic framework for studying formation feasibility of multi agent systems in particular we consider undirected formations for centralized formations and directed formations for decentralized formations in each case we determine differential geometric conditions that guarantee formation feasibility given the individual agent kinematics our framework also enables us to extract a smaller control system that describes the group kinematics while maintaining all formation constraints location oriented integration of internet information mobile info search information on the internet is becoming more attractive and useful for our daily life it provides things on the town happenings on the city and learning of the real world if we can utilize such information for the interaction between the human and the city it can enhance the value and the function of the city in this paper we introduce the research project mobile info search in which we study the method of integrating heterogeneous information in a location oriented way for providing it in a handy form with mobile computing we have a prototype of mobile info search at http www kokono net a location based search engine local information such as yellow pages maps and relevant web pages at any location of japan are provided with a simple interface from the analysis of test services we will discuss the user issues and information source issues what kind of local information is welcomed what can we learn from collected documents through the experience of handling various contents related to the real world we describe the potential of the internet information for the digital city efforts 1 mobile info search information integration for location aware computing this paper we introduce a research project entitled mobile info search mis and describe its goal architecture implementation and experimental results the goal of mis is to collect structure and integrate distributed and diverse local information from the internet in a practicable form and make it available through a simple interface to mobile users in various situations or contexts we do this in a location oriented way the experimental mis application features a location oriented meta search for web database servers a location oriented robot based search called kokono search for distributed web documents and a simple interface based on the latitude and longitude of the user s location which are obtained from the user s phs or gps analysis of a trial service shows that multiple services such as maps and textual information are often requested together for one location within a controlling the robots of web search engines robots are deployed by a web search engine for collecting information from different web servers in order to maintain the currency of its data base of web pages in this paper we investigate the number of robots to be used by a search engine so as to maximize the currency of the data base without putting an unnecessary load on the network we use a queueing model to represent the system the arrivals to the queueing system are web pages brought by the robots service corresponds to the indexing of these pages the objective is to find the number of robots and thus the arrival rate of the queueing system such that the indexing queue is neither starved nor saturated for this we consider a finite buffer queueing system and define the cost function to be minimized as a weighted sum of the loss rate and the fraction of time that the system is empty both static and dynamic policies are considered in the static setting the number of robots is held fixed in the dynamic setting robots may be re learning case classification for improving case based reasoning this paper we consider unconstrained graphs which may be either directed or undirected and possibly labelled or unlabelled computing structural similarity is essentially based on graph matching which actually means computing subgraph isomorphism this subproblem is known to be np complete it becomes more tractable when using labelled and directed graphs the particular approach developed and investigated throughout the present workshop presentation can be easily generalized to other structural similarity concepts for readability we confine ourselves to graph representations because of np completeness of the subgraph isomorphism problem classical case retrieval is extremely expensive if every member of a usually huge case base is potentially queried the performance of this search can be increased enormously by prestructuring the case base in case classes i e clustering preferably a given case base cb is clustered with respect to structural similarity i e graphs of a close structural relationship are grouped together and represented by the graph s describing their common structural similarity let us assume that any given case base cb is separated in n partitions or classes each class cb i i 1 n consists of a set of graph represented cases and is determined by a graph theta the infocockpit providing location and place to aid human memory our work focuses on building and evaluating computer system interfaces that make information memorable psychology research tells us people remember spatially distributed information based on its location relative to their body as well as the environment in which the information was learned we apply these principles in the implementation of a multimodal prototype system the infocockpit for information cockpit the infocockpit not only uses multiple monitors surrounding the user to engage human memory for location but also provides ambient visual and auditory displays to engage human memory for place we report a user study demonstrating a 56 increase in memory for information presented with our infocockpit system as compared to a standard desktop system trust relationships in a mobile agent system the notion of trust is presented as an important component in a security infrastructure for mobile agents a trust model that can be used in tackling the aspect of protecting mobile agents from hostile platforms is proposed we dene several trust relationships in our model and present a trust derivation algorithm that can be used to infer new relationships from existing ones an example of how such a model can be utilized in a practical system is provided 1 software infrastructure for ubiquitous computing environments supporting synchronous collaboration with heterogeneous devices in ubiquitous computing environments multiple users work with a wide range of different devices in many cases users interact and collaborate using multiple heterogeneous devices at the same time the configuration of the devices should be able to change frequently due to a highly dynamic flexible and mobile nature of new work practices this produces new requirements for the architecture of an appropriate software infrastructure in this paper an architecture designed to meet these requirements is proposed to test its applicability this architecture was used as the basis for the implementation of beach the software infrastructure of i land the ubiquitous computing environment at gmd ipsi it provides the functionality for synchronous cooperation and interaction with roomware components i e room elements with integrated information technology in conclusion our experiences with the current implementation are presented mining semi structured data by path expressions a new data model for ltering semi structured texts is presented jinni intelligent mobile agent programming at the intersection of java and prolog jinni java inference engine and networked interactor is a lightweight multi threaded logic programming language intended to be used as a flexible scripting tool for gluing together knowledge processing components and java objects in distributed applications jinni threads are coordinated through blackboards local to each process associative search based on term unification a variant of linda is used as the basic synchronization mechanism threads are controlled with tiny interpreters following a scripting language based on a subset of prolog mobile threads implemented by capturing first order continuations in a compact data structure sent over the network allow jinni to interoperate with remote high performance binprolog servers for cpu intensive knowledge processing and with other jinni components over the internet the synergy of these features makes jinni a convenient development platform for distributed ai and in particular for building intelligent autonomous agent cache management in corba distributed object systems for many distributed data intensive applications the default remote invocation of corba objects by clients is not acceptable because of performance degradation caching enables clients to invoke operations locally on distributed objects instead of fetching them from remote servers this paper addresses the design and implementation of a specific caching approach for corba based systems we propose a new removal algorithm which uses a double linked structure and a hash table for eviction we also present a new variation of optimistic two phase locking for consistency control which does not require a lock at the client side by using a perprocess caching design with the experiments we have performed we demonstrate that the proposed caching approach provides an important performance gain the caching with half buffer saves up to 45 of access time and the caching with full buffer saves up to 50 of access time the common object request broker architecture corba provides several adv temporal matching under uncertainty temporal matching is the problem of matching observations to predefined temporal patterns or templates this problem arises in many applications including medical and model based diagnosis plan recognition and temporal databases this work examines the sources of uncertainty in temporal matching and presents a probabilistic technique to perform temporal matching under uncertainty this technique is then applied to the problem of finding the onset of infection with toxoplasma gondii 1 introduction temporal matching is the process of matching a limited set of observations to known temporal evolution patterns in order to identify the stage of evolution or determine the onset of the temporally evolving pattern given a sequence of observations and some temporal evolution patterns a temporal match consistent with the sequence of observations maps observation times to particular points in a pattern formally s is a set of states tr is the set of time points measured rel virtual keyboards this paper describes a novel scheme for vision based human computer interaction in which traditional input and output devices monitors keyboards and mice are replaced with augmented reality displays projection systems and cameras user input is accomplished by projecting an image of the interface onto a flat surface in the scene which is monitored with a video camera the scheme hinges on the observation that the relationship between the three surfaces of interest the work surface the virtual keyboard and the image obtained by the camera can be characterized by projective transformations of rp 2 this observation leads to a fast and accurate online calibration algorithm the basic advantage of the vision based interaction techniques proposed in this paper is that they do not involve mechanical input devices such as keyboards mice and touch screens there are no moving parts and no wires to connect to the interface surface by avoiding a physical instantiation of t continuous state space q learning for control of nonlinear systems contents 1 introduction 1 1 1 control 2 1 1 1 designing the state feedback controller 3 1 1 2 unknown systems 5 1 2 reinforcement learning 7 1 3 problem statement 8 1 4 overview of this thesis 9 2 reinforcement learning 11 2 1 a discrete deterministic optimal control task 11 2 1 1 the problem 11 2 1 2 the solution 13 2 2 the stochastic optimization tasks 13 2 2 1 the markov decision process automatic detection of human faces in natural scene images by use of a skin color model and of invariant moments we use a skin color model based on the mahalanobis metric and a shape analysis based on invariant moments to automatically detect and locate human faces in two dimensional natural scene images first color segmentation of an input image is performed by thresholding in a perceptually plausible hue saturation color space where the effects of the variability of human skin color and the dependency of chrominance on changes in illumination are reduced we then group regions of the resulting binary image which have been classified as face candidates into clusters of connected pixels performing median filtering on the image and discarding the smallest remaining clusters ensures that only a small number of clusters will be used for further analysis fully translation scale and in plane rotation invariant moments are calculated for each remaining cluster finally in order to distinguish faces from distractors a multilayer perceptron neural network is used with the invariant moments as the input vector supervised learning of the network is implemented with the backpropagation algorithm at first for frontal views of faces preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds 1 evaluating emergent collaboration on the web links between web sites can be seen as evidence of a type of emergent collaboration among web site authors we report here on an empirical investigation into emergent collaboration we developed a webcrawling algorithm and tested its performance on topics volunteered by 30 subjects our findings include some topics exhibit emergent collaboration some do not the presence of commercial sites reduces collaboration when sites are linked with other sites they tend to group into one large tightly connected component connectivity can serve as the basis for collaborative filtering human experts rate connected sites as significantly more relevant and of higher quality keywords social filtering collaborative filtering computer supported cooperative work human computer interaction information access information retrieval introduction the field of cscw sees collaboration as involving people who know they are working together e g to edit a document to carry out a soft constructing organizing and visualizing collections of topically related web resources for many purposes the web page is too small a unit of interaction and analysis web sites are structured multimedia documents consisting of many pages and users often are interested in obtaining and evaluating entire collections of topically related sites once such a collection is obtained users face the challenge of exploring comprehending and organizing the items we report four innovations that address these user needs we replaced the web page with the web site as the basic unit of interaction and analysis we defined a new information structure the clan graph that groups together sets of related sites we augment the representation of a site with a site profile information about site structure and content that helps inform user evaluation of a site we invented a new graph visualization the auditorium visualization that reveals important structural and content properties of sites within a clan graph detailed analysis and user studies document the utility of this approach the clan graph construction algorithm tends to filter out irrelevant sites and discover additional relevant items the auditorium visualization augmented with drill down capabilities to explore site profile data helps users to find high quality sites as well as sites that serve a particular function g day mate let me introduce you to everyone an infrastructure for scalable human system interaction we are exposed to physical and virtual systems every day they consist of computers pdas wireless devices and increasingly robots each provides services to individual or groups of users whether they are local or remote to the system services offered by these systems may be useful beyond these users to others however connecting many of these systems to more users presents a challenging problem the primary goal of the research presented in this paper is to demonstrate a scalable approach for connecting multiple users to the services provided by multiple systems such an approach must be simple robust and general to contend with the heterogeneous capabilities of the services an infrastructure is presented that addresses these scalability requirements and establishes the foundation for contending with heterogeneous services additionally it allows services to be linked to form higher level abstractions the infrastructure is demonstrated in simulation on several similar multirobot systems with multiple users the results propose it as a solution for large scale human system interaction disseminating mobile agents for distributed information filtering an often claimed benefit of mobile agent technology is the reduction of communication cost especially the area of information filtering has been proposed for the application of mobile filter agents however an effective coordination of agents which takes into account the current network conditions is difficult to achieve this contribution analyses the situation that data distributed among various remote data servers has to be examined with mobile filter agents we present an approach for coordinating the agents employment which minimizes communication costs validation studies on the possible cost savings for various constellations show that savings up to 90 can be achieved in the face of actual internet conditions 1 introduction an often claimed benefit of mobile agent technology is the reduction of communication cost either for decreasing an application s latency or for reducing the load on a network this prediction has been made especially for scenarios of information adaptation and plasticity of user interfaces this paper introduces the notion of plasticity a new property of interactive systems that denotes a particular type of user interface adaptation it also presents a generic framework inspired from the model based approach for supporting the development of plastic user interfaces this framework is illustrated with simple case studies keyswords user interface adaptation plasticity 1 introduction a design space for adaptation in hci adaptation is modeled as two complementary system properties adaptability and adaptivity adaptability is the capacity of the system to allow users to customize their system from a predefined set of parameters adaptivity is the capacity of the system to perform adaptation automatically without deliberate action from the user s part whether adaptation is performed on human requests or automatically the design space for adaptation includes three additional orthogonal axes see figure 1 the target for adaptation this axis denotes the enti searching the world wide web in low connectivity communities the internet has the potential to deliver information to communities around the world that have no other information resources high telephone and isp fees in combination with lowbandwidth connections make it unaffordable for many people to browse the web online we are developing the tek system to enable users to search the web using only email tek stands for quot time equals knowledge quot since the user exchanges time waiting for email for knowledge the system contains three components 1 the client which provides a graphical interface for the end user 2 the server which performs the searches from mit and 3 a reliable email based communication protocol between the client and the server the tek search engine differs from others in that it is designed to return low bandwidth results which are achieved by special filtering analysis and compression on the server side we believe that tek will bring web resources to people who otherwise would not be able to afford them creating a customized access method for blobworld we present the design and analysis of a customized access method for the content based image retrieval system blobworld using the amdb access method analysis tool we analyze three existing multidimensional access methods that support nearest neighbor search in the context of the blobworld application based on this analysis we propose several variants of the r tree tailored to address the problems the analysis revealed we implemented the access methods we propose in the generalized search trees gist framework and analyzed them using amdb a tool that enables visualization and performance analysis of access methods we found that two of our access methods have better performance characteristics for the blobworld application than any of the traditional multi dimensional access methods we examined based on this experience we draw conclusions for nearest neighbor access method design and for the task of constructing custom access methods tailored to particular applications in particular we found that our top x jagged bites quot bounding predicate performed better than all the other access methods we tested 1 a wearable computer system with augmented reality to support terrestrial navigation to date augmented realities are typically operated in only a small defined area in the order of a large room this paper reports on our investigation into expanding augmented realities to outdoor environments the project entails providing visual navigation aids to users awearable computer system with a see through display digital compass and a differential gps are used to provide visual cues while performing a standard orienteering task this paper reports the outcomes of a set of trials using an off the shelf wearable computer equipped with a custom built navigation software package map in the hat desires and defaults a framework for planning with inferred goals this paper develops a formalism designed to integrate reasoning about desires with planning the resulting logic bdp is capable of modeling a wide range of common sense practical arguments and can serve as a more general and flexible model for agent architectures active learning for natural language parsing and information extraction in natural language acquisition it is difficult to gather the annotated data needed for supervised learning however unannotated data is fairly plentiful active learning methods attempt to select for annotation and training only the most informative examples and therefore are potentially very useful in natural language applications however existing results for active learning have only considered standard classification tasks to reduce annotation effort while maintaining accuracy we apply active learning to two non classification tasks in natural language processing semantic parsing and information extraction we show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance for these complex tasks keywords active learning natural language learning learning for parsing learning for information extraction email address of contact author cthomp csli stanford edu phone number of contact author 650 8 a framework for programming embedded systems initial design and results this paper describes ces a proto type of a new programming language for robots and other embedded systems equipped with sensors and actuators ces contains two new ideas currently not found in other programming languages support of computing with uncertain information and support of adaptation and teaching as a means of programming these innovations facilitate the rapid development of software for embedded systems as demonstrated by two mobile robot applications this research is sponsoredin part by darpa via afmsc contract number f04701 97 c 0022 tacom contract number daae07 98 c l032 and rome labs contract number f30602 98 2 0137 the views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing official policies or endorsements either expressed or implied of darpa afmsc tacom rome labs or the united states government keywords artificial intelligence embedded system machine learning mobil map learning and high speed navigation in rhino this chapter surveys basic methods for learning maps and high speed autonomous navigation for indoor mobile robots the methods have been developed in our lab over the past few years and most of them have been tested thoroughly in various indoor environments the chapter is targeted towards researchers and engineers who attempt to build reliable mobile robot navigation software key instructions solving the code location problem for optimized code there are many difficulties to be overcome in the process of designing and implementing a debugger for optimized code one of the first problems facing the designer of such a debugger is determining how to accurately map between locations in the source program and locations in the corresponding optimized binary the solution to this problem is critical for many aspects of debugger design from setting breakpoints to implementing single stepping to reporting error locations previous approaches to debugging optimized code have presented many different techniques for solving this location mapping problem commonly known as the code location problem these techniques are often very complex and sometimes incomplete identifying key instructions allows for a simple yet formal way of mapping between locations in the source program and the optimized target program in this paper we present the concept of key instructions we give a formal definition of key instructions and present algorit a practical robust method for generating variable range tables in optimized programs the location in which the current value of a single source variable may reside typically varies as the computation progresses a debugger for optimized code needs to know all of the locations both registers and memory addresses in which a variable resides and which locations are valid for which portions of the computation determining this information is known as the data location problem 3 7 because optimizations frequently move variables around between registers and memory or from one register to another the compiler must build a table to keep track of this information such a table is known as a variable range table 3 once a variable range table has been constructed finding a variable s current location reduces to the simple task of looking up the appropriate entry in the table the difficulty lies in collecting the data for building the table previous methods for collecting this data depend on which optimizations the compiler performs and ho boosting image retrieval we present an approach for image retrieval using a very large number of highly selective features and efficient online learning our approach is predicated on the assumption that each image is generated by a sparse set of visual causes and that images which are visually similar share causes we propose a mechanism for computing a very large number of highly selective features which capture some aspects of this causal structure in our implementation there are over 45 000 highly selective features at query time a user selects a few example images and a technique known as boosting is used to learn a classification function in this feature space by construction the boosting procedure learns a simple classifier which only relies on 20 of the features as a result a very large database of images can be scanned rapidly perhaps a million images per second finally we will describe a set of experiments performed using our retrieval system on a database of 3000 images 1 introductio multiagent architecture for d sifter a modern approach to flexible information filtering in dynamic environments enterprise agents solving conflicts the cobac approach autonomous agents seem to be a promising approach for application in computermediated supply webs supporting the management and integration of the planning scheduling and controlling processes they can be used as enterprise delegates however this leads to a problem with common autonomous agent architectures as they are not designed to model complex decision behavior of entire companies in this paper we present an innovative approach to model intelligent agents in the context of the new agent architecture enterprise agents which is using conflicts of interests explicitly to determine agent s behavior and discuss its application to ebusiness keywords agent control conflict management adaptation bdi architecture enterprise agents 1 from corporate memories to supply web memory modern production has discovered knowledge as an additional factor of production and a new trend of research development and implementation of corporate memory systems is arising the global economy leads to tighter corporation relations between enterprises therefore the knowledge of one product does not exist in a single company but within participating companies respective the supply chain a modern product centered knowledge management has to face the difficult task of the integration of distributed knowledge sources this contribution states our interest in research on the integration of corporate memory in a first step we are focusing on single products leading to supply chain memories further research and development will lead to supply web memory 1 synthesis and adaptation of multiagent communication protocols in the production engineering domain the application of multiagent systems is often based on the claim that there will be an emergent behavior within these systems to reach the emergent behavior many researcher propagate to plan it within design and analysis of specific systems as it will not occur by chance we are presenting a new way to adaptive agent communication protocols with respect to a possible gain of emergent behavior communication protocols can be generated refined or adapted by the agents autonomously the approach uses basic concepts of machine learning flexible process planning and production control using co operative agent systems nowadays one of the greatest challenges companies have to face is the change towards flexible and demand driven production more information has to be handled and a considerable speed up of development and manufacturing processes is needed however the actual situation is characterized by strong borderlines between process planning production control and scheduling systems caused by extreme specialisation and independent historical paths of system evolution this gap implies loss of time and of information thus there is a strong need for innovative concepts for management and control of integrated information logistics production scheduling and process planning agent based information technologies like the innovative concept of co operative agent systems are promising approaches for more flexible and distributed production networks agents are autonomously co operatively and goal oriented acting intelligent software units the use of agents for managing information within production control and process planning makes short term and flexible reaction to unexpected events and disturbances in manufacturing e g break down of machines or lack of other resources as well as unexpected change of market situation possible thus enterprises will react to changing requirements in a more flexible way and will be able to face the challenges of international competition successfully keywords agile manufacturing intelligent manufacturing capp cam ppc operations management distributed artificial intelligence multiagent systems optimizing the parsom neural network implementation for data mining with distributed memory systems and cluster computing the self organizing map is a prominent unsupervised neural network model which lends itself to the analysis of high dimensional input data and data mining applications however the high execution times required to train the map put a limit to its application in many high performance data analysis application domains in this paper we discuss the parsom implementation a software based parallel implementation of the selforganizing map and its optimization for the analysis of high dimensional input data using distributed memory systems and clusters the original parsom algorithm scales very well in a parallel execution environment with low communication latencies and exploits parallelism to cope with memory latencies however it suffers from poor scalability on distributed memory computers we present optimizations to further decouple the subprocesses simplify the communication model and improve the portability of the system 1 introduction the self organizing map som 5 is a pr restricted bayes optimal classifiers we introduce the notion of restricted bayes optimal classifiers these classifiers attempt to combine the flexibility of the generative approach to classification with the high accuracy associated with discriminative learning they first create a model of the joint distribution over class labels and features instead of choosing the decision boundary induced directly from the model they restrict the allowable types of decision boundaries and learn the one that minimizes the probability of misclassification relative to the estimated joint distribution in this paper we investigate two particular instantiations of this approach the first uses a non parametric density estimator parzen windows with gaussian kernels and hyperplane decision boundaries we show that the resulting classifier is asymptotically equivalent to a maximal margin hyperplane classifier a highly successful discriminative classifier we therefore provide an alternative justification for maximal margin hyperplane classifiers the second instantiation uses a mixture of gaussians as the estimated density in experiments on real world data we show that this approach allows data with missing values to be handled in a principled manner leading to improved performance over regular discriminative approaches advanced deinterlacing techniques with the use of zonal based algorithms this paper describes a new highly efficient aleinterlacing approach based on motion estimation and compensation techniques the proposed technique mainly benefits from the motion vector properties of zonal based algorithms such as the advanced predictive diamond zonal search apdzs and the predictive motion vector field adaptive search technique pmvfast multihypothesis motion compensation but also an additional motion classification phase where depending on the motion of a pixel additional spatial and temporal information is also considered to further improve performance extensive simulations demonstrate the efficacy of these algorithms especially when compared to standard deinterlacing techniques such as the line doubling and line averaging algorithms temporal interpolation of video sequences using zonal based algorithms temporal interpolation has been recently proposed as a solution for increasing temporal resolution or even for predicting missing or corrupted frames within a video sequence in this paper new techniques on temporal interpolation are presented by mainly exploiting properties of the very popular and highly efficient zonal based motion estimation algorithms and by introducing several other techniques such as multihypothesis motion compensation motion classification and temporal spatial filtering in addition we further give an analysis on when temporal interpolation should be employed thus possibly avoiding unwanted artifacts created from this process while at the same time significantly improving the overall performance of the interpolation side surfer a spontaneous information discovery and exchange system development of wireless communications enables the rise of networking applications in embedded systems web interactions which are the most spread are nowadays available on wireless pdas moreover we can observe a development of ubiquitous computing based on this concept many works aim to consider user s context as part of the parameters of the applications the context notion can include the user s location his social activity taking part from emerging technologies enabling short range and direct wireless communications which allow to define a proximity context the aim of our study is to design a new kind of application extending the web paradigm spontaneous and proximate web interactions active views for electronic commerce electronic commerce is emerging as a major web supported application in this paper we argue that database technology can and should provide the backbone for a wide range of such applications more precisely we present here the activeviews system which relaying on an extensive use of database features including views active rules triggers and enhanced mechanisms for notification access control and logging tracing of users activities provides the needed basis for electronic commerce based on the emerging xml standards dom query languages for xml etc the system offers a novel declarative view specification language describing the relevant data and activities of all actors e g vendors and clients participating in electronic commerce activities then acting as an application generator the system generates an actual possibly customized web application that allows users to perform the given set of controlled activities and to work interactively on the specified dat videograph a graphical object based model for representing and querying video data modeling video data poses a great challenge since they do not have as clear an underlying structure as traditional databases do we propose a graphical object based model called videograph in this paper this scheme has the following advantages 1 in addition to semantics of video individual events we capture their temporal relationships as well 2 the inter event relationships allow us to deduce implicit video information 3 uncertainty can also be handled by associating the video event with a temporal boolean like expression this also allows us to exploit incomplete information the above features make videograph very flexible in representing various metadata types extracted from diverse information sources to facilitate video retrieval we also introduce a formalism for the query language based on path expressions query processing involves only simple traversal of the video graphs 1 introduction we deal with the modeling aspect of video database manageme a reactive deliberative model of dialogue agency for an agent to engage in substantive dialogues with other agents there are several complexities which go beyond the scope of standard models of rational agency in particular an agent must reason about social attitudes that span more than one agent as well as the dynamic and fallible process of plan execution in this paper we sketch a theory of plan execution which allows the representation of failure and repair extend the underlying agency model with social attitudes of mutual belief obligation and multi agent plan execution and describe an implemented dialogue agent which uses these notions reacting to its environment and mental state and deliberating and planning action only when more pressing concerns are absent 1 overview for autonomous agents that operate in a realm of heterogeneous agents including human agents an agent theory should allow many of the features of natural language dialogue the agent communication protocols should allow flexible turn taking and speech acts for dialogue agents this paper by the u s army research office under contract grant number daah 04 95 10628 and the u s national science foundation under grant iri9311988 some of the work described above was developed in collaboration with james allen and supported by onr darpa under grant number n00014 92j 1512 by onr under research grant number n00014 90 j 1811 and by nsf under grant number iri 9003841 a description logic for vague knowledge this work introduces the concept language alcfm which is an extension of alc to many valued logics alcfm allows to express vague concepts e g more or less enlarged or very small to realize this extension to many valued logics the classical notions of satisfiability and subsumption had to be modified appropriately for example alcfm concepts are no longer either satisfiable or unsatisfiable but they are satisfiable to a certain degree the main contribution of this paper is a sound and complete method for computing the degree of subsumption between two alcfm concepts 1 introduction this work takes its motivation from the occurrence of vague concept descriptions in different application areas often application inherent information is characterized by a very high degree of vagueness appropriate information systems must be able to process this kind of data so far there are no systems that really solve the corresponding problems due to the lack of powerful basic methods a a gesture interface for human robot interaction we present a person independent gesture interface implemented on a real robot which allows the user to give simple commands e g how to grasp an object and where to put it the gesture analysis relies on realtime tracking of the user s hand and a re thetaned analysis of the hand s shape in the presence of varying complex backgrounds 1 introduction robots of the future will interact with humans in a natural way they will understand spoken and gestural commands and will articulate themselves by speech and gesture we are especially interested in gestural interfaces for robots operating in uncontrolled real world environments this imposes several constraints on human robot interaction as a special case of human computer interaction 1 the robot visual system must cope with variable and possibly complex backgrounds a system requiring uniform background is not exible enough for real world applications 2 the system must be person independent many users should be able to operate feedback from video for virtual reality navigation important preconditions for wide acceptance of virtual reality systems include their comfort ease and naturalness to use most existing trackers suer from discomfortrelated issues for example body based trackers such as hand controllers joysticks or helmet attachments restrict spontaneity and naturalness of motion whereas groundbased devices e g hand controllers limit the workspace by literally binding an operator to the ground controls have similar problems this paper describes using real time video with registered depth information from a commercially available camera for virtual reality navigation a camera based setup can replace cumbersome trackers the method includes selective depth processing for increased speed and a robust skin color segmentation for handling illumination variations fuzzy meta learning preliminary results learning from distributed data is becoming in our times a necessity but it is also a complex and challenging task approaches developed so far have not dealt with the uncertainty imprecision and vagueness involved in distributed learning meta learning a successful approach for distributed data mining is in this paper extended to handle the imprecision and uncertainty of the local models and the vagueness that characterizes the meta learning process the proposed approach fuzzy meta learning uses a fuzzy inductive algorithm to meta learn a global model from the degrees of certainty of the output of local classifiers this way more accurate models of collective knowledge can be acquired from data with application both to inherently distributed databases and parts of a very large database preliminary results are promising and encourage further research towards this direction 1 subspace information criterion for non quadratic regularizers model selection for sparse regressors non quadratic regularizers in particular the 1 norm regularizer can yield sparse solutions that generalize well in this work we propose the generalized subspace information criterion gsic that allows to predict the generalization error for this useful family of regularizers we show that under some technical assumptions gsic is an asymptotically unbiased estimator of the generalization error gsic is demonstrated to have a good performance in experiments with the 1 norm regularizer as we compare with the network information criterion and cross validation in relatively large sample cases however in the small sample case gsic tends to fail to capture the optimal model due to its large variance therefore also a biased version of gsic is introduced which achieves reliable model selection in the relevant and challenging scenario of high dimensional data and few samples subspace information criterion for non quadratic regularizers 2 1 perceptual user interfaces for some time graphical user interfaces guis have been the dominant platform for human computer interaction the gui based style of interaction has made computers simpler and easier to use especially for office productivity applications where computers are used as tools to accomplish specific tasks however as the way we use computers changes and computing becomes more pervasive and ubiquitous guis will not easily support the range of interactions necessary to meet users needs in order to accommodate a wider range of scenarios tasks users and preferences we need to move toward interfaces that are natural intuitive adaptive and unobtrusive the aim of a new focus in hci called perceptual user interfaces puis is to make human computer interaction more like how people interact with each other and with the world this paper describes the emerging pui field and then reports on three puimotivated projects computer vision based techniques to visually perceive relevant information about the user 1 moving from guis to puis for some time graphical user interfaces guis have been the dominant platform for human computer interaction the gui based style of interaction has made computers simpler and easier to use especially for office productivity applications however as the way we use computers changes and computing becomes more pervasive and ubiquitous guis will not easily support the range of interactions necessary to meet users needs in order to accommodate a wider range of scenarios tasks users and preferences we need to move toward interfaces that are natural intuitive adaptive and unobtrusive the aim of a new focus in hci called perceptual user interfaces puis is to make human computer interaction more like how people interact with each other and with the world this paper describes the emerging pui field and then reports on three pui motivated components computer vision based techniques to visually perceive relevant information about the user 1 introduction recent research i improving the scalability of multi agent systems there is an increasing demand for designers and developers to construct ever larger multi agent systems such systems will be composed of hundreds or even thousands of autonomous agents moreover in open and dynamic environments the number of agents in the system at any one time will uctuate signicantly to cope with these twin issues of scalability and variable numbers we hypothesize that multiagent systems need to be both self building able to determine the most appropriate organizational structure for the system by themselves at runtime and adaptive able to change this structure as their environment changes to evaluate this hypothesis we have implemented such a multiagent system and have applied it to the domain of automated trading preliminary results supporting the rst part of this hypothesis are presented adaption and self organization do indeed make the system better able to cope with large numbers of agents 1 introduction when designing or buildin learning algorithms for keyphrase extraction many academic journals ask their authors to provide a list of about five to fifteen keywords to appear on the first page of each article since these key words are often phrases of two or more words we prefer to call them keyphrases there is a wide variety of tasks for which keyphrases are useful as we discuss in this paper we approach the problem of automatically extracting keyphrases from text as a supervised learning task we treat a document as a set of phrases which the learning algorithm must learn to classify as positive or negative examples of keyphrases our first set of experiments applies the c4 5 decision tree induction algorithm to this learning task we evaluate the performance of nine different configurations of c4 5 the second set of experiments applies the genex algorithm to the task we developed the genex algorithm specifically for automatically extracting keyphrases from text the experimental results support the claim that a custom designed algorithm genex learning to extract keyphrases from text many academic journals ask their authors to provide a list of about five to fifteen key words to appear on the first page of each article since these key words are often phrases of two or more words we prefer to call them keyphrases there is a surprisingly wide variety of tasks for which keyphrases are useful as we discuss in this paper recent commercial software such as microsoft s word 97 and verity s search 97 includes algorithms that automatically extract keyphrases from documents in this paper we approach the problem of automatically extracting keyphrases from text as a supervised learning task we treat a document as a set of phrases which the learning algorithm must learn to classify as positive or negative examples of keyphrases our first set of experiments applies the c4 5 decision tree induction algorithm to this learning task the second set of experiments applies the genex algorithm to the task we developed the genex algorithm specifically for this task t a survey of agent oriented software engineering agent oriented software engineering is the one of the most recent contributions to the field of software engineering it has several benefits compared to existing development approaches in particular the ability to let agents represent high level abstractions of active entities in a software system this paper gives an overview of recent research and industrial applications of both general high level methodologies and on more specific design methodologies for industry strength software engineering democratic data fusion for information retrieval mediators our research presented in this paper concerns the problem of fusing the results returned by the underlying systems to a mediating retrieval system also called meta retrieval system meta search engine or mediator we propose a fusion technique which is based solely on the actual results returned by each system for each query the final fused ordering of documents is derived by aggregating the orderings of each system in a democratic manner in addition the fused ordering is accompanied by a level of democracy alternatively construed as the level of confidence multiversion linear quadtree for spatio temporal data research in spatio temporal databases has largely focused on extensions of access methods for the proper handling of time changing spatial information in this paper we present the multiversion linear quadtree mvlq a spatio temporal access method based on multiversion b trees mvbt 2 embedding ideas from linear region quadtrees 4 more specifically instead of storing independent numerical data having a different transaction time each for every consecutive image we store a group of codewords that share the same transaction time whereas each codeword represents a spatial subregion thus the new structure may be used as an index mechanism for storing and accessing evolving raster images we also conducted a thorough experimentation using sequences of real and synthetic raster images in particular we examined the time performance of temporal window queries and provide results for a variety of parameter settings overlapping linear quadtrees and spatio temporal query processing indexing in spatio temporal databases by using the technique of overlapping is investigated overlapping has been previously applied in various access methods to combine consecutive structure instances into a single structure without storing identical sub structures in this way space is saved without sacrificing time performance a new access method overlapping linear quadtrees is introduced this structure is able to store consecutive historical raster images a database of evolving images moreover it can be used to support query processing in such a database five such spatio temporal queries along with the respective algorithms that take advantage of the properties of the new structure are introduced the new access method was implemented and extensive experimental studies for space efficiency and query processing performance were conducted a number of results of these experiments are presented as far as space is concerned these results indicate that in the case of similar consecutive images considerable storage is saved in comparison to independent linear quadtrees in the case of query processing the results indicate that the proposed algorithmic approaches outperform the respective straightforward algorithms in most cases the region data sets used in experiments were real images of meteorological satellite views and synthetic random images with specified aggregation time split linear quadtree for indexing image databases the time split b tree tsbt is modified for indexing a database of evolving binary images this is accomplished by embedding ideas from linear region quadtrees that make the tsbt able to support spatio temporal query processing to improve query performance additional pointers are added to the leaf nodes of the tsbt the resulting access method is called time split linear quadtree tslq algorithms for processing five spatio temporal queries have been adapted to the new structure such queries appear in multimedia systems or geographical information systems gis when searched by content the tslq was implemented and results of extensive experiments on query time performance are presented indicating that the proposed algorithmic approaches outbalance respective straightforward algorithms the region data sets used in the experiments were real images of meteorological satellite views and synthetic raster images overlapping b trees an implementation of a transaction time access method a new variation of overlapping b trees is presented which provides efficient indexing of transaction time and keys in a two dimensional key time space modification operations i e insertions deletions and updates are allowed at the current version whereas queries are allowed to any temporal version i e either in the current or in past versions using this structure snapshot and range timeslice queries can be answered optimally however the fundamental objective of the proposed method is to deliver efficient performance in case of a general pure key query i e history of a key the trade off is a small increase in time cost for version operations and storage requirements processing of spatiotemporal queries in image databases overlapping linear quadtrees is a structure suitable for storing consecutive raster images according to transaction time a database of evolving images this structure saves considerable space without sacrificing time performance in accessing every single image moreover it can be used for answering efficiently window queries for a number of consecutive images spatio temporal queries in this paper we present three such temporal window queries strict containment border intersect and cover besides based on a method of producing synthetic pairs of evolving images random images with specified aggregation we present empirical results on the i o performance of these queries modeling and simulation of mobile agents agent oriented software implies the realization of software components which are mobile autonomous and solve problems by creating new software components during run time moving between locations initiating or joining groups of other software components modeling and simulating those multiagent systems requires specific mechanisms for variable structure modeling james a java based agent modeling environment for simulation realizes variable structure models including mobility from the perspective of single autonomous agents james itself is based on parallel devs and adopts its abstract simulator model simulation takes place as a sending of messages between concurrently active and locally distributed entities which reflect the model s current structure thus modeling and simulation are coined equally by an agent based perspective 1 introduction the definition of agents subsumes a multitude of different facets 30 agents are reactive deliberative or combine reactive with plug and test software agents in virtual environments james a java based agent modeling environment for simulation has been developed to support the compositional construction of test beds for multi agent systems and their execution in distributed environments the modeling formalism of james imposes only few constraints on the modeling of agents and facilitates a plug and test with pieces of agent code which has been demonstrated in earlier work however even entire agents can be run in james as they are run in their run time environment the integration of agents as a whole is based on model templates which serve as the agents interface and representative during the simulation run the eort which is put into dening model templates for selected agent systems obviates the need for the single agent programmer to get acquainted with the underlying modeling and simulation formalism instead the agent programmer can compose the experimental frame and test the programmed agents as they are the approach is illustrated with agents of the mobile agent system mole 1 appearance based obstacle detection with monocular color vision this paper presents a new vision based obstacle detection method for mobile robots each individual image pixel is classified as belonging either to an obstacle or the ground based on its color appearance the method uses a single passive color camera performs in real time and provides a binary obstacle image at high resolution the system is easily trained by simply driving the robot through its environment in the adaptive mode the system keeps learning the appearance of the ground during operation the system has been tested successfully in a variety of environments indoors as well as outdoors 1 introduction obstacle detection is an important task for many mobile robot applications most mobile robots rely on range data for obstacle detection popular sensors for range based obstacle detection systems include ultrasonic sensors laser rangefinders radar stereo vision optical flow and depth from focus because these sensors measure the distances from obstacles t adaboost for query by example in text this paper describes an implementation of query by example or relevance feedback for text the implementation uses google s search engine to perform a keyword query as requested by the user if the user requires more information the user may score documents in the result set as relevant or irrelevant an implementation of the adaboost algorithm is then used choose words that separate the relevant documents from a random document set examples of negative document sets are also tested an example query and refinements of the query is presented the results seem promising the system seems to propose new keywords that are sensible to the requested context many of the keywords prove useful in constructing new queries however refinement using exactly the new terms predicted by the system does not seem to return noticeably better or worse results this may be the result of an inexact fit between the design of adaboost and the capabilities of google as a back end engine dynamic pipeline scheduling for improving interactive query performance interactive query performance is becoming an important criterion for online systems where delivering query results in a timely fashion is critical pipelined execution is a promising query execution style that can produce the initial portion of the result early and in a continuous fashion in this paper we propose techniques for delivering results faster in a pipelined query plan we distinguish between two cases for cases where the tuples in the query result are of the same importance we propose a dynamic rate based pipeline scheduling policy that produces more results during the early stages of query execution cost based query scrambling for initial delays remote data access from disparate sources across a wide area network such as the internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites traditional static query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays query scrambling has been proposed to address this problem scrambling modifies query execution plans on the fly when delays are encountered during runtime in its original formulation scrambling was based on simple heuristics which although providing good performance in many cases were also shown to be susceptible to problems resulting from bad scrambling decisions in this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices we propose three different approaches to using query optimization for scramblin cryptographic traces for mobile agents mobile code systems are technologies that allow applications to move their code and possibly the corresponding state among the nodes of a wide area network code mobility is a flexible and powerful mechanism that can be exploited to build distributed applications in an internet scale at the same time the ability to move code to and from remote hosts introduces serious security issues these issues include authentication of the parties involved and protection of the hosts from malicious code however the most difficult task is to protect mobile code against attacks coming from hosts this paper presents a mechanism based on execution tracing and cryptography that allows one to detect attacks against code state and execution flow of mobile software components 1 introduction mobile code technologies are languages and systems that exploit some form of code mobility in an internet scale setting in this framework the network is populated by several loosely coupled co privacy preserving association rule mining in vertically partitioned data privacy considerations often constrain data mining projects this paper addresses the problem of association rule mining where transactions are distributed across sources each site holds some attributes of each transaction and the sites wish to collaborate to identify globally valid association rules however the sites must not reveal individual transaction data we present a two party algorithm for efficiently discovering frequent itemsets with minimum support levels without either site revealing individual transaction values algebraic topology for knowledge representation in analogy solving we propose a computational model for analogy solving based on a topological formalism of representation the source and the target analogs are represented as simplexes and the analogy solving is modeled as a topological deformation of these simplexes along a polygonal chain and according to some constraints we apply this framework to the resolution of iq tests typically presented as given a b and c find d such that a is to b what c is to d 1 introduction in this paper we present a topological framework for knowledge representation based on the concept of simplicial complex we present then the esqimo system which is the application of this framework to an analogy solving problem the underlying idea developed here is that spatial relationships and more precisely topological relationships such as neighbor border dimension obstruction deformation separabitily path etc enable the building and structuration of knowledge representation more precisely we explore the possi hitch hiker s guide to esqimo computational model for analogy solving in iq tests esqimo is a computational model for analogy solving based on a topological formalism of representation the source and the target analogs are represented as simplexes and the analogy solving is modeled as a topological deformation of these simplexes along a polygonal chain and according to some constraints we apply this framework to the resolution of iq tests typically presented as igiven a b and c nd d such that a is to b what c is to dj keywords knowledge representation diagrammatic reasoning simplicial complexes analogy solving cat 4 introduction esqimo is a system that automatically solves the following type of iq tests given three gures a b and c nd a gure d such that d is to c what b is to a these iq tests are a paradigmatic example of analogy solving problems esqimo is thus a computational model for analogy solving the framework used by esqimo to solve analogies is called cat for combinatorial algebraic topology more precisely esqimo models knowledg foleyautomatic physically based sound effects for interactive simulation and animation animations for which sound effects were automatically added by our system demonstrated in the accompanying video a a real wok in which a pebble is thrown the pebble rattles around the wok and comes to rest after wobbling b a simulation of a pebble thrown in wok with all sound effects automatically generated c a ball rolling back and forth on a ribbed surface d interaction with a sonified object we describe algorithms for real time synthesis of realistic sound effects for interactive simulations e g games and animation these sound effects are produced automatically from 3d models using dynamic simulation and user interaction we develop algorithms that are efficient physicallybased and can be controlled by users in natural ways we develop effective techniques for producing high quality continuous contact sounds from dynamic simulations running at video rates which are slow relative to audio synthesis we accomplish this using modal models driven by contact forces modeled at audio rates which are much higher than the graphics frame rate the contact forces can be computed from simulations or can be custom designed we demonstrate the effectiveness with complex realistic simulations contextual deontic logic normative agents violations and independence this paper we discuss when and how to use deontic logic in multi agent systems diagnosis and decision making in normative reasoning diagnosis theory reasons about incomplete knowledge and only considers the past it distinguishes between violations and non violations qualitative decision theory reasons about decision variables and considers the future it distinguishes between fulfilled goals and unfulfilled goals in this paper we formalize normative diagnoses and decisions in the special purpose formalism dio de 2 as well as in extensions of the preference based deontic logic pdl the diagnostic and decision theoretic framework for deontic reasoning dio de 2 formalizes reasoning about violations and fulfillments and is used to characterize the distinction between normative diagnosis theory and qualitative decision theory the extension of the preference based deontic logic pdl shows how normative diagnostic and decision theoretic reasoning i e reasoning about violations and fulfillments can be formalized as an extension of deontic reasoning 1 introduction in the ai and law literature it is violation contexts and deontic independence in this paper we discuss the role of context and independence in normative reasoning first deontic operators obligations prohibitions permissions referring to the ideal context may conflict with operators referring to a violation or contrary to duty context second deontic independence is a powerful concept to derive deontic operators from such operators of other violation contexts these two concepts are used to determine how to proceed once a norm has been violated a key issue of deontic logic applications in computer science we also show how violation contexts and deontic independence can be used to give a new analysis of several notorious paradoxes of deontic logic 1 introduction deontic logic is a modal logic in which flp is read as p ought to be done fp as p is forbidden to be done and pp as p is permitted to be done deontic logic has traditionally been used by philosophers to analyze the structure of the normative use of language in the eightie generalised object oriented concepts for inter agent communication in this paper we describe a framework to program open societies of concurrently operating agents the agents maintain a subjective theory about their environment and interact with each other via a communication mechanism suited for the exchange of information which is a generalisation of the traditional rendez vous communication mechanism from the object oriented programming paradigm moreover following object oriented programming agents are grouped into agent classes according to their particular characteristics viz the program that governs their behaviour the language they employ to represent information and most interestingly the questions they can be asked to answer we give and operational model of the programming language in terms of a transition system for the formal derivation of computations of multi agent programs 1 introduction the field of multi agent systems is a rapidly growing research area although in this field there is no real consensus on what specialising the other way around in this paper we present a program transformation based on bottom up evaluation of logic programs we explain that using this technique programs can be specialised w r t a set of unit clauses instead of a query moreover examples indicate that excellent specialisation can be obtained when this bottom up transformation is combined with a more traditional top down approach resulting in conceptually cleaner techniques requiring a less complicated control than one overall approach 1 bottom up partial deduction of logic programs in this paper we develop a solid theoretical foundation for a bottom up program transformation capable of specialising a logic program with respect to a set of unit clauses extending a well known operator we define a bottom up partial deduction operator and prove correctness of the transformation with respect to the s semantics we also show how within this framework a concrete control strategy can be designed the transformation can be used as a stand alone specialisation technique useful when a program needs to be specialised with respect to its internal structure e g a library of predicates with respect to an abstract data type instead of a single goal moreover the bottom up transformation can be usefully combined with a more traditional top down partial deduction strategy teaching context to applications enhancing applications by adding one or more sensors is not new incorporating machinelearning techniques to fuse the data from the sensors into a high level context description is less obvious this paper describes an architecture that combines a hierarchy of self organizing networks and a markov chain to enable on line context recognition depending on both user and application the user can teach a context description to the system whenever he or she likes to as long as the behavior of the sensors is different enough finally consequences and complications of this new approach are discussed 1 what shall we teach our pants if a wearable device can register what the wearer is currently doing it can anticipate and adjust its behavior to avoid redundant interaction with the user however the relevance and properties of the activities that should be recognized depend on both the application and the user this requires an adaptive recognition of the activities where the user instead of the designer can teach the device what he she is doing as a case study we connected a pair of pants with accelerometers to a laptop to interpret the raw sensor data using a combination of machine learning techniques such as kohonen maps and probabilistic models we build a system that is able to learn activities while requiring minimal user attention this approach to context awareness is more universal since it requires no a priori knowledge about the contexts or the user 1 combining the self organizing map and k means clustering for on line classification of sensor data many devices like mobile phones use contextual profiles like in the car or in a meeting to quickly switch between behaviors achieving automatic context detection usually by analysis of small hardware sensors is a fundamental problem in human computer interaction however mapping the sensor data to a context is a difficult problem involving near real time classification and training of patterns out of noisy sensor signals this paper proposes an adaptive approach that uses a kohonen self organizing map augmented with on line k means clustering for classification of the incoming sensor data overwriting of prototypes on the map especially during the untangling phase of the self organizing map is avoided by a refined k means clustering of labeled input vectors real time analysis of data from many sensors with neural networks much research has been conducted that uses sensorbased modules with dedicated software to automatically distinguish the user s situation or context the best results were obtained when powerful sensors such as cameras or gps systems and or sensor specific algorithms like sound analysis were applied a somewhat new approach is to replace the one smart sensor by many simple sensors we argue that neural networks are ideal algorithms to analyze the data coming from these sensors and describe how we came to one specific algorithm that gives good results by giving an overview of several requirements finally wearable implementations are given to show the feasibility and benefits of this approach and its implications 1 process and agent based modelling techniques for dialogue systems and virtual environments this text presents results of ongoing research which is aimed at developing a framework for developing multimodal natural language dialogue systems operating within virtual environments the aspects of multimodality and presence in a virtual environment are chosen as the main focus of this research it may be argued that specification techniques would form the basis of such a framework therefore a general overview and evaluation is given of existing specification techniques for interactive systems based on both literature and previous research results this includes the object oriented model process algebras interactor models and agent systems agent systems are further subdivided into intentional logics production rule systems agent communication languages agent platforms and agent architectures a new agent system is proposed which is based on update notification mechanisms as found in interactor models and the facilitator function as found in some agent platfo structuring distributed virtual environments using a relational database model this paper discusses a specification technique that is based on a traditional entity relationship database model to model the architecture of complex interactive systems in particular multimodal and multi user user interfaces user interface components and other software components report on siks course on interactive and multi agent systems 30 november 4 december 1998 contents 1 introduction 2 2 interactive systems 2 2 1 miscellaneous 2 2 1 1 overview development methods for interactive systems 2 2 1 2 prototyping 2 2 1 3 techniques tools development environment and management 2 2 2 techniques per development stage 3 2 2 1 task analysis and global design specification 3 2 2 2 detailed design stage 3 2 2 3 scenario based design 3 2 2 4 evaluation criteria 4 2 2 5 evaluation techniques 4 3 multi agent systems 4 3 1 agent systems and models speechbot a speech recognition based audio indexing system for the web we have developed an audio search engine incorporating speech recognition technology this allows indexing of spoken documents from the world wide web when no transcription is available this site indexes several talk and news radio shows covering a wide range of topics and speaking styles from a selection of public web sites with multimedia archives our web site is similar in spirit to normal web search sites it contains an index not the actual multimedia content the audio from these shows suffers in acoustic quality due to bandwidth limitations coding compression and poor acoustic conditions the shows are typically sampled at 8 khz and transmitted realaudio compressed at 6 5 kbps our word error rate results using appropriately trained acoustic models show remarkable resilience to the high compression though many factors combine to increase the average word error rates over standard broadcast news benchmarks we show that even if the transcription is inaccurate we can st patterns as tools for user interface design designing usable systems is difficult and designers need effective tools that are usable themselves effective design tools should be based on proven knowledge of design capturing knowledge about the successful design of usable systems is important for both novice and experienced designers and traditionally this knowledge has largely been described in guidelines however guidelines have shown to have problems concerning selection validity and applicability patterns have emerged as a possible solution to some of the problems from which guidelines suffer patterns focus on the context of a problem and solution thereby guiding the designer in using the design knowledge patterns for architecture or software engineering are not identical in structure and user interface design also requires its own structure for patterns focusing on usability this paper explores how patterns for user interface design must be structured in order to be effective and usable tools for desig searching documents on the intranet searching for documents on the internet with today s search engines which are mainly based on words in a document is not satisfactory results can be improved by also taking the content of a document into account the extensible markup language xml enables us to do semantic tagging and to make the structure of a document explicit but this describes a document only at the syntactical level a more ideal situation would be when the xml tagging is also used to define the document at the semantical level to realize this we allow an author of a document to describe the relevant concepts by means of tags like he would design an object oriented database schema in our approach a user searching for a particular document is presented a graphical description of such a schema that describes the concepts defined for the webspace of an intranet via this interface the user can formulate oo like queries or navigate to relevant web pages to realize our ideas we are building an architecture based on the concept of an index database a prototype is up and running the wristcam as input device we show how images of a user s hand from a video camera attached to the underside of the wrist can be processed to yield finger movement information discrete and discreet movements of the fingers away from a rest position are translated into a small set of base symbols these are interpreted as input to a wearable computer providing unobtrusive control ant colony optimisation for virtual wavelength path routing and wavelength allocation ant colony optimisation aco is applied to the problem of routing and wavelength allocation in a multi wavelength all optical virtual wavelength path routed transport network three variants of our aco algorithm are proposed local update lu global update distance gu d and global update occupancy gu o all three extend the usual practice that ants are attracted by the pheromone trail of ants from their own colony in our work the artificial ants are also repelled by the pheromone of other colonies overall the best aco variant gu o provides results that approach those of an earlier problem specific heuristic on small and medium sized networks 1 introduction multi wavelength all optical transport networks have attracted considerable interest in recent years because of their potential by using multiple wavelengths in both optical transmission and optical switching to provide the huge bandwidths necessary if broadband services are to be widely adopted 1 in addition bayesian representations and learning mechanisms for content based image retrieval we have previously introduced a bayesian framework for content based image retrieval cbir that relies on a generative model for feature representation based on embedded mixtures this is a truly generic image representation that can jointly model color and texture and has been shown to perform well across a broad spectrum of image databases in this paper we expand the bayesian framework along two directions first we show that the formulation of cbir as a problem of bayesian inference leads to a natural criteria for evaluating local image similarity without requiring any image segmentation this allows the practical implementation of retrieval systems where users can provide image regions or objects as queries region based queries are significantly less ambiguous than queries based on entire images leading to significant improvements in retrieval precision second we present a bayesian learning algorithm that relies on belief propagation to integrate feedback provided by the probabilistic retrieval new insights and experimental results we present new insights on the relations between a recently introduced probabilistic formulation of the content based retrieval problem and standard solutions new experimental results are presented providing evidence that probabilistic retrieval has superior performance finally a unified representation for texture and color is introduced 1 introduction the problem of retrieving images or video from a database is naturally formulated as a problem of pattern recognition given a representation or feature space f for the entries in the database the design of a retrieval system consists of finding a map g f m f1 kg x y from f to the set m of classes identified as useful for the retrieval operation k the cardinality of m can be as large as the number of items in the database in which case each item is a class by itself or smaller if the goal of the retrieval system is to minimize the probability of error i e p g x 6 y it is well known that the opt modeling and executing the data warehouse refreshment process data warehouse refreshment is often viewed as a problem of maintaining a system for extraction of temporal expressions from french texts we present a system for extraction of temporal expressions from french texts the identication of the temporal expressions is based on a context scanning strategy css which is carried out by two complementary techniques search for regular expressions and left to right and right to left local chart parsing a system for extraction of temporal expressions from french texts paper id acl 2001 xxxx 1 introduction the identication and the interpretation of temporal and aspectual information plays an important role in text understanding this information is encoded in the natural languages by a wide array of linguistic means ranging from grammatical morpho syntactic to lexical verbs and adverbials or strictly syntactic phenomena temporal anaphora webber 1988 or argument structure of the verb verkuyl 1972 verkuyl 1993 in this paper we present a system for identi cation of lexical non verbal means of expressing temporal information in french texts the system det rdf based architecture for semantic integration of heterogeneous information sources the proposed integration architecture aims at exploiting data semantics in order to provide a coherent and meaningful with respect to a given conceptual model view of the integrated heterogeneous information sources the architecture is split into five separate layers to assure modularization providing description requirements and interfaces for each it favors the lazy retrieval paradigm over the data warehousing approach the novelty of the architecture lies in the combination of semantic and on demand driven retrieval this line of attack offers several advantages but brings also challenges both of which we discuss with respect to rdf the architecture s underlying model 1 introduction background and related work with the vast expansion of the world wide web during the last few years the integration of heterogeneous information sources has become a hot topic a solution to this integration problem allows for the design of applications that provide a uniform access to dat learning strategy knowledge incrementally modern industrial processes require advanced computer tools that should adapt to the user requirements and to the tasks being solved strategy learning consists of automating the acquisition of patterns of actions used while solving particular tasks current intelligent strategy learning systems acquire operational knowledge to improve the efficiency of a particular problem solver however these strategy learning tools should also provide a way of achieving low cost solutions according to user specific criteria in this paper we present a learning system hamlet which is integrated in a planning architecture prodigy and acquires control knowledge to guide prodigy to efficiently produce cost effective plans hamlet learns from planning episodes by explaining why the correct decisions were made and later refines the learned strategy knowledge to make it incrementally correct with experience playing soccer with legged robots sony has provided a remarkable platform for research and development in robotic agents namely fully autonomous legged robots in this paper we describe our work using sony s legged robots to participate at the robocup 98 legged robot demonstration and competition robotic soccer represents a very challenging environment for research into systems with multiple robots that need to achieve concrete objectives particularly in the presence of an adversary furthermore robocup 98 offers an excellent opportunity for robot entertainment we introduce the robocup context and briefly present sony s legged robot we developed a vision based navigation and a bayesian localization algorithm team strategy is achieved through pre defined behaviors and learning by instruction 1 introduction problem solving in complex domains necessarily involves multiple agents dynamic environments and the need for learning from feedback and previous experience robotic soccer is an example of one such complex the cmunited 98 champion small robot team abstract in this chapter we present the main research contributions of our champion cmunited 98 small robot team the team is a multiagent robotic system with global perception and distributed cognition and action we describe the main features of the hardware design of the physical robots including di erential drive robust mechanical structure and a kicking device we brie y review the cmunited 98 global vision processing algorithm which is the same as the one used by the previous champion cmunited 97 team we introduce our new robot motion algorithm which reactively generates motion control to account for the target point the desired robot orientation and obstacle avoidance our robots exhibit successful collision free motion in the highly dynamic robotic soccer environment at the strategic and decision making level we present the role based behaviors of the cmunited 98 robotic agents team collaboration is remarkably achieved through a new algorithm that allows for team agents to anticipate possible collaboration opportunities robots position themselves strategically in open positions that increase passing opportunities the chapter terminates with a summary of the results of the robocup 98 games in which the cmunited 98 small robot team scored a total of 25 goals and su ered 6 goals in the 5 games that it played 1 isocrob intelligent society of robots abstract the socrob project was born as a challenge for multidisciplinary research on broad and generic approaches for the design of a cooperative robot society involving control robotics and artificial intelligence researchers a case study on robotic soccer played by a team of 3 robots has started two years ago and was first tested last year during robocup98 this experience has clearly revealed that the robotic soccer environment is a sufficiently rich complex and dynamic testbed to study new methodologies both on robotics and artificial intelligence therefore the socrob team is currently working on the socrob project improvements and intends to compete at the world cup of robotic soccer robocup99 held in stockholm sweden in the middle size league in this paper the basic aspects of last year implementation as well as the improvements made meanwhile are briefly recalled and presented naturally a special emphasis is given here to the novel solutions proposed for this year implementation the results obtained and the expected future developments 1 a web odyssey from codd to xml introduction the web presents the database area with vast opportunities and commensurate challenges databases and the web are organically connected at many levels web sites are increasingly powered by databases collections of linked web pages distributed across the internet are themselves tempting targets for a database the emergence of xml as the lingua franca of the web brings some much needed order and will greatly facilitate the use of database techniques to manage web information this paper will discuss some of the developments related to the web from the viewpoint of database theory as we shall see the web scenario requires revisiting some of the basic assumptions of the area to be sure database theory remains as valid as ever in the classical setting and the database industry will continue to representamulti billion dollar target of applicability for the foreseeable future but the web represents an opportunityofanentirely di erent scale we are th collections adapting the display of personal objects for different audiences although current networked systems and online applications provide new opportunities for displaying and sharing personal information they do not account for the underlying social contexts that frame such interactions existing categorization and management mechanisms for digital content have been designed to focus on the data they handle without much regard for the social circumstances within which their content is shared as we share large collections of personal information over mediated environments our tools need to account for the social scenarios that surround our interactions this thesis presents collections an application for the management of digital pictures according to their intended audiences the goal is to create a graphical interface that supports the creation of fairly complex privacy decisions concerning the display of digital photographs simple graphics are used to enable the collector to create a wide range of audience arrangements for her digital pho a quantification of distance bias between evaluation metrics in classification this paper provides a characterization of bias for evaluation metrics in classification e g information gain gini 2 etc our characterization provides a uniform representation for all traditional evaluation metrics such representation leads naturally to a measure for the distance between the bias of two evaluation metrics we give a practical value to our measure by observing if the distance between the bias of two evaluation metrics correlates with differences in predictive accuracy when we compare two versions of the same learning algorithm that differ in the evaluation metric only experiments on real world domains show how the expectations on accuracy differences generated by the distance bias measure correlate with actual differences when the learning algorithm is simple e g search for the best single feature or the best single rule the correlation however weakens with more complex algorithms e g learning decision trees our results sh rule induction of computer events introduction monitoring systems are able to capture an assortment of different events from a network environment a single event signals an abnormal situation on either one host e g cpu utilization is above a critical threshold or the network e g communication link is down a monitoring system can capture thousands of events in a short time period several days applying data analysis techniques e g machine learning data mining on those events may reveal useful patterns characterizing a network problem data analysis techniques have proved useful in the area of network fault management in this paper we consider the following scenario a computer network is under continuous monitoring a user is interested in identifying what triggers a specific kind of events which we refer to as target events the user would like to know what events correlate to each target event within a fixed time window we describe a data mining technique that takes as input an agent infrastructure to build and evaluate multi agent systems the java agent framework and multi agent system simulator in this paper we describe our agent framework and address secure mobile code the javaseal experiment mobile agents are programs that move between sites during execution to benefit from the services and information present at each site to gain wide acceptance strong security guarantees must be given sites must be protected from malicious agents and agents must be protected from each other software based protection is widely viewed as the most efficient way of enforcing agent security in the first part of the paper we review programming language support for security this review also helps to highlight weaknesses in the java security model in the second part of the paper we make good on the lessons learned in the review to design a security architecture for the javaseal agent platform 1 introduction wide area networks such as the internet hold the promise of a brave new wired world of global computing the hope is to have distributed applications that scale to the size of the internet and seamlessly provide access to massive amounts of information and value added services bu external memory algorithms and data structures data sets in large applications are often too massive to fit completely inside the computer s internal memory the resulting input output communication or i o between fast internal memory and slower external memory such as disks can be a major performance bottleneck in this paper we survey the state of the art in the design and analysis of external memory algorithms and data structures which are sometimes referred to as em or i o or out of core algorithms and data structures em algorithms and data structures are often designed and analyzed using the parallel disk model pdm the three machine independent measures of performance in pdm are the number of i o operations the cpu time and the amount of disk space pdm allows for multiple disks or disk arrays and parallel cpus and it can be generalized to handle tertiary storage and hierarchical memory we discuss several important paradigms for how to solve batched and online problems efficiently in external memory programming tools and environments are available for simplifying the programming task the tpie system transparent parallel i o programming environment is both easy to use and efficient in terms of execution speed we report on some experiments using tpie in the domain of spatial databases the newly developed em algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice toward scalability in asl recognition breaking down signs into phonemes in this paper we present a novel approach to continuous whole sentence asl recognition that uses phonemes instead of whole signs as the basic units our approach is based on a sequential phonological model of asl according to this model the asl signs can be broken into movements and holds which are both considered phonemes this model does away with the distinction between whole signs and epenthesis movements that we made in previous work 13 instead epenthesis movements are just like the other movements that constitute the signs we subsequently train hidden markov models hmms to recognize the phonemes instead of whole signs and epenthesis movements that we recognized previously 13 because the number of phonemes is limited hmm based training and recognition of the asl signal becomes computationally more tractable and has the potential to lead to the recognition of large scale vocabularies we experimented with a 22 word vocabulary and we achieved similar recognition r how much more is better characterizing the effects of adding more ir systems to a combination we present the results of some expansion experiments for solving the routing data fusion problem using trec5 systems the experiments address the question how much more is better when combining the results of multiple information retrieval systems using a linear combination weighted sum model by investigating all 2way 3 way 4 way and 10 way combinations of 10 ir systems on 10 queries we show that 1 one can expect potentially significant amounts of improvement in performance over the best system used in the combination if enough systems are used 2 for this number of candidate systems the point of diminishing returns is reached when around four systems are used in the combination 3 queries generally have too few relevant documents causing little correlation in performance between the training set and test set thus making it difficult to get test set improvement even when multiple systems are used and 4 if one knows the relative past performance of the candidate s fusion via a linear combination of scores we present a thorough analysis of the capabilities of the linear combination lc model for fusion of information retrieval systems the lc model combines the results lists of multiple ir systems by scoring each document using a weighted sum of the scores from each of the component systems we first present both empirical and analytical justification for the hypotheses that such a model should only be used when the systems involved have high performance a large overlap of relevant documents and a small overlap of nonrelevant documents the empirical approach allows us to very accurately predict the performance of a combined system we also derive a formula for a theoretically optimal weighting scheme for combining 2 systems we introduce d the difference between the average score on relevant documents and the average score on nonrelevant documents as a performance measure which not only allows mathematical reasoning about system performance but also allows the selection of w a neuro fuzzy solution for integrated visual and force control in this paper the use of a b spline neuro fuzzy model for different tasks such as vision based fine positioning using uncalibrated cameras and force control is presented it is shown that neuro fuzzy controllers can be used not only for low dimensional problems like force control but also for high dimensional problems like vision based sensorimotor control and for fusing input from different sensors controllers of this type can be modularly combined to solve a given assembly problem 1 introduction it is well known that general fuzzy rule descriptions of systems with a large number of input variables suffer from the problem of the curse of dimensionality in many realworld applications it is difficult to identify the decisive input parameters and thus to reduce the number of input variables to the minimum a general solution to building fuzzy models is not only interesting from a theoretical point it may also extend the range of applications of fuzzy control to more complex intel bare hand human computer interaction in this paper we describe techniques for barehanded interaction between human and computer barehanded means that no device and no wires are attached to the user who controls the computer directly with the movements of his her hand our approach is centered on the needs of the user we therefore define requirements for real time barehanded interaction derived from application scenarios and usability considerations based on those requirements a finger finding and hand posture recognition algorithm is developed and evaluated to demonstrate the strength of the algorithm we build three sample applications finger tracking and hand posture recognition are used to paint virtually onto the wall to control a presentation with hand postures and to move virtual items on the wall during a brainstorming session we conclude the paper with user tests which were conducted to prove the usability of bare hand human computer interaction categories and subject descriptors h 5 2 information interfaces and presentation user interfaces input devices and strategies i 5 5 pattern recognition implementation interactive systems general terms algorithms design experimentation human factors keywords computer vision human computer interaction real time finger tracking hand posture recognition bare hand control 1 ontology based integration of information a survey of existing approaches we review the use on ontologies for the integration of heterogeneous information sources based on an in depth evaluation of existing approaches to this problem we discuss how ontologies are used to support the integration task we evaluate and compare the languages used to represent the ontologies and the use of mappings between ontologies as well as to connect ontologies with information sources we also ask for ontology engineering methods and tools used to develop ontologies for information integration based on the results of our analysis we summarize the state of the art in ontology based information integration and name areas of further research activities 1 motivation the so called information society demands for complete access to available information which is often heterogeneous and distributed in order to establish efficient information sharing many technical problems have to be solved first a suitable information source must be located that might conta lifelike gesture synthesis and timing for conversational agents besides the inclusion of gesture recognition devices as an intuitive input modality the synthesis of lifelike gesture is finding growing attention in human computer interface research in particular the generation of synthetic gesture in connection with text to speech systems is one of the goals for embodied conversational agents which have become a new paradigm for the study of gesture and for human computer interface 1 embodied conversational agents are computer generated characters that demonstrate similar properties as humans in face to face conversation including the ability to produce and respond to verbal and nonverbal communication they may represent the computer in an interaction with a human or represent their human users as quot avatars quot in a computational environment in this context this contribution focusses on an approach for synthesizing lifelike gestures for an articulated virtual agent with particular emphasis on how to achieve temporal coordination with external information such as the signal generated by a text to speech system the context of this research is the conception of an quot articulated communicator quot that conducts multimodal dialogue with a human partner in cooperating on a construction task gesture production and performance in humans is a complex and multi stage process informedia search and summarization in the video medium the informedia system provides full content search and retrieval of current and past tv and radio news and documentary broadcasts the system implements a fully automatic intelligent process to enable daily content capture analysis and storage in on line archives the current library consists of approximately a 2 000 hours 1 5 terabyte library of daily cnn news captured over the last 3 years and documentaries from public television and government agencies this database allows for rapid retrieval of individual video paragraphs which satisfy an arbitrary spoken or typed subject area query based on a combination of the words in the soundtrack images recognized in the video plus closed captioning when available and informational text overlaid on the screen images there are also capabilities for matching of similar faces and images generation of related map based displays the latest work attempts to produce a visualization and summarization of the content across all the stories concall an information service for researchers based on edinfo in this paper we present new types of web information services where users and information brokers collaborate in creating a r hqhf vo oor information service such services impose a novel task on information brokers they become responsible for maintaining the inference strategies used in user modeling in return information brokers obtain more accurate information about user needs since the adaptivity ensures that user profiles are kept up to date and consistent with what users actually prefer not only what they say that they prefer we illustrate the approach by an example application in which conference calls are collected and distributed to interested readers keywords adaptive information services intelligent information filtering agents www adaptivity user modeling user profiling introduction the rapid development of information sources such as the world wide web has left readers with an acute problem of information overflow the problem is not simply one an xml based multimedia middleware for mobile online auctions pervasive internet services today promise to provide users with a quick and convenient access to a variety of commercial applications however due to unsuitable architectures and poor performance user acceptance is still low to be a major success mobile services have to provide device adapted content and advanced value added web services innovative enabling technologies like xml and wireless communication may for the first time provide a facility to interact with online applications anytime anywhere we present a prototype implementing an efficient multimedia middleware approach towards ubiquitous value added services using an auction house as a sample application advanced multi feature retrieval technologies are combined with enhanced content delivery to show the impact of modern enterprise information systems on today s e commerce applications keywords mobile commerce online auctions middleware architectures pervasive internet technology multimedia database appli artificial agents and logic programming artificial agents represent a new paradigm in software engineering and artificial intelligence as complex software controlled systems they are capable of flexible autonomous behavior in dynamic and unpredictable environments over the past few years researchers in computer science have begun to recognise that the technology of artificial agents provides the key to solving many problems in distributed computing and intelligent control for which traditional software engineering techniques offer no solution the field of logic programming includes many important concepts such as declarativity unification meta logic programming and deduction rules from which the new technology of multiagent systems can benefit 1 introduction although the idea of agent systems is intuitively appealing and there are a number of implemented systems that claim to realize this popular idea the basic concepts underlying these systems are often not well understood and no attempt is made to define t toward generalized organizationally contexted agent control generalized domain independent approaches to agent control enable control components to be used for a wide variety of applications this abstraction from the domain context implies that contextual behavior is not possible or that it requires violation of the domain independent objective we discuss how context is used in the generalized framework and our current focus on the addition of organizational context in agent control 1 introduction from the vantage point of a long history of research in agents and agent control components for building distributed ai and multi agent systems we have focused our recent efforts on approaching agent control from a generalized domainindependent perspective in implementation terms the objective is to develop a set of agent control components that can be bundled with domain problem solvers or legacy applications to create agents that can meet real time deadlines and real resource constraints and coordinate activities with other agents this pap constrained genetic algorithms and their applications in nonlinear constrained optimization this chapter presents a framework that unifies various search mechanisms for solving constrained nonlinear programming nlp problems these problems are characterized by functions that are not necessarily differentiable and continuous our proposed framework is based on the first order necessary and sufficient condition developed for constrained local minimization in discrete space that shows the equivalence between discrete neighborhood saddle points and constrained local minima to look for discrete neighborhood saddle points we formulate a discrete constrained nlp in an augmented lagrangian function and study various mechanisms for performing ascents of the augmented function in the original variable subspace and descents in the lagrange multiplier subspace our results show that csaga a combined constrained simulated annealing and genetic algorithm performs well when using crossovers mutations and annealing to generate trial points finally we apply iterative deepening to de real time frp functional reactive programming frp is a declarative programming paradigm where the basic notions are continuous time varying behaviors and discrete event based reactivity frp has been used successfully in many reactive programming domains such as animation robotics and graphical user interfaces the success of frp in these domains encourages us to consider its use in real time applications where it is crucial that the cost of running a program be bounded and known before run time but previous work on the semantics and implementation of frp was not explicitly concerned about the issues of cost in fact the resource consumption of frp programs in the current implementation is often hard to predict as a first step argumentation based abduction in disjunctive logic programming in this paper we propose an argumentation based semantic framework called das for disjunctive logic programming the basic idea is to translate a disjunctive logic program into an argumentationtheoretic framework one unique feature of our proposed framework is to consider the disjunctions of negative literals as possible assumptions so as to represent incomplete information in our framework three semantics pdh cdh and wfdh are defined by three kinds of acceptable hypotheses to represent credulous moderate and skeptical reasoning in ai respectively further more our semantic framework can be extended to a wider class than that of disjunctive programs called bi disjunctive logic programs in addition to being a first serious attempt of establishing an argumentation theoretic framework for disjunctive logic programming das integrates and naturally extends many key semantics such as the minimal models egcwa the well founded model and the disjunctive stable models in particular novel and interesting argumentation theoretic characterizations of the egcwa and the disjunctive stable semantics are shown thus the framework presented in this paper does not only provide a new way of performing argumentation abduction in disjunctive deductive databases but also is a simple intuitive and unifying semantic framework for disjunctive logic programming concept hierarchy based text database categorization document categorization as a technique to improve the retrieval of useful documents has been extensively investigated one important issue in a large scale metasearch engine is to select text databases that are likely to contain useful documents for a given query we believe that database categorization can be a potentially effective technique for good database selection especially in the internet environment where short queries are usually submitted in this paper we propose and evaluate several database categorization algorithms this study indicates that while some document categorization algorithms could be adopted for database categorization algorithms that take into consideration the special characteristics of databases may be more effective preliminary experimental results are provided to compare the proposed database categorization algorithms a prototype database categorization system based on one of the proposed algorithms has been developed experience paper implementing a multi agent architecture for cooperative software engineering the paper describes experiences we have earned from implementing a multiagent architecture used to support cooperative software engineering before starting to implement a multi agent architecture important decisions and considerations must be taken into account you have to decide how to provide efficient inter agent communication support what language should the agents talk should the agents be stationary or mobile and what technology should be used to build the architecture this paper describes how we implemented our multi agent system and the experiences we gained from building it keywords cooperative software engineering agents multi agent system kqml xml aglets jatlite corba 1 introduction the last couple of years distributed computing and agent technology have become more and more popular when researchers are developing prototypes the choice of technologies and how to use different technologies is getting more and more complicated this paper descri nonmonotonic reasoning in ldl deductive database systems have made major advances on efficient support for nonmonotonic reasoning a first generation of deductive database systems supported the notion of stratification for programs with negation and set aggregates stratification is simple to understand and efficient to implement but it is too restrictive therefore a second generation of systems seeks efficient support for more powerful semantics based on notions such as well founded models and stable models in this respect a particularly powerful set of constructs is provided by the recently enhanced ldl system that supports i monotonic user defined aggregates ii xy stratified programs and iii the nondeterministic choice constructs under stable model semantics this integrated set of primitives supports a terse formulation and efficient implementation for complex computations such as greedy algorithms and data mining functions yielding levels of expressive power unmatched by other deductive using software agents to support evolution of distributed workflow models this paper outlines a high level design of how software agents can be used combined with an existing cagis process centred environment to deal with evolution of distributed fragmented workflow models our process centred environment allows process fragments of the same workflow model to be located in workspaces that are geographically distributed these process fragments can be changed independently in local workspaces causing consistency problems we propose to use software mobile agents offering awareness services solving conflicting updates of process fragment our solution is illustrated using some scenarios keywords process centred environments software agents workflow model consistency workflow model evolution distribution fragmentation 1 introduction dealing with evolution of workflow processes is not a trivial matter one simple solution to this problem is to have one centralised workflow model that cannot be changed after it is instanciated in practice it is ho agent oriented requirements engineering using congolog and i agent oriented approaches are becoming popular in software engineering both as architectural frameworks and as modeling frameworks for requirements engineering and design i is an informal diagram based language for early phase requirements engineering that supports the modeling of social dependencies between agents and how process design choices affect the agents goals both functional and non functional congolog is an expressive logic based formalism for specifying processes that involves multiple agents tools are being developed to support the validation of congolog process models though simulation and verification the two formalisms complement each other well and in this work we develop a methodology for their combined use in requirements engineering the i sr diagram language is extended with process specification annotations which allow the sr model of a system to be refined and then mapped into a congolog model the mapping must satisfy a set of mapping rules which ensure that it specifies which elements in the two models are related and that the models are consistent the methodology is illustrated on a meeting scheduling application example 1 supporting workspace awareness in distance learning environments issues and experiences in the development of a collaborative learning system in recent years we have witnessed an enormous growth in networks and related technologies course materials are increasingly published on web servers and students are encouraged to access these at leisure distance learning via the www shifted the education paradigm from teacher centered instruction to user centered collaborative learning systems that allow users to learn collaboratively are increasingly interesting to scientific communities and learning organizations we initially designed and prototyped a collaborative system to support collaborative learning over the internet a usability study of the first prototype revealed the importance of awareness information our review of three wellknown collaborative systems finds that such systems today also lack support for awareness information especially workspace awareness we then consider various types of awareness in collaborative learning situations and set out the design requirements of our system from these requirements we have designed and prototyped several awareness widgets for a typical collaborative tool the shared electronic whiteboard these widgets help learners maintain awareness of other learners interactions with the shared workspace quasi stable semantics of logic programs this paper we introduce a new semantic theory for logic programs we choose the well founded semantics wfs 23 as our starting point because it has many desirable features for any logic program there exists a unique well founded partial model which can be defined in a constructive way wfs naturally extends the semantics for a large class of logic programs including stratified and locally stratified programs see 17 and 23 despite its merits it can be argued that the wfs is too sceptical for many programs it gives the empty set as their intended meaning it is thus silent on all atoms though it may be reasonable to expect that something should be concluded from the information given for an illustrative example see program p 1 in section 4 there are many proposals for extending the wfs such as the generalised well founded semantics gwfs 1 the well founded by case semantics wfsc 20 the extended well founded semantics wfse 11 the strong well founded semantics wfs s 5 and the o semantics 15 the relationship between these semantic theories for logic programming is also studied in 6 here we propose another nondeterministic extension of the wfs the other dominant semantic model for logic programs is the stable model semantics 10 compared with the wfs the stable model semantics is credulous it derives much more information than the wfs though this is non deterministic or disjunctive in the sense that there may be several stable models for a given logic program the chief drawback of this semantics is that a stable model is not defined for all logic programs in addition stable model semantics gives rise to anomalies in some circumstances see the program p 3 in section 4 borrowed from 23 a modification of the stable model seman the structure of object transportation and orientation in human computer interaction an experiment was conducted to investigate the relationship between object transportation and object orientation by the human hand in the context of humancomputer interaction hci this work merges two streams of research the structure of interactive manipulation in hci and the natural hand prehension in human motor control it was found that object transportation and object orientation have a parallel interdependent structure which is generally persistent over different visual feedback conditions the notion of concurrency and interdependence of multidimensional visuomotor control structure can provide a new framework for human computer interface evaluation and design keywords direct manipulation input device multi dimensional control visuomotor control visual conditions information processing interface design virtual reality introduction object manipulation is a basic operation in humancomputer interaction hci modern computer technology advances towards affording m a critical note on stable model semantics in this paper we argue that both the stable model semantics and its three valued version are conceptually flawed 1 introduction the semantics of logic programming and deductive databases has been extensively studied in the past two decades a whole spectrum of semantic theories for logic programs have been proposed ranging from those that infer very little information from a logic program skeptical to those that infer a great deal credulous the most skeptical semantics is the well founded semantics 4 while the most credulous is the the stable model semantics 6 and its different but equivalent three valued versions including 3 stable models 8 partial stable models 9 and preferred extensions 2 the chief drawback of the stable semantics is that a two valued stable model is not defined for all logic programs in the three valued variant a model is always defined but that model may leave all atoms undetermined 8 the original problem of the non existence of a discovering structural association of semistructured data many semistructured objects are similarly though not identically structured we study the problem of discovering typical substructures of a collection of semistructured objects the discovered structures can serve the following purposes a the table of contents for gaining general information of a source b a road map for browsing and querying information sources c a basis for clustering documents d partial schemas for providing standard database access methods e user customer s interests and browsing patterns the discovery task is impacted by structural features of semistructured data in a non trivial way and traditional data mining frameworks are inapplicable we define this discovery problem and propose a solution 1 introduction 1 1 motivation many on line documents such as html latex bibtex sgml files and those found in digital libraries are semistructured semistructured data arises when the source does not impose a rigid structure such as the hierarchical optimization of policy coupled semi markov decision processes one general strategy for approximately solving large markov decision processes is divide and conquer the original problem is decomposed into sub problems which interact with each other but yet can be solved independently by taking into account the nature of the interaction in this paper we focus on a class of policy coupled semi markov decision processes smdps which arise in many nonstationary real world multi agent tasks such as manufacturing and robotics the nature of the interaction among sub problems agents is more subtle than that studied previously the components of a sub smdp namely the available states and actions transition probabilities and rewards depend on the policies used in solving the neighboring sub smdps this strongly coupled interaction among subproblems causes the approach of solving each sub smdp in parallel to fail we present a novel approach whereby many variants of each sub smdp are solved explicitly taking into account the different mod negation in logic and deductive databases this thesis studies negation in logic and deductive databases among other things two kinds of negation are discussed in detail strong negation and nonmonotonic negation in the logic part we have constructed a first order logic cf 0 of strong negation with bounded quantifiers the logic is based on constructive logics in particular thomason s logic cf however unlike constructive logic quantifiers in our system as in thomason s are static rather than dynamic for the logic cf 0 the usual kripke formal semantics is defined but based on situations instead of conventional possible worlds a sound and complete axiomatic system of cf 0 is established based on the axiomatic systems of constructive logics with strong negation and thomason s completeness proof techniques cf 0 is proposed as the underlying logic for situation theory thus the connection between cf 0 and infon logic is briefly discussed in the database part based on the study of some main existing semant the s sup2 tree an index structure for subsequence matching of spatial objects we present the s tree an indexing method for subsequence matching of spatial objects the s tree locates subsequences within a collection of spatial sequences i e sequences made up of spatial objects such that the subsequences match a given query pattern within a specified tolerance our method is based on i the string searching techniques that locate substrings within a string of symbols drawn from a discrete alphabet e g ascii characters and ii the spatial access methods that index unsequenced spatial objects particularly the s tree can be applied to solve problems such as subsequence matching of time series data where features of subsequences are often extracted and mapped into spatial objects moreover it supports queries such as what is the longest common pattern of the two time series which previous subsequence matching algorithms find difficult to solve efficiently reaching for objects in vr displays lag and frame rate this article reports the results from three experimental studies of reaching behavior in a head coupled stereo display system with a hand tracking subsystem for object selection it is found that lag in the head tracking system is relatively unimportant in predicting performance whereas lag in the hand tracking system is critical the effect of hand lag can be modeled by means of a variation on fitts law with the measured system lag introduced as a multiplicative variable to the fitts law index of difilculty this means that relatively small lags can cause considerable degradation in performance if the targets are small another finding is that errors are higher for movement in and out of the screen as compared to movements in the plane of the screen and there is a small 10 time penalty for movement in the z direction in all three experiments low frame rates cause a degradation in performance however this can be attributed to the lag which is caused by low frame rates particularly if double buffering is used combined with early sampling of the hand tracking device rotating virtual objects with real handles times for virtual object rotations reported in the literature are of the order of ten seconds or more and this is far longer than it takes to manually orient a real object such as a cup this is a report of a series of experiments designed to investigate the reasons for this difference and to help design interfaces for object manipulation the results suggest that two major factors are important having the hand physically in the same location as the virtual object being manipulated is one the other is based on whether the object is being rotated to a new randomly determined orientation or is always rotated to the same position making the object held in the hand have the same physical shape as the object being visually manipulated was not found to be a significant factor the results are discussed in the context of interactive virtual environments categories and subject descriptors h 1 2 models and principles user machine systems human factors i 3 6 computer graphics evaluating stereo and motion cues for visualizing information nets in three dimensions this article concerns the benefits of presenting abstract data in 3d two experiments show that motion cues combined with stereo viewing can substantially increase the size of tbe graph that can be perceived the first experiment was designed to provide quantitative measurements of how much more or less can be understood in 3d than in 2d tbe 3d display used was configured so that the image on the monitor was coupled to the user s actual eye positions and it was updated in real time as the user moved as well as being in stereo thus the effect was like a local virtual reality display located in the vicinity of the computer monitor the results from this study show that head coupled stereo viewing can increase the size of an abstract graph that can be understood by a factor of three using stereo alone provided an increase by a factor of 1 6 and bead coupling alone produced an increase by a factor of 2 2 tbe second experiment examined a variety of motion cues provided by head coupled perspective as in virtual reality displays hand guided motion and automatic rotation respectively both with and without stereo in each case the results show that structured 3d motion and stereo viewing both help in understanding but that the kind of motion is not particularly important all improve performance and all are more significant than stereo cues these results provide strong reasons for using advanced 3d graphics for interacting with a large variety of information structures facilitating hard active database applications machine interface 26 3 2 3 concurrency control 27 3 3 venusdb language semantics an evaluation 28 3 3 1 related work 30 3 3 2 the mortgage pool allocation problem 33 3 3 3 quantitative results 37 3 3 4 discussion and conclusions 43 chapter 4 application semantics for active log monitoring applications 45 4 1 motivation 46 4 1 1 coupling modes 47 4 1 2 example 1 48 4 2 background 50 4 2 1 lmas datalog and confluence 50 4 2 2 previous work monitoring network logs for anomalous activity we report on the progress of the venusdb active database system as driven by watchdog an application in network intrusion detection the application is typical of a class of problems we coin monotonic log monitoring systems these are systems where real time data sources are logged to a database for transactional assurances and the database further provides services for decision support milestones comprise the successful layering of a venus language executable with oracle through the use of the venus abstract machine interface ami a data abstraction interface and oracle s native trigger mechanism the identification of monotonic logging systems as an interesting application class enables us to limit coupling modes and to identify an effective layered architecture rule based query optimization revisited we present an overview and initial performance assessment of a rule based query optimizer written in venusdb venusdb is an active database rule language embedded in c following the developments in extensible database query optimizers first in rule based form followed by optimizers written as object oriented programs the venusdb optimizer avails the advantages of both to date development of rule based query optimizers have included the definition and implementation of custom rule languages thus extensibility required detailed understanding and often further development of the underlying search mechanism of the rule system objectoriented query optimizers appear to have achieved their goals with respect to a clear organization and encapsulation of an optimizer s elements they do not however provide for the concise declarative expression of domain specific heuristics our experience demonstrates that a rule based query optimizer developed in venusdb can be well structured embodied evolution embodying an evolutionary algorithm in a population of robots we introduce embodied evolution ee as a methodology for the automatic design of robotic controllers ee is an evolutionary robotics er technique that avoids the pitfalls of the simulate and transfer method allows the speed up of evaluation time by utilizing parallelism and is particularly suited to future work on multi agent behaviors in ee an evolutionary algorithm is distributed amongst and embodied within a population of physical robots that reproduce with one another while situated in the task environment we have built a population of eight robots and successfully implemented our first experiments the controllers evolved by ee compare favorably to hand designed solutions for a simple task we detail our methodology report our initial results and discuss the application of ee to more advanced and distributed robotics tasks 1 introduction our work is inspired by the following vision a large number of robots freely interact with each other in a shared environment atte unsupervised learning of models for recognition abstract we present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition we focus on a particular type of model where objects are represented as flexible constellations of rigid parts features the variability within a class is represented by a joint probability density function pdf on the shape of the constellation and the output of part detectors in a first stage the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator it then learns the statistical shape model using expectation maximization the method achieves very good classification results on human faces and rear views of cars 1 introduction and related work we are interested in the problem of recognizing members of object classes where we define an object class as a collection of objects which share characteristic features or parts that are visually similar and occur in similar spatial configurations when building models for object classes of this type one is faced with three problems see fig 1 viewpoint invariant learning and detection of human heads we present a method to learn models of human heads for the purpose of detection from different viewing angles we focus on a model where objects are represented as constellations of rigid features parts variability is represented by a joint probability density function pdf on the shape of the constellation in a first stage the method automatically identifies distinctive features in the training set using an interest operator followed by vector quantization the set of model parameters including the shape pdf is then learned using expectation maximization experiments show good generalization performance to novel viewpoints and unseen faces performance is above correct with less than s computation time per image a textual case based reasoning framework for knowledge management applications knowledge management km systems manipulate organizational knowledge by storing and redistributing corporate memories that are acquired from the organization s members in this paper we introduce a textual casebased reasoning tcbr framework for km systems that manipulates organizational knowledge embedded in artifacts e g best practices alerts lessons learned the tcbr approach acquires knowledge from human users via knowledge elicitation and from text documents via knowledge extraction using template based information extraction methods a subset of natural language and a domain ontology organizational knowledge is stored in a case base and is distributed in the context of targeted processes i e within external distribution systems the knowledge artifacts in the case base have to be translated into the format of the external distribution systems a domain ontology supports knowledge elicitation and extraction storage of knowledge artifacts in a case base and artifact translation agent based digital libraries decentralization and coordination this paper describes agent based systems and explains why digital libraries should be built with this type of architecture the primary advantage of agent based architecture is decentralization which enables scaling flexibility and extensibility the corresponding requirement is the need to coordinate agent activity we describe the approach taken by the university of michigan digital library project 2 1 introduction digital libraries are just beginning to evolve no one is certain what capabilities are needed nor how they should be organized it is therefore important to design digital libraries to be as open as possible so that new collections and services can be easily added to the system furthermore it is essential that libraries be able to scale to become quite large for us this implies a decentralized architecture where there are few if any shared resources and where as much decision making is done as locally as possible an example of such a distributed system is t on building flexible agents this paper focuses on the challenge of building technical agents that act exibly in modern computing and information environments it is argued that existing agent architectures tend to inherently limit an agent s exibility because they imply a discrete social and cognitive behavior space a generic constraint centered architectural framework is proposed that aims at enabling agents to act in a continuous behavior space and thus to achieve higher exibility 1 introduction modern computing platforms and information environments are becoming more and more distributed large open dynamic and heterogeneous computers are no longer stand alone systems but have become tightly connected both with each other and their users the increasing technological complexity of such platforms and environments goes together with an increasing complexity of their potential applications this development has led to a rapidly growing research and application interest in agents as a powerful predicting telecommunication equipment failures from sequences of network alarms the computer and telecommunication industries rely heavily on knowledge based expert systems to manage the performance of their networks these expert systems are developed by knowledge engineers who must first interview domain experts to extract the pertinent knowledge this knowledge acquisition process is laborious and costly and typically is better at capturing qualitative knowledge than quantitative knowledge this is a liability especially for domains like the telecommunication domain where enormous amounts of data are readily available for analysis data mining holds tremendous promise for the development of expert systems for monitoring network performance since it provides a way of automatically identifying subtle yet important patterns in data this case study describes a project in which a temporal data mining system called timeweaver is used to identify faulty telecommunication equipment from logs of network alarm messages project overview managing the p answer network monitoring using object oriented rules this paper describes answer the expert system responsible for monitoring at t s 4ess switches these switches are extremely important since they handle virtually all of at t s long distance traffic answer is implemented in r a rule based extension to the c object oriented programming language and is innovative because it employs both rule based and object oriented programming paradigms the use of object technology in answer has provided a principled way of modeling the 4ess and of reasoning about failures within the 4ess this has resulted in an expert system that is more clearly organized easily understood and maintainable than its predecessor which was implemented using the rule based paradigm alone answer has been deployed for more than a year and handles all 140 of at t s 4ess switches and processes over 100 000 4ess alarms per week introduction network reliability is of critical concern to at t since its reputation for network reliability has taken many years to intelligent telecommunication technologies anage telecommunication networks building such applications involved acquiring valuable telecommunication knowledge from human experts and then applying this knowledge typically by embedding it in an expert system this knowledge acquisition process is so time consuming that it is referred to as the knowledge acquisition bottleneck data mining techniques are now being applied to industrial applications to break this bottleneck by replacing the manual knowledge acquisition process with automated knowledge discovery telecommunication networks which routinely generate tremendous amounts of data are ideal candidates for data mining 1 this section will describe expert system and data mining technologies and how they are evolving to solve complex industrial problems 1 1 expert systems expert systems are programs which represent and apply factual knowledge of specific areas of expertise to solve problems 2 expert systems have been applied extensively within learning to predict rare events in event sequences learning to predict rare events from sequences of events with categorical features is an important real world problem that existing statistical and machine learning methods are not well suited to solve this paper describes timeweaver a genetic algorithm based machine learning system that predicts rare events by identifying predictive temporal and sequential patterns timeweaver is applied to the task of predicting telecommunication equipment failures from 110 000 alarm messages and is shown to outperform existing learning methods introduction an event sequence is a sequence of timestamped observations each described by a fixed set of features in this paper we focus on the problem of predicting rare events from sequences of events which contain categorical non numerical features predicting telecommunication equipment failures from alarm messages is one important problem which has these characteristics for at t where most traffic is handled by 4ess switches the specific achieving coordination through combining joint planning and joint learning there are two major approaches to activity coordination in multiagent systems first by endowing the agents with the capability to jointly plan that is to jointly generate hypothetical activity sequences second by endowing the agents with the capability to jointly learn that is to jointly choose the actions to be executed on the basis of what they know from experience about the interdependencies of their actions this paper describes a new algorithm called jpjl joint planning and joint learning that combines both approaches the primary motivation behind this algorithm is to bring together the advantages of joint planning and joint learning while avoiding their disadvantages experimental results are provided that illustrate the potential benefits and shortcomings of the jpjl algorithm 1 motivation multiagent systems mas systems in which several interacting intelligent and autonomous entities called agents pursue some set of goals or perform some set of tasks have a multiagent framework for planning reacting and learning there are two main approaches to activity coordination in multiagent systems plan based coordination and reactive coordination each having its specic advantages and disadvantages this paper describes a framework called m dyna q that aims at combining these two approaches such that their advantages are retained while their disadvantages are avoided the key idea behind m dyna q which is a multiagent variant of a single agent framework known as dyna q is to achieve this combination through equipping the agents with the capability to learn information that is relevant to their planning and reacting activities m dyna q thus oers an integrated view of planning reacting and learning in multiagent systems keywords m dyna q multiagent systems reactive coordination plan based coordination learning 1 introduction the past years have witnessed a rapidly growing interest in multiagent systems mas that is in systems in which several interacting intelligent and autono cbr for dynamic situation assessment in an agent oriented setting in this paper we describe an approach of using case based reasoning in an agent oriented setting cbr is used in a real time environment to select actions of soccer players based on previously collected experiences encoded as cases keywords case based reasoning artificial soccer agentoriented programming 1 introduction in recent years agent oriented technologies have caught much attention both in research and commercial areas to provide a testbed for developing evaluating and testing various agent architectures robocup federation started the robot world cup initiative robocup which is an attempt to foster ai and intelligent robotics research by providing a somewhat standardized problem where wide range of technologies from ai and robotics can be integrated and examined in particular during the annual robocup championships different teams utilizing different models and methods compete with each other in the domain of soccer playing key issues in this domain are that an incremental learning algorithm with automatically derived discriminating features we propose a new technique which incrementally derive discriminating features in the input space this technique casts both classification problems class labels as outputs and regression problems numerical values as outputs into a unified regression problem the virtual labels are formed by clustering in the output space we use these virtual labels to extract discriminating features in the input space this procedure is performed recursively we organize the resulting discriminating subspace in a coarse to fine fashion and store the information in a decision tree such an incrementally hierarchical discriminating regression ihdr decision tree can be realized as a hierarchical probability distribution model we also introduce a sample size dependent negativelog likelihood nll metric to deal with large sample size cases small sample size cases and unbalanced sample size cases this is very essential since the number of training samples per class are different at each internal node of the ihdr tree we report experimental results for two types of data face image data along with comparison with some major appearance based method and decision trees hall way images with driving directions as outputs for the automatic navigation problem a regression application vision guided navigation using shoslif this paper presents an unconventional approach to vision guided autonomous navigation the system recalls information about scenes and navigational experience using content based retrieval from a visual database to achieve a high applicability to various road types we do not impose a priori scene features such as road edges that the system must use but rather the system automatically derives features from images during supervised learning to accomplish this the system uses principle component analysis and linear discriminant analysis to automatically derive the most expressive features mef for scene reconstruction or the most discriminating features mdf for scene classi cation these features best describe or classify the population of the scenes and approximate complex decision regions using piecewise linear boundaries up to a desired accuracy a new self organizing scheme called recursive partition tree rpt is used for automatic construction of a vision and control da face recognition identifying a human individual from his or her face is one of the most nonintrusive modalities in biometrics however it is also one of the most challenging ones this chapter discusses why it is challenging and the factors that a practitioner can take advantage of in developing a practical face recognition system some major existing approaches are discussed along with some algorithmic considerations a face recognition algorithm is presented as an example along with some experimental data some possible future research directions are outlined at the end of the chapter 1 1 introduction face recognition from images is a sub area of the general object recognition problem it is of particular interest in a wide variety of applications applications in law enforcement for mugshot identification verification for personal identification such as driver s licenses and credit cards gateways to limited access areas surveillance of crowd behavior are all potential applications of a succes ayllu distributed port arbitrated behavior based control distributed control of a team of mobile robots presents a number of unique challenges including highly unreliable communication real world task and safety constraints scalability dynamic reconfigurability heterogenous platforms and a lack of standardized tools or techniques similar problems plagued development of single robots applications until the behavior based revolution led to new techniques for robot control based on port arbitrated behaviors pab though there are now many implementations of systems for behavior based control of single robots the potential for distributing such control across robots for multi agent control has not until now been fully realized this paper presents ayllu a system for distributed multi robot behavioral control ayllu allows standard pab interaction message passing inhibition and suppression to take place over ip networks and extends the pab paradigm to provide for arbitrary scalability we give a brief overview of the broadcast emergent neural computational architectures based on neuroscience present approaches for computing do not have the performance flexibility and reliability of neural information processing systems in order to overcome this conventional computing systems could benefit from various characteristics of the brain such as modular organisation robustness timing and synchronisation and learning and memory storage in the central nervous system this overview incorporates some of the key research issues in the field of biologically inspired computing systems a competitive layer model for feature binding and sensory segmentation we present a recurrent neural network for feature binding and sensory segmentation the competitive layer model clm the clm uses topographically structured competitive and cooperative interactions in a layered network to partition a set of input features into salient groups the dynamics is formulated within a standard additive recurrent network with linear threshold neurons contextual relations among features are coded by pairwise compatibilities which define an energy function to be minimized by the neural dynamics due to the usage of dynamical winner take all circuits the model gains more flexible response properties than spin models of segmentation by exploiting amplitude information in the grouping process we prove analytic results on the convergence and stable attractors of the clm which generalize earlier results on winner take all networks and incorporate deterministic annealing for robustness against local minima the piecewise linear dynamics of the clm allows a linear eigensubspace analysis which we use to analyze the dynamics of binding in conjunction with annealing for the example of contour detection we show how the clm can integrate figure ground segmentation and grouping into a unified model case based and symbolic classification algorithms contrary to symbolic learning approaches that represent a learned concept explicitly case based approaches describe concepts implicitly by a pair cb sim i e by a measure of similarity sim and a set cb of cases this poses the question if there are any differences concerning the learning power of the two approaches in this article we will study the relationship between the case base the measure of similarity and the target concept of the learning process to do so we transform a simple symbolic learning algorithm the version space algorithm into an equivalent case based variant the achieved results strengthen the hypothesis of the equivalence of the learning power of symbolic and casebased methods and show the interdependency between the measure used by a case based algorithm and the target concept 1 introduction in this article we want to compare the learning power of two important learning paradigms the symbolic and the case based approach 1 4 as a first step semantic query optimization through abduction and constraint handling the use of integrity constraints to perform semantic query optimization sqo in deductive databases can be formalized in a way similar to the use of integrity constraints in abductive logic programming alp and the use of constraint handling rules in constraint logic programming clp based on this observation and on the similar role played by respectively extensional abducible and constraint predicates in sqo alp and clp we present a unified framework from which variants of sqo alp and clp can be obtained as special instances the framework relies on a proof procedure which combines backward reasoning with logic programming clauses and forward reasoning with integrity constraints 1 introduction semantic query optimization sqo in deductive databases uses implicit knowledge coded in integrity constraints ics to transform queries into new queries that are easier to evaluate and ideally contain only atoms of extensional predicates sqo sometimes allows for unsatisfiable towards multi swarm problem solving in networks this paper describes how multiple interacting swarms of adaptive mobile agents can be used to solve problems in networks the paper introduces a new architectural description for an agent that is chemically inspired and proposes chemical interaction as the principal mechanism for inter swarm communication agents within a given swarm have behavior that is inspired by the foraging activities of ants with each agent capable of simple actions and knowledge of a global goal is not assumed the creation of chemical trails is proposed as the primary mechanism used in distributed problem solving arising from self organization of swarms of agents the paper proposes that swarm chemistries can be engineered in order to apply the principal ideas of the subsumption architecture in the domain of mobile agents the paper presents applications of the new architecture in the domain of communications networks and describes the essential elements of a mobile agent framework that is being considered fo jotmail a voicemail interface that enables you to see what was said stevew julia urs research att com voicemail is a pervasive but under researched tool for workplace communication despite potential advantages of voicemail over email current phone based voicemail uis are highly problematic for users we present a novel web based voicemail interface jotmail the design was based on data from several studies of voicemail tasks and user strategies the gui has two main elements a personal annotations that serve as a visual analogue to underlying speech b automatically derived message header information we evaluated jotmail in an 8 week field trial where people used it as their only means for accessing voicemail jotmail was successful in supporting most key voicemail tasks although users electronic annotation and archiving behaviors were different from our initial predictions our results argue for the utility of a combination of annotation based indexing and automatically derived information as a general technique for accessing speech archives compiling for fast state capture of mobile agents saving transporting and restoring the state of a mobile agent is one of the main problems in implementing a mobile agents system we present an approach implemented as part of our messengers system that represents a trade off between the unrestricted use of pointers and the ability to perform fully transparent state capture when writing the code for an agent the programmer has a choice between two types of functions c functions are fully general and may use unrestricted pointers but they are not allowed to invoke any migration commands messengers functions may cause migration but their use of pointers is restricted to only a special type of a dynamic array structure under these restrictions the local variables the program counter and the calling stack of an agent can all be made machine independent and can be captured restored transparently during migration 1 introduction saving transporting and restoring the state of a mobile agent is one of the main problem in implem wsq dsq a practical approach for combined querying of databases and the web www db stanford edu we present wsq dsq pronounced wisk disk a new approach for combining the query facilities of traditional databases with existing search engines on the web wsq for web supported database queries leverages results from web searches to enhance sql queries over a relational database dsq for database supported web queries uses information stored in the database to enhance and explain web searches this paper focuses primarily on wsq describing a simple low overhead way to support wsq in a relational dbms and demonstrating the utility of wsq with a number of interesting queries and results the queries supported by wsq are enabled by two virtual tables whose tuples represent web search results generated dynamically during query execution wsq query execution may involve many high latency calls to one or more search engines during which the query processor is idle we present a lightweight technique called asynchronous iteration that can be integrated easily into a standard sequential query processor to enable concurrency between query processing and multiple web search requests asynchronous iteration has broader applications than wsq alone and it opens up many interesting query optimization issues we have developed a prototype implementation of wsq by extending a dbms with virtual tables and asynchronous iteration performance results are reported 1 dynamic modeling and learning user profile in personalized news agent finding relevant information effectively on the internet is a challenging task although the information is widely available exploring web sites and finding information relevant to a user s interest can be a time consuming and tedious task as a result many software agents have been employed to perform autonomous information gathering and filtering on behalf of the user one of the critical issues in such an agent is the capability of the agent to model its users and adapt itself over time to changing user interests in this thesis a novel scheme is proposed to learn user profile the proposed scheme is designed to handle multiple domains of long term and short term users interests simultaneously which are learned through positive and negative user feedback a 3 descriptor interest category representation approach is developed to achieve this objective using such a representation the learning algorithm is derived by imitating human personal assistants doing the same task based on experimental evaluation the scheme performs very well and adapts quickly to significant changes in user interest value added mediation in large scale information systems many information processing tasks can be part of multiple customer applications as summarizing stock prices integrating catolog information from several companies in the same line of business predicting the weather and checking on transportation resources we assign such sharable services to an active middleware layer interposed between clients and servers we define domain specific mediator modules to populate this layer such mediating services must be of value to the customers so that it will benefit their client applications to access mediators rather than the server sources directly several categories of value can be considered improvement in access and coverage improvement of content and delegation of maintenance we will define criteria for mediating modules ownership by party who assumes responsibility for the rseults of the services domain specificity to delimit the scope of such a responsibility and of course conformance with interface standards that the conceptual basis for mediation services mediator modules comprise a layer of intelligent middleware services in information systems linking data resources and application programs earlier programs that led to the concept of mediation were either constructed to support specific applications or provided extended services from databases intelligent mediators are being built now by careful domain knowledge acquisition and hand crafting the required code in this paper we present the conceptual underpinning for automating the mediation process automation does not extend to fully automatic code generation since additional knowledge is necessary to provide added value the generation concept is based on the extraction of a hierarchical domain model out of the general network representing the available resources associated with the method are domain ontologies ontologies list the terms used by the models and document their relationships these terms provide the semantic foundation needed to perform the generation this paper a systematic classification of replicated database protocols based on atomic broadcast database replication protocols based on group communication primitives have recently emerged as a promising technology to improve database faulttolerance and performance roughly speaking this approach consists in exploiting the order and atomicity properties provided by group communication primitives or more specifically atomic broadcast to guarantee transaction properties this paper proposes a systematic classification of non voting database replication algorithms based on atomic broadcast 1 on mas scalability in open dynamic multi agent environments the number of agents can vary significantly within very short periods of time very few if any current multi agent systems have however been designed to cope with large scale distributed applications scalability requires increasing numbers of new agents and resources to have no noticeable effect on performance nor to increase administrative complexity in this paper a number of implications for techniques and management are discussed current research on agent middleware is briefly described 1 on query optimization in a temporal spc algebra tuples of a temporal relation are equipped with a valid time period a simple extension of the spc selectionprojection cross product algebra for temporal relations is defined which conforms to primitives in existing temporal query languages in particular temporal projection involves coalescing of time intervals which results in non monotonic queries also the select from where normal form is no longer available in this temporal extension generalizing temporal dependencies for non temporal dimensions recently there has been a lot of interest in temporal granularity and its applications in temporal dependency theory and data mining generalization hierarchies used in multi dimensional databases and olap serve a role similar to that of time granularity in temporal databases but they also apply to non temporal dimensions like space a string based model for infinite granularities extended abstract jef wijsen universit e de mons hainaut jef wijsen umh ac be abstract in the last few years the concept of time granularity has been defined by several researchers and a glossary of time granularity concepts has been published these definitions often view a time granularity as a mostly infinite sequence of time granules although this view is conceptually clean it is extremely inefficient or even practically impossible to represent a time granularity in this manner in this paper we present a practical formalism for the finite representation of infinite granularities the formalism is string based allows symbolic reasoning and can be extended to multiple dimensions to accommodate for example space introduction in the last few years formalisms to represent and to reason about temporal and spatial granularity have been developed in several areas of computer science although several researchers have used different definitions of time granularity they comm temporal fds on complex objects ing with credit is permitted to copy otherwise to republish to post on servers or to redistribute to lists requires prior speci c permission and or a fee request permissions from publications dept acm inc fax 1 212 869 0481 or permissions acm org 2 j wijsen in reality regardless of the past and future in many applications information about the past and future is just as important as information about the present incorporating the time dimension into existing database theory and practice is interesting and important temporal databases extend classical snapshot databases by supporting the storage and the access of time related information when history is taken into account integrity constraints can place restrictions on the evolution of data in time this paper deals with extending dependency theory for temporal databases with complex objects in general classical dependencies can be applied to temporal databases in a straightforward manner from an abstr digital library resources as a basis for collaborative work the creation of large networked digital document resources has greatly facilitated information access and dissemination we suggest that such resources can further enhance how we work with information namely that they can provide a substrate that supports collaborative work we focus on one form of collaboration annotation by which we mean any of an open ended number of creative document manipulations which are useful to record and to share with others widespread digital document dissemination required technological enablers such as web clients and servers the resulting infrastructure is one in which information may be widely shared by individuals across administrative boundaries to achieve the same ubiquitous availability for annotation requires providing support for spontaneous collaboration that is for collaboration across administrative boundaries without significant prior agreements annotation is not more commonplace we suggest because the technological needs of sp introducing trusted third parties to the mobile agent paradigm abstract the mobile agent paradigm gains ever more acceptance for the creation of distributed applications particularly in the domain of electronic commerce in such applications a mobile agent roams the global internet in search of services for its owner one of the problems with this approach is that malicious service providers on the agent s itinerary can access con dential information contained in the agent or tamper with the agent in this article we identify trust as a major issue in this context and propose a pessimistic approach to trust that tries to prevent malicious behaviour rather than correcting it the approach relies on a trusted and tamper resistant hardware device that provides the mobile agent with the means to protect itself finally weshow that the approach is not limited to protecting the mobile agents of a user but can also be extended to protect the mobile agents of a trusted third party inorder to take fulladvantage of the mobile agent paradigm 1 a framework for learning adaptation knowledge based on knowledge light approaches in this paper we present a framework for learning adaptation knowledge with knowledge light approaches for case based reasoning cbr systems knowledge light means that these approaches use knowledge already acquired and represented inside the cbr system therefore we describe the sources of knowledge inside a cbr system along with the different knowledge containers next we present our framework in terms of these knowledge containers further we apply our framework to two very different knowledge light approaches for learning adaptation knowledge after that we point out some issues which should be addressed during the design or the use of such algorithms for learning adaptation knowledge from our point of view many of these issues should be the topic of further research finally we close with a short discussion 1 introduction one of the major challenges during designing a case based reasoning cbr system is the modeling of appropriate adaptation knowledge usually adaptati searchable words on the web in designing data structures for text databases it is valuable to know how many different words are likely to be encountered in a particular collection for example vocabulary accumulation is central to index construction for text database systems it is useful to be able to estimate the space requirements and performance characteristics of the main memory data structures used for this task however it is not clear how many distinct words will be found in a text collection or whether new words will continue to appear after inspecting large volumes of data we propose practical definitions of a word and investigate new word occurrences under these models in a large text collection we inspected around two billion word occurrences in 45 gigabytes of world wide web documents and found just over 9 74 million different words in 5 5 million documents overall 1 word in 200 was new we observe that new words continue to occur even in very large data sets and that choosing stricter definitions of what constitutes a word has only limited impact on the number of new words found approximating discrete probability distributions with bayesian networks i generalise the arguments of chow liu 1968 to show that a bayesian network satisfying some arbitrary constraint that best approximates a probability distribution is one for which mutual information weight is maximised i give a practical procedure for nding an approximation network and evaluate its application on a range of data sets articial intelligence requires the ability to reach conclusions that may be far from certain for example an expert system for medical diagnosis may be given the symptoms of some patient and asked to provide a diagnosis even though the background knowledge and symptom information may not be enough to determine for sure which problem actually besets the patient probability theory provides a plausible model for reasoning under uncertainty since one would expect a diagnosis to be relatively probable given the symptoms this paper addresses practical issues to do with the implementation of probabilistic reasoning the plan is rst to discuss foundations for bayesian networks bayesian networks are normally given one of two types of foundations they are either treated purely formally as an abstract way of representing probability functions or they are interpreted with some causal interpretation given to the graph in a network and some standard interpretation of probability given to the probabilities specified in the network in this chapter i argue that current foundations are problematic and put forward new foundations which involve aspects of both the interpreted and the formal approaches one standard approach is to interpret a bayesian network objectively the graph in a bayesian network represents causality in the world and the specified probabilities are objective empirical probabilities such an interpretation founders when the bayesian network independence assumption often called the causal markov condition fails to hold in 2 i catalogue the occasions when the independence assumption fails and show that such failures are pervasive next in 3 i show that even where the independence assumption does hold objectively an agent s causal knowledge is unlikely to satisfy the assumption with respect to her subjective probabilities and that slight differences between an agent s subjective bayesian network and an objective bayesian network can lead to large differences between probability distributions determined by these networks to overcome these difficulties i put forward logical bayesian foundations in 5 i show that if the graph and probability specification in a bayesian network are thought of as an agent s background knowledge then the agent is most rational if she adopts the probability distribution determined by the random probability functions random probability functions are required in monte carlo simulations for expert systems testing here i give an empirical method for generating such functions while this is a practical problem its solution raises interesting philosophical questions the design of expert systems is an important problem in the field of artificial intelligence one task for instance is to give a general methodology for diagnosis which can be used to construct computer experts in a whole range of areas such as the diagnosis of hepatitis and fault finding in circuit boards clearly before these artificial experts can be used to make critical decisions they must be thoroughly tested and their reliability ascertained two approaches to testing stand out empirical experiments and monte carlo simulations empirical testing is certainly desirable but there may not be enough empirical data readily available to ascertain the reliability of a particular diagnostic expert system one often requires a considerab an open software infrastructure for reconfigurable control systems recent advances in software technology have the potential to revolutionize control system design this paper describes a new software infrastructure for complex control systems which exploits new and emerging software technologies it presents an open control platform ocp for complex systems including those that must be reconfigured or customized in real time for extreme performance applications an application of the ocp to the control system design of an autonomous aerial vehicle is described 1 introduction complex dynamic systems such as aircraft power systems and telecommunications networks present major challenges to control systems designers both the military and civilian sectors of our economy are demanding new and highly sophisticated capabilities from these systems that the current state of the art is not offering among them are the following ability to adapt to a changing environment ability to reconfigure the control algorithms plug and play exten an open platform for reconfigurable control bility openness reconfigurability and component interchangeability require software architectures that are flexible and that support tools and algorithms from a variety of sources and domains this requires a shift away from traditional control system implementation which tends to be practiced with a particular apjune 2001 ieee control systems magazine 49 by linda wills suresh kannan sam sander murat guler bonnie heck j v r prasad daniel schrage and george vachtsevanos wills linda wills ece gatech edu sander guler heck and vachtsevanos are with the school of electrical and computer engineering georgia institute of technology atlanta ga 30332 0250 u s a kannan prasad and schrage are with the school of aerospace engineering georgia institute of technology atlanta ga 30332 0250 u s a 1998 corbis corp 0272 1708 01 10 002001ieee plication in mind and which makes rigid a framework for linking distributed simulations using software agents this paper presents the basic ideas behind the use of software agent technology for distributed simulation and data assimilation a software agent is an autonomous computer program that operates on behalf of someone or something a mobile agent has the ability to migrate during execution from machine to machine in a heterogeneous network while a stationary agent executes only on the system on which it began execution flexible and scalable digital library search in this report the development of a specialised search engine for a digital library is described the proposed system architecture consists of three levels the conceptual the logical and the physical level the conceptual level schema enables by its exposure of a domain specific schema semantically rich conceptual search the logical level provides a description language to achieve a high degree of flexibility for multimedia retrieval the physical level takes care of scalable and e cient persistent data storage the role played by each level changes during the various stages of a search engine s lifecycle 1 modeling the index 2 populating and maintaining the index and 3 querying the index the integration of all this functionality allows the combination of both conceptual and content based querying in the query stage a search engine for the australian open tennis tournament website is used as a running example which shows the power of the complete architecture and its various components simplifying the development of intelligent agents intelligent agents is a powerful artificial intelligence technology which shows considerable promise as a new paradigm for mainstream software development however despite their promise intelligent agents are still scarce in the market place a key reason for this is that developing intelligent agent software requires significant training and skill a typical developer or undergraduate struggles to develop good agent systems using the belief desire intention bdi model or similar models this paper identifies the concept set which we have found to be important in developing intelligent agent systems and the relationships between these concepts this concept set was developed with the intention of being clearer simpler and easier to use than current approaches we also describe briefly a very simplified example from one of the projects we have worked on roborescue illustrating the way in which these concepts are important in designing and developing intelligent software agents keywords ai architectures distributed ai multiagent systems reactive control software agents 1 ambient displays turning architectural space into an interface between people and digital information we envision that the architectural space we inhabit will be a new form of interface between humans and online digital information this paper discusses ambient displays a new approach to interfacing people with online digital information ambient displays present information within a space through subtle changes in light sound and movement which can be processed in the background of awareness we describe the design and implementation of two example ambient displays the ambientroom and ambient fixtures additionally we discuss applications of ambient displays and propose theories of design of such interfaces based on our initial experiences 1 introduction ambient am bi ent a surrounding encircling encompassing and environing oxford english dictionary display dis play n an opening or unfolding exhibition manifestation webster s revised unabridged dictionary 1913 nature is filled with subtle beautiful and expressive ambient displays that engage building a public digital library based on full text retrieval digital libraries are expensive to create and maintain and generally restricted to a particular corporation or group of paying subscribers while many indexes to the world wide web are freely available the quality of what is indexed is extremely uneven the digital analog of a public library a reliable quality community service has yet to appear this paper demonstrates the feasibility of a cost effective collection of high quality public domain information available free over the internet one obstacle to the creation of a digital library is the difficulty of providing formal cataloguing information without a title author and subject database it seems hard to offer the searching facilities normally available in physical libraries full text retrieval provides a way of approximating these services without a concomitant investment of resources a second is the problem of finding a suitable corpus of material computer science research reports form the focus of ou power to the people end user building of digital library collections naturally digital library systems focus principally on the reader the consumer of the material that constitutes the library in contrast this paper describes an interface that makes it easy for people to build their own library collections collections may be built and served locally from the user s own web server or given appropriate permissions remotely on a shared digital library host end users can easily build new collections styled after existing ones from material on the web or from their local files or both and collections can be updated and new ones brought on line at any time the interface which is intended for non professional end users is modeled after widely used commercial software installation packages lest one quail at the prospect of end users building their own collections on a shared system we also describe an interface for the administrative user who is responsible for maintaining a digital library installation greenstone a comprehensive open source digital library software system this paper describes the greenstone digital library software a comprehensive open source system for the construction and presentation of information collections collections built with greenstone offer effective full text searching and metadata based browsing facilities that are attractive and easy to use moreover they are easily maintainable and can be augmented and rebuilt entirely automatically the system is extensible software plugins accommodate different document and metadata types network management by knowledge distribution using mobile agents d to each other functionality provided by the different agents and managers are located at given places in the system such a system may have difficulties coping with a highly dynamic network environment by introducing mechanisms for moving programcode a more flexible system can be designed a specific part e g an agent having a certain set of abilities can be moved or move itself closer to or even into the ne in need of management bandwidth can be saved and expert agents can be utilised autonomous moving program code is known as mobile agents 3 3 collective behavior when designing agents for complex problem solving you can either create a few highly advanced agents 2 or try to divide and distribute the problem solving task between a number of less complex agents mother nature can give several examples of successful implementations based on the latter strategy an ant colony is such an example where a high number of less inte expert system technology in observing tools over the past two years the scientist s expert assistant team from nasa s goddard space flight center and the space telescope science institute has been prototyping tools to support general observer proposal development for the hubble space telescope and the next generation space telescope one aspect of this effort has been the exploration of the use of expert systems in guiding the user in preparing their observing program the initial goal was to provide the user with a question and answer style of interaction where the software would interview the user for their science needs and recommend instrument settings this design ultimately failed the reasons for this failure and the resulting evolution of our approach are an interesting case study in the use of expert system technology for observing tools although the interview approach failed we felt that expert systems can still be used in the tools environment this paper describes our current approach to the use of expert syste general principles of learning based multi agent systems we consider the problem of how to design large decentralized multi agent systems mas s in an automated fashion with little or no hand tuning our approach has each agent run a reinforcement learning algorithm this converts the problem into one of how to automatically set update the reward functions for each of the agents so that the global goal is achieved in particular we do not want the agents to work at cross purposes as far as the global goal is concerned we use the term artificial collective intelligence coin to refer to systems that embody solutions to this problem in this paper we present a summary of a mathematical framework for coins we then investigate the real world applicability of the core concepts of that framework via two computer experiments we show that our coins perform near optimally in a difficult variant of arthur s bar problem 1 and in particular avoid the tragedy of the commons for that problem and we also illustrate optimal performance for our coins in the leader follower problem 1 design of rapidbase an active measurement database system in data intensive industrial on line applications utilizing live process data one faces an unusual set of database requirements the process measurement data need to be acquired at great speed organized in time series and made available for time based retrieval active capabilities and functional extensibility are needed to implement a flexible data driven processing paradigm an efficient transaction logging and recovery mechanism is needed in order not to impede the data acquisition flow rapidbase is a system that meets these requirements it utilizes a main memory database a unique temporal relational data model for handling time series and an elaborate trigger subsystem it is implemented as a server program equipped with interfaces of high power of expression 1 introduction although the requirement for data management is omnipresent in various advanced applications the main stream notion of a database may be not a best choice in all cases certain application classes requ fuzzy triggers incorporating imprecise reasoning into active databases traditional event condition action triggers active database rules include a boolean predicate as a trigger condition we propose fuzzy triggers whereby fuzzy inference is utilized in the condition evaluation this way approximate reasoning may be integrated with a traditional crisp database the new approach paves the way for intuitive expression of application semantics of imprecise nature in database bound applications two fuzzy triggers models are proposed firstly a set of fuzzy rules is encapsulated into a boolean valued function called a rule set function leading to the c fuzzy trigger model subsequently actions are expressed also in fuzzy terms and the corresponding ca fuzzy trigger model is proposed examples are provided to illustrate how fuzzy triggers can be applied to a real life drive control system in an industrial installation 1 introduction there has been considerable interest in active database rules called triggers in commercial applications and in th spatio temporal representation and reasoning based on rcc 8 this paper is to introduce a hierarchy of languages intended for qualitative spatio temporal representation and reasoning provide these languages with topological temporal semantics construct effective reasoning algorithms and estimate their computational complexity contextual rules for text analysis in this paper we describe a rule based formalism for the analysis and labelling of texts segments the rules are contextual rewriting rules with a restricted form of negation they allow to underspecify text segments not considered relevant to a given task and to base decisions upon context a parser for these rules is presented and consistence and completeness issues are discussed some results of an implementation of this parser with a set of rules oriented to the segmentation of texts in propositions are shown an overview of the multiagent systems engineering methodology to solve complex problems agents work cooperatively with other agents in heterogeneous environments we are interested in coordinating the local behavior of individual agents to provide an appropriate system level behavior the use of intelligent agents provides an even greater amount of flexibility to the ability and configuration of the system itself with these new intricacies software development is becoming increasingly difficult therefore it is critical that our processes for building the inherently complex distributed software that must run in this environment be adequate for the task this paper introduces a methodology for designing these systems of interacting agents 1 computationally grounded theories of agency in this paper i motivate define and illustrate the notion of computationally grounded theories of agency a theory of agency is said to be computationally grounded if we can give the theory an interpretation in terms of some concrete computational model this requirement is essential if we are to claim that the theories we develop can be understood as expressing properties of real multiagent systems after introducing and formally defining the concept of a computationally grounded theory of agency i illustrate the idea with reference to vsk logic a formalism for reasoning about agent systems that has a semantics defined with respect to an automata like model of agents vsk logic is an extension of modal epistemic logic which allows us to represent what information is visible to an agent what it sees and what it knows we are able to prove that formulae of vsk logic correspond directly to properties of agents 1 introduction artificial intelligence ai is a broad church whic model checking multi agent systems with mable mable is a language for the design and automatic verification of multi agent systems mable is essentially a conventional imperative programming language enriched by constructs from the agent oriented programming paradigm a mable system contains a number of agents programmed using the mable imperative programming language agents in mable have a mental state consisting of beliefs desires and intentions agents communicate using request and inform performatives in the style of the fipa agent communication language mable systems may be augmented by the addition of formal claims about the system expressed using a quantified linear temporal belief desire intention logic mable has been fully implemented and makes use of the spin model checker to automatically verify the truth or falsity of claims a logic of bdi agents with procedural knowledge in this paper we present a new logic for specifying the behaviour of multi agent systems in this logic agents are viewed as bdi systems in that their state is characterised in terms of beliefs desires and intentions the semantics of the bdi component of the logic are based on the wellknown system of rao and georgeff in addition agents have available to them a library of plans representing their know how procedural knowledge about how to achieve their intentions these plans are in effect programs that specify how a group of agents can work in parallel to achieve certain ends the logic provides a rich set of constructs for describing the structure and execution of plans some properties of the logic are investigated in particular those relating to plans and some comments on future work are presented 1 introduction there is currently much international interest in computer systems that go under the banner of intelligent agents 17 crudely an intelligent agent i practical reasoning with procedural knowledge a logic of bdi agents with know how in this paper we present a new logic for specifying the behaviour of multi agent systems in this logic agents are viewed as bdi systems in that their state is characterised in terms of beliefs desires and intentions the semantics of the bdi component of the logic are based on the well known system of rao and georgeff in addition agents have available to them a library of plans representing their know how procedural knowledge about how to achieve their intentions these plans are in effect programs that specify how a group of agents can work in parallel to achieve certain ends the logic provides a rich set of constructs for describing the structure and execution of plans some properties of the logic are investigated in particular those relating to plans and some comments on future work are presented 1 introduction there is currently much international interest in computer systems that go under the banner of intelligent agents 16 crudely an intel intention reconsideration reconsidered abstract in this paper we consider the issue of designing agents that successfully balance the amount of time spent in reconsidering their intentions against the amount of time spent acting to achieve them following a brief review of the various ways in which this problem has previously been analysed we motivate and introduce a simple formal model of agents which is closely related to the well known belief desire intention model in this model an agent is explicitly equipped with mechanisms for deliberation and action selection as well as a meta level control function which allows the agent to choose between deliberation and action using the formal model we define what it means for an agent to be optimal with respect to a task environment and explore how various properties of an agent s task environment can impose certain requirements on its deliberation and meta level control components we then show how the model can capture a number of interesting practical reasoning scenarios and illustrate how our notion of meta level control can easily be extended to encompass higherorder meta level reasoning we conclude with a discussion and pointers to future work 1 a tableau based proof method for temporal logics of knowledge and belief in this paper we define two logics kln and bln and present tableau based decision procedures for both kln is a temporal logic of knowledge thus in addition to the usual connectives of linear discrete temporal logic it contains a set of unary modal connectives for representing the knowledge possessed by agents the logic bln is somewhat similar it is a temporal logic that contains connectives for representing the beliefs of agents in addition to a complete formal definition of the two logics and their decision procedures the paper includes a brief review of their applications in ai and mainstream computer science correctness proofs for the decision procedures a number of worked examples illustrating the decision procedures and some pointers to further work keywords temporal logics of knowledge and belief theorem proving tableau 1 introduction this paper presents two logics called kln and bln respectively and gives tableau based decision procedures for both the l a methodology for agent oriented analysis and design this article presents gaia a methodology for agent oriented analysis and design the gaia methodology is both general in that it is applicable to a wide range of multi agent systems and comprehensive in that it deals with both the macro level societal and the micro level agent aspects of systems gaia is founded on the view of a multi agent system as a computational organisation consisting of various interacting roles we illustrate gaia through a case study an agent based business process management system 1 introduction progress in software engineering over the past two decades has been made through the development of increasingly powerful and natural high level abstractions with which to model and develop complex systems procedural abstraction abstract data types and most recently objects and components are all examples of such abstractions it is our belief that agents represent a similar advance in abstraction they may be used by software developers to more n analyzing relational databases using rough set based methods one of the most important problems in kdd applications is a size of real world databases in practical problems data may contain millions of records in many data tables bounded by relations on the other hand most of theoretical works and practical applications concentrate on relatively small data sets collected in single tables the paper proposes a new approach to the problem of analysis of relational databases the proposed methodology woks in adaptive way if a considered data table is not sufficient to create satisfactory set of rules new attributes generated by arithmetical operations or based on database relations are added to information system an adaptive self organizing color segmentation algorithm with application to robust real time human hand localization in proc asian conf on computer vision taiwan 2000 this paper describes an adaptive self organizing color segmentation algorithm and a transductive learning algorithm used to localize human hand in video sequences the color distribution at each time frame is approximated by the proposed 1 d self organizing map som in which schemes of growing pruning and merging are facilitated to find an appropriate number of color cluster automatically due to the dynamic backgrounds and changing lighting conditions the distribution of color over time may not be stationary an algorithm of som transduction is proposed to learn the nonstationary color distribution in hsi color space by combining supervised and unsupervised learning paradigms color cue and motion cue are integrated in the localization system in which motion cue is employed to focus the attention of the system this approach is also applied to other tasks such as human face tracking and color indexing our localization system discriminant em algorithm with application to image retrieval in many vision applications the practice of supervised learning faces several difficulties one of which is that insufficient labeled training data result in poor generalization in image retrieval we have very few labeled images from query and relevance feedback so that it is hard to automatically weight image features and select similarity metrics for image classification this paper investigates the possibility of including an unlabeled data set to make up the insufficiency of labeled data different from most current research in image retrieval the proposed approach tries to cast image retrieval as a transductive learning problem in which the generalization of an image classifier is only defined on a set of images such as the given image database formulating this transductive problem in a probabilistic framework the proposed algorithm discriminant em d em not only estimates the parameters of a generative model but also finds a linear transformation to relax the assumption of pro self supervised learning for visual tracking and recognition of human hand due to the large variation and richness of visual inputs statistical learning gets more and more concerned in the practice of visual processing such as visual tracking and recognition statistical models can be trained from a large set of training data however in many cases since it is not trivial to obtain a large labeled and representative training data set it would be difficult to obtain a satisfactory generalization another difficulty is how to automatically select good features for representation by combining both labeled and unlabeled training data this paper proposes a new learning paradigm selfsupervised learning to investigate the issues of learning bootstrapping and model transduction inductive learning and transductive learning are the two main cases of self supervised learning in which the proposed algorithm discriminant em d em is a specific learning technique vision based gesture capturing natural hand articulation vision based motion capturing of hand articulation is a challenging task since the hand presents a motion of high degrees of freedom model based approaches could be taken to approach this problem by searching in a high dimensional hand state space and matching projections of a hand model and image observations however it is highly inefficient due to the curse of dimensionality fortunately natural hand articulation is highly constrained which largely reduces the dimensionality of hand state space this paper presents a model based method to capture hand articulation by learning hand natural constraints our study shows that natural hand articulation lies in a lower dimensional configurations space characterized by a union of linear manifolds spanned by a set of basis configurations by integrating hand motion constraints an efficient articulated motion capturing algorithm is proposed based on sequential monte carlo techniques our experiments show that this algorithm is robust and accurate for tracking natural hand movements this algorithm is easy to extend to other articulated motion capturing tasks towards a highly scalable and effective metasearch engine a metasearch engine is a system that supports unified access to multiple local search engines database selection is one of the main challenges in building a large scale metasearch engine the problem is to efficiently and accurately determine a small number of potentially useful local search engines to invoke for each user query in order to enable accurate selection metadata that reect the contents of each search engine need to be collected and used in this paper we propose a highly scalable and accurate database selection method this method has several novel features first the metadata for representing the contents of all search engines are organized into a single integrated representative such a representative yields both computation efficiency and storage efficiency second our selection method is based on a theory for ranking search engines optimally experimental results indicate that this new method is very effective an operational prototype system has been built based on the proposed approach first order polynomial based theorem proving introduction the boolean ring or first order polynomial based theorem proving began with the work of hsiang 1982 1985 hsiang extended the idea of using boolean polynomials to represent propositional formulae to the case of first order predicate calculus based on the completion procedure of knuth and bendix 1970 the n strategy was proposed later on by imitating the framework of buchberger s algorithm to compute the grobner bases of polynomial ideals buchberger 1985 kapur and narendran 1985 developed another approach which is also referred to as the grobner basis method one obvious advantage of using boolean polynomials is that every propositional formula has a unique representation and sometimes it is easy to be generalized to some non classical logic systems chazarain et al 1991 wu and tan 1994 stimulated by them some approaches and results have been reported bachmair and dershowitz 1987 dietrich 1986 kapur and zhang 1989 wu and liu 1998 zhang 198 xml declarative description a language for the semantic web this document you agree to all provisions of the copyright laws protecting it a web resource s semantics it employs xml as its bare syntax and enhances xml expressive power by employing declarative description theory 8 a description in xdd is a set of ordinary xml elements extended xml elements with variables and the xml elements relationships in terms of xml clauses an ordinary xml element denotes a semantic unit and is a surrogate of an information item in the real application domain an extended xml element represents implicit information or a set of semantic units clauses express rules conditional relationships integrity constraints and ontological axioms we define the precise and formal semantics of an xdd description as a set of ordinary xml elements without employing other formalisms important axioms that are missing in xml and rdf but expressible in xdd include symmetry composition of and inverse relations as an example of the inverse relation axiom consider the xml clauses a and b in figure 1a they model the creator and pubray8 xw properties inverses assumed by some particular domain figure 1b then gives an example of representing an rdf statement c a creator of a document entitled xdd language is john the semantics of an xdd description which comprises the clauses a b and c will also contain an rdf statement a publication of john is xdd language figure 1c hence allowing inverse inference of such implicit information obviously this axiom cannot be represented in rdf xdd can directly represent all xmlbased application markup languages it also can simply represent xml applications that provide common conventions of semantics syntax and structures for certain specific domains in addition to rdf these domai comprehensive hardware and software support for operating systems to exploit mp memory hierarchies abstract high performance multiprocessor workstations are becoming increasingly popular since many of the workloads running on these machines are operating system intensive we are interested in exploring the types of support for the operating system that the memory hierarchy of these machines should provide in this paper we evaluate a comprehensive set of hardware and software supports that minimize the performance losses for the operating system in a sophisticated cache hierarchy these supports selected from recent papers are code layout optimization guarded sequential instruction prefetching instruction stream buffers support for block operations support for coherence activity and software data prefetching we evaluate these supports under a simulated environment we show that they have a largely complementary impact and that when combined speed up the operating system by an average of 40 percent finally a cost performance comparison of these schemes suggests that the most cost effective ones are code layout optimization and block operation support while the least cost effective one is software data prefetching index terms cache hierarchies shared memory multiprocessors architectural support for operating system prefetching tracedriven simulations performance block operations 1 locality in search engine queries and its implications for caching caching is a popular technique for reducing both server load and user response time in distributed systems in this paper we are interested in the question of whether caching might be effective for search engines as well we study two real search engine traces by examining query locality and its implications for caching our trace analysis results show that 1 queries have significant locality with query frequency following a zipf distribution very popular queries are shared among different users and can be cached at servers or proxies while 16 to 22 of the queries are from the same users and should be cached at the user side multiple word queries are shared less and should be cached mainly at the user side 2 if caching is to be done at the user side short term caching for hours will be enough to cover query temporal locality while server proxy caching should be based on longer periods such as days 3 most users have small lexicons when submitting queries frequent users who submit many search requests tend to reuse a small subset of words to form queries thus with proxy or user side caching prefetching based on user lexicon looks promising maintaining horizontally partitioned warehouse views data warehouses usually store large amounts of information representing an integration of base data from different data sources over a long time period aggregate views can be stored as a set of its horizontal fragments for the purposes of reducing warehouse query response time and maintenance cost this paper proposes a scheme that efficiently maintains horizontally partitioned data warehouse views using the proposed scheme only one view fragment holding the relevant subset of tuples of the view is accessed for each update the scheme also includes an approach to reduce the refresh time for maintaining views that compute aggregate functions min and max keywords data warehouse applications view maintenance horizontal partitioning performance improvement 1 an agent based petri net model with application to seller buyer design in electronic commerce agents are becoming one of the most important topics in distributed and autonomous decentralized systems ads and there are increasing attempts to use agent technologies to develop software systems in electronic commerce such systems are complex and there is a pressing need for system modeling techniques to support reliable maintainable and extensible design g nets are a type of petri net defined to support modeling of a system as a set of independent and loosely coupled modules in this paper we first introduce an extension of g net agent based g net as a generic model for agent design then new communication mechanisms are introduced to support asynchronous message passing among agents to illustrate our formal modeling technique is effective for agent modeling in electronic commerce a pricenegotiation protocol example between buyers and sellers is provided finally by analyzing an ordinary petri net reduced from our agent based g net models we conclude that our agent based g net models are l3 live concurrent and effective for agent communications 1 modeling and verifying multi agent behaviors using predicate transition nets in a multi agent system how agents accomplish a goal task is usually specified by multi agent plans built from basic actions e g operators of which the agents are capable a critical problem with such an approach is how can the designer make sure the plans are reliable to tackle this problem this paper presents a formal approach for modeling and analyzing multi agent behaviors using predicate transition prt nets a high level formalism of petri nets we construct a multi agent model by representing agent capabilities as transitions to verify a multiagent prt model we adapt the planning graphs as a compact structure for the reachability analysis we also demonstrate that based on the prt model whether parallel actions specified in multi agent plans can be executed in parallel and whether the plans guarantee the achievement of the goal can be verified by analyzing the dependency relations among the transitions a neural network model for monotonic diagnostic problem solving the task of diagnosis is to find a hypothesis that best explains a set of manifestations observations generally it is computationally expensive to find a hypothesis because the number of the potential hypotheses is exponentially large recently many efforts have been made to find parallel processing methods to solve the above difficulty in this paper we propose a neural network model for diagnostic problem solving where a diagnostic problem is treated as a combinatorial optimisation problem one feature of the model is that the causal network is directly used as the network another feature is that the errors between the observations and the current activations of manifestation nodes are used to guide the network computing for finding optimal diagnostic hypotheses 1 introduction for a set of manifestations observations the diagnostic inference is to find the most plausible faults or disorders which can explain why the manifestations are present in general an individual d a variable depth search algorithm for the generalized assignment problem a variable depth search procedure abbreviated as vds is a generalization of the local search method which was rst successfully applied by lin and kernighan to the traveling salesman problem and the graph partitioning problem the main idea is to adaptively change the size of neighborhood so that it can eectively traverse larger search space while keeping the amount of computational time reasonable in this paper we propose a heuristic algorithm based on vds for the generalized assignment problem which is one of the representative combinatorial optimization problems known to be np hard to the authors knowledge most of the previously proposed algorithms with some exceptions conduct the search within the feasible region however there are instances for which the search within feasible region is not advantageous because the feasible region is very small or is combinatorially complicated to search therefore we allow in our algorithm to search into the infeasible region as w improving text categorization methods for event tracking automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small the majority of training documents are unlabelled and most of the events have a short duration in time we adapted several supervised text categorization methods specifically several new variants of the k nearest neighbor knn algorithm and a rocchio approach to track events all of these methods showed significant improvement up to 71 reduction in weighted error rates over the performance of the original knn algorithm on tdt benchmark collections making knn among the top performing systems in the recent tdt3 official evaluation furthermore by combining these methods we significantly reduced the variance in performance of our event tracking system over different a shopping agent that automatically constructs wrappers for semi structured online vendors this paper proposes a shopping agent with a robust inductive learning method that automatically constructs wrappers for semistructured online stores strong biases assumed in many existing systems are weakened so that the real stores with reasonably complex document structures can be handled our method treats a logical line as a basic unit and recognizes the position and the structure of product descriptions by finding the most frequent pattern from the sequence of logical line information in output html pages this method is capable of analyzing product descriptions that comprise multiple logical lines and even those with extra or missing attributes experimental tests on over 60 sites show that it successfully constructs correct wrappers for most real stores 1 introduction a shopping agent is a mediator system that extracts the product descriptions from several online stores on a user s behalf since the stores are heterogeneous a procedure for extracting the cont temporal view self maintenance view self maintenance refers to maintaining materialized views without accessing base data self maintenance is particularly useful in data warehousing settings where base data comes from sources that may be inaccessible selfmaintenance has been studied for nontemporal views but is even more important when a warehouse stores temporal views over the history of source data since the source history needed to perform view maintenance may no longer exist this paper tackles the self maintenance problem for temporal views we show how to derive auxiliary data to be stored at the warehouse so that the warehouse views and auxiliary data can be maintained without accessing the sources the temporal view self maintenance problem is considerably harder than the nontemporal case because a temporal view may need to be maintained not only when source data is modified but also as time advances and these two dimensions of change interact in subtle ways we also seek to minimize the amount of au pms a pvc management system for atm networks reported in this paper is the developed pms a pvc management system for atm networks pms provides a scalable end to end path management solution required for managing todays complex atm networks it aims to assist the network operators to perform pvc operations with simplified procedures and automatic optimum route selection it also aims to provide effective decision making support for pvc fault identification and prevention to the network operators 1 introduction atm communication network is playing more and more important role in todays telecommunication networks it has been widely used in backbone networks transmission networks access networks and even enterprise networks such emerging large heterogeneous atm networks have raised many new challenges for researchers and developers in the area of network management in the management of atm communication networks that have increased dramatically in size and complexity the pvc permanent virtual circuit managemen extraction and classification of visual motion patterns for hand gesture recognition we present a new method for extracting and classifying motion patterns to recognize hand gestures first motion segmentation of the image sequence is generated based on a multiscale transform and attributed graph matching of regions across frames this produces region correspondences and their affine transformations second color information of motion regions is used to determine skin regions third human head and palm regions are identified based on the shape and size of skin areas in motion finally affine transformations defining a region s motion between successive frames are concatenated to construct the region s motion trajectory gestural motion trajectories are then classified by a time delay neural network trained with backpropagation learning algorithm our experimental results show that hand gestures can be recognized well using motion patterns 1 introduction this paper is concerned with the problem of detecting two dimensional motion across image frames and classifyi feature subset selection using a genetic algorithm practical pattern classification and knowledge discovery problems require selection of a subset of attributes or features from a much larger set to represent the patterns to be classified this paper presents an approach to the multi criteria optimization problem of feature subset selection using a genetic algorithm our experiments demonstrate the feasibility of this approach for feature subset selection in the automated design of neural networks for pattern classification and knowledge discovery 1 introduction many practical pattern classification tasks e g medical diagnosis require learning of an appropriate classification function that assigns a given input pattern typically represented using a vector of attribute or feature values to one of a finite set of classes the choice of features attributes or measurements used to represent patterns that are presented to a classifier affect among other things ffl the accuracy of the classification function that can be learn mixtures of linear subspaces for face detection we present two methods using mixtures of linear subspaces for face detection in gray level images one method uses a mixture of factor analyzers to concurrently perform clustering and within each cluster perform local dimensionality reduction the parameters of the mixture model are estimated using an em algorithm a face is detected if the probability of an input sample is above a predened threshold the other mixture of subspaces method uses kohonen s self organizing map for clustering and fisher linear discriminant to nd an optimal projection and a gaussian distribution to model the class conditional density function of the projected samples for each class the parameters of the class conditional density functions are maximum likelihood estimates and the decision rule is also based on maximum likelihood a wide range of face images including ones in dierent poses with dierent expressions and under dierent lighting conditions are used as the training set to capture the varia scenario customization for information extraction information extraction ie is an emerging nlp technology whose function is to process unstructured natural language text to locate specific pieces of information or facts in the text and to use these facts to fill a database ie systems today are commonly based on pattern matching the core ie engine uses a cascade of sets of patterns of increasing linguistic complexity each pattern consists of a regular expression and an associated mapping from syntactic to logical form the pattern sets are customized for each new topic as defined by the set of facts to be extracted construction of a pattern base for a new topic is recognized as a time consuming and expensive process a principal roadblock to wider use of ie technology in the large an e ective pattern base must be precise and have wide coverage this thesis addresses the portability probl making use of population information in evolutionary artificial neural networks this paper is concerned with the simultaneous evolution of artificial neural network ann architectures and weights the current practice in evolving ann s is to choose the best ann in the last generation as the final result this paper proposes a different approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population this approach regards a population of ann s as an ensemble and uses a combination method to integrate them although there has been some work on integrating ann modules 2 3 little has been done in evolutionary learning to make best use of its population information four linear combination methods have been investigated in this paper to illustrate our ideas three real world data sets have been used in our experimental studies which show that the recursive least square rls algorithm always produces an integrated system that outperforms the best individual the results confirm that a population contains more information than a single individual evolutionary learning should exploit such information to improve generalization of learned systems memory hierarchies as a metaphor for academic library collections research libraries and their collections are a cornerstone of the academic tradition representing 2000 years of development of the western civilization they make written history widely accessible at low cost computer memories are a range of physical devices used for storing digital information that have undergone much formal study over 40 years and are well understood this paper draws parallels between the organisation of research collections and computer memories in particular examining their hierarchical structure and examines the implication for digital libraries cast collaborative agents for simulating teamwork psychological studies on teamwork have shown that an effective team often can anticipate information needs of teammates based on a shared mental model existing multi agent models for teamwork are limited in their ability to support proactive information exchange among teammates to address this issue we have developed and implemented a multi agent architecture called cast that simulates teamwork and supports proactive information exchange in a dynamic environment we present a formal model for proactive information exchange knowledge regarding the structure and process of a team is described in a language called mallet beliefs about shared team processes and their states are represented using petri nets based on this model cast agents offer information proactively to those who might need it using an algorithm called diarg empirical evaluations using a multi agent synthetic testbed application indicate that cast enhances the effectiveness of teamwork among agents without sacrificing a high cost for communications 1 architecture centric object oriented design method for multi agent systems this paper introduces an architecture centric object oriented design method for mas multi agent systems using the extended uml unified modeling language the uml extension is based on design principles that are derived from characteristics of mas and concept of software architecture which helps to design reusable and wellstructured multi agent architecture the extension allows one to use original objectoriented method without syntactic or semantic changes which implies the preservation of oo productivity i e the availability of developers and tools the utilization of past experiences and knowledge and the seamless integration with other systems keywords multi agent systems architecture object oriented development methods 1 introduction software agents provide a new way of analyzing designing and implementing complex software systems currently agent technology is used in wide variety of applications with range from comparatively small systems such as a knowledge based approach for designing intelligent team training systems this paper presents a knowledge approach to designing team training systems using intelligent agents we envision a computer based training system in which teams are trained by putting them through scenarios which allow them to practice their team skills there are two important roles that intelligent agents can play these are virtual team members and tutors to carry out these functions these agents must be equipped with an understanding of the task domain the team structure the selected decision making process and their beliefs about other team members mental states even though existing agent teamwork models incorporate many of the elements listed above they have not focused on analyzing information needs of team members to support proactive agent interactions to encode the team knowledge we have developed a representation language based on the bdi model called mallet a petri net model of an individual agent s plans and information needs can be derived from the role des secure multi agent dynamic programming based on homomorphic encryption and its application to combinatorial auctions this paper presents a secure dynamic programming protocol that utilizes homomorphic encryption by using this method multiple agents can solve a combinatorial optimization problem among them without leaking their private information more specifically in this method multiple servers cooperatively perform dynamic programming procedures for solving a combinatorial optimization problem by using the private information sent from agents as inputs although the severs can compute the optimal solution correctly the inputs are kept secret even from the servers cooperative coevolution of multi agent systems in certain tasks such as pursuit and evasion multiple agents need to coordinate their behavior to achieve a common goal an interesting question is how can such behavior best be evolved when the agents are controlled with neural networks a powerful method is to coevolve them in separate subpopulations and test together in the common task in this paper such a method called multi agent esp enforced subpopulations is presented and demonstrated in a prey capture task the approach is shown more efficient and robust than evolving a single central controller for all agents the role of communication in such domains is also studied and shown to be unnecessary and even detrimental if effective behavior in the task can be expressed as role based cooperation rather than synchronization 1 impulse location based agent assistance in the physical world a user experiences products and places explores physical surroundings and participates in location specific activities software agents trapped in their electronic world offer users valuable assistance online for example by personalizing searches and queries the impulse research project at the mit media lab 1 examines what happens when the rich experience of the physical world is augmented with the low search costs and information resources available through the internet this paper presents a subset and implementation of one aspect of the impulse vision a scenario demonstrating a mobile device which uses location aware queries to digitally augment and explore the physical world project overview related research on learning agents within wireless devices 6 and the combined work of wearable computing and ubiquitous computing 5 explore placing agents into our physical environment our work takes these previous explorations and introduces the idea of calibrating parameters of cost functionals we propose a new framework for calibrating parameters of energy functionals as used in image analysis the method learns parameters from a family of correct examples and given a probabilistic construct for generating wrong examples from correct ones we introduce a measure of frustration to penalize cases in which wrong responses are preferred to correct ones and we design a stochastic gradient algorithm which converges to parameters which minimize this measure of frustration we also present a first set of experiments in this context and introduce extensions to deal with data dependent energies keywords learning variational method parameter estimation image reconstruction bayesian image models 1 1 description of the method many problems in computer vision are addressed through the minimization of a cost functional u this function is typically defined on a large finite set omega for example the set of pictures with fixed dimensions and the minimizer of x indexing the distance an efficient method to knn processing in this paper we present an efficient method called idistance for k nearest neighbor knn search in a high dimensional space idistance partitions the data and selects a reference point for each partition the data in each cluster are transformed into a single dimensional space based on their similarity with respect to a reference point this allows the points to be indexed using a b tree structure and knn search be performed using one dimensional range search the choice of partition and reference point provides the idistance technique with degrees of freedom most other techniques do not have we describe how appropriate choices here can effectively adapt the index structure to the data distribution we conducted extensive experiments to evaluate the idistance technique and report results demonstrating its effectiveness progressive knn search using b trees in high dimensional databases nearest neighbor nn search is computationally expensive however for many applications where small errors can be tolerated determining approximate answers quickly has become an acceptable alternative in this paper we present a new algorithm that exploits the iminmax scheme 16 to facilitate nearest neighbor processing the proposed algorithm distinguishes itself from existing algorithms in two ways first it evaluates knn queries progressively i e it begins by examining a small amount of data to produce approximate answers quickly as more data are examined the approximate answers are refined second unlike existing progressive algorithms that access data randomly the algorithm searches the data space in a systematic and optimal manner our performance study shows that the proposed scheme can produce approximate answers very quickly more importantly the quality of the answers are not sacrified significantly such an extension is applicable to other relevant methods such as the pyramid technique 2 efficient and effective metasearch for a large number of text databases metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources text databases in a metasearch engine the contents of each local database is represented by a representative each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search when the number of databases is very large say in the order of tens of thousands or more then a traditional metasearch engine may become inefficient as each query needs to be evaluated against too many database representatives furthermore the storage requirement on the site containing the metasearch engine can be very large in this paper we propose to use a hierarchy of database representatives to improve the efficiency we provide an algorithm to search the hierarchy we show that the retrieval effectiveness of our algorithm is the same as that of evaluating the user query against all database representatives we als finding the most similar documents across multiple text databases in this paper we present a methodology for finding the n most similar documents across multiple text databases for any given query and for any positive integer n this methodology consists of two steps first databases are ranked in a certain order next documents are retrieved from the databases according to the order and in a particular way if the databases containing the n most similar documents for a given query can be ranked ahead of other databases the methodology will guarantee the retrieval of the n most similar documents for the query a statistical method is provided to identify databases each of which is estimated to contain at least one of the n most similar documents then a number of strategies is presented to retrieve documents from the identified databases experimental results are given to illustrate the relative performance of different strategies 1 introduction the internet has become a vast information source in recent years and can be considered as the w a methodology to retrieve text documents from multiple databases in this paper we present a methodology for finding the n most similar documents across multiple text databases for any given query and for any positive integer n this methodology consists of two steps first the contents of databases are indicated approximately by database representatives databases are ranked using their representatives in a certain order with respect to the given query we provide a necessary and sufficient condition to rank the databases optimally in order to satisfy this necessary and sufficient condition we provide three estimation methods one estimation method is intended for short queries the other two are for all queries second we provide an algorithm optdocretrv to retrieve documents from the databases according to their rank and in a particular way we show that if the databases containing the n most similar documents for a given query are ranked ahead of other databases our methodology will guarantee the retrieval of the n most similar d self learning techniques for grapheme to phoneme conversion in this article we present a comprehensive review of various experiences with different self learning techniques applied to the task of converting a graphemic string into the corresponding phonemic sequence we also report some experiments carried out both with english words and french proper names these experiments support the view that taking full advantage of the huge pronunciation dictionaries that we have been developing during the onomastica project is possible only if the traditional understanding of grapheme tophoneme conversion as a classification problem is questioned discovering web access patterns and trends by applying olap and data mining technology on web logs as a confluence of data mining and www technologies it is now possible to perform data mining on web log records collected from the internet web page access history the behaviour of the web page readers is imprinted in the web server log files analyzing and exploring regularities in this behaviour can improve system performance enhance the quality and delivery of internet information services to the end user and identify population of potential customers for electronic commerce thus by observing people using collections of data data mining can bring considerable contribution to digital library designers in a joint effort between the telelearning nce project on virtual university and nce iris project on data mining we have been developing the knowledge discovery tool weblogminer for mining web server log files this paper presents the design of the weblogminer reports the current progress and outlines the future work in this direction sequence mining in categorical domains incorporating constraints we present cspade an efficient algorithm for mining frequent sequences considering a variety of syntactic constraints these take the form of length or width limitations on the sequences minimum or maximum gap constraints on consecutive sequence elements applying a time window on allowable sequences incorporating item constraints and finding sequences predictive of one or more classes even rare ones our method is efficient and scalable experiments on a number of synthetic and real databases show the utility and performance of considering such constraints on the set of mined sequences 1 on the emergence of macro spatial structures in dissipative cellular automata and its implications for agent based distributed computing this paper describes the peculiar behavior observed in a class of cellular automata that we have defined as dissipative i e cellular automata that are open and makes it possible for the environment to influence the evolution of the automata peculiar in the dynamic evolution of this class of cellular automata is that stable macro level spatial structures emerge from local interactions among cells a behavior that does not emerge when the cellular automaton is closed i e when the state of a cell is not influenced by the external world on this basis the paper discusses the relations of the performed experiments with the area of open distributed computing and in particular of agent based distributed computing the basic intuition is that dissipative cellular automata express characteristics that strongly resembles those of wide area open distributed systems based on autonomous and situated active components as agents are accordingly similar sorts of macrolevel behaviors are likely to emerge and need to be studied controlled and possibly fruitfully exploited organisational rules as an abstraction for the analysis and design of multi agent systems multi agent systems in this paper we introduce three additional organisational concepts organisational rules organisational structures and organisational patterns and discuss why we believe they are necessary for the complete specification of computational organisations in particular we focus on the concept of organisational rules and introduce a formalism based on temporal logic to specify them this formalism is then used to drive the definition of the organisational structure and the identification of the organisational patterns finally the paper sketches some guidelines for a methodology for agent oriented systems based on our expanded set of organisational abstractions web document clustering a feasibility demonstration abstract users of web search engines are often forced to sift through the long ordered list of document snippets returned by the engines the ir community has explored document clustering as an alternative method of organizing retrieval results but clustering has yet to be deployed on the major search engines the paper articulates the unique requirements of web document clustering and reports on the first evaluation of clustering methods in this domain a key requirement is that the methods create their clusters based on the short snippets returned by web search engines surprisingly we find that clusters based on snippets are almost as good as clusters created using the full text of web documents to satisfy the stringent requirements of the web domain we introduce an incremental linear time in the document collection size algorithm called suffix tree clustering stc which creates clusters based on phrases shared between documents we show that stc is faster than standard clustering methods in this domain and argue that web document clustering via stc is both feasible and potentially beneficial 1 grouper a dynamic clustering interface to web search results users of web search engines are often forced to sift through the long ordered list of document snippets returned by the engines the ir community has explored document clustering as an alternative method of organizing retrieval results but clustering has yet to be deployed on most major search engines the northernlight search engine organizes its output into custom folders based on pre computed document labels but does not reveal how the folders are generated or how well they correspond to users interests in this paper we introduce grouper an interface to the results of the huskysearch meta search engine which dynamically groups the search results into clusters labeled by phrases extracted from the snippets in addition we report on the first empirical comparison of user web search behavior on a standard ranked list presentation versus a clustered presentation by analyzing huskysearch logs we are able to demonstrate substantial differences in the number of documents f information extraction by text classification corpus mining for features this paper describes a method for building an information extraction ie system using standard text classification machine learning techniques and datamining for complex features on a large corpus of example texts that are only superficially annotated we have successfully used this method to build an ie system textractor for job advertisements 1 introduction for rapid development of an information extraction system in a large new domain the usual methods of semicorpusbased hand crafting of extraction rules are often simply too laborious therefore one must turn to the use of machine learning techniques and try to induce the knowledge needed for extraction from annotated training samples techniques for the induction of extraction rules are e g described by freitag 1998 improving short text classification using unlabeled background knowledge to assess document similarity we describe a method for improving the classification of short text strings using a combination of labeled training data plus a secondary corpus of unlabeled but related longer documents we show that such unlabeled background knowledge can greatly decrease error rates particularly if the number of examples or the size of the strings in the training set is small this is particularly useful when labeling text is a labor intensive job and when there is a large amount of information available about a particular problem on the world wide web our approach views the task as one of information integration using whirl a tool that combines database functionalities with techniques from the information retrieval literature 1 introduction the task of classifying textual data that has been culled from sites on the world wide web is both difficult and intensively studied cohen hirsh 1998 joachims 1998 nigam et al 1999 applications of various machine learning techniqu dyda dynamic data warehouse maintenance in a fully concurrent environment data warehouse is an emerging technology to support high level decision making by gathering data from several distributed information sources into one repository in dynamic environments data warehouses must be maintained in order to stay consistent with the underlying sources recently proposed view maintenance algorithms tackle the problem of data warehouse maintenance under concurrent source data updates while the view synchronization is to handle non concurrent source schema changes however the concurrency between interleaved schema changes and data updates still remain unexplored problems in this paper we propose a solution framework called dyda that successfully addresses this problem the dyda framework detects concurrent schema changes by a broken query scheme and conicting concurrent data updates by a local timestamp scheme a fundamental idea of the dyda framework is the development of a two layered architecture that separates the concerns for concurrent data updates and concurrent schema changes handling without imposing any restrictions on the sourse update transactions at the lower level of the framework it employs a local compensation algorithm to handle concurrent data updates and a metadata name mapping strategy to handle concurrent source rename operations at the higher level it addresses the problem of concurrent source drop operations for the latter problem we design a strategy for the detection and correction of such concurrency and nd an executable plan for the aected updates we then develop a new view adaption algorithm called batch va for execution of such plan to incrementally adapt the view put together these algorithms are the rst to provide a complete solution to data warehouse management in a fully concurrent environment efficient computation of temporal aggregates with range predicates a temporal aggregation query is an important but costly operation for applications that maintain timeevolving data data warehouses temporal databases etc due to the large volume of such data performance improvements for temporal aggregation queries are critical in this paper we examine techniques to compute temporal aggregates that include key range predicates range temporal aggregates in particular we concentrate on sum count and avg aggregates this problem is novel to handle arbitrary key ranges previous methods would need to keep a separate index for every possible key range we propose an approach based on a new index structure called the multiversion sb tree which incorporates features from both the sb tree and the multiversion b tree to handle arbitrary key range temporal sum count and avg queries we analyze the performance of our approach and present experimental results that show its efficiency 1 an evolutionary approach to materialized views selection in a data warehouse environment a data warehouse contains multiple views accessed by queries one of the most important decisions in designing a data warehouse is selecting views to materialize for the purpose of eciently supporting decision making the search space for possible materialized views is exponentially large therefore heuristics have been used to search for a near optimal solution in this paper we explore the use of an evolutionary algorithm for materialized view selection based on multiple global processing plans for queries we apply a hybrid evolutionary algorithm to solve three related problems the rst is to optimize queries the second is to choose the best global processing plan from multiple global processing plans the third is to select materialized views from a given global processing plan our experiment shows that the hybrid evolutionary algorithm delivers better performance than either the evolutionary algorithm or heuristics used alone in terms of the minimal query and maintenance cost and the evaluation cost to obtain the minimal cost keywords evolutionary algorithms materialised view selection data warehousing data mining i maximum likelihood estimation for filtering thresholds information filtering systems based on statistical retrieval models usually compute a numeric score indicating how well each document matches each profile documents with scores above profile specific dissemination thresholds are delivered an optimal dissemination threshold is one that maximizes a given utility function based on the distributions of the scores of relevant and non relevant documents the parameters of the distribution can be estimated using relevance information but relevance information obtained while filtering is biased this paper presents a new method of adjusting dissemination thresholds that explicitly models and compensates for this bias the new algorithm which is based on the maximum likelihood principle jointly estimates the parameters of the density distributions for relevant and nonrelevant documents and the ratio of the relevant document in the corpus experiments with trec 8 and trec 9 filtering track data demonstrate the effectiveness of the algorithm keywords information filtering dissemination threshold maximum likelihood estimation 1 personalized web document filtering using reinforcement learning abstract document filtering is increasingly deployed in web environments to reduce information overload of users we formulate online information filtering as a reinforcement learning problem i e td 0 the goal is to learn user profiles that best represent his information needs and thus maximize the expected value of user relevance feedback a method is then presented that acquires reinforcement signals automatically by estimating user s implicit feedback from direct observations of browsing behaviors this learning by observation approach is contrasted with conventional relevance feedback methods which require explicit user feedbacks field tests have been performed which involved 10 users reading a total of 18 750 html documents during 45 days compared to the existing document filtering techniques the proposed learning method showed superior performance in information quality and adaptation speed to user preferences in online filtering 1 pvm parallel view maintenance under concurrent data updates of distributed sources data warehouses dw are built by gathering information from distributed information sources iss and integrating it into one customized repository in recent years work has begun to address the problem of view maintenance of dws under concurrent data updates of different iss popular solutions such as eca and strobe achieve such concurrent maintenance however with the requirement of quiescence of the iss more recently the sweep solution releases this quiescence requirement using a local compensation strategy that now processes all update messages in a sequential manner to optimize upon this sequential processing we have developed a parallel view maintenance algorithm called pvm that incorporates all benefits of previous maintenance approaches while offering improved performance due to parallelism in order to perform parallel view maintenance we have identified two critical issues 1 detecting maintenance concurrent data updates in a parallel mode and 2 correcting the problem that the dw commit order may not correspond to the dw update processing order due to parallel maintenance handling in this work we provide solutions to both issues given a modular component based system architecture we insert a middle layer timestamp assignment module for detecting maintenance concurrent data updates without requiring any global clock synchronization in addition we introduce the negative counter concept as a simple yet elegant solution to solve the problem of variant orders of committing effects of data updates to the dw we have proven the correctness of pvm to guarantee that our strategy indeed generates the correct final dw state we have implemented both sweep and pvm in our eve data warehousing system our performance study demonstrates on supporting containment queries in relational database management systems virtually all proposals for querying xml include a class of query we term containment queries it is also clear that in the foreseeable future a substantial amount of xml data will be stored in relational database systems this raises the question of how to support these containment queries the inverted list technology that underlies much of information retrieval is well suited to these queries but should we implement this technology a in a separate loosely coupled ir engine or b using the native tables and query execution machinery of the rdbms with option b more than twenty years of work on rdbms query optimization query execution scalability and concurrency control and recovery immediately extend to the queries and structures that implement these new operations but all this will be irrelevant if the performance of option b lags that of a by too much in this paper we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine our performance study shows that while rdbmss are generally poorly suited for such queries under certain conditions they can outperform an inverted list engine our analysis further identifies two significant causes that differentiate the performance of the ir and rdbms implementations the join algorithms employed and the hardware cache utilization our results suggest that contrary to most expectations with some modifications a native implementation in an rdbms can support this class of query much more efficiently evolving materialized views in data warehouse a data warehouse contains multiple views accessed by queries one of the most important decisions in designing a data warehouse is the selection of materialized views for the purpose of efficiently implementing decision making the search space for the selection of materialized views is exponentially large therefore heuristics have been used to search a small fraction of the space to get a near optimal solution in this paper we explore the use of a genetic algorithm for the selection of materialized views based on multiple global processing plans for many queries our experimental studies indicate that the genetic algorithm delivers better solutions than some heuristics 1 introduction a data warehouse dw is a repository of integrated information available for querying and analysis dw is an approach to the integration of data from multiple possibly very large distributed heterogeneous databases and other information sources in this paper dw data model is based on spj sele query formulation from high level concepts for relational databases a new query formulation system based on a semantic graph model is presented the graph provides a semantic model for the data in the database with userdefined relationships the query formulator allows users to specify their requests and constraints in highlevel concepts the query candidates are formulated based on the user input by a graph search algorithm and ranked according to a probabilistic information measure english like query descriptions can also be provided for users to resolve ambiguity when multiple queries are formulated from a user input for complex queries we introduce an incremental approach which assists users to achieve a complex query goal by formulating a series of simple queries a prototype system with a multimodal interface using the high level query formulation techniques has been implemented on top of a cooperative database system cobase at ucla 1 introduction many database applications require users to formulate ad hoc queries instead of invocation the sdcc framework for integrating existing algorithms for diverse data warehouse maintenance tasks recently proposed view maintenance algorithms tackle the problem of concurrent data updates happening at different autonomous iss whereas the eve system addresses the maintenance of a data warehouse after schema changes of iss the concurrency of schema changes and data updates still remains an unexplored problem however this paper now provides a first solution that guarantees concurrent view definition evolution and view extent maintenance of a dw defined over distributed iss for this problem we introduce a framework called sdcc schema change and data update concurrency control system sdcc integrates existing algorithms designed to address view maintenance subproblems such as view extent maintenance after is data updates view definition evolution after is schema changes and view extent adaptation after view definition changes into one system by providing protocols that enable them to correctly co exist and collaborate sdcc tracks any potential faulty updates of the dw ca situated neuro fuzzy control for vision based robot localisation we introduce a neuro fuzzy system for localising mobile robot solely based on raw vision data without relying on landmarks or artificial symbols in an initial learning step the system is trained on the compressed input data so as to classify different situations and to associate appropriate behaviours to these situations input data may for example be generated by an omnidirectional vision system obviating the need for active cameras at run time the compressed input data are fed into different b spline fuzzy controllers which determine the correspondence between the actual situation and the situation they were trained for the matching controller may then directly drive the actuators to realise the desired behaviour the system thus realises a tight coupling between a very high dimensional input parameter space and the robot actuators it is completely free of any internal models such as maps of the environment the algorithms are straightforward to implement and the computational towards transparent control of large and complex systems system identi cation unlike with markovian decision processes some systems output depends not only on the current state but also the previous input output as a training data set for nonlinear system identi cation the box jenkins gas furnace data bj70 is often studied and compared the furnace input is the gas ow rate x t the output y t is the co 2 concentration at least 10 candidate inputs are considered x t 6 x t 5 x t 1 y t 1 y t 4 if all of them are used building a fuzzy controller means to solve a 10 input 1 output problem if each input is de ned by 5 linguistic terms this would result in a fuzzy rule system of about 10 million rules the modelling and prediction o sfs based view synthesis for robust face recognition sensitivity to variations in pose is a challenging problem in face recognition using appearance based methods more specifically the appearance of a face changes dramatically when viewing and or lighting directions change various approaches have been proposed to solve this difficult problem they can be broadly divided into three classes 1 multiple image based methods where multiple images of various poses per person are available 2 hybrid methods where multiple example images are available during learning but only one database image per person is available during recognition and 3 single image based methods where no example based learning is carried out in this paper we present a method that comes under class 3 this method based on shape from shading sfs improves the performance of a face recognition system in handling variations due to pose and illumination via image synthesis 1 introduction face recognition has become one of the most active areas of research in image criterion functions for document clustering experiments and analysis in recent years we have witnessed a tremendous growth in the volume of text documents available on the internet digital libraries news sources and company wide intranets this has led to an increased interest in developing methods that can help users to effectively navigate summarize and organize this information with the ultimate goal of helping them to find what they are looking for fast and high quality document clustering algorithms play an important role towards this goal as they have been shown to provide both an intuitive navigation browsing mechanism by organizing large amounts of information into a small number of meaningful clusters as well as to greatly improve the retrieval performance either via cluster driven dimensionality reduction term weighting or query expansion this ever increasing importance of document clustering and the expanded range of its applications led to the development of a number of new and novel algorithms with different complexity quality trade offs among them a class of clustering algorithms that have relatively low computational requirements are those that treat the clustering problem as an optimization process which seeks to maximize or minimize a particular clustering criterion function defined over the entire clustering solution empirical performance analysis of linear discriminant classifiers in face recognition literature holistic template matching systems and geometrical local feature based systems have been pursued in the holistic approach pca principal component analysis and lda linear discriminant analysis are popular ones more recently the combination of pca and lda has been proposed as a superior alternative over pure pca and lda in this paper we illustrate the rationales behind these methods and the pros and cons of applying them to pattern classification task a theoretical performance analysis of lda suggests applying lda over the principal components from the original signal space or the subspace the improved performance of this combined approach is demonstrated through experiments conducted on both simulated data and real data 1 introduction statistical pattern recognition techniques have been successfully applied to many problems including speech recognition automatic target recognition and image classification for a given pattern classificat discriminant analysis based feature extraction we propose a new feature extraction scheme called discriminant component analysis the new scheme decomposes a signal into orthonormal bases such that for each base there is an eigenvalue representing the discriminatory power of projection in that direction the bases and eigenvalues are obtained based on certain classification criterion for simplicity a criterion used in fisher s discriminant analysis da is chosen and is applied iteratively to implement the scheme we illustrate the motivation of this new scheme and show how it can be used to construct new distance metrics we then argue that these new distance metrics are more robust than da based metrics finally very good classification performance on simulation data and real face images are demonstrated using these new distance metrics 1 introduction it is important that for different applications we use different representations for the same signal 1 for example pca principal component analysis or wavelet decompos integrating boosting and stochastic attribute selection committees for further improving the performance of decision tree learning techniques for constructing classifier committees including boosting and bagging have demonstrated great success especially boosting for decision tree learning this type of technique generates several classifiers to form a committee by repeated application of a single base learning algorithm the committee members vote to decide the final classification boosting and bagging create different classifiers by modifying the distribution of the training set sasc stochastic attribute selection committees uses an alternative approach to generating classifier committees by stochastic manipulation of the set of attributes considered at each node during tree induction but keeping the distribution of the training set unchanged in this paper we propose a method for improving the performance of boosting this technique combines boosting and sasc it builds classifier committees by manipulating both the distribution of the training set and the set of attributes available during induction in stochastic attribute selection committees classifier committee learning methods generate multiple classifiers to form a committee by repeated application of a single base learning algorithm the committee members vote to decide the final classification two such methods bagging and boosting have shown great success with decision tree learning they create different classifiers by modifying the distribution of the training set this paper studies a different approach stochastic attribute selection committee learning of decision trees it generates classifier committees by stochastically modifying the set of attributes but keeping the distribution of the training set unchanged an empirical evaluation of a variant of this method namely sasc in a representative collection of natural domains shows that the sasc method can significantly reduce the error rate of decision tree learning on average sasc is more accurate than bagging and less accurate than boosting although a one tailed sign test fails to show that these differences are significant at a level of 0 05 in addition it is found that like bagging sasc is more stable than boosting in terms of less frequently obtaining significantly higher error rates than c4 5 and when error is raised producing lower error rate increases moreover like bagging sasc is amenable to parallel and distributed processing while boosting is not implementation of a linear tabling mechanism delaying based tabling mechanisms such as the one adopted in xsb are nonlinear in the sense that the computation state of delayed calls has to be preserved incorporating quality metrics in centralized distributed information retrieval on the world wide web most information retrieval systems on the internet rely primarily on similarity ranking algorithms based solely on term frequency statistics information quality is usually ignored this leads to the problem that documents are retrieved without regard to their quality we present an approach that combines similarity based similarity ranking with quality ranking in centralized and distributed search environments six quality metrics including the currency availability information to noise ratio authority popularity and cohesiveness were investigated search effectiveness was significantly improved when the currency availability information to noise ratio and page cohesiveness metrics were incorporated in centralized search the improvement seen when the availability information to noise ratio popularity and cohesiveness metrics were incorporated in site selection was also significant finally incorporating the popularity metric in information fusion resulted in a significan on line analytical mining of association rules with wide applications of computers and automated data collection tools massive amounts of data have been continuously collected and stored in databases which creates an imminent need and great opportunities for mining interesting knowledge from data association rule mining is one kind of data mining techniques which discovers strong association or correlation relationships among data the discovered rules may help market basket or cross sales analysis decision making and business management in this thesis we propose and develop an interesting association rule mining approach called on line analytical mining of association rules which integrates the recently developed olap on line analytical processing technology with some efficient association mining methods it leads to flexible multi dimensional multi level association rule mining with high performance several algorithms are developed based on this approach for mining various kinds of associations in multi dimensional caselp a rapid prototyping environment for agent based software intelligent agents and multi agent systems are increasingly recognized as an innovative approach for analyzing designing and implementing complex heterogeneous and distributed software applications the agent based view offers a powerful and high level conceptualization that software engineers can exploit to considerably improve the way in which software is realized agent based software engineering is a recent and very interesting research area due to its novelty there is still no evidence of well established practices for the development of agent based applications and thus experimentation in this direction is very important this dissertation representing coordination relationships with influence diagrams it is well know the necessity of managing relationships among agents in a multi agent system to achieve coordinated behavior one approach to manage such relationships consists of using an explicit representation of them allowing each agent to choose its actions based on them previous work in the area have considered ideal situations such as fully known environments static relationships and shared mental states in this paper we propose to represent relationships among agents and entities in a multi agent system by using influence diagrams 